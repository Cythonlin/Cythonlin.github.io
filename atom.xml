<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://cythonlin.github.io</id>
    <title>Cython_lin</title>
    <updated>2020-10-12T12:43:52.057Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://cythonlin.github.io"/>
    <link rel="self" href="https://cythonlin.github.io/atom.xml"/>
    <logo>https://cythonlin.github.io/images/avatar.png</logo>
    <icon>https://cythonlin.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, Cython_lin</rights>
    <entry>
        <title type="html"><![CDATA[PY => éŸ³å£°åˆæˆ]]></title>
        <id>https://cythonlin.github.io/post/py-greater-yin-sheng-he-cheng/</id>
        <link href="https://cythonlin.github.io/post/py-greater-yin-sheng-he-cheng/">
        </link>
        <updated>2020-10-12T12:20:17.000Z</updated>
        <content type="html"><![CDATA[<h1 id="èƒŒæ™¯">èƒŒæ™¯</h1>
<p>éŸ³å£°åˆæˆ åŸºäº å¾ˆä¹…ä¹‹å‰å†™çš„æ–‡ç«  <a href="https://www.cklin.top/post/py-greater-yin-sheng-fen-chi/">éŸ³å£°åˆ†ç¦»</a><br>
ä¸€äº› Light Music çš„ Electronic Drum å¤ªåµäº†ã€‚<br>
äºæ˜¯çªå‘å¥‡æƒ³ï¼Œå¦‚ä½•  N v 1 åˆ†ç¦»å‡º Drum å¹¶ä¸” Drop</p>
<h1 id="éŸ³å£°åˆ†ç¦»æ›´æ–°ä¸º5stems-16khz-model">éŸ³å£°åˆ†ç¦»ï¼ˆæ›´æ–°ä¸º5stems-16kHz Modelï¼‰</h1>
<p>2stems (vocals / accompaniment)</p>
<pre><code>spleeter separate  -o audio_output -i audio_example.mp3
</code></pre>
<p>4stems (vocals / bass / drums / other )</p>
<pre><code>spleeter separate -o audio_output -p spleeter:4stems  -i audio_example.mp3
</code></pre>
<p>5stems (vocals / bass / drums / piano / other)</p>
<pre><code>spleeter separate -o audio_output -p spleeter:5stems-16kHz -i audio_example.mp3
</code></pre>
<p>è¿™æ¬¡ç”¨çš„æ˜¯ 5stemsé¢„è®­ç»ƒæ¨¡å‹ï¼Œ å¾—åˆ°äº†å¦‚ä¸‹5ä¸ªæ–‡ä»¶ï¼š</p>
<pre><code>bass.wav
drums.wav
other.wav
piano.wav
vocals.wav
</code></pre>
<h1 id="å¯»æ‰¾è§£å†³æ–¹æ¡ˆ">å¯»æ‰¾è§£å†³æ–¹æ¡ˆ</h1>
<p>æœ€å¼€å§‹ä¸çŸ¥é“ä»ä½•æœèµ·ï¼Œåæ¥ç›´æ¥ç´¢æ€§Githubè´´äº†ä¸€ä¸ª <a href="https://github.com/deezer/spleeter/issues/506">Question Issues</a>ã€‚<br>
æœ‰äººç»™å‡ºstackçš„<a href="https://stackoverflow.com/questions/14498539/how-to-overlay-downmix-two-audio-files-using-ffmpeg">è§£å†³æ–¹æ¡ˆ</a>ï¼Œ ä¸ªäººç®€åŒ–ä½¿ç”¨å¦‚ä¸‹ï¼š</p>
<pre><code>ffmpeg -i other.wav -i vocals.wav -i bass.wav -i piano.wav -filter_complex amix=inputs=4:duration=longest output.mp3
</code></pre>
<h1 id="ç»“æœ">ç»“æœ</h1>
<p>æœ€ç»ˆæˆåŠŸæŠŠ Electronic Drum å£° Dropã€‚<br>
å”¯ä¸€ç¾ä¸­ä¸è¶³çš„å°±æ˜¯ï¼Œ5ä¸ªStemsé¢„è®­ç»ƒæ¨¡å‹åˆ†çš„ä¸å¤Ÿç»†è‡´, Github Wikiæœ€æ–°æ–¹æ¡ˆçš„å°±æ˜¯5stems~</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => K8Sï¼ˆæœªå®Œå¾…ç»­ï¼‰]]></title>
        <id>https://cythonlin.github.io/post/py-greater-k8s/</id>
        <link href="https://cythonlin.github.io/post/py-greater-k8s/">
        </link>
        <updated>2020-10-06T22:18:23.000Z</updated>
        <content type="html"><![CDATA[<h1 id="å®‰è£…">å®‰è£…</h1>
<p>å®˜æ¡£ï¼š <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a><br>
å®˜æ¡£ä¸­çš„é•œåƒæ—¶Googleçš„ï¼Œéœ€è¦æ¢æˆé˜¿é‡Œæºã€‚</p>
<h3 id="ubuntu">Ubuntu</h3>
<pre><code>apt-get update &amp;&amp; apt-get install -y apt-transport-https

curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 

cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF

apt-get update
apt-get install -y kubelet kubeadm kubectl
</code></pre>
<h3 id="centos">CentOS</h3>
<pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

setenforce 0
yum install -y kubelet kubeadm kubectl
systemctl enable kubelet &amp;&amp; systemctl start kubelet</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => Github-Cli]]></title>
        <id>https://cythonlin.github.io/post/py-greater-github-cli-quan-tao/</id>
        <link href="https://cythonlin.github.io/post/py-greater-github-cli-quan-tao/">
        </link>
        <updated>2020-10-03T01:53:58.000Z</updated>
        <content type="html"><![CDATA[<h1 id="ä¸‹è½½">ä¸‹è½½</h1>
<p>é€‰ä¸ªOSç‰ˆæœ¬ï¼ˆæˆ‘ç”¨çš„Winï¼‰ï¼š<a href="https://github.com/cli/cli/releases">https://github.com/cli/cli/releases  </a></p>
<h1 id="åˆ—å‡ºé…ç½®">åˆ—å‡ºé…ç½®</h1>
<pre><code>git config --list
</code></pre>
<p>ç”±äºæˆ‘github2ä¸ªå·åˆ‡æ¢ï¼Œå¯¼è‡´ï¼Œpushçš„æ—¶å€™æœ‰403é”™è¯¯æ··æ·†ï¼Œ<br>
æ‰€ä»¥åˆ é™¤äº†å®¶ç›®å½•çš„ .gitconfig(åº”è¯¥æ˜¯è¿™ä¸ªæœ‰äº›è®°ä¸æ¸…äº†)</p>
<h1 id="è®¾ç½®ä»£ç†">è®¾ç½®ä»£ç†</h1>
<p>ä¸ºäº†åŠ é€Ÿcloneï¼Œè¿™é‡Œå…ˆè®¾ç½®ï¼Œè‹¥æ²¡æœ‰PROXY, é‚£æ­¤æ­¥å¯ç•¥è¿‡</p>
<pre><code>git config --global http.proxy &quot;socks5://127.0.0.1:7890&quot;
git config --global https.proxy &quot;socks5://127.0.0.1:7890&quot;
</code></pre>
<p>æ¸…é™¤ä»£ç†ä¹Ÿå¾ˆç®€å•</p>
<pre><code>git config --global --unset http.proxy
git config --global --unset https.proxy
</code></pre>
<h1 id="è®¾ç½®ssh-key">è®¾ç½®SSH Key</h1>
<h3 id="è¯´æ˜">è¯´æ˜</h3>
<pre><code>å¦‚æœä½ ä¸ä¹ æƒ¯ç”¨SSHï¼Œè€Œæ˜¯ä¹ æƒ¯ç”¨HTTPçš„æ–¹å¼ï¼Œé‚£è¿™æ­¥å¯çœ
</code></pre>
<h3 id="ç”Ÿæˆå¯†é’¥å‘½ä»¤">ç”Ÿæˆå¯†é’¥å‘½ä»¤</h3>
<pre><code>ssh-keygen
</code></pre>
<p>è¿›å…¥ç”¨æˆ·å®¶ç›®å½•ï¼ŒæŠŠid_rsa.pubå…¬é’¥å¤åˆ¶å‡ºæ¥<br>
ç²˜è´´åˆ°-&gt; <a href="https://github.com/settings/ssh/">https://github.com/settings/ssh/new</a></p>
<h1 id="ç™»å½•">ç™»å½•</h1>
<pre><code>gh auth login
</code></pre>
<p>æå‰å£°æ˜ï¼Œé‡åˆ°é€‰é¡¹ï¼Œéƒ½æ˜¯ç”¨ä¸Šä¸‹ç®­å¤´é€‰æ‹©<br>
ç¬¬1ä¸ªé€‰é¡¹ï¼š é€‰æ‹©Github.comï¼ˆä¹Ÿå°±æ˜¯ä¸ªäººç”¨æˆ·ï¼‰<br>
ç¬¬2ä¸ªé€‰é¡¹ï¼šé€‰æ‹© Login with a web browser<br>
Commandä¸­ä¼šç»™ä¸€ä¸²ä»£ç ï¼Œå¤åˆ¶ä»£ç -&gt;CMDå›è½¦-&gt;è‡ªåŠ¨è·³è½¬åˆ°Web-&gt;ç²˜è´´ä»£ç -&gt;ç¡®è®¤-&gt;ç¡®è®¤æˆæƒ</p>
<pre><code>è¿™é‡Œä¹Ÿå¯ä»¥é€‰æ‹©ä½¿ç”¨ Token ä»£æ›¿ web browserã€‚ä½†æ˜¯è¿™ç§æ–¹å¼éœ€è¦ç”Ÿæˆä¸€ä¸ªToken
ç”ŸæˆURLå¦‚ä¸‹ï¼š
:  -&gt;   https://github.com/settings/tokens

ç‚¹å‡» Generate bew token ï¼Œæ–°å»ºä¸€ä¸ªæ–° tokenï¼š
    æ³¨æ„ï¼š éœ€è¦æŠŠ repoçš„æ‰€æœ‰æƒé™å‹¾ä¸Š
               å¤–åŠ ä¸€ä¸ªadmin:orgä¸‹é¢çš„  read:org  é€‰é¡¹
    æ¸©é¦¨æç¤ºï¼š  read:org  å¿…é¡»å‹¾ä¸Šï¼Œä¸ç„¶åˆ›å»ºå¤±è´¥ã€‚
</code></pre>
<p>ç¬¬3ä¸ªé€‰é¡¹ï¼šé€‰æ‹©SSHï¼ˆHTTPSä¹Ÿå¯ä»¥ï¼‰</p>
<h3 id="æŸ¥çœ‹ç™»å½•çŠ¶æ€">æŸ¥çœ‹ç™»å½•çŠ¶æ€</h3>
<pre><code>gh auth status
</code></pre>
<h3 id="é€€å‡ºç™»å½•">é€€å‡ºç™»å½•</h3>
<pre><code>gh auth logout
</code></pre>
<h1 id="ä»“åº“">ä»“åº“</h1>
<h3 id="åˆ›å»ºä»“åº“">åˆ›å»ºä»“åº“</h3>
<pre><code>gh repo create my-gh
    -&gt; Public
    -&gt; xxx  in your current directoryï¼ˆY/Nï¼‰ y   (å›è½¦é»˜è®¤å°±æ˜¯yesï¼Œä¸‹åŒ)
    -&gt; Create a local project directory for xxx ï¼ˆY/Nï¼‰y
</code></pre>
<h3 id="æŸ¥çœ‹è¿œç¨‹æƒé™">æŸ¥çœ‹è¿œç¨‹æƒé™</h3>
<pre><code>git remote -v
    origin  https://github.com/Cythonlin/my-gh.git (fetch)
    origin  https://github.com/Cythonlin/my-gh.git (push)
</code></pre>
<h3 id="å…‹éš†">å…‹éš†</h3>
<pre><code>gh repo clone gin-gonic/gin
cd gin
git remote -v
    origin  https://github.com/gin-gonic/gin.git (fetch)
    origin  https://github.com/gin-gonic/gin.git (push)
    # æˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œè¿™åˆ†æ”¯å¹¶ä¸æ˜¯æˆ‘ä»¬çš„è‡ªå·±çš„
    # æ‰€ä»¥æˆ‘ä»¬å¯ä»¥ fork ä¸‹æ¥
</code></pre>
<h3 id="fork">fork</h3>
<p>fork æŒ‡çš„æ˜¯ï¼ŒæŠŠå…‹éš†åˆ°è‡ªå·±çš„ä»“åº“ï¼Œä½œä¸ºä¸Šæ¸¸ï¼ˆupstreamï¼‰é¡¹ç›®ï¼Œç„¶åè‡ªå·±å°±å¯è‡ªç”±åŒæ­¥å®ƒ</p>
<pre><code>cd gin  # ä¸Šé¢å·²ç»è¿›æ­¤è·¯å¾„ï¼Œè¿™æ­¥å¯çœ
gh repo fork
    -&gt; Would you like to add a remote for the fork? (Y/n) å›è½¦yes
</code></pre>
<p>ä¸Šé¢æ˜¯å…ˆclone,ç„¶åè¿›å…¥è·¯å¾„ï¼Œå†fork<br>
å¦‚æœäº‹å…ˆæœªcloneï¼Œ ä¹Ÿå¯ä»¥ç”¨gh repo fork + ç”¨æˆ·å/ä»“åº“åï¼Œ ç›´æ¥ fork+cloneä¸€æ­¥åˆ°ä½</p>
<pre><code># è¿™å°±ä¸éœ€è¦åƒä¸Šé¢å…ˆ cdè¿›å…¥cloneçš„ç›®å½•ä¸‹å†forkäº†ï¼Œè¿™ç§æ–¹å¼ç›´æ¥forkå³å¯
gh repo fork pytorch/pytorch
</code></pre>
<h1 id="gist">Gist</h1>
<p>gistæ˜¯githubåˆ†äº«æ•°æ®å†…å®¹çš„å¹³å° -&gt; <a href="https://gist.github.com/">https://gist.github.com/</a><br>
ä¸Šé¢çš„åœ°å€å¯ä»¥åˆ†äº«å…¬æœ‰/ç§æœ‰çš„æ–‡ä»¶ï¼Œåˆ›å»ºä¸Šä¼ åï¼ŒGithubä¼šè·³è½¬ç”Ÿæˆä¸€ä¸ªé“¾æ¥ï¼Œæ¥ç»™æˆ‘ä»¬ä½¿ç”¨</p>
<h3 id="github-cliå®ç°gist">github-cliå®ç°Gist</h3>
<p>é»˜è®¤æ˜¯ç§æœ‰çš„ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š</p>
<pre><code>gh gist create 1.txt
</code></pre>
<p>å…¬æœ‰å‘½ä»¤å¦‚ä¸‹ï¼š</p>
<pre><code>gh gist create 1.txt --public
</code></pre>
<p>äºŒè€…éƒ½ä¼šç”Ÿæˆä¸ªURLï¼Œå³å¯è®¿é—®ã€‚<br>
ä¹Ÿå¯ä»¥ä¸€ä¸ªgistä¸­å­˜2ä¸ªæ–‡ä»¶ï¼š</p>
<pre><code>gh gist create 1.txt 2.txt
</code></pre>
<h3 id="ä¿®æ”¹-gistå…±äº«çš„æ–‡ä»¶å‚æ•°ä¸ºç”Ÿæˆurlçš„å°¾éƒ¨è·¯å¾„å‚æ•°">ä¿®æ”¹ Gistå…±äº«çš„æ–‡ä»¶ï¼Œå‚æ•°ä¸ºç”Ÿæˆurlçš„å°¾éƒ¨è·¯å¾„å‚æ•°</h3>
<pre><code>gh gist edit 5ff497631f0cc1e0a4463079a6a9eeff
</code></pre>
<h3 id="åˆ—å‡º-ä¸Šä¼ è¿‡çš„gistæ–‡ä»¶-publicä»£è¡¨å…¬æœ‰æ–‡ä»¶-secretç§æœ‰-ä¸åŠ å‚æ•°ä»£è¡¨æ‰€æœ‰">åˆ—å‡º ä¸Šä¼ è¿‡çš„Gistæ–‡ä»¶, --publicä»£è¡¨å…¬æœ‰æ–‡ä»¶ï¼Œ--secretç§æœ‰, ä¸åŠ å‚æ•°ä»£è¡¨æ‰€æœ‰</h3>
<pre><code>gh gist list
gh gist list --secret  
gh gist list --public
</code></pre>
<h1 id="pr-pull-request">PR (Pull Requestï¼‰</h1>
<h2 id="præ¦‚å¿µ">PRæ¦‚å¿µ</h2>
<p>&quot;æˆ‘forkäº†ä½ ä»¬çš„ä»£ç ï¼Œç°åœ¨æˆ‘å‘é€ä¸€ä¸ªè¯·æ±‚ï¼Œè¯·ä½ ä»¬å›æ”¶æˆ‘çš„ä»£ç &quot;ğŸ˜‚</p>
<h2 id="præµç¨‹">PRæµç¨‹</h2>
<ol>
<li>forkåˆ«äººä»“åº“ï¼ˆå…ˆforkåœ¨cloneï¼Œ å‰é¢å·²ç»æåˆ° gh å¯ä»¥ç›´æ¥forkä¸€æ­¥åˆ°ä½äº†ï¼‰</li>
<li>åˆ‡æ¢åˆ†æ”¯ï¼ˆä¹Ÿå¯ä»¥åœ¨ master ä¸‹ï¼‰ï¼Œadd,commit,push ä¿®æ”¹ä»£ç ã€‚</li>
<li>åœ¨ä½ forkåçš„ä»“åº“ä¸»é¡µç‚¹å‡»å³ä¸Šè§’çš„ Compare &amp; pull request æäº¤åˆå¹¶ç”³è¯·</li>
<li>ç­‰å¾…åˆ«äººåˆå¹¶ä½ çš„è¯·æ±‚</li>
</ol>
<h2 id="å®éªŒæµç¨‹">å®éªŒæµç¨‹</h2>
<h3 id="ä¸€-ç”¨å½“å‰çš„å·å»-forkå¦ä¸€ä¸ªå·å¦å¤–é‚£ä¸ªä¹Ÿæ˜¯è‡ªå·±çš„å·æ–¹ä¾¿åšå®éªŒçš„ä»“åº“">ä¸€ã€ç”¨å½“å‰çš„å·ï¼Œå» forkå¦ä¸€ä¸ªå·ï¼ˆå¦å¤–é‚£ä¸ªä¹Ÿæ˜¯è‡ªå·±çš„å·æ–¹ä¾¿åšå®éªŒï¼‰çš„ä»“åº“ã€‚</h3>
<pre><code>gh repo fork hacker-lin/bio2bioes
cd bio2bioes
</code></pre>
<h3 id="äºŒ-åˆ›å»ºåˆ‡æ¢åˆ†æ”¯addcommitpushadd">äºŒã€ åˆ›å»º+åˆ‡æ¢åˆ†æ”¯+add+commit+push+add</h3>
<pre><code>git checkout -b dev
echo 111 &gt; 1.txt
git add . &amp;&amp; git commit -m &quot;my_test&quot; 
åˆ°è¿™é‡Œå³å¯ï¼Œå…ˆä¸è¦pushï¼ˆä¸‹é¢PRåˆ›å»ºçš„è¿‡ç¨‹ï¼Œä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬pushï¼Œè¿™é‡Œpushæˆ‘è¯•äº†ä¼šå‡ºé”™ï¼‰
</code></pre>
<h3 id="ä¸‰-ç”¨github-cliå‘½ä»¤-ä»£æ›¿-ç‚¹å‡»compare-pull-request-æŒ‰é’®æäº¤åˆå¹¶ç”³è¯·">ä¸‰ã€ç”¨GitHUB-Cliå‘½ä»¤ ä»£æ›¿ ç‚¹å‡»Compare &amp; pull request æŒ‰é’®æäº¤åˆå¹¶ç”³è¯·</h3>
<p>åˆ›å»ºPR</p>
<pre><code>gh pr create
     -&gt; Where should we push the 'dev' branch # é€‰ç¬¬ä¸€ä¸ª
     -&gt; Title  # éšä¾¿å†™ä¸ª  some update
     -&gt; Body # ç›´æ¥ å›è½¦ è·³è¿‡å°±è¡Œã€‚
     -&gt; What's next? # é€‰ Submit  æäº¤å³å¯ 
</code></pre>
<p>åˆ—å‡ºæäº¤çš„PR</p>
<pre><code>gh pr list 
# æ‰§è¡Œåï¼Œä½ ä¼šå‘ç° PRä¿¡æ¯ï¼Œ # åé¢çš„æ•°å­—è®°ä½ä¸‹é¢Closedå’Œdiffç”¨å¾—åˆ°
</code></pre>
<p>Merge PR</p>
<pre><code>gh pr merge
    -&gt; What merge method would you like to use # Create a merge commitå³å¯
</code></pre>
<p>Closed PR</p>
<pre><code>gh pr close 3
# è¿™ä¸ª3 å°±æ˜¯ä¸Šé¢ gh pr list çš„ç»“æœ
</code></pre>
<p>æ¯”è¾ƒ PR ä¿¡æ¯</p>
<pre><code>gh pr diff 3
# æœ€ä¸‹é¢æ˜¯æœ€æ–°çš„
</code></pre>
<p>æŸ¥çœ‹ PR è¯¦ç»†ä¿¡æ¯</p>
<pre><code>gh pr status
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[GO => äº¤æ¢å…ƒç´ çš„å››ç§æ–¹å¼]]></title>
        <id>https://cythonlin.github.io/post/go-greater-jiao-huan-yuan-su-de-si-chong-fang-shi/</id>
        <link href="https://cythonlin.github.io/post/go-greater-jiao-huan-yuan-su-de-si-chong-fang-shi/">
        </link>
        <updated>2020-09-29T04:22:34.000Z</updated>
        <content type="html"><![CDATA[<h1 id="python-and-go">Python and Go</h1>
<p>ä¸‹é¢ä»£ç ï¼Œé™¤äº†æ•°æ®å£°æ˜ä¸å®šä¹‰ï¼Œå…¶ä»–Py å’Œ Go çš„è¯­æ³•éƒ½æ˜¯ä¸€æ‘¸ä¸€æ ·çš„ï¼ˆä¸»è¦å¼ºè°ƒ å¼‚æˆ–æ–¹å¼ï¼‰<br>
a := 1<br>
b := 3</p>
<h3 id="æ–¹å¼0">æ–¹å¼0</h3>
<pre><code>c := 0
c = a
a = b
b = c
</code></pre>
<h3 id="æ–¹å¼1">æ–¹å¼1</h3>
<pre><code>a,b = b,a
</code></pre>
<h3 id="æ–¹å¼2">æ–¹å¼2</h3>
<pre><code>a = a + b
b = a - b
a = a - b
</code></pre>
<h3 id="æ–¹å¼3-æ³¨å¿…é¡»æ˜¯æ•´å½¢-pyä¹Ÿä¸€æ ·">æ–¹å¼3 ï¼ˆæ³¨ï¼šå¿…é¡»æ˜¯æ•´å½¢ï¼Œ Pyä¹Ÿä¸€æ ·ï¼‰</h3>
<pre><code>a = a ^ b
b = a ^ b
a = a ^ b

fmt.Println(a)
fmt.Println(b)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RS => æ¨èç³»ç»Ÿï¼ˆä¸‰ï¼‰ç¦»çº¿å¬å›ä¸æ’åº ]]></title>
        <id>https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-san-chi-xian-zhao-hui-yu-pai-xu/</id>
        <link href="https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-san-chi-xian-zhao-hui-yu-pai-xu/">
        </link>
        <updated>2020-09-29T04:21:44.000Z</updated>
        <content type="html"><![CDATA[<h1 id="å¬å›è®¾è®¡">å¬å›è®¾è®¡</h1>
<h3 id="å¬å›æ’åºæµç¨‹">å¬å›æ’åºæµç¨‹</h3>
<h4 id="åŒ¿åç”¨æˆ·">åŒ¿åç”¨æˆ·ï¼š</h4>
<pre><code>é€šå¸¸ä½¿ç”¨ç”¨æˆ·å†·å¯åŠ¨æ–¹æ¡ˆï¼ŒåŒºåˆ«åœ¨äºuser_idä¸ºåŒ¿åç”¨æˆ·æ‰‹æœºè¯†åˆ«å·(é»‘é©¬å¤´æ¡ä¸å…è®¸åŒ¿åç”¨æˆ·)
æ‰€æœ‰åªæ­£é’ˆå¯¹äºç™»å½•ç”¨æˆ·ï¼š
</code></pre>
<h4 id="ç”¨æˆ·å†·å¯åŠ¨å‰æœŸç‚¹å‡»è¡Œä¸ºè¾ƒå°‘æƒ…å†µ">ç”¨æˆ·å†·å¯åŠ¨ï¼ˆå‰æœŸç‚¹å‡»è¡Œä¸ºè¾ƒå°‘æƒ…å†µï¼‰</h4>
<p>éä¸ªæ€§åŒ–æ¨è<br>
çƒ­é—¨å¬å›ï¼šè‡ªå®šä¹‰çƒ­é—¨è§„åˆ™ï¼Œæ ¹æ®å½“å‰æ—¶é—´æ®µçƒ­ç‚¹å®šæœŸæ›´æ–°ç»´æŠ¤äººç‚¹æ–‡ç« åº“<br>
æ–°æ–‡ç« å¬å›ï¼šä¸ºäº†æé«˜æ–°æ–‡ç« çš„æ›å…‰ç‡ï¼Œå»ºç«‹æ–°æ–‡ç« åº“ï¼Œè¿›è¡Œæ¨è<br>
ä¸ªæ€§åŒ–æ¨èï¼š<br>
åŸºäºå†…å®¹çš„ååŒè¿‡æ»¤åœ¨çº¿å¬å›ï¼šåŸºäºç”¨æˆ·å®æ—¶å…´è¶£ç”»åƒç›¸ä¼¼çš„å¬å›ç»“æœç”¨äºé¦–é¡µçš„ä¸ªæ€§åŒ–æ¨è</p>
<h4 id="åæœŸç¦»çº¿éƒ¨åˆ†ç”¨æˆ·ç‚¹å‡»è¡Œä¸ºè¾ƒå¤šç”¨æˆ·ç”»åƒå®Œå–„">åæœŸç¦»çº¿éƒ¨åˆ†ï¼ˆç”¨æˆ·ç‚¹å‡»è¡Œä¸ºè¾ƒå¤šï¼Œç”¨æˆ·ç”»åƒå®Œå–„ï¼‰</h4>
<p>å»ºç«‹ç”¨æˆ·é•¿æœŸå…´è¶£ç”»åƒï¼ˆè¯¦ç»†ï¼‰ï¼šåŒ…æ‹¬ç”¨æˆ·å„ä¸ªç»´åº¦çš„å…´è¶£ç‰¹å¾<br>
è®­ç»ƒæ’åºæ¨¡å‹<br>
LRæ¨¡å‹ã€FTRLã€Wide&amp;Deep<br>
ç¦»çº¿éƒ¨åˆ†çš„å¬å›ï¼š<br>
åŸºäºæ¨¡å‹ååŒè¿‡æ»¤æ¨èç¦»çº¿å¬å›ï¼šALS<br>
åŸºäºå†…å®¹çš„ç¦»çº¿å¬å›ï¼šæˆ–è€…ç§°åŸºäºç”¨æˆ·ç”»åƒçš„å¬å›</p>
<h3 id="å¬å›è¡¨è®¾è®¡ä¸æ¨¡å‹å¬å›">å¬å›è¡¨è®¾è®¡ä¸æ¨¡å‹å¬å›</h3>
<h4 id="å¬å›è¡¨è®¾è®¡">å¬å›è¡¨è®¾è®¡</h4>
<p>æˆ‘ä»¬çš„å¬å›æ–¹å¼æœ‰å¾ˆå¤šç§ã€‚<br>
å¤šè·¯å¬å›ç»“æœå­˜å‚¨æ¨¡å‹å¬å› ä¸ å†…å®¹å¬å›çš„ç»“æœ éœ€è¦è¿›è¡Œç›¸åº”é¢‘é“æ¨èåˆå¹¶ã€‚<br>
æ–¹æ¡ˆï¼šåŸºäºæ¨¡å‹ä¸åŸºäºå†…å®¹çš„å¬å›ç»“æœå­˜å…¥åŒä¸€å¼ è¡¨ï¼Œé¿å…å¤šå¼ è¡¨è¿›è¡Œè¯»å–å¤„ç†<br>
ç”±äºHBASEæœ‰å¤šä¸ªç‰ˆæœ¬æ•°æ®åŠŸèƒ½å­˜åœ¨çš„æ”¯æŒ<br>
TTL=&gt;7776000, VERSIONS=&gt;999999<br>
å¦‚ä¸‹ï¼š<br>
create 'cb_recall', {NAME=&gt;'als', TTL=&gt;7776000, VERSIONS=&gt;999999}<br>
alter 'cb_recall', {NAME=&gt;'content', TTL=&gt;7776000, VERSIONS=&gt;999999}<br>
alter 'cb_recall', {NAME=&gt;'online', TTL=&gt;7776000, VERSIONS=&gt;999999}</p>
<pre><code># ä¾‹å­ï¼ˆå¤šç‰ˆæœ¬ï¼‰ï¼š
put 'cb_recall', 'recall:user:5', 'als:1',[45,3,5,10]
put 'cb_recall', 'recall:user:5', 'als:1',[289,11,65,52,109,8]
put 'cb_recall', 'recall:user:5', 'als:2',[1,2,3,4,5,6,7,8,9,10]
put 'cb_recall', 'recall:user:2', 'content:1',[45,3,5,10,289,11,65,52,109,8]
put 'cb_recall', 'recall:user:2', 'content:2',[1,2,3,4,5,6,7,8,9,10]


hbase(main):084:0&gt; desc 'cb_recall'
Table cb_recall is ENABLED                                                                             
cb_recall                                                                                              
COLUMN FAMILIES DESCRIPTION                                                                            
{NAME =&gt; 'als', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false'
, KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 
'7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE
_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_
OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                    
{NAME =&gt; 'content', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'fa
lse', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL
          =&gt; '7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', C
ACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS
_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                
{NAME =&gt; 'online', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'fal
se', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL 
=&gt; '7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CA
CHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_
ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                 
3 row(s)	
</code></pre>
<p>ï¼ˆå‡ ä¹ä¸ç”¨ï¼‰åœ¨HIVEç”¨æˆ·æ•°æ®æ•°æ®åº“ä¸‹å»ºç«‹HIVEå¤–éƒ¨è¡¨,è‹¥hbaseè¡¨æœ‰ä¿®æ”¹ï¼Œåˆ™è¿›è¡ŒHIVE è¡¨åˆ é™¤æ›´æ–°<br>
create external table cb_recall_hbase(<br>
user_id STRING comment &quot;userID&quot;,<br>
als map&lt;string, ARRAY<BIGINT>&gt; comment &quot;als recall&quot;,<br>
content map&lt;string, ARRAY<BIGINT>&gt; comment &quot;content recall&quot;,<br>
online map&lt;string, ARRAY<BIGINT>&gt; comment &quot;online recall&quot;)<br>
COMMENT &quot;user recall table&quot;<br>
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'<br>
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,als:,content:,online:&quot;)<br>
TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;cb_recall&quot;);</p>
<h4 id="å¢åŠ ä¸€ä¸ªå†å²å¬å›ç»“æœè¡¨">å¢åŠ ä¸€ä¸ªå†å²å¬å›ç»“æœè¡¨</h4>
<pre><code>create 'history_recall', {NAME=&gt;'channel', TTL=&gt;7776000, VERSIONS=&gt;999999}

put 'history_recall', 'recall:user:5', 'als:1',[1,2,3]
put 'history_recall', 'recall:user:5', 'als:1',[4,5,6,7]
put 'history_recall', 'recall:user:5', 'als:1',[8,9,10]
</code></pre>
<p>ä¸ºä»€ä¹ˆå¢åŠ å†å²å¬å›è¡¨ï¼Ÿ<br>
1ã€ç›´æ¥åœ¨å­˜å‚¨å¬å›ç»“æœéƒ¨åˆ†è¿›è¡Œè¿‡æ»¤ï¼Œæ¯”ä¹‹åæ’åºè¿‡æ»¤ï¼ŒèŠ‚çœæ’åºæ—¶é—´<br>
2ã€é˜²æ­¢Redisç¼“å­˜æ²¡æœ‰æ¶ˆè€—å®Œï¼Œé€ æˆé‡å¤æ¨èï¼Œä»æºå¤´è¿›è¡Œè¿‡æ»¤</p>
<h3 id="åŸºäºæ¨¡å‹å¬å›é›†åˆè®¡ç®—">åŸºäºæ¨¡å‹å¬å›é›†åˆè®¡ç®—</h3>
<h4 id="alsæ¨¡å‹æ¨èå®ç°">ALSæ¨¡å‹æ¨èå®ç°</h4>
<p>æ­¥éª¤ï¼š<br>
1ã€æ•°æ®ç±»å‹è½¬æ¢,clickedä»¥åŠç”¨æˆ·IDä¸æ–‡ç« IDå¤„ç†<br>
2ã€ALSæ¨¡å‹è®­ç»ƒä»¥åŠæ¨è<br>
3ã€æ¨èç»“æœè§£æå¤„ç†<br>
4ã€æ¨èç»“æœå­˜å‚¨<br>
æ•°æ®ç±»å‹è½¬æ¢,clicked( bool è½¬ int)<br>
ur.spark.sql(&quot;use profile&quot;)<br>
user_article_click = ur.spark.sql(&quot;select * from user_article_basic&quot;).<br>
select(['user_id', 'article_id', 'clicked'])<br>
# æ›´æ¢ç±»å‹<br>
def change_types(row):<br>
return row.user_id, row.article_id, int(row.clicked)</p>
<pre><code>user_article_click = user_article_click.rdd.map(change_types).toDF(['user_id', 'article_id', 'clicked'])
</code></pre>
<p>è¿™æ­¥å¤„ç†ç»“æœæ ¼å¼å¦‚ä¸‹ï¼š<br>
user_id	article_id	clicked<br>
0<br>
1<br>
ç”¨æˆ·IDä¸æ–‡ç« IDå¤„ç†ï¼Œç¼–ç¨‹IDç´¢å¼•ï¼ˆåŸç”¨æˆ·IDå’Œæ–‡ç« IDæ˜¯é•¿å­—ç¬¦ä¸²ï¼ŒALSæ¨¡å‹ä¸èƒ½å¤„ç†ï¼Œè¦é‡æ–°ç¼–æ’IDç´¢å¼•ï¼‰<br>
from pyspark.ml.feature import StringIndexer<br>
from pyspark.ml import Pipeline<br>
# ç”¨æˆ·å’Œæ–‡ç« IDè¶…è¿‡ALSæœ€å¤§æ•´æ•°å€¼ï¼Œéœ€è¦ä½¿ç”¨StringIndexerè¿›è¡Œè½¬æ¢<br>
user_id_indexer = StringIndexer(inputCol='user_id', outputCol='als_user_id')<br>
article_id_indexer = StringIndexer(inputCol='article_id', outputCol='als_article_id')<br>
pip = Pipeline(stages=[user_id_indexer, article_id_indexer])<br>
pip_fit = pip.fit(user_article_click)<br>
als_user_article_click = pip_fit.transform(user_article_click)<br>
ALS æ¨¡å‹è®­ç»ƒä¸æ¨èï¼ˆALSæ¨¡å‹éœ€è¦è¾“å‡ºç”¨æˆ·IDåˆ—ï¼Œæ–‡ç« IDåˆ—ä»¥åŠç‚¹å‡»åˆ—ï¼‰<br>
from pyspark.ml.recommendation import ALS<br>
# æ¨¡å‹è®­ç»ƒå’Œæ¨èé»˜è®¤æ¯ä¸ªç”¨æˆ·å›ºå®šæ–‡ç« ä¸ªæ•°<br>
als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1)<br>
model = als.fit(als_user_article_click)<br>
recall_res = model.recommendForAllUsers(100)<br>
ç»“æœï¼š<br>
als_user_id	recommendations<br>
1			[[article_id, åˆ†æ•°]]</p>
<h4 id="æ¨èç»“æœå¤„ç†">æ¨èç»“æœå¤„ç†</h4>
<p>é€šè¿‡StringIndexerå˜æ¢åçš„ä¸‹æ ‡çŸ¥é“åŸæ¥çš„å’Œç”¨æˆ·ID<br>
# recall_reså¾—åˆ°éœ€è¦ä½¿ç”¨StringIndexerå˜æ¢åçš„ä¸‹æ ‡<br>
# ä¿å­˜åŸæ¥çš„ä¸‹è¡¨æ˜ å°„å…³ç³»<br>
refection_user = als_user_article_click.groupBy(['user_id']).max('als_user_id').withColumnRenamed(<br>
'max(als_user_id)', 'als_user_id')<br>
refection_article = als_user_article_click.groupBy(['article_id']).max('als_article_id').withColumnRenamed(<br>
'max(als_article_id)', 'als_article_id')</p>
<pre><code># Joinæ¨èç»“æœä¸ refection_useræ˜ å°„å…³ç³»è¡¨
# +-----------+--------------------+-------------------+
# | als_user_id | recommendations | user_id |
# +-----------+--------------------+-------------------+
# | 8 | [[163, 0.91328144]... | 2 |
 # | 0 | [[145, 0.653115], ... | 1106476833370537984 |
 
recall_res = recall_res.join(refection_user, on=['als_user_id'], how='left').select(
['als_user_id', 'recommendations', 'user_id'])
</code></pre>
<p>å¯¹æ¨èæ–‡ç« IDåå¤„ç†ï¼šå¾—åˆ°æ¨èåˆ—è¡¨,è·å–æ¨èåˆ—è¡¨ä¸­çš„IDç´¢å¼•<br>
# Joinæ¨èç»“æœä¸ refection_articleæ˜ å°„å…³ç³»è¡¨<br>
# +-----------+-------+----------------+<br>
# | als_user_id | user_id | als_article_id |<br>
# +-----------+-------+----------------+<br>
# | 8 | 2 | [163, 0.91328144] |<br>
# | 8 | 2 | [132, 0.91328144] |<br>
import pyspark.sql.functions as F<br>
recall_res = recall_res.withColumn('als_article_id', F.explode('recommendations')).drop('recommendations')</p>
<pre><code># +-----------+-------+--------------+
# | als_user_id | user_id | als_article_id |
# +-----------+-------+--------------+
# | 8 | 2 | 163 |
# | 8 | 2 | 132 |
def _article_id(row):
	return row.als_user_id, row.user_id, row.als_article_id[0]
</code></pre>
<p>è¿›è¡Œç´¢å¼•å¯¹åº”æ–‡ç« IDè·å–<br>
als_recall = recall_res.rdd.map(_article_id).toDF(['als_user_id', 'user_id', 'als_article_id'])<br>
als_recall = als_recall.join(refection_article, on=['als_article_id'], how='left').select(<br>
['user_id', 'article_id'])<br>
# å¾—åˆ°æ¯ä¸ªç”¨æˆ·ID å¯¹åº”æ¨èæ–‡ç« <br>
# +-------------------+----------+<br>
# | user_id |				 article_id |<br>
# +-------------------+----------+<br>
# | 1106476833370537984 |   44075 |<br>
# | 1 | 					 44075 |<br>
è·å–æ¯ä¸ªæ–‡ç« å¯¹åº”çš„é¢‘é“ï¼Œæ¨èç»™ç”¨æˆ·æ—¶æŒ‰ç…§é¢‘é“å­˜å‚¨:<br>
ur.spark.sql(&quot;use toutiao&quot;)<br>
news_article_basic = ur.spark.sql(&quot;select article_id, channel_id from news_article_basic&quot;)</p>
<pre><code>als_recall = als_recall.join(news_article_basic, on=['article_id'], how='left')
als_recall = als_recall.groupBy(['user_id', 'channel_id']).agg(F.collect_list('article_id')).withColumnRenamed(
                    'collect_list(article_id)', 'article_list')

als_recall = als_recall.dropna()
</code></pre>
<h4 id="å¬å›ç»“æœå­˜å‚¨">å¬å›ç»“æœå­˜å‚¨</h4>
<p>HBASEè¡¨è®¾è®¡æ¦‚è§ˆï¼š<br>
put 'cb_recall', 'recall:user:5', 'als:1',[45,3,5,10,289,11,65,52,109,8]<br>
put 'cb_recall', 'recall:user:5', 'als:2',[1,2,3,4,5,6,7,8,9,10]<br>
å­˜å‚¨ä»£ç å¦‚ä¸‹ï¼š<br>
def save_offline_recall_hbase(partition):<br>
&quot;&quot;&quot;ç¦»çº¿æ¨¡å‹å¬å›ç»“æœå­˜å‚¨<br>
&quot;&quot;&quot;<br>
import happybase<br>
pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)<br>
for row in partition:<br>
with pool.connection() as conn:<br>
# è·å–å†å²çœ‹è¿‡çš„è¯¥é¢‘é“æ–‡ç« <br>
history_table = conn.table('history_recall')<br>
# å¤šä¸ªç‰ˆæœ¬<br>
data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),<br>
'channel:{}'.format(row.channel_id).encode())</p>
<pre><code>            history = []
            if len(data) &gt;= 2:
                for l in data[:-1]:
                    history.extend(eval(l))
            else:
                history = []

            # è¿‡æ»¤reco_articleä¸history
            reco_res = list(set(row.article_list) - set(history))

            if reco_res:

                table = conn.table('cb_recall')
                # é»˜è®¤æ”¾åœ¨æ¨èé¢‘é“
                table.put('recall:user:{}'.format(row.user_id).encode(),
                          {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                # æ”¾å…¥å†å²æ¨èè¿‡æ–‡ç« 
                history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                  {'channel:{}'.format(row.channel_id): str(reco_res).encode()})
            conn.close()

als_recall.foreachPartition(save_offline_recall_hbase)
</code></pre>
<h3 id="ç¦»çº¿ç”¨æˆ·åŸºäºå†…å®¹å¬å›é›†">ç¦»çº¿ç”¨æˆ·åŸºäºå†…å®¹å¬å›é›†</h3>
<p>ç›®æ ‡<br>
çŸ¥é“ç¦»çº¿å†…å®¹å¬å›çš„æ¦‚å¿µ<br>
çŸ¥é“å¦‚ä½•è¿›è¡Œå†…å®¹å¬å›è®¡ç®—å­˜å‚¨è§„åˆ™<br>
åº”ç”¨<br>
åº”ç”¨sparkå®Œæˆç¦»çº¿ç”¨æˆ·åŸºäºå†…å®¹çš„ååŒè¿‡æ»¤æ¨è</p>
<h4 id="åŸºäºå†…å®¹å¬å›å®ç°æ–‡ç« å‘é‡ä¹‹å‰å·²ç»å¼„å¥½äº†">åŸºäºå†…å®¹å¬å›å®ç°ï¼ˆæ–‡ç« å‘é‡ä¹‹å‰å·²ç»å¼„å¥½äº†ï¼‰</h4>
<p>è¿‡æ»¤ç”¨æˆ·ç‚¹å‡»çš„æ–‡ç« <br>
# åŸºäºå†…å®¹ç›¸ä¼¼å¬å›ï¼ˆç”»åƒå¬å›ï¼‰<br>
ur.spark.sql(&quot;use profile&quot;)<br>
user_article_basic = self.spark.sql(&quot;select * from user_article_basic&quot;)<br>
user_article_basic = user_article_basic.filter(&quot;clicked=True&quot;)</p>
<pre><code>def save_content_filter_history_to__recall(partition):
    &quot;&quot;&quot;è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„æ¯ä¸ªæ“ä½œæ–‡ç« çš„ç›¸ä¼¼æ–‡ç« ï¼Œè¿‡æ»¤ä¹‹åï¼Œå†™å…¥contentå¬å›è¡¨å½“ä¸­ï¼ˆæ”¯æŒä¸åŒæ—¶é—´æˆ³ç‰ˆæœ¬ï¼‰
    &quot;&quot;&quot;
    import happybase
    pool = happybase.ConnectionPool(size=10, host='hadoop-master')

    # è¿›è¡Œä¸ºç›¸ä¼¼æ–‡ç« è·å–
    with pool.connection() as conn:

        # key:   article_id,    column:  similar:article_id
        similar_table = conn.table('article_similar')
        # å¾ªç¯partition
        for row in partition:
            # è·å–ç›¸ä¼¼æ–‡ç« ç»“æœè¡¨
            similar_article = similar_table.row(str(row.article_id).encode(),
                                                columns=[b'similar'])
            # ç›¸ä¼¼æ–‡ç« ç›¸ä¼¼åº¦æ’åºè¿‡æ»¤ï¼Œå¬å›ä¸éœ€è¦å¤ªå¤§çš„æ•°æ®ï¼Œ ç™¾ä¸ªï¼Œåƒ
            _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True)
            if _srt:
                # æ¯æ¬¡è¡Œä¸ºæ¨è10ç¯‡æ–‡ç« 
                reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10]

                # è·å–å†å²çœ‹è¿‡çš„è¯¥é¢‘é“æ–‡ç« 
                history_table = conn.table('history_recall')
                # å¤šä¸ªç‰ˆæœ¬
                data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                           'channel:{}'.format(row.channel_id).encode())

                history = []
                if len(data) &gt;= 2:
                    for l in data[:-1]:
                        history.extend(eval(l))
                else:
                    history = []

                # è¿‡æ»¤reco_articleä¸history
                reco_res = list(set(reco_article) - set(history))

                # è¿›è¡Œæ¨èï¼Œæ”¾å…¥åŸºäºå†…å®¹çš„å¬å›è¡¨å½“ä¸­ä»¥åŠå†å²çœ‹è¿‡çš„æ–‡ç« è¡¨å½“ä¸­
                if reco_res:
                    # content_table = conn.table('cb_content_recall')
                    content_table = conn.table('cb_recall')
                    content_table.put(&quot;recall:user:{}&quot;.format(row.user_id).encode(),
                                      {'content:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                    # æ”¾å…¥å†å²æ¨èè¿‡æ–‡ç« 
                    history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                      {'channel:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

        conn.close()

user_article_basic.foreachPartition(save_content_filter_history_to__recall)
</code></pre>
<h3 id="ç¦»çº¿ç”¨æˆ·å¬å›å®šæ—¶æ›´æ–°">ç¦»çº¿ç”¨æˆ·å¬å›å®šæ—¶æ›´æ–°</h3>
<h4 id="å®šæ—¶æ›´æ–°ä»£ç ">å®šæ—¶æ›´æ–°ä»£ç </h4>
<pre><code>import os
import sys
# å¦‚æœå½“å‰ä»£ç æ–‡ä»¶è¿è¡Œæµ‹è¯•éœ€è¦åŠ å…¥ä¿®æ”¹è·¯å¾„ï¼Œå¦åˆ™åé¢çš„å¯¼åŒ…å‡ºç°é—®é¢˜
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(BASE_DIR))
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline

from pyspark.ml.recommendation import ALS
from offline import SparkSessionBase
from datetime import datetime
import time
import numpy as np


class UpdateRecall(SparkSessionBase):

    SPARK_APP_NAME = &quot;updateRecall&quot;
    ENABLE_HIVE_SUPPORT = True

    def __init__(self, number):

        self.spark = self._create_spark_session()
        self.N = number

    def update_als_recall(self):
        &quot;&quot;&quot;
        æ›´æ–°åŸºäºæ¨¡å‹ï¼ˆALSï¼‰çš„ååŒè¿‡æ»¤å¬å›é›†
        :return:
        &quot;&quot;&quot;
        # è¯»å–ç”¨æˆ·è¡Œä¸ºåŸºæœ¬è¡¨
        self.spark.sql(&quot;use profile&quot;)
        user_article_click = self.spark.sql(&quot;select * from user_article_basic&quot;).\
            select(['user_id', 'article_id', 'clicked'])

        # æ›´æ¢ç±»å‹
        def change_types(row):
            return row.user_id, row.article_id, int(row.clicked)

        user_article_click = user_article_click.rdd.map(change_types).toDF(['user_id', 'article_id', 'clicked'])
        # ç”¨æˆ·å’Œæ–‡ç« IDè¶…è¿‡ALSæœ€å¤§æ•´æ•°å€¼ï¼Œéœ€è¦ä½¿ç”¨StringIndexerè¿›è¡Œè½¬æ¢
        user_id_indexer = StringIndexer(inputCol='user_id', outputCol='als_user_id')
        article_id_indexer = StringIndexer(inputCol='article_id', outputCol='als_article_id')
        pip = Pipeline(stages=[user_id_indexer, article_id_indexer])
        pip_fit = pip.fit(user_article_click)
        als_user_article_click = pip_fit.transform(user_article_click)

        # æ¨¡å‹è®­ç»ƒå’Œæ¨èé»˜è®¤æ¯ä¸ªç”¨æˆ·å›ºå®šæ–‡ç« ä¸ªæ•°
        als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1)
        model = als.fit(als_user_article_click)
        recall_res = model.recommendForAllUsers(self.N)

        # recall_reså¾—åˆ°éœ€è¦ä½¿ç”¨StringIndexerå˜æ¢åçš„ä¸‹æ ‡
        # ä¿å­˜åŸæ¥çš„ä¸‹è¡¨æ˜ å°„å…³ç³»
        refection_user = als_user_article_click.groupBy(['user_id']).max('als_user_id').withColumnRenamed(
            'max(als_user_id)', 'als_user_id')
        refection_article = als_user_article_click.groupBy(['article_id']).max('als_article_id').withColumnRenamed(
            'max(als_article_id)', 'als_article_id')

        # Joinæ¨èç»“æœä¸ refection_useræ˜ å°„å…³ç³»è¡¨
        # +-----------+--------------------+-------------------+
        # | als_user_id | recommendations | user_id |
        # +-----------+--------------------+-------------------+
        # | 8 | [[163, 0.91328144]... | 2 |
        #        | 0 | [[145, 0.653115], ... | 1106476833370537984 |
        recall_res = recall_res.join(refection_user, on=['als_user_id'], how='left').select(
            ['als_user_id', 'recommendations', 'user_id'])

        # Joinæ¨èç»“æœä¸ refection_articleæ˜ å°„å…³ç³»è¡¨
        # +-----------+-------+----------------+
        # | als_user_id | user_id | als_article_id |
        # +-----------+-------+----------------+
        # | 8 | 2 | [163, 0.91328144] |
        # | 8 | 2 | [132, 0.91328144] |
        import pyspark.sql.functions as F
        recall_res = recall_res.withColumn('als_article_id', F.explode('recommendations')).drop('recommendations')

        # +-----------+-------+--------------+
        # | als_user_id | user_id | als_article_id |
        # +-----------+-------+--------------+
        # | 8 | 2 | 163 |
        # | 8 | 2 | 132 |
        def _article_id(row):
            return row.als_user_id, row.user_id, row.als_article_id[0]

        als_recall = recall_res.rdd.map(_article_id).toDF(['als_user_id', 'user_id', 'als_article_id'])
        als_recall = als_recall.join(refection_article, on=['als_article_id'], how='left').select(
            ['user_id', 'article_id'])
        # å¾—åˆ°æ¯ä¸ªç”¨æˆ·ID å¯¹åº”æ¨èæ–‡ç« 
        # +-------------------+----------+
        # | user_id | article_id |
        # +-------------------+----------+
        # | 1106476833370537984 | 44075 |
        # | 1 | 44075 |
        # åˆ†ç»„ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·ï¼Œæ¨èåˆ—è¡¨
        # als_recall = als_recall.groupby('user_id').agg(F.collect_list('article_id')).withColumnRenamed(
        #     'collect_list(article_id)', 'article_list')
        self.spark.sql(&quot;use toutiao&quot;)
        news_article_basic = self.spark.sql(&quot;select article_id, channel_id from news_article_basic&quot;)
        als_recall = als_recall.join(news_article_basic, on=['article_id'], how='left')
        als_recall = als_recall.groupBy(['user_id', 'channel_id']).agg(F.collect_list('article_id')).withColumnRenamed(
            'collect_list(article_id)', 'article_list')
        als_recall = als_recall.dropna()

        # å­˜å‚¨
        def save_offline_recall_hbase(partition):
            &quot;&quot;&quot;ç¦»çº¿æ¨¡å‹å¬å›ç»“æœå­˜å‚¨
            &quot;&quot;&quot;
            import happybase
            pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)
            for row in partition:
                with pool.connection() as conn:
                    # è·å–å†å²çœ‹è¿‡çš„è¯¥é¢‘é“æ–‡ç« 
                    history_table = conn.table('history_recall')
                    # å¤šä¸ªç‰ˆæœ¬
                    data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                               'channel:{}'.format(row.channel_id).encode())

                    history = []
                    if len(data) &gt;= 2:
                        for l in data[:-1]:
                            history.extend(eval(l))
                    else:
                        history = []

                    # è¿‡æ»¤reco_articleä¸history
                    reco_res = list(set(row.article_list) - set(history))

                    if reco_res:

                        table = conn.table('cb_recall')
                        # é»˜è®¤æ”¾åœ¨æ¨èé¢‘é“
                        table.put('recall:user:{}'.format(row.user_id).encode(),
                                  {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()})
                        conn.close()

                        # æ”¾å…¥å†å²æ¨èè¿‡æ–‡ç« 
                        history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                          {'channel:{}'.format(row.channel_id): str(reco_res).encode()})
                    conn.close()

        als_recall.foreachPartition(save_offline_recall_hbase)

    def update_content_recall(self):
        &quot;&quot;&quot;
        æ›´æ–°åŸºäºå†…å®¹ï¼ˆç”»åƒï¼‰çš„æ¨èå¬å›é›†, word2vecç›¸ä¼¼
        :return:
        &quot;&quot;&quot;
        # åŸºäºå†…å®¹ç›¸ä¼¼å¬å›ï¼ˆç”»åƒå¬å›ï¼‰
        ur.spark.sql(&quot;use profile&quot;)
        user_article_basic = self.spark.sql(&quot;select * from user_article_basic&quot;)
        user_article_basic = user_article_basic.filter(&quot;clicked=True&quot;)

        def save_content_filter_history_to__recall(partition):
            &quot;&quot;&quot;è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„æ¯ä¸ªæ“ä½œæ–‡ç« çš„ç›¸ä¼¼æ–‡ç« ï¼Œè¿‡æ»¤ä¹‹åï¼Œå†™å…¥contentå¬å›è¡¨å½“ä¸­ï¼ˆæ”¯æŒä¸åŒæ—¶é—´æˆ³ç‰ˆæœ¬ï¼‰
            &quot;&quot;&quot;
            import happybase
            pool = happybase.ConnectionPool(size=10, host='hadoop-master')

            # è¿›è¡Œä¸ºç›¸ä¼¼æ–‡ç« è·å–
            with pool.connection() as conn:

                # key:   article_id,    column:  similar:article_id
                similar_table = conn.table('article_similar')
                # å¾ªç¯partition
                for row in partition:
                    # è·å–ç›¸ä¼¼æ–‡ç« ç»“æœè¡¨
                    similar_article = similar_table.row(str(row.article_id).encode(),
                                                        columns=[b'similar'])
                    # ç›¸ä¼¼æ–‡ç« ç›¸ä¼¼åº¦æ’åºè¿‡æ»¤ï¼Œå¬å›ä¸éœ€è¦å¤ªå¤§çš„æ•°æ®ï¼Œ ç™¾ä¸ªï¼Œåƒ
                    _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True)
                    if _srt:
                        # æ¯æ¬¡è¡Œä¸ºæ¨è10ç¯‡æ–‡ç« 
                        reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10]

                        # è·å–å†å²çœ‹è¿‡çš„è¯¥é¢‘é“æ–‡ç« 
                        history_table = conn.table('history_recall')
                        # å¤šä¸ªç‰ˆæœ¬
                        data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                                   'channel:{}'.format(row.channel_id).encode())

                        history = []
                            if len(_history_data) &gt; 1:
                                for l in _history_data:
                                    history.extend(l)

                        # è¿‡æ»¤reco_articleä¸history
                        reco_res = list(set(reco_article) - set(history))

                        # è¿›è¡Œæ¨èï¼Œæ”¾å…¥åŸºäºå†…å®¹çš„å¬å›è¡¨å½“ä¸­ä»¥åŠå†å²çœ‹è¿‡çš„æ–‡ç« è¡¨å½“ä¸­
                        if reco_res:
                            # content_table = conn.table('cb_content_recall')
                            content_table = conn.table('cb_recall')
                            content_table.put(&quot;recall:user:{}&quot;.format(row.user_id).encode(),
                                              {'content:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                            # æ”¾å…¥å†å²æ¨èè¿‡æ–‡ç« 
                            history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                              {'channel:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                conn.close()

        user_article_basic.foreachPartition(save_content_filter_history_to__recall)


if __name__ == '__main__':
    ur = UpdateRecall(500)
    ur.update_als_recall()
    ur.update_content_recall()
</code></pre>
<p>å®šæ—¶æ›´æ–°ä»£ç ï¼Œåœ¨main.pyå’Œupdate.pyä¸­æ·»åŠ ä»¥ä¸‹ä»£ç ï¼š<br>
from offline.update_recall import UpdateRecall<br>
from schedule.update_profile import update_user_profile, update_article_profile, update_recall</p>
<pre><code>def update_recall():
    &quot;&quot;&quot;
    æ›´æ–°ç”¨æˆ·çš„å¬å›é›†
    :return:
    &quot;&quot;&quot;
    udp = UpdateRecall(200)
    udp.update_als_recall()
    udp.update_content_recall()
</code></pre>
<p>mainä¸­æ·»åŠ <br>
scheduler.add_job(update_recall, trigger='interval', hour=3)</p>
<h1 id="æ’åºè®¾è®¡">æ’åºè®¾è®¡</h1>
<h3 id="æ’åºæ¨¡å‹">æ’åºæ¨¡å‹</h3>
<p>å®½æ¨¡å‹ + ç‰¹å¾â¼¯ç¨‹<br>
LR/MLR + éIDç±»ç‰¹å¾(â¼ˆâ¼¯ç¦»æ•£/GBDT/FM)<br>
spark ä¸­å¯ä»¥ç›´æ¥ä½¿ç”¨<br>
å®½æ¨¡å‹ + æ·±æ¨¡å‹<br>
wide&amp;deep,DeepFM<br>
ä½¿ç”¨TensorFlowè¿›è¡Œè®­ç»ƒ<br>
æ·±æ¨¡å‹ï¼š<br>
DNN + ç‰¹å¾embedding<br>
ä½¿ç”¨TensorFlowè¿›è¡Œè®­ç»ƒ</p>
<h3 id="ç‰¹å¾å¤„ç†åŸåˆ™">ç‰¹å¾å¤„ç†åŸåˆ™</h3>
<p>ç¦»æ•£æ•°æ®<br>
one-hotç¼–ç <br>
è¿ç»­æ•°æ®<br>
å½’ä¸€åŒ–<br>
å›¾ç‰‡/æ–‡æœ¬<br>
æ–‡ç« æ ‡ç­¾/å…³é”®è¯æå–<br>
embedding</p>
<h3 id="ä¼˜åŒ–è®­ç»ƒæ–¹å¼">ä¼˜åŒ–è®­ç»ƒæ–¹å¼</h3>
<p>ä½¿ç”¨Batch SGDä¼˜åŒ–<br>
åŠ å…¥æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ</p>
<h3 id="spark-lr-è¿›è¡Œé¢„ä¼°">spark LR è¿›è¡Œé¢„ä¼°</h3>
<p>ç›®çš„ï¼šé€šè¿‡LRæ¨¡å‹è¿›è¡ŒCTRé¢„ä¼°<br>
æ­¥éª¤ï¼š<br>
1ã€éœ€è¦é€šè¿‡sparkè¯»å–HIVEå¤–éƒ¨è¡¨ï¼Œéœ€è¦æ–°çš„sparksessioné…ç½®<br>
å¢åŠ HBASEé…ç½®<br>
2ã€è¯»å–ç”¨æˆ·ç‚¹å‡»è¡Œä¸ºè¡¨ï¼Œä¸ç”¨æˆ·ç”»åƒå’Œæ–‡ç« ç”»åƒï¼Œæ„é€ è®­ç»ƒæ ·æœ¬<br>
3ã€LRæ¨¡å‹è¿›è¡Œè®­ç»ƒ<br>
4ã€LRæ¨¡å‹é¢„æµ‹ã€ç»“æœè¯„ä¼°</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => HBase]]></title>
        <id>https://cythonlin.github.io/post/py-greater-hbase/</id>
        <link href="https://cythonlin.github.io/post/py-greater-hbase/">
        </link>
        <updated>2020-09-29T04:21:09.000Z</updated>
        <content type="html"><![CDATA[<h1 id="æŠ¥é”™">æŠ¥é”™</h1>
<p>è‹¥list æˆ–å…¶ä»–å‘½ä»¤ æœ‰å¦‚ä¸‹é”™è¯¯ï¼š<br>
ERROR: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing<br>
åˆ™ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ï¼š<br>
cd $HBASE_HOME/bin<br>
./hbase zkcli<br>
ls /<br>
rmr /hbase<br>
ls /<br>
é€€å‡º zookeeper cliï¼Œ åˆ é™¤hdfsä¸­çš„ /hbase<br>
hdfs dfs -rm -r /hbase<br>
ç„¶åé‡å¯hbase:<br>
cd $HBASE_HOME/bin<br>
./stop-hbase.sh<br>
./start-hbase.sh<br>
è‹¥stop hbaseçš„æ—¶å€™å‡ºç° ..... åœæ­¢ä¸æ‰ï¼Œ åˆ™ï¼š<br>
cd $HBASE_HOME/bin<br>
./hbase-daemons.sh stop regionserver</p>
<pre><code># ./start-hbase.sh
# kill -9 pidæ¥ç»ˆæ­¢hbaseçš„è¿›ç¨‹
</code></pre>
<h1 id="hbaseå‘½ä»¤">HBaseå‘½ä»¤</h1>
<p>https://blog.csdn.net/vbirdbest/article/details/88236575</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RS => æ¨èç³»ç»Ÿï¼ˆäºŒï¼‰ç¦»çº¿ç”»åƒæ„å»º ]]></title>
        <id>https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-er-chi-xian-hua-xiang-gou-jian/</id>
        <link href="https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-er-chi-xian-hua-xiang-gou-jian/">
        </link>
        <updated>2020-09-29T04:20:35.000Z</updated>
        <content type="html"><![CDATA[<h1 id="æ–‡ç« ç¦»çº¿ç”»åƒæ„å»º">æ–‡ç« ç¦»çº¿ç”»åƒæ„å»º</h1>
<h3 id="sparké…ç½®åŸºç±»æŠ½å–">Sparké…ç½®åŸºç±»æŠ½å–</h3>
<pre><code>from pyspark import SparkConf
from pyspark.sql import SparkSession
import os

class SparkSessionBase(object):

	SPARK_APP_NAME = None
	SPARK_URL = &quot;yarn&quot;

	SPARK_EXECUTOR_MEMORY = &quot;2g&quot;
	SPARK_EXECUTOR_CORES = 2
	SPARK_EXECUTOR_INSTANCES = 2

	ENABLE_HIVE_SUPPORT = False

	def _create_spark_session(self):
		conf = SparkConf()  # åˆ›å»ºspark configå¯¹è±¡
		config = (
			(&quot;spark.app.name&quot;, self.SPARK_APP_NAME),  # è®¾ç½®å¯åŠ¨çš„sparkçš„appåç§°ï¼Œæ²¡æœ‰æä¾›ï¼Œå°†éšæœºäº§ç”Ÿä¸€ä¸ªåç§°
			(&quot;spark.executor.memory&quot;, self.SPARK_EXECUTOR_MEMORY),  # è®¾ç½®è¯¥appå¯åŠ¨æ—¶å ç”¨çš„å†…å­˜ç”¨é‡ï¼Œé»˜è®¤2g
	 		(&quot;spark.master&quot;, self.SPARK_URL),  # spark masterçš„åœ°å€
			(&quot;spark.executor.cores&quot;, self.SPARK_EXECUTOR_CORES),  # è®¾ç½®spark executorä½¿ç”¨çš„CPUæ ¸å¿ƒæ•°ï¼Œé»˜è®¤æ˜¯1æ ¸å¿ƒ
			(&quot;spark.executor.instances&quot;, self.SPARK_EXECUTOR_INSTANCES),
			(&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;),
		)

	conf.setAll(config)

	# åˆ©ç”¨configå¯¹è±¡ï¼Œåˆ›å»ºspark session
	if self.ENABLE_HIVE_SUPPORT:
		return SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()
	else:
		return SparkSession.builder.config(conf=conf).getOrCreate()
</code></pre>
<h3 id="ä¸»åº”ç”¨å¯¼å…¥åŸºç±»">ä¸»åº”ç”¨å¯¼å…¥åŸºç±»</h3>
<pre><code># pip install pyspark
# pip install findspark

import findspark
findspark.init()

import os
import sys
# å¦‚æœå½“å‰ä»£ç æ–‡ä»¶è¿è¡Œæµ‹è¯•éœ€è¦åŠ å…¥ä¿®æ”¹è·¯å¾„ï¼Œé¿å…å‡ºç°åå¯¼åŒ…é—®é¢˜
BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))
sys.path.insert(0, os.path.join(BASE_DIR))
print(BASE_DIR)
PYSPARK_PYTHON = &quot;/miniconda2/envs/reco_sys/bin/python&quot;
# å½“å­˜åœ¨å¤šä¸ªç‰ˆæœ¬æ—¶ï¼Œä¸æŒ‡å®šå¾ˆå¯èƒ½ä¼šå¯¼è‡´å‡ºé”™
os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHON
os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHON
from offline import SparkSessionBase

class OriginArticleData(SparkSessionBase):

	SPARK_APP_NAME = &quot;mergeArticle&quot;
	SPARK_URL = &quot;yarn&quot;

	ENABLE_HIVE_SUPPORT = True

	def __init__(self):
		self.spark = self._create_spark_session()

oa = OriginArticleData()   # oaå°±æ˜¯å¸¦æœ‰é…ç½®çš„ sparkSessionçš„å®ä¾‹åŒ–å¯¹è±¡
</code></pre>
<h3 id="æ–‡ç« -è¡¨-åˆå¹¶">æ–‡ç«  è¡¨ åˆå¹¶</h3>
<p>æ–‡ç« åŸºæœ¬ä¿¡æ¯è¡¨+æ–‡ç« å†…å®¹è¡¨+é¢‘é“è¡¨ï¼š<br>
titlce_content = oa.spark.sql(&quot;select a.article_id, a.channel_id, a.title, b.content from news_article_basic a inner join news_article_content b on a.article_id=b.article_id)<br>
å› ä¸ºå¾—åˆ°çš„æ˜¯ DFç±»å‹ï¼Œæƒ³è¦ç”¨SQLï¼Œå¯ä»¥æŠŠDFæ³¨å†Œä¸ºä¸´æ—¶è¡¨<br>
titlce_content.registerTempTable('temptable')<br>
å†æŠŠ é¢‘é“è¡¨ çš„ é¢‘é“å åˆå¹¶è¿›æ¥<br>
channel_title_content = oa.spark.sql(&quot;select t.*, n.channel_name from temptable t left join news_channel n on t.channel_id=n.channel_id&quot;)</p>
<h3 id="æ–‡ç« -å­—æ®µ-åˆå¹¶">æ–‡ç«  å­—æ®µ åˆå¹¶</h3>
<p>å°† æ–‡ç« æ ‡é¢˜+æ–‡ç« å†…å®¹+æ–‡ç« é¢‘é“ çš„åˆ—ï¼Œæ‹¼æ¥æˆä¸€ä¸ªå¤§å­—ç¬¦ä¸²<br>
import pyspark.sql.functions as F<br>
import gc</p>
<pre><code># å¢åŠ channelçš„åå­—ï¼Œåé¢ä¼šä½¿ç”¨
basic_content.registerTempTable(&quot;temparticle&quot;)
channel_basic_content = oa.spark.sql(
	&quot;select t.*, n.channel_name from temparticle t left join news_channel n on t.channel_id=n.channel_id&quot;
)

# åˆ©ç”¨concat_wsæ–¹æ³•ï¼Œå°†å¤šåˆ—æ•°æ®åˆå¹¶ä¸ºä¸€ä¸ªé•¿æ–‡æœ¬å†…å®¹ï¼ˆé¢‘é“ï¼Œæ ‡é¢˜ä»¥åŠå†…å®¹åˆå¹¶ï¼‰
oa.spark.sql(&quot;use article&quot;)
sentence_df = channel_basic_content.select(&quot;article_id&quot;, &quot;channel_id&quot;, &quot;channel_name&quot;, &quot;title&quot;, &quot;content&quot;, \
	F.concat_ws(
		&quot;,&quot;,											# æŒ‡å®šå¤§å­—ç¬¦ä¸²åˆ†éš”ç¬¦
		channel_basic_content.channel_name,         
		channel_basic_content.title,
		channel_basic_content.content					
		).alias(&quot;sentence&quot;)                    # æ–°åˆ— å¤§å­—ç¬¦ä¸² å–å
	)
del basic_content
del channel_basic_content
gc.collect()

# sentence_df.write.insertInto(&quot;article_data&quot;)       # å†™å…¥æå‰åˆ›å»ºå¥½çš„Hiveè¡¨ä¸­
</code></pre>
<h3 id="åˆ†è¯">åˆ†è¯</h3>
<pre><code>def segmentation(partition):          # å°±è¿™ä¸€è¡Œçš„ç¼©è¿›éœ€è¦è°ƒæ•´ä¸‹
import os
import re

import jieba
import jieba.analyse
import jieba.posseg as pseg
import codecs

abspath = &quot;/root/words&quot;

# ç»“å·´åŠ è½½ç”¨æˆ·è¯å…¸
userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;)
jieba.load_userdict(userDict_path)

# åœç”¨è¯æ–‡æœ¬
stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;)

def get_stopwords_list():
    &quot;&quot;&quot;è¿”å›stopwordsåˆ—è¡¨&quot;&quot;&quot;
    stopwords_list = [i.strip()
                      for i in codecs.open(stopwords_path).readlines()]
    return stopwords_list

# æ‰€æœ‰çš„åœç”¨è¯åˆ—è¡¨
stopwords_list = get_stopwords_list()

# åˆ†è¯
def cut_sentence(sentence):
    &quot;&quot;&quot;å¯¹åˆ‡å‰²ä¹‹åçš„è¯è¯­è¿›è¡Œè¿‡æ»¤ï¼Œå»é™¤åœç”¨è¯ï¼Œä¿ç•™åè¯ï¼Œè‹±æ–‡å’Œè‡ªå®šä¹‰è¯åº“ä¸­çš„è¯ï¼Œé•¿åº¦å¤§äº2çš„è¯&quot;&quot;&quot;
    # print(sentence,&quot;*&quot;*100)
    # eg:[pair('ä»Šå¤©', 't'), pair('æœ‰', 'd'), pair('é›¾', 'n'), pair('éœ¾', 'g')]
    seg_list = pseg.lcut(sentence)
    seg_list = [i for i in seg_list if i.flag not in stopwords_list]
    filtered_words_list = []
    for seg in seg_list:
        # print(seg)
        if len(seg.word) &lt;= 1:
            continue
        elif seg.flag == &quot;eng&quot;:
            if len(seg.word) &lt;= 2:
                continue
            else:
                filtered_words_list.append(seg.word)
        elif seg.flag.startswith(&quot;n&quot;):
            filtered_words_list.append(seg.word)
        elif seg.flag in [&quot;x&quot;, &quot;eng&quot;]:  # æ˜¯è‡ªå®šä¸€ä¸ªè¯è¯­æˆ–è€…æ˜¯è‹±æ–‡å•è¯
            filtered_words_list.append(seg.word)
    return filtered_words_list

for row in partition:
    sentence = re.sub(&quot;&lt;.*?&gt;&quot;, &quot;&quot;, row.sentence)    # æ›¿æ¢æ‰æ ‡ç­¾æ•°æ®
    words = cut_sentence(sentence)
    yield row.article_id, row.channel_id, words
</code></pre>
<h3 id="è®¡ç®—-tf-idf">è®¡ç®— TF-IDF</h3>
<p>TF:<br>
ktt.spark.sql(&quot;use article&quot;)<br>
article_dataframe = ktt.spark.sql(&quot;select * from article_data limit 20&quot;)<br>
words_df = article_dataframe.rdd.mapPartitions(segmentation).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;words&quot;])</p>
<pre><code>from pyspark.ml.feature import CountVectorizer
# æ€»è¯æ±‡çš„å¤§å°ï¼Œæ–‡æœ¬ä¸­å¿…é¡»å‡ºç°çš„æ¬¡æ•°
cv = CountVectorizer(inputCol=&quot;words&quot;, outputCol=&quot;countFeatures&quot;, vocabSize=200*10000, minDF=1.0)
# è®­ç»ƒè¯é¢‘ç»Ÿè®¡æ¨¡å‹
cv_model = cv.fit(words_df)
cv_model.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;)

# cv_model.vocabulary æŸ¥çœ‹ç»Ÿè®¡è¯è¡¨ï¼ˆç›¸å½“äºgroupbyç»“æœçš„ key,  ä½†ä¸åŒ…æ‹¬valueï¼‰
</code></pre>
<p>è®­ç»ƒTF-IDF:<br>
# è¯è¯­ä¸è¯é¢‘ç»Ÿè®¡<br>
from pyspark.ml.feature import CountVectorizerModel<br>
cv_model = CountVectorizerModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;)<br>
# å¾—å‡ºè¯é¢‘å‘é‡ç»“æœ<br>
cv_result = cv_model.transform(words_df)<br>
# è®­ç»ƒIDFæ¨¡å‹ (æŠŠ tfç»“æœä¼ è¿›å»ï¼Œå…¶å®è¯´æ˜¯ IDFæ¨¡å‹ï¼Œè®¡ç®—ç»“æœå¾—å‡ºçš„å°±æ˜¯ TF-IDF)<br>
from pyspark.ml.feature import IDF<br>
idf = IDF(inputCol=&quot;countFeatures&quot;, outputCol=&quot;idfFeatures&quot;)<br>
idfModel = idf.fit(cv_result)<br>
idfModel.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/IDF.model&quot;)</p>
<pre><code># idfModel.idf.toArray()[:20]    æŸ¥çœ‹é€†æ–‡æ¡£é¢‘ç‡çŸ©é˜µ
</code></pre>
<p>TF-IDFç»“æœæ•°æ®æ ¼å¼ï¼š<br>
åˆ—1ï¼Œ åˆ—...ï¼Œ åˆ— TF-IDF<br>
(1000,[804,1032],[6.349777077,7.0761797]) ã€‚ã€‚ã€‚<br>
ä½¿ç”¨TF-IDFæ¨¡å‹ï¼Œå–Top-Kä¸ªè¯ï¼š<br>
from pyspark.ml.feature import CountVectorizerModel<br>
cv_model = CountVectorizerModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model&quot;)<br>
from pyspark.ml.feature import IDFModel<br>
idf_model = IDFModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model&quot;)<br>
cv_result = cv_model.transform(words_df)<br>
tfidf_result = idf_model.transform(cv_result)</p>
<pre><code>def func(partition):            
	TOPK = 20
	for row in partition:
		# æ‰¾åˆ°ç´¢å¼•ä¸IDFå€¼å¹¶è¿›è¡Œæ’åº
		_ = list(zip(row.idfFeatures.indices, row.idfFeatures.values))   # [ (indexes,values)ï¼Œ  (indexes,values)]
		_ = sorted(_, key=lambda x: x[1], reverse=True)
		result = _[:TOPK]
		for word_index, tfidf in result:
			yield row.article_id, row.channel_id, int(word_index), round(float(tfidf), 4)
			# yieldè¿™å¥æ³¨å®šäº†è¿”å›ç»“æœæ ¼å¼ (å¤šå±‚forå¾ªç¯ yieldï¼Œ åŸæœ¬ä¸€è¡Œæ•°æ®æŒ‰æ¯ä¸ªå•è¯çˆ†ç‚¸å±•å¼€)
			# article_id,   channel_id,   word_index,   tfidf
			# 1				 100         40         15.5
			# 1				 100         14         10.3         
			# 1				 100         23         13.2
            
            
_keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;index&quot;, &quot;tfidf&quot;])
</code></pre>
<p>æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ï¼š æ„æˆ  è¯+TFIDFå€¼, è€Œä¸æ˜¯ç´¢å¼•+TFIDF<br>
cv_model.vocabulary ç»“æœæ˜¯æ‰€æœ‰å•è¯çš„åˆ—è¡¨ã€‚ ä¸Šé¢çš„ indexå°±æ˜¯å¯¹åº”è¿™ä¸ªåˆ—è¡¨çš„ç´¢å¼•<br>
æœ€ç»ˆæ„å»ºä¸€ä¸ªè¯å…¸+ç´¢å¼•è¡¨ï¼š<br>
index	word<br>
..		..	<br>
ç„¶åå°† ä¸»è¡¨ï¼ˆæ–‡ç« id,é¢‘é“id,ç´¢å¼•ï¼Œtfidfï¼‰ä¸ è¯å…¸è¡¨ï¼ˆindex+wordï¼‰ åˆå¹¶<br>
å¾—åˆ°  ï¼ˆæ–‡ç« id,é¢‘é“id, è¯ï¼Œ tfidfï¼‰</p>
<h3 id="è®¡ç®—-textrank">è®¡ç®— TextRank</h3>
<p>TextRankå’Œæ ¸å¿ƒå°±æ˜¯è®¾å®šä¸€ä¸ªå›ºå®šçª—å£æ¥æ»‘åŠ¨<br>
æŠŠæ¯ä¸ªçª—å£å†…çš„æ¯ä¸ªè¯ï¼Œ è®¾ä¸ºå­—å…¸çš„Key, valueå°±æ˜¯ä»–é™„è¿‘çš„nä¸ªè¯çš„åˆ—è¡¨<br>
ç„¶åæ¯ä¸ªè¯éƒ½è¿™æ ·åšï¼Œ é‡åˆ°ç›¸åŒçš„è¯å°±è¿½åŠ åˆ°å­—å…¸çš„value åˆ—è¡¨ä¸­<br>
# åˆ†è¯<br>
def textrank(partition):<br>
import os</p>
<pre><code>import jieba
import jieba.analyse
import jieba.posseg as pseg
import codecs

abspath = &quot;/root/words&quot;

# ç»“å·´åŠ è½½ç”¨æˆ·è¯å…¸
userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;)
jieba.load_userdict(userDict_path)

# åœç”¨è¯æ–‡æœ¬
stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;)

def get_stopwords_list():
    &quot;&quot;&quot;è¿”å›stopwordsåˆ—è¡¨&quot;&quot;&quot;
    stopwords_list = [i.strip()
                      for i in codecs.open(stopwords_path).readlines()]
    return stopwords_list

# æ‰€æœ‰çš„åœç”¨è¯åˆ—è¡¨
stopwords_list = get_stopwords_list()

class TextRank(jieba.analyse.TextRank):
    def __init__(self, window=20, word_min_len=2):
        super(TextRank, self).__init__()
        self.span = window  # çª—å£å¤§å°
        self.word_min_len = word_min_len  # å•è¯çš„æœ€å°é•¿åº¦
        # è¦ä¿ç•™çš„è¯æ€§ï¼Œæ ¹æ®jieba github ï¼Œå…·ä½“å‚è§https://github.com/baidu/lac
        self.pos_filt = frozenset(
            ('n', 'x', 'eng', 'f', 's', 't', 'nr', 'ns', 'nt', &quot;nw&quot;, &quot;nz&quot;, &quot;PER&quot;, &quot;LOC&quot;, &quot;ORG&quot;))

    def pairfilter(self, wp):
        &quot;&quot;&quot;è¿‡æ»¤æ¡ä»¶ï¼Œè¿”å›Trueæˆ–è€…False&quot;&quot;&quot;

        if wp.flag == &quot;eng&quot;:
            if len(wp.word) &lt;= 2:
                return False

        if wp.flag in self.pos_filt and len(wp.word.strip()) &gt;= self.word_min_len \
                and wp.word.lower() not in stopwords_list:
            return True
# TextRankè¿‡æ»¤çª—å£å¤§å°ä¸º5ï¼Œå•è¯æœ€å°ä¸º2
textrank_model = TextRank(window=5, word_min_len=2)
allowPOS = ('n', &quot;x&quot;, 'eng', 'nr', 'ns', 'nt', &quot;nw&quot;, &quot;nz&quot;, &quot;c&quot;)

for row in partition:
    tags = textrank_model.textrank(row.sentence, topK=20, withWeight=True, allowPOS=allowPOS, withFlag=False)
    for tag in tags:
        yield row.article_id, row.channel_id, tag[0], tag[1]

# è®¡ç®—textrank
textrank_keywords_df = article_dataframe.rdd.mapPartitions(textrank).toDF(
	[&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;textrank&quot;]
)

# textrank_keywords_df.write.insertInto(&quot;textrank_keywords_values&quot;)
</code></pre>
<p>textrankè¿è¡Œç»“æœå¦‚ä¸‹ï¼š<br>
hive&gt; select * from textrank_keywords_values limit 20;<br>
OK<br>
æ–‡ç« ID  channel   word    textrank<br>
98319   17      var     20.6079<br>
98323   17      var     7.4938<br>
98326   17      var     104.9128<br>
ç„¶åå’Œ tfidfä¸€æ · æ ¹æ® textrankå€¼ï¼Œ  å–TOP-Kä¸ªè¯</p>
<h3 id="è®¡ç®—-ä¸»é¢˜è¯-å’Œ-å…³é”®è¯">è®¡ç®— ä¸»é¢˜è¯ å’Œ å…³é”®è¯</h3>
<p>å…³é”®è¯ï¼šTEXTRANKè®¡ç®—å‡ºçš„ç»“æœTOPKä¸ªè¯ä»¥åŠæƒé‡<br>
ä¸»é¢˜è¯ï¼šTEXTRANKçš„TOPKè¯ ä¸ ITFDFè®¡ç®—çš„TOPKä¸ªè¯çš„äº¤é›†<br>
æ ¼å¼å¦‚ä¸‹ï¼š<br>
hive&gt; desc article_profile;<br>
OK<br>
article_id              int                     article_id<br>
channel_id              int                     channel_id<br>
keywords               map								 keywords<br>
topics						  array								topics</p>
<pre><code>hive&gt; select * from article_profile limit 1;    
# è¿™é‡ŒæŠŠç»“æœæŒ‰è¡Œæ’åˆ—å¼€æ–¹ä¾¿è§‚çœ‹
article_id			26
channel_id		   17            
å…³é”®è¯å­—å…¸		  {&quot;ç­–ç•¥&quot;:0.3973770571351729,&quot;jpg&quot;:0.9806348975390871,&quot;ç”¨æˆ·&quot;:1.2794959063944176,&quot;strong&quot;:1.6488457985625076,&quot;æ–‡ä»¶&quot;:0.28144603583387057,&quot;é€»è¾‘&quot;:0.45256526469610714,&quot;å½¢å¼&quot;:0.4123994242601279,&quot;å…¨è‡ª&quot;:0.9594604850547191,&quot;h2&quot;:0.6244481634710125,&quot;ç‰ˆæœ¬&quot;:0.44280276959510817,&quot;Adobe&quot;:0.8553618185108718,&quot;å®‰è£…&quot;:0.8305037437573172,&quot;æ£€æŸ¥æ›´æ–°&quot;:1.8088946300014435,&quot;äº§å“&quot;:0.774842382276899,&quot;ä¸‹è½½é¡µ&quot;:1.4256311032544344,&quot;è¿‡ç¨‹&quot;:0.19827163395829256,&quot;json&quot;:0.6423301791599972,&quot;æ–¹å¼&quot;:0.582762869780791,&quot;é€€å‡ºåº”ç”¨&quot;:1.2338671268242603,&quot;Setup&quot;:1.004399549339134} 

ä¸»é¢˜è¯åˆ—è¡¨				[&quot;Electron&quot;,&quot;å…¨è‡ªåŠ¨&quot;,&quot;äº§å“&quot;,&quot;ç‰ˆæœ¬å·&quot;,&quot;å®‰è£…åŒ…&quot;,&quot;æ£€æŸ¥æ›´æ–°&quot;,&quot;æ–¹æ¡ˆ&quot;,&quot;ç‰ˆæœ¬&quot;,&quot;é€€å‡ºåº”ç”¨&quot;,&quot;é€»è¾‘&quot;,&quot;å®‰è£…è¿‡ç¨‹&quot;,&quot;æ–¹å¼&quot;,&quot;å®šæ€§&quot;,&quot;æ–°ç‰ˆæœ¬&quot;,&quot;Setup&quot;,&quot;é™é»˜&quot;,&quot;ç”¨æˆ·&quot;]
</code></pre>
<h3 id="å¢é‡æ›´æ–°-ç¦»çº¿æ–‡ç« ç”»åƒ">å¢é‡æ›´æ–° ç¦»çº¿æ–‡ç« ç”»åƒ</h3>
<p>æ›´æ–°æµç¨‹ï¼š<br>
1ã€toutiao æ•°æ®åº“ä¸­ï¼Œnews_article_content ä¸news_article_basicâ€”&gt;æ›´æ–°åˆ°articleæ•°æ®åº“ä¸­article_dataè¡¨ï¼Œæ–¹ä¾¿æ“ä½œ<br>
2. ç¬¬ä¸€æ¬¡ï¼šæ‰€æœ‰æ›´æ–°ï¼Œåé¢å¢é‡æ¯å¤©çš„æ•°æ®æ›´æ–°26æ—¥ï¼š1ï¼š00<sub>2ï¼š00ï¼Œ2ï¼š00</sub>3ï¼š00ï¼Œå·¦é—­å³å¼€,ä¸€ä¸ªå°æ—¶æ›´æ–°ä¸€æ¬¡<br>
3ã€åˆšæ‰æ–°æ›´æ–°çš„æ–‡ç« ï¼Œé€šè¿‡å·²æœ‰çš„idfè®¡ç®—å‡ºtfidfå€¼ä»¥åŠhive çš„textrank_keywords_values<br>
4ã€æ›´æ–°hiveçš„article_profile</p>
<p>ç¦»çº¿æ›´æ–°æ–‡ç« ç”»åƒ ä»£ç ç»„è£…ï¼šPycharm<br>
æ³¨æ„åœ¨Pycharmä¸­è¿è¡Œè¦è®¾ç½®ç¯å¢ƒï¼š</p>
<pre><code>PYTHONUNBUFFERED=1
JAVA_HOME=/root/bigdata/jdk
SPARK_HOME=/root/bigdata/spark
HADOOP_HOME=/root/bigdata/hadoop
PYSPARK_PYTHON=/root/anaconda3/envs/reco_sys/bin/python
PYSPARK_DRIVER_PYTHON=/root/anaconda3/envs/reco_sys/bin/python
</code></pre>
<p>å…·ä½“ä»£ç å¦‚ä¸‹ï¼š<br>
import os<br>
import sys<br>
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(<strong>file</strong>)))<br>
sys.path.insert(0, os.path.join(BASE_DIR))<br>
from offline import SparkSessionBase<br>
from datetime import datetime<br>
from datetime import timedelta<br>
import pyspark.sql.functions as F<br>
import pyspark<br>
import gc</p>
<pre><code>class UpdateArticle(SparkSessionBase):
&quot;&quot;&quot;
æ›´æ–°æ–‡ç« ç”»åƒ
&quot;&quot;&quot;
SPARK_APP_NAME = &quot;updateArticle&quot;
ENABLE_HIVE_SUPPORT = True

SPARK_EXECUTOR_MEMORY = &quot;7g&quot;

def __init__(self):
    self.spark = self._create_spark_session()

    self.cv_path = &quot;hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model&quot;
    self.idf_path = &quot;hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model&quot;

def get_cv_model(self):
    # è¯è¯­ä¸è¯é¢‘ç»Ÿè®¡
    from pyspark.ml.feature import CountVectorizerModel
    cv_model = CountVectorizerModel.load(self.cv_path)
    return cv_model

def get_idf_model(self):
    from pyspark.ml.feature import IDFModel
    idf_model = IDFModel.load(self.idf_path)
    return idf_model

@staticmethod
def compute_keywords_tfidf_topk(words_df, cv_model, idf_model):
    &quot;&quot;&quot;ä¿å­˜tfidfå€¼é«˜çš„20ä¸ªå…³é”®è¯
    :param spark:
    :param words_df:
    :return:
    &quot;&quot;&quot;
    cv_result = cv_model.transform(words_df)
    tfidf_result = idf_model.transform(cv_result)
    # print(&quot;transform compelete&quot;)

    # å–TOP-Nçš„TFIDFå€¼é«˜çš„ç»“æœ
    def func(partition):
        TOPK = 20
        for row in partition:
            _ = list(zip(row.idfFeatures.indices, row.idfFeatures.values))
            _ = sorted(_, key=lambda x: x[1], reverse=True)
            result = _[:TOPK]
            #         words_index = [int(i[0]) for i in result]
            #         yield row.article_id, row.channel_id, words_index

            for word_index, tfidf in result:
                yield row.article_id, row.channel_id, int(word_index), round(float(tfidf), 4)

    _keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;index&quot;, &quot;tfidf&quot;])

    return _keywordsByTFIDF

def merge_article_data(self):
    &quot;&quot;&quot;
    åˆå¹¶ä¸šåŠ¡ä¸­å¢é‡æ›´æ–°çš„æ–‡ç« æ•°æ®
    :return:
    &quot;&quot;&quot;
    # è·å–æ–‡ç« ç›¸å…³æ•°æ®, æŒ‡å®šè¿‡å»ä¸€ä¸ªå°æ—¶æ•´ç‚¹åˆ°æ•´ç‚¹çš„æ›´æ–°æ•°æ®
    # å¦‚ï¼š26æ—¥ï¼š1ï¼š00~2ï¼š00ï¼Œ2ï¼š00~3ï¼š00ï¼Œå·¦é—­å³å¼€
    self.spark.sql(&quot;use toutiao&quot;)
    _yester = datetime.today().replace(minute=0, second=0, microsecond=0)
    start = datetime.strftime(_yester + timedelta(days=0, hours=-1, minutes=0), &quot;%Y-%m-%d %H:%M:%S&quot;)
    end = datetime.strftime(_yester, &quot;%Y-%m-%d %H:%M:%S&quot;)

    # åˆå¹¶åä¿ç•™ï¼šarticle_idã€channel_idã€channel_nameã€titleã€content
    # +----------+----------+--------------------+--------------------+
    # | article_id | channel_id | title | content |
    # +----------+----------+--------------------+--------------------+
    # | 141462 | 3 | test - 20190316 - 115123 | ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œå¿ƒæƒ…å¾ˆç¾ä¸½ï¼ï¼ï¼ |
    basic_content = self.spark.sql(
        &quot;select a.article_id, a.channel_id, a.title, b.content from news_article_basic a &quot;
        &quot;inner join news_article_content b on a.article_id=b.article_id where a.review_time &gt;= '{}' &quot;
        &quot;and a.review_time &lt; '{}' and a.status = 2&quot;.format(start, end))
    # å¢åŠ channelçš„åå­—ï¼Œåé¢ä¼šä½¿ç”¨
    basic_content.registerTempTable(&quot;temparticle&quot;)
    channel_basic_content = self.spark.sql(
        &quot;select t.*, n.channel_name from temparticle t left join news_channel n on t.channel_id=n.channel_id&quot;)

    # åˆ©ç”¨concat_wsæ–¹æ³•ï¼Œå°†å¤šåˆ—æ•°æ®åˆå¹¶ä¸ºä¸€ä¸ªé•¿æ–‡æœ¬å†…å®¹ï¼ˆé¢‘é“ï¼Œæ ‡é¢˜ä»¥åŠå†…å®¹åˆå¹¶ï¼‰
    self.spark.sql(&quot;use article&quot;)
    sentence_df = channel_basic_content.select(&quot;article_id&quot;, &quot;channel_id&quot;, &quot;channel_name&quot;, &quot;title&quot;, &quot;content&quot;, \
                                               F.concat_ws(
                                                   &quot;,&quot;,
                                                   channel_basic_content.channel_name,
                                                   channel_basic_content.title,
                                                   channel_basic_content.content
                                               ).alias(&quot;sentence&quot;)
                                               )
    del basic_content
    del channel_basic_content
    gc.collect()

    sentence_df.write.insertInto(&quot;article_data&quot;)
    return sentence_df

def generate_article_label(self, sentence_df):
    &quot;&quot;&quot;
    ç”Ÿæˆæ–‡ç« æ ‡ç­¾  tfidf, textrank
    :param sentence_df: å¢é‡çš„æ–‡ç« å†…å®¹
    :return:
    &quot;&quot;&quot;
    # è¿›è¡Œåˆ†è¯
    words_df = sentence_df.rdd.mapPartitions(segmentation).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;words&quot;])
    cv_model = self.get_cv_model()
    idf_model = self.get_idf_model()

    # 1ã€ä¿å­˜æ‰€æœ‰çš„è¯çš„idfçš„å€¼ï¼Œåˆ©ç”¨idfä¸­çš„è¯çš„æ ‡ç­¾ç´¢å¼•
    # å·¥å…·ä¸ä¸šåŠ¡éš”ç¦»
    _keywordsByTFIDF = UpdateArticle.compute_keywords_tfidf_topk(words_df, cv_model, idf_model)

    keywordsIndex = self.spark.sql(&quot;select keyword, index idx from idf_keywords_values&quot;)

    keywordsByTFIDF = _keywordsByTFIDF.join(keywordsIndex, keywordsIndex.idx == _keywordsByTFIDF.index).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;tfidf&quot;])

    keywordsByTFIDF.write.insertInto(&quot;tfidf_keywords_values&quot;)

    del cv_model
    del idf_model
    del words_df
    del _keywordsByTFIDF
    gc.collect()

    # è®¡ç®—textrank
    textrank_keywords_df = sentence_df.rdd.mapPartitions(textrank).toDF(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;textrank&quot;])
    textrank_keywords_df.write.insertInto(&quot;textrank_keywords_values&quot;)

    return textrank_keywords_df, keywordsIndex

def get_article_profile(self, textrank, keywordsIndex):
    &quot;&quot;&quot;
    æ–‡ç« ç”»åƒä¸»é¢˜è¯å»ºç«‹
    :param idf: æ‰€æœ‰è¯çš„idfå€¼
    :param textrank: æ¯ä¸ªæ–‡ç« çš„textrankå€¼
    :return: è¿”å›å»ºç«‹å·å¢é‡æ–‡ç« ç”»åƒ
    &quot;&quot;&quot;
    keywordsIndex = keywordsIndex.withColumnRenamed(&quot;keyword&quot;, &quot;keyword1&quot;)
    result = textrank.join(keywordsIndex, textrank.keyword == keywordsIndex.keyword1)

    # 1ã€å…³é”®è¯ï¼ˆè¯ï¼Œæƒé‡ï¼‰
    # è®¡ç®—å…³é”®è¯æƒé‡
    _articleKeywordsWeights = result.withColumn(&quot;weights&quot;, result.textrank * result.idf).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;weights&quot;])

    # åˆå¹¶å…³é”®è¯æƒé‡åˆ°å­—å…¸
    _articleKeywordsWeights.registerTempTable(&quot;temptable&quot;)
    articleKeywordsWeights = self.spark.sql(
        &quot;select article_id, min(channel_id) channel_id, collect_list(keyword) keyword_list, collect_list(weights) weights_list from temptable group by article_id&quot;)
    def _func(row):
        return row.article_id, row.channel_id, dict(zip(row.keyword_list, row.weights_list))
    articleKeywords = articleKeywordsWeights.rdd.map(_func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keywords&quot;])

    # 2ã€ä¸»é¢˜è¯
    # å°†tfidfå’Œtextrankå…±ç°çš„è¯ä½œä¸ºä¸»é¢˜è¯
    topic_sql = &quot;&quot;&quot;
            select t.article_id article_id2, collect_set(t.keyword) topics from tfidf_keywords_values t
            inner join 
            textrank_keywords_values r
            where t.keyword=r.keyword
            group by article_id2
            &quot;&quot;&quot;
    articleTopics = self.spark.sql(topic_sql)

    # 3ã€å°†ä¸»é¢˜è¯è¡¨å’Œå…³é”®è¯è¡¨è¿›è¡Œåˆå¹¶ï¼Œæ’å…¥è¡¨
    articleProfile = articleKeywords.join(articleTopics,
                                          articleKeywords.article_id == articleTopics.article_id2).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keywords&quot;, &quot;topics&quot;])
    articleProfile.write.insertInto(&quot;article_profile&quot;)

    del keywordsIndex
    del _articleKeywordsWeights
    del articleKeywords
    del articleTopics
    gc.collect()

    return articleProfile


if __name__ == '__main__':
ua = UpdateArticle()
sentence_df = ua.merge_article_data()
if sentence_df.rdd.collect():
    rank, idf = ua.generate_article_label(sentence_df)
    articleProfile = ua.get_article_profile(rank, idf)
</code></pre>
<p>ä½¿ç”¨å·¥å…·ï¼šSupervisor+Apscheduler<br>
# pip install APScheduler<br>
from apscheduler.schedulers.blocking import BlockingScheduler<br>
from apscheduler.executors.pool import ProcessPoolExecutor</p>
<pre><code>from scheduler.update import update_article_profile

# åˆ›å»ºschedulerï¼Œå¤šè¿›ç¨‹æ‰§è¡Œ
executors = {
	'default': ProcessPoolExecutor(3)
}
scheduler = BlockingScheduler(executors=executors)
# æ·»åŠ å®šæ—¶æ›´æ–°ä»»åŠ¡æ›´æ–°æ–‡ç« ç”»åƒ,æ¯éš”ä¸€å°æ—¶æ›´æ–°ï¼Œ triggerè¿˜æœ‰å…¶ä»–å®šæ—¶æ–¹å¼
scheduler.add_job(update_article_profile, trigger='interval', hours=1)
scheduler.start()
</code></pre>
<p>è‡ªå®šä¹‰Logger:<br>
import logging<br>
import logging.handlers<br>
import os</p>
<pre><code>logging_file_dir = '/root/logs/'
def create_logger():
	# ç¦»çº¿å¤„ç†æ›´æ–°æ‰“å°æ—¥å¿—
	 log_trace = logging.getLogger('offline')
     
	trace_file_handler = logging.FileHandler(
        os.path.join(logging_file_dir, 'offline.log')
    )
	 trace_file_handler.setFormatter(logging.Formatter('%(message)s'))
    
    log_trace.addHandler(trace_file_handler)
    log_trace.setLevel(logging.INFO)
</code></pre>
<p>supervisorç®¡ç†apscheduler:<br>
[program:offline]<br>
environment=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python<br>
command=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/scheduler/main.py<br>
directory=/root/toutiao_project/scheduler<br>
user=root<br>
autorestart=true<br>
redirect_stderr=true<br>
stdout_logfile=/root/logs/offlinesuper.log<br>
loglevel=info<br>
stopsignal=KILL<br>
stopasgroup=true<br>
killasgroup=true</p>
<h3 id="word2vecä¸æ–‡ç« ç›¸ä¼¼åº¦">Word2Vecä¸æ–‡ç« ç›¸ä¼¼åº¦</h3>
<pre><code>w2v.spark.sql(&quot;use article&quot;)
article = w2v.spark.sql(&quot;select * from article_data where channel_id=18 limit 2&quot;)
words_df = article.rdd.mapPartitions(segmentation).toDF(['article_id', 'channel_id', 'words'])
</code></pre>
<p>Spark Word2Vec APIä»‹ç»ï¼š<br>
æ¨¡å—ï¼šfrom pyspark.ml.feature import Word2Vec</p>
<pre><code>APIï¼šclass pyspark.ml.feature.Word2Vec(vectorSize=100, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, seed=None, inputCol=None, outputCol=None, windowSize=5, maxSentenceLength=1000)
å‚æ•°è¯´æ˜ï¼š
	vectorSize=100: è¯å‘é‡é•¿åº¦
	minCountï¼šè¿‡æ»¤æ¬¡æ•°å°äºé»˜è®¤5æ¬¡çš„è¯
	windowSize=5ï¼šè®­ç»ƒæ—¶å€™çš„çª—å£å¤§å°
	inputCol=Noneï¼šè¾“å…¥åˆ—å
	outputCol=Noneï¼šè¾“å‡ºåˆ—å
</code></pre>
<p>Spark Word2Vecè®­ç»ƒä¿å­˜æ¨¡å‹ï¼š<br>
new_word2Vec = Word2Vec(vectorSize=100, inputCol=&quot;words&quot;, outputCol=&quot;model&quot;, minCount=3)<br>
new_model = new_word2Vec.fit(words_df)<br>
new_model.save(&quot;hdfs://hadoop-master:9000/headlines/models/test.word2vec&quot;)<br>
ä¸Šä¼ å†å²æ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼š<br>
hadoop dfs -put ./word2vec_model /headlines/models/</p>
<h3 id="å¢é‡æ›´æ–°-æ–‡ç« å‘é‡è®¡ç®—">å¢é‡æ›´æ–°-æ–‡ç« å‘é‡è®¡ç®—</h3>
<p>æœ‰äº†è¯å‘é‡ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°ä¸€ç¯‡æ–‡ç« çš„å‘é‡äº†ï¼Œä¸ºäº†åé¢å¿«é€Ÿä½¿ç”¨æ–‡ç« çš„å‘é‡ï¼Œæˆ‘ä»¬ä¼šå°†æ¯ä¸ªé¢‘é“æ‰€æœ‰çš„æ–‡ç« å‘é‡ä¿å­˜èµ·æ¥ã€‚</p>
<p>ç›®çš„ï¼šä¿å­˜æ‰€æœ‰å†å²è®­ç»ƒçš„æ–‡ç« å‘é‡<br>
æ­¥éª¤ï¼š<br>
1ã€åŠ è½½æŸä¸ªé¢‘é“æ¨¡å‹ï¼Œå¾—åˆ°æ¯ä¸ªè¯çš„å‘é‡<br>
2ã€è·å–é¢‘é“çš„æ–‡ç« ç”»åƒï¼Œå¾—åˆ°æ–‡ç« ç”»åƒçš„å…³é”®è¯(æ¥ç€ä¹‹å‰å¢é‡æ›´æ–°çš„æ–‡ç« article_profile)<br>
3ã€è®¡ç®—å¾—åˆ°æ–‡ç« æ¯ä¸ªè¯çš„å‘é‡<br>
4ã€è®¡ç®—å¾—åˆ°æ–‡ç« çš„å¹³å‡è¯å‘é‡å³æ–‡ç« çš„å‘é‡<br>
åŠ è½½æŸä¸ªé¢‘é“æ¨¡å‹ï¼Œå¾—åˆ°æ¯ä¸ªè¯çš„å‘é‡<br>
from pyspark.ml.feature import Word2VecModel<br>
channel_id = 18<br>
channel = &quot;python&quot;<br>
wv_model = Word2VecModel.load(<br>
&quot;hdfs://hadoop-master:9000/headlines/models/word2vec_model/channel_%d_%s.word2vec&quot; % (channel_id, channel))<br>
vectors = wv_model.getVectors()<br>
è·å–æ–°å¢çš„æ–‡ç« ç”»åƒï¼Œå¾—åˆ°æ–‡ç« ç”»åƒçš„å…³é”®è¯ï¼š<br>
# é€‰å‡ºæ–°å¢çš„æ–‡ç« çš„ç”»åƒåšæµ‹è¯•ï¼Œä¸ŠèŠ‚è®¡ç®—çš„ç”»åƒä¸­æœ‰ä¸åŒé¢‘é“çš„ï¼Œæˆ‘ä»¬é€‰å–Pythoné¢‘é“çš„è¿›è¡Œè®¡ç®—æµ‹è¯•<br>
# æ–°å¢çš„æ–‡ç« ç”»åƒè·å–éƒ¨åˆ†<br>
profile = w2v.spark.sql(&quot;select * from article_profile where channel_id=18 limit 10&quot;)<br>
# profile = articleProfile.filter('channel_id = {}'.format(channel_id))</p>
<pre><code>profile.registerTempTable(&quot;incremental&quot;)
articleKeywordsWeights = w2v.spark.sql(
                &quot;select article_id, channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight&quot;)
_article_profile = articleKeywordsWeights.join(vectors, vectors.word==articleKeywordsWeights.keyword, &quot;inner&quot;)
</code></pre>
<p>è®¡ç®—å¾—åˆ°æ–‡ç« çš„å¹³å‡è¯å‘é‡å³æ–‡ç« çš„å‘é‡<br>
def avg(row):<br>
x = 0<br>
for v in row.vectors:<br>
x += v<br>
#  å°†å¹³å‡å‘é‡ä½œä¸ºarticleçš„å‘é‡<br>
return row.article_id, row.channel_id, x / len(row.vectors)</p>
<pre><code>articleKeywordVectors.registerTempTable(&quot;tempTable&quot;)
articleVector = w2v.spark.sql(
	&quot;select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from tempTable group by article_id&quot;).rdd \
    # åˆ†ç»„ä¹‹åï¼Œ æ±‚mapçš„å¹³å‡ä¹‹å‰ï¼Œ ç»“æœæ˜¯  artile_id, channel_id, vector_list # vector_list æ˜¯äºŒç»´æ•°ç»„ã€‚
    .map(avg).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;articleVector&quot;])
    # æ±‚mapçš„å¹³å‡ä¹‹åç»“æœæ˜¯ article_id, channel_id, article_vector # article_vector ä»£è¡¨æ–‡ç« å‘é‡
</code></pre>
<h3 id="æ–‡ç« ç›¸ä¼¼åº¦è®¡ç®—">æ–‡ç« ç›¸ä¼¼åº¦è®¡ç®—</h3>
<h4 id="å­˜åœ¨çš„é—®é¢˜ä»¥è¿›è¡ŒæŸé¢‘é“å…¨é‡æ‰€æœ‰çš„ä¸¤ä¸¤ç›¸ä¼¼åº¦è®¡ç®—-ä½†æ˜¯äº‹å®å½“æ–‡ç« é‡è¾¾åˆ°åƒä¸‡çº§åˆ«æˆ–è€…ä¸Šäº¿çº§åˆ«ç‰¹å¾ä¹Ÿä¼šä¸Šäº¿çº§åˆ«è®¡ç®—é‡å°±ä¼šå¾ˆå¤§-ä»¥ä¸‹æœ‰ä¸¤ç§ç±»å‹è§£å†³æ–¹æ¡ˆ">å­˜åœ¨çš„é—®é¢˜ï¼šä»¥è¿›è¡ŒæŸé¢‘é“å…¨é‡æ‰€æœ‰çš„ä¸¤ä¸¤ç›¸ä¼¼åº¦è®¡ç®—ã€‚ä½†æ˜¯äº‹å®å½“æ–‡ç« é‡è¾¾åˆ°åƒä¸‡çº§åˆ«æˆ–è€…ä¸Šäº¿çº§åˆ«ï¼Œç‰¹å¾ä¹Ÿä¼šä¸Šäº¿çº§åˆ«ï¼Œè®¡ç®—é‡å°±ä¼šå¾ˆå¤§ã€‚ä»¥ä¸‹æœ‰ä¸¤ç§ç±»å‹è§£å†³æ–¹æ¡ˆï¼š</h4>
<ol>
<li>æ¯ä¸ªé¢‘é“çš„æ–‡ç« å…ˆè¿›è¡Œèšç±»ï¼ˆç¼ºç‚¹ï¼Œï¼ˆåˆ†æˆå‡ ä¸ªç°‡ï¼‰ä¹Ÿæ˜¯ä¸ªè¶…å‚æ•°ï¼‰</li>
<li>å±€éƒ¨æ•æ„Ÿå“ˆå¸ŒLSH(Locality Sensitive Hashing)<br>
åŸºæœ¬æ€æƒ³1ï¼šLSHç®—æ³•åŸºäºä¸€ä¸ªå‡è®¾ï¼Œå¦‚æœä¸¤ä¸ªæ–‡æœ¬åœ¨åŸæœ‰çš„æ•°æ®ç©ºé—´æ˜¯ç›¸ä¼¼çš„ï¼Œé‚£ä¹ˆåˆ†åˆ«ç»è¿‡å“ˆå¸Œå‡½æ•°è½¬æ¢ä»¥åçš„å®ƒä»¬ä¹Ÿå…·æœ‰å¾ˆé«˜çš„ç›¸ä¼¼åº¦<br>
åŸºæœ¬æ€æƒ³2: ç»å¸¸ä½¿ç”¨çš„å“ˆå¸Œå‡½æ•°ï¼Œå†²çªæ€»æ˜¯éš¾ä»¥é¿å…ã€‚LSHå´ä¾èµ–äºå†²çªï¼Œåœ¨è§£å†³NNS(Nearest neighbor search )æ—¶ï¼Œæˆ‘ä»¬æœŸæœ›ï¼š<br>
ç¦»å¾—è¶Šè¿‘çš„å¯¹è±¡ï¼Œå‘ç”Ÿå†²çªçš„æ¦‚ç‡è¶Šé«˜<br>
ç¦»å¾—è¶Šè¿œçš„å¯¹è±¡ï¼Œå‘ç”Ÿå†²çªçš„æ¦‚ç‡è¶Šä½</li>
</ol>
<h4 id="å±€éƒ¨æ•æ„Ÿå“ˆå¸Œlshlocality-sensitive-hashing-lshè¿‡ç¨‹">å±€éƒ¨æ•æ„Ÿå“ˆå¸ŒLSH(Locality Sensitive Hashing) LSHè¿‡ç¨‹ï¼š</h4>
<p>mini hashing(ç•¥)	<br>
Random Projectionï¼ˆç‰¹å¾å‹ç¼©ï¼‰ï¼š<br>
Random Projectionæ˜¯ä¸€ç§éšæœºç®—æ³•.éšæœºæŠ•å½±çš„ç®—æ³•æœ‰å¾ˆå¤šï¼Œå¦‚PCAã€Gaussian random projection - é«˜æ–¯éšæœºæŠ•å½±ã€‚<br>
éšæœºæ¡¶æŠ•å½±æ˜¯ç”¨äºæ¬§å‡ é‡Œå¾·è·ç¦»çš„ LSH familyã€‚å…¶LSH familyå°†xç‰¹å¾å‘é‡æ˜ å°„åˆ°éšæœºå•ä½çŸ¢é‡vï¼Œå¹¶å°†æ˜ å°„ç»“æœåˆ†ä¸ºå“ˆå¸Œæ¡¶ä¸­ã€‚å“ˆå¸Œè¡¨ä¸­çš„æ¯ä¸ªä½ç½®è¡¨ç¤ºä¸€ä¸ªå“ˆå¸Œæ¡¶ã€‚	<br>
ä½¿å¾—ï¼š<br>
ç¦»å¾—è¶Šè¿‘çš„å¯¹è±¡ï¼Œå‘ç”Ÿå†²çªçš„æ¦‚ç‡è¶Šé«˜<br>
ç¦»å¾—è¶Šè¿œçš„å¯¹è±¡ï¼Œå‘ç”Ÿå†²çªçš„æ¦‚ç‡è¶Šä½</p>
<h4 id="ä»£ç å®ç°">ä»£ç å®ç°ï¼š</h4>
<p>è¯»å–æ•°æ®ï¼Œè¿›è¡Œç±»å‹å¤„ç†(æ•°ç»„è½¬æ¢ç±»å‹ä¸ºVector)ï¼š<br>
from pyspark.ml.linalg import Vectors<br>
# é€‰å–éƒ¨åˆ†æ•°æ®åšæµ‹è¯•<br>
article_vector = w2v.spark.sql(&quot;select article_id, articlevector from article_vector where channel_id=18 limit 10&quot;)<br>
train = articlevector.select(['article_id', 'articleVector'])</p>
<pre><code>def _array_to_vector(row):
	return row.article_id, Vectors.dense(row.articleVector)

train = train.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])
</code></pre>
<p>ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆBRPè¿›è¡ŒFITï¼‰ï¼š<br>
å‡½æ•°å‚æ•°è¯´æ˜ï¼š<br>
class pyspark.ml.feature.BucketedRandomProjectionLSH(inputCol=None, outputCol=None, seed=None, numHashTables=1, bucketLength=None)<br>
inputCol=Noneï¼šè¾“å…¥ç‰¹å¾åˆ—<br>
outputCol=Noneï¼šè¾“å‡ºç‰¹å¾åˆ—<br>
numHashTables=1ï¼šå“ˆå¸Œè¡¨æ•°é‡ï¼Œå‡ ä¸ªhash functionå¯¹æ•°æ®è¿›è¡Œhashæ“ä½œ<br>
bucketLength=Noneï¼šæ¡¶çš„æ•°é‡ï¼Œå€¼è¶Šå¤§ç›¸åŒæ•°æ®è¿›å…¥åˆ°åŒä¸€ä¸ªæ¡¶çš„æ¦‚ç‡è¶Šé«˜<br>
method:<br>
# è®¡ç®—df1æ¯ä¸ªæ–‡ç« ç›¸ä¼¼çš„df2æ•°æ®é›†çš„æ•°æ®<br>
approxSimilarityJoin(df1, df2, 2.0, distCol='EuclideanDistance')  # è½¬ä¸ºå‘é‡</p>
<pre><code># ä»£ç è°ƒç”¨ï¼š
from pyspark.ml.feature import BucketedRandomProjectionLSH

brp = BucketedRandomProjectionLSH(inputCol='articleVector', outputCol='hashes', numHashTables=4.0, bucketLength=10.0)
model = brp.fit(æ—§æ–‡ç« å‘é‡)
</code></pre>
<p>è®¡ç®—ç›¸ä¼¼çš„æ–‡ç« ä»¥åŠç›¸ä¼¼åº¦<br>
# è®¡ç®—æ–‡ç« å’Œæ–‡ç« ä¹‹é—´çš„ç›¸ä¼¼åº¦<br>
similar = model.approxSimilarityJoin(æ–°å¢æ–‡ç« å‘é‡, æ–°å¢æ–‡ç« å‘é‡, 2.0, distCol='Similarity')  # è¾“å‡ºåˆ—å<br>
similar.sort(['EuclideanDistance']).show()<br>
è®¡ç®—ç»“æœï¼š<br>
datasetA(æ–°å¢),    datasetBï¼ˆæ—§çš„ï¼‰,     Similarity<br>
[2,[æ–‡ç« å‘é‡]]		[5,[æ–‡ç« å‘é‡]]			0.0051<br>
[1,[æ–‡ç« å‘é‡]]		[3,[æ–‡ç« å‘é‡]]			0.0054<br>
[2,[æ–‡ç« å‘é‡]]		[8,[æ–‡ç« å‘é‡]]			0.0053<br>
[1,[æ–‡ç« å‘é‡]]		[4,[æ–‡ç« å‘é‡]]			0.0052<br>
[2,[æ–‡ç« å‘é‡]]		[7,[æ–‡ç« å‘é‡]]			0.0055<br>
...</p>
<h3 id="æ–‡ç« ç›¸ä¼¼åº¦å­˜å‚¨-hbase">æ–‡ç« ç›¸ä¼¼åº¦å­˜å‚¨ HBase</h3>
<h4 id="å­˜å‚¨ç›®æ ‡å­˜å‚¨-æ–‡ç« ç›¸ä¼¼æ–‡ç« -ç›¸ä¼¼åº¦">å­˜å‚¨ç›®æ ‡ï¼šå­˜å‚¨ æ–‡ç« ï¼Œç›¸ä¼¼æ–‡ç« ï¼Œ ç›¸ä¼¼åº¦</h4>
<p>è°ƒç”¨foreachPartitionï¼š<br>
foreachPartitionä¸åŒäºmapå’ŒmapPartitionã€‚<br>
æ— è¿”å›ç»“æœï¼Œä¸»è¦ç”¨äºç¦»çº¿åˆ†æä¹‹åçš„æ•°æ®ï¼ˆæ•°æ®åº“å­˜å‚¨ç­‰ï¼‰è½åœ°<br>
å¦‚æœæƒ³è¦è¿”å›æ–°çš„ä¸€ä¸ªæ•°æ®DFï¼Œå°±ä½¿ç”¨mapåè€…ã€‚<br>
æˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä¸ªHBaseå­˜å‚¨æ–‡ç« ç›¸ä¼¼åº¦çš„è¡¨ï¼š<br>
create 'article_similar', 'similar'</p>
<pre><code># å­˜å‚¨æ ¼å¼å¦‚ä¸‹ï¼š
		 è¡¨           row_key  column_family   value
	put 'article_similar', '1', 	'similar:1',    0.2
	put 'article_similar', '1', 	'similar:2',    0.34
</code></pre>
<p>HBase å¼€å¯å¤±è´¥å¯èƒ½çš„åŸå› çš„ï¼š</p>
<ol>
<li>
<p>æ—¶é—´æœªåŒæ­¥çš„è§£å†³åŠæ³•ï¼š<br>
ntpdate 0.cn.pool.ntp.org<br>
æˆ–<br>
ntpdate ntp1.aliyun.com</p>
</li>
<li>
<p>thriftæœåŠ¡æœªå¼€å¯çš„è§£å†³åŠæ³•ï¼š<br>
hbase-daemon.sh start thrift<br>
happybaseä»£ç å®ç°ï¼š<br>
def save_hbase(partition):<br>
import happybase<br>
pool = happybase.ConnectionPool(size=3, host='hadoop-master')</p>
<p>with pool.connection() as conn:<br>
# å»ºè®®è¡¨çš„è¿æ¥<br>
table = conn.table('article_similar')<br>
for row in partition:<br>
if row.datasetA.article_id == row.datasetB.article_id:<br>
pass<br>
else:<br>
table.put(str(row.datasetA.article_id).encode(),<br>
{&quot;similar:{}&quot;.format(row.datasetB.article_id).encode(): b'%0.4f' % (row.EuclideanDistance)})<br>
# æ‰‹åŠ¨å…³é—­æ‰€æœ‰çš„è¿æ¥<br>
conn.close()</p>
<p>similar.foreachPartition(save_hbase)</p>
</li>
</ol>
<h3 id="æ–‡ç« ç›¸ä¼¼åº¦å¢é‡æ›´æ–°ä»£ç æ•´ç†">æ–‡ç« ç›¸ä¼¼åº¦å¢é‡æ›´æ–°ä»£ç æ•´ç†</h3>
<pre><code>def compute_article_similar(self, articleProfile):
    &quot;&quot;&quot;
    è®¡ç®—å¢é‡æ–‡ç« ä¸å†å²æ–‡ç« çš„ç›¸ä¼¼åº¦ word2vec
    :return:
    &quot;&quot;&quot;
    # å¾—åˆ°è¦æ›´æ–°çš„æ–°æ–‡ç« é€šé“ç±»åˆ«(ä¸é‡‡ç”¨)
    # all_channel = set(articleProfile.rdd.map(lambda x: x.channel_id).collect())
    def avg(row):
        x = 0
        for v in row.vectors:
            x += v
        #  å°†å¹³å‡å‘é‡ä½œä¸ºarticleçš„å‘é‡
        return row.article_id, row.channel_id, x / len(row.vectors)

    for channel_id, channel_name in CHANNEL_INFO.items():

        profile = articleProfile.filter('channel_id = {}'.format(channel_id))
        wv_model = Word2VecModel.load(
            &quot;hdfs://hadoop-master:9000/headlines/models/channel_%d_%s.word2vec&quot; % (channel_id, channel_name))
        vectors = wv_model.getVectors()

        # è®¡ç®—å‘é‡
        profile.registerTempTable(&quot;incremental&quot;)
        articleKeywordsWeights = ua.spark.sql(
            &quot;select article_id, channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight where channel_id=%d&quot; % channel_id)

        articleKeywordsWeightsAndVectors = articleKeywordsWeights.join(vectors,
                                                        vectors.word == articleKeywordsWeights.keyword, &quot;inner&quot;)
        articleKeywordVectors = articleKeywordsWeightsAndVectors.rdd.map(
            lambda r: (r.article_id, r.channel_id, r.keyword, r.weight * r.vector)).toDF(
            [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;weightingVector&quot;])

        articleKeywordVectors.registerTempTable(&quot;tempTable&quot;)
        articleVector = self.spark.sql(
            &quot;select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from tempTable group by article_id&quot;).rdd.map(
            avg).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;articleVector&quot;])

        # å†™å…¥æ•°æ®åº“
        def toArray(row):
            return row.article_id, row.channel_id, [float(i) for i in row.articleVector.toArray()]
        articleVector = articleVector.rdd.map(toArray).toDF(['article_id', 'channel_id', 'articleVector'])
        articleVector.write.insertInto(&quot;article_vector&quot;)

        import gc
        del wv_model
        del vectors
        del articleKeywordsWeights
        del articleKeywordsWeightsAndVectors
        del articleKeywordVectors
        gc.collect()

        # å¾—åˆ°å†å²æ•°æ®, è½¬æ¢æˆå›ºå®šæ ¼å¼ä½¿ç”¨LSHè¿›è¡Œæ±‚ç›¸ä¼¼
        train = self.spark.sql(&quot;select * from article_vector where channel_id=%d&quot; % channel_id)

        def _array_to_vector(row):
            return row.article_id, Vectors.dense(row.articleVector)
        train = train.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])
        test = articleVector.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])

        brp = BucketedRandomProjectionLSH(inputCol='articleVector', outputCol='hashes', seed=12345,
                                          bucketLength=1.0)
        model = brp.fit(train)
        similar = model.approxSimilarityJoin(test, train, 2.0, distCol='EuclideanDistance')

        def save_hbase(partition):
            import happybase
            for row in partition:
                pool = happybase.ConnectionPool(size=3, host='hadoop-master')
                # article_similar article_id similar:article_id sim
                with pool.connection() as conn:
                    table = connection.table(&quot;article_similar&quot;)
                    for row in partition:
                        if row.datasetA.article_id == row.datasetB.article_id:
                            pass
                        else:
                            table.put(str(row.datasetA.article_id).encode(),
                                      {b&quot;similar:%d&quot; % row.datasetB.article_id: b&quot;%0.4f&quot; % row.EuclideanDistance})
                    conn.close()
        similar.foreachPartition(save_hbase)
</code></pre>
<p>æ·»åŠ å‡½æ•°åˆ°ä¸»å‡½æ•°ä¸­æ–‡ä»¶ä¸­ï¼Œä¿®æ”¹updateæ›´æ–°ä»£ç ï¼š<br>
ua = UpdateArticle()<br>
sentence_df = ua.merge_article_data()<br>
if sentence_df.rdd.collect():<br>
rank, idf = ua.generate_article_label(sentence_df)<br>
articleProfile = ua.get_article_profile(rank, idf)<br>
ua.compute_article_similar(articleProfile)</p>
<h1 id="ç”¨æˆ·ç”»åƒæ„å»ºä¸æ›´æ–°">ç”¨æˆ·ç”»åƒæ„å»ºä¸æ›´æ–°</h1>
<h3 id="ç»„æˆæˆåˆ†">ç»„æˆæˆåˆ†</h3>
<p>ç”¨æˆ·åŸºæœ¬ä¿¡æ¯+ç”¨æˆ·è¡Œä¸º(å†å²+æ–°å¢)<br>
ç”¨æˆ·è¡Œä¸ºåŒ…æ‹¬ï¼š<br>
hive&gt; select * from user_action limit 1;<br>
OK<br>
2019-03-05 10:19:40		0		{&quot;action&quot;:&quot;exposure&quot;,&quot;userId&quot;:&quot;2&quot;,&quot;articleId&quot;:&quot;[16000, 44371, 16421, 16181, 17454]&quot;,&quot;algorithmCombine&quot;:&quot;C2&quot;} 2019-03-05<br>
æˆ‘ä»¬éœ€è¦å¯¹ç”¨æˆ·è¡Œä¸ºï¼ˆå­—å…¸ï¼‰æ•°æ®æ ¼å¼å¹³é“ºå¤„ç†<br>
user_id	action_time	article_id	share	click	collected	exposure	read_time</p>
<h3 id="æ­¥éª¤">æ­¥éª¤ï¼š</h3>
<p>1ã€åˆ›å»ºHIVEåŸºæœ¬æ•°æ®è¡¨<br>
2ã€è¯»å–å›ºå®šæ—¶é—´å†…çš„ç”¨æˆ·è¡Œä¸ºæ—¥å¿—<br>
3ã€è¿›è¡Œç”¨æˆ·æ—¥å¿—æ•°æ®å¤„ç†<br>
4ã€å­˜å‚¨åˆ°user_article_basicè¡¨ä¸­</p>
<h4 id="åˆ›å»ºhiveåŸºæœ¬æ•°æ®è¡¨">åˆ›å»ºHIVEåŸºæœ¬æ•°æ®è¡¨</h4>
<pre><code>create table user_article_basic(
user_id BIGINT comment &quot;userID&quot;,
action_time STRING comment &quot;user actions time&quot;,
article_id BIGINT comment &quot;articleid&quot;,
channel_id INT comment &quot;channel_id&quot;,
shared BOOLEAN comment &quot;is shared&quot;,
clicked BOOLEAN comment &quot;is clicked&quot;,
collected BOOLEAN comment &quot;is collected&quot;,
exposure BOOLEAN comment &quot;is exposured&quot;,
read_time STRING comment &quot;reading time&quot;)
COMMENT &quot;user_article_basic&quot;
CLUSTERED by (user_id) into 2 buckets
STORED as textfile
LOCATION '/user/hive/warehouse/profile.db/user_article_basic';
</code></pre>
<h4 id="è¯»å–å¢é‡ç”¨æˆ·è¡Œä¸ºæ•°æ®-å›ºå®šæ—¶é—´å†…çš„ç”¨æˆ·è¡Œä¸ºæ—¥å¿—">è¯»å–å¢é‡ç”¨æˆ·è¡Œä¸ºæ•°æ®-å›ºå®šæ—¶é—´å†…çš„ç”¨æˆ·è¡Œä¸ºæ—¥å¿—</h4>
<p>å…³è”å†å²æ—¥æœŸæ–‡ä»¶<br>
# åœ¨è¿›è¡Œæ—¥å¿—ä¿¡æ¯çš„å¤„ç†ä¹‹å‰ï¼Œå…ˆå°†æˆ‘ä»¬ä¹‹å‰å»ºç«‹çš„user_actionè¡¨ä¹‹é—´è¿›è¡Œæ‰€æœ‰æ—¥æœŸå…³è”ï¼Œspark hiveä¸ä¼šè‡ªåŠ¨å…³è”<br>
import pandas as pd<br>
from datetime import datetime</p>
<pre><code>def datelist(beginDate, endDate):
	date_list=[datetime.strftime(x,'%Y-%m-%d') for x in list(pd.date_range(start=beginDate, end=endDate))]
	return date_list

dl = datelist(&quot;2019-03-05&quot;, time.strftime(&quot;%Y-%m-%d&quot;, time.localtime()))

fs = pyhdfs.HdfsClient(hosts='hadoop-master:50070')
for d in dl:
	try:
		_localions = '/user/hive/warehouse/profile.db/user_action/' + d
		if fs.exists(_localions):
			uup.spark.sql(&quot;alter table user_action add partition (dt='%s') location '%s'&quot; % (d, _localions))
	except Exception as e:
		# å·²ç»å…³è”è¿‡çš„å¼‚å¸¸å¿½ç•¥,partitionä¸hdfsæ–‡ä»¶ä¸ç›´æ¥å…³è”
		pass
sqlDF = uup.spark.sql(
	&quot;select actionTime, readTime, channelId, param.articleId, param.algorithmCombine, param.action, param.userId from user_action where dt={}&quot;.format(time_str)
)
</code></pre>
<h4 id="åŸå§‹æ•°æ®æ ¼å¼ä¸ç›®æ ‡æ•°æ®æ ¼å¼">åŸå§‹æ•°æ®æ ¼å¼ä¸ç›®æ ‡æ•°æ®æ ¼å¼</h4>
<p>åŸå§‹æ•°æ®æ ¼å¼ï¼šï¼ˆè¡Œä¸ºå‚æ•°éƒ½ç®—åœ¨ actionåˆ—å†…ï¼‰<br>
actionTime	readTime	channelID	articleId	ç®—æ³•åç§°	action		userId<br>
123						1						exposure	1<br>
321						1						click		1<br>
ç›®æ ‡æ•°æ®æ ¼å¼ï¼ˆè¡Œä¸ºå‚æ•°1æ‹†4ï¼Œactionåˆ—è¢«çˆ†ç‚¸ä¸º4åˆ—ï¼Œå‡ä¸ºboolç±»å‹ï¼‰<br>
user_id	action_time	article_id	shared	clicked	collected	expore	read_time<br>
1					1			false	false	false		true<br>
1					2			false	true	false		true</p>
<h4 id="è¿›è¡Œç”¨æˆ·æ—¥å¿—æ•°æ®æ ¼å¼å¤„ç†">è¿›è¡Œç”¨æˆ·æ—¥å¿—æ•°æ®æ ¼å¼å¤„ç†</h4>
<pre><code>if sqlDF.collect():
def _compute(row):
    # è¿›è¡Œåˆ¤æ–­è¡Œä¸ºç±»å‹
    _list = []
    if row.action == &quot;exposure&quot;:
        for article_id in eval(row.articleId):
            _list.append(
                [row.userId, row.actionTime, article_id, row.channelId, False, False, False, True, row.readTime])
        return _list
    else:
        class Temp(object):
            shared = False
            clicked = False
            collected = False
            read_time = &quot;&quot;

        _tp = Temp()
        if row.action == &quot;share&quot;:
            _tp.shared = True
        elif row.action == &quot;click&quot;:
            _tp.clicked = True
        elif row.action == &quot;collect&quot;:
            _tp.collected = True
        elif row.action == &quot;read&quot;:
            _tp.clicked = True
        else:
            pass
        _list.append(
            [row.userId, row.actionTime, int(row.articleId), row.channelId, _tp.shared, _tp.clicked, _tp.collected,
             True,
             row.readTime])
        return _list
# è¿›è¡Œå¤„ç†
# æŸ¥è¯¢å†…å®¹ï¼Œå°†åŸå§‹æ—¥å¿—è¡¨æ•°æ®è¿›è¡Œå¤„ç†
_res = sqlDF.rdd.flatMap(_compute)
data = _res.toDF([&quot;user_id&quot;, &quot;action_time&quot;,&quot;article_id&quot;, &quot;channel_id&quot;, &quot;shared&quot;, &quot;clicked&quot;, &quot;collected&quot;, &quot;exposure&quot;, &quot;read_time&quot;])
</code></pre>
<h4 id="å°†ä¸Šè¿°ç›®æ ‡æ ¼å¼çš„æ•°æ®æŒ‰ç…§-userid-å’Œ-articleid-åˆ†ç»„">å°†ä¸Šè¿°ç›®æ ‡æ ¼å¼çš„æ•°æ®æŒ‰ç…§ userid å’Œ articleid åˆ†ç»„</h4>
<p>å…ˆåˆå¹¶å†å²æ•°æ®ï¼Œå­˜å‚¨åˆ°user-article-basicè¡¨ä¸­<br>
# åˆå¹¶å†å²æ•°æ®ï¼Œæ’å…¥è¡¨ä¸­<br>
old = uup.spark.sql(&quot;select * from user_article_basic&quot;)<br>
# ç”±äºåˆå¹¶çš„ç»“æœä¸­ä¸æ˜¯å¯¹äºuser_idå’Œarticle_idå”¯ä¸€çš„ï¼Œä¸€ä¸ªç”¨æˆ·ä¼šå¯¹æ–‡ç« å¤šç§æ“ä½œ<br>
new_old = old.unionAll(data)<br>
HIVEç›®å‰æ”¯æŒhiveç»ˆç«¯æ“ä½œACIDï¼Œä¸æ”¯æŒpythonçš„pysparkåŸå­æ€§æ“ä½œï¼Œå¹¶ä¸”å¼€å¯é…ç½®ä¸­å¼€å¯åŸå­æ€§ç›¸å…³é…ç½®ä¹Ÿä¸è¡Œã€‚<br>
new_old.registerTempTable(&quot;temptable&quot;)<br>
# æŒ‰ç…§ç”¨æˆ·ï¼Œæ–‡ç« åˆ†ç»„å­˜æ”¾è¿›å»<br>
uup.spark.sql(<br>
&quot;insert overwrite table user_article_basic select user_id, max(action_time) as action_time, &quot;<br>
&quot;article_id, max(channel_id) as channel_id, max(shared) as shared, max(clicked) as clicked, &quot;<br>
&quot;max(collected) as collected, max(exposure) as exposure, max(read_time) as read_time from temptable &quot;<br>
&quot;group by user_id, article_id&quot;)</p>
<h3 id="ç”¨æˆ·ç”»åƒæ ‡ç­¾æƒé‡è®¡ç®—">ç”¨æˆ·ç”»åƒæ ‡ç­¾æƒé‡è®¡ç®—</h3>
<h4 id="å¦‚ä½•å­˜å‚¨">å¦‚ä½•å­˜å‚¨</h4>
<p>ç”¨æˆ·ç”»åƒï¼Œä½œä¸ºç‰¹å¾æä¾›ç»™ä¸€äº›ç®—æ³•æ’åºï¼Œæ–¹ä¾¿ä¸å¿«é€Ÿè¯»å–ä½¿ç”¨<br>
é€‰æ‹©å­˜å‚¨åœ¨Hbaseå½“ä¸­ã€‚<br>
ç„¶åç”¨ Hive å¤–è¡¨å…³è” hbase<br>
å¦‚æœç¦»çº¿åˆ†æä¹Ÿæƒ³è¦ä½¿ç”¨æˆ‘ä»¬å¯ä»¥å»ºç«‹HIVEåˆ°Hbaseçš„å¤–éƒ¨è¡¨ã€‚</p>
<h4 id="hbaseè¡¨è®¾è®¡">HBaseè¡¨è®¾è®¡</h4>
<pre><code>		table_name		column1 column2  column3 
create 'user_profile', 'basic','partial','env'

					row_key   column_family					value
put 'user_profile', 'user:2', 'partial:{channel_id}:{topic}': weights
put 'user_profile', 'user:2', 'basic:{info}': value
put 'user_profile', 'user:2', 'env:{info}': value
</code></pre>
<h4 id="hiveè¡¨è®¾è®¡">Hiveè¡¨è®¾è®¡</h4>
<pre><code>create external table user_profile_hbase(
user_id STRING comment &quot;userID&quot;,
information map&lt;string, DOUBLE&gt; comment &quot;user basic information&quot;,
article_partial map&lt;string, DOUBLE&gt; comment &quot;article partial&quot;,
env map&lt;string, INT&gt; comment &quot;user env&quot;)
COMMENT &quot;user profile table&quot;
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,basic:,partial:,env:&quot;)
TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;user_profile&quot;);
</code></pre>
<h4 id="spark-sqlå…³è”è¡¨è¯»å–é—®é¢˜">Spark SQLå…³è”è¡¨è¯»å–é—®é¢˜</h4>
<p>åˆ›å»ºå…³è”è¡¨ä¹‹åï¼Œç¦»çº¿è¯»å–è¡¨å†…å®¹éœ€è¦ä¸€äº›ä¾èµ–åŒ…ã€‚è§£å†³åŠæ³•ï¼š<br>
æ‹·è´/root/bigdata/hbase/lib/ä¸‹é¢hbase-<em>.jar åˆ° /root/bigdata/spark/jars/ç›®å½•ä¸‹<br>
æ‹·è´/root/bigdata/hive/lib/h</em>.jar åˆ° /root/bigdata/spark/jars/ç›®å½•ä¸‹<br>
ä¸Šè¿°æ“ä½œä¸‰å°è™šæ‹Ÿæœºéƒ½æ‰§è¡Œä¸€éã€‚</p>
<h4 id="ç”¨æˆ·ç”»åƒé¢‘é“å…³é”®è¯è·å–ä¸æƒé‡è®¡ç®—">ç”¨æˆ·ç”»åƒé¢‘é“å…³é”®è¯è·å–ä¸æƒé‡è®¡ç®—</h4>
<p>ç›®æ ‡ï¼šè·å–ç”¨æˆ·1~25é¢‘é“(ä¸åŒ…æ‹¬æ¨èé¢‘é“)çš„å…³é”®è¯ï¼Œå¹¶è®¡ç®—æƒé‡<br>
1ã€è¯»å–user-article-basicè¡¨ï¼Œåˆå¹¶è¡Œä¸ºè¡¨ä¸æ–‡ç« ç”»åƒä¸­çš„ä¸»é¢˜è¯<br>
2ã€è¿›è¡Œç”¨æˆ·æƒé‡è®¡ç®—å…¬å¼ã€åŒæ—¶è½åœ°å­˜å‚¨<br>
# è·å–åŸºæœ¬ç”¨æˆ·è¡Œä¸ºä¿¡æ¯ï¼Œç„¶åè¿›è¡Œæ–‡ç« ç”»åƒçš„ä¸»é¢˜è¯åˆå¹¶<br>
uup.spark.sql(&quot;use profile&quot;)<br>
# å–å‡ºæ—¥å¿—ä¸­çš„channel_id<br>
user_article_ = uup.spark.sql(&quot;select * from user_article_basic&quot;).drop('channel_id')<br>
uup.spark.sql('use article')<br>
article_label = uup.spark.sql(&quot;select article_id, channel_id, topics from article_profile&quot;)<br>
# åˆå¹¶ä½¿ç”¨æ–‡ç« ä¸­æ­£ç¡®çš„channel_id<br>
click_article_res = user_article_.join(article_label, how='left', on=['article_id'])<br>
å°†å…³é”®è¯å­—æ®µçš„åˆ—è¡¨çˆ†ç‚¸<br>
import pyspark.sql.functions as F<br>
click_article_res = click_article_res.withColumn('topic', F.explode('topics')).drop('topics')<br>
çˆ†ç‚¸åæ ¼å¼å¦‚ä¸‹ï¼š<br>
user_id article_id topic ...<br>
1     1			 python<br>
1     1			 golang<br>
1     1			 linux<br>
...    ...		  ...</p>
<h4 id="ç”¨æˆ·ç”»åƒä¹‹æ ‡ç­¾æƒé‡ç®—æ³•">ç”¨æˆ·ç”»åƒä¹‹æ ‡ç­¾æƒé‡ç®—æ³•</h4>
<p>ç”¨æˆ·æ ‡ç­¾æƒé‡ =( è¡Œä¸ºç±»å‹æƒé‡ä¹‹å’Œ) Ã— æ—¶é—´è¡°å‡<br>
è¡Œä¸ºç±»å‹æƒé‡ çš„ åˆ†å€¼ çš„ç¡®å®šéœ€è¦æ•´ä½“åå•†<br>
è¡Œä¸º					åˆ†å€¼<br>
é˜…è¯»æ—¶é—´&lt;1000ms		  1<br>
é˜…è¯»æ—¶é—´&gt;=1000ms	  2<br>
æ”¶è—					2<br>
åˆ†äº«					3<br>
ç‚¹å‡»					5<br>
æ—¶é—´è¡°å‡: 1/(log(t)+1) ,tä¸ºæ—¶é—´å‘ç”Ÿæ—¶é—´è·ç¦»å½“å‰æ—¶é—´çš„å¤§å°ã€‚<br>
# è®¡ç®—æ¯ä¸ªç”¨æˆ·å¯¹æ¯ç¯‡æ–‡ç« çš„æ ‡ç­¾çš„æƒé‡<br>
def compute_weights(rowpartition):<br>
&quot;&quot;&quot;å¤„ç†æ¯ä¸ªç”¨æˆ·å¯¹æ–‡ç« çš„ç‚¹å‡»æ•°æ®<br>
&quot;&quot;&quot;<br>
weightsOfaction = {<br>
&quot;read_min&quot;: 1,<br>
&quot;read_middle&quot;: 2,<br>
&quot;collect&quot;: 2,<br>
&quot;share&quot;: 3,<br>
&quot;click&quot;: 5<br>
}</p>
<pre><code>import happybase
from datetime import datetime
import numpy as np
#  ç”¨äºè¯»å–hbaseç¼“å­˜ç»“æœé…ç½®
pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090)

# è¯»å–æ–‡ç« çš„æ ‡ç­¾æ•°æ®
# è®¡ç®—æƒé‡å€¼
# æ—¶é—´é—´éš”
for row in rowpartition:
    t = datetime.now() - datetime.strptime(row.action_time, '%Y-%m-%d %H:%M:%S')
    # æ—¶é—´è¡°å‡ç³»æ•°
    time_exp = 1 / (np.log(t.days + 1) + 1)

    if row.read_time == '':
        r_t = 0
    else:
        r_t = int(row.read_time)
    # æµè§ˆæ—¶é—´åˆ†æ•°
    is_read = weightsOfaction['read_middle'] if r_t &gt; 1000 else weightsOfaction['read_min']

    # æ¯ä¸ªè¯çš„æƒé‡åˆ†æ•°
    weigths = time_exp * (
                row.shared * weightsOfaction['share'] + row.collected * weightsOfaction['collect'] + row.
                clicked * weightsOfaction['click'] + is_read)

#        with pool.connection() as conn:
#            table = conn.table('user_profile')
#            table.put('user:{}'.format(row.user_id).encode(),
#                      {'partial:{}:{}'.format(row.channel_id, row.topic).encode(): json.dumps(
#                          weigths).encode()})
#            conn.close()

click_article_res.foreachPartition(compute_weights)
</code></pre>
<p>è½åœ°Hbaseä¸­ä¹‹åï¼Œåœ¨HBASEä¸­æŸ¥è¯¢ï¼Œhappybaseæˆ–è€…hbaseç»ˆç«¯<br>
import happybase<br>
# ç”¨äºè¯»å–hbaseç¼“å­˜ç»“æœé…ç½®<br>
pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090)</p>
<pre><code>with pool.connection() as conn:
	table = conn.table('user_profile')
	# è·å–æ¯ä¸ªé”® å¯¹åº”çš„æ‰€æœ‰åˆ—çš„ç»“æœ
	data = table.row(b'user:2', columns=[b'partial'])
	conn.close()

# ç­‰ä»·äº  hbase(main):015:0&gt; get 'user_profile', 'user:2'
</code></pre>
<h3 id="åŸºç¡€ä¿¡æ¯ç”»åƒæ›´æ–°">åŸºç¡€ä¿¡æ¯ç”»åƒæ›´æ–°</h3>
<pre><code>def update_user_info(self):
    &quot;&quot;&quot;
    æ›´æ–°ç”¨æˆ·çš„åŸºç¡€ä¿¡æ¯ç”»åƒ
    :return:
    &quot;&quot;&quot;
    self.spark.sql(&quot;use toutiao&quot;)

    user_basic = self.spark.sql(&quot;select user_id, gender, birthday from user_profile&quot;)

    # æ›´æ–°ç”¨æˆ·åŸºç¡€ä¿¡æ¯
    def _udapte_user_basic(partition):
        &quot;&quot;&quot;æ›´æ–°ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
        &quot;&quot;&quot;
        import happybase
        #  ç”¨äºè¯»å–hbaseç¼“å­˜ç»“æœé…ç½®
        pool = happybase.ConnectionPool(size=10, host='172.17.0.134', port=9090)
        for row in partition:

            from datetime import date
            age = 0
            if row.birthday != 'null':
                born = datetime.strptime(row.birthday, '%Y-%m-%d')
                today = date.today()
                age = today.year - born.year - ((today.month, today.day) &lt; (born.month, born.day))

            with pool.connection() as conn:
                table = conn.table('user_profile')
                table.put('user:{}'.format(row.user_id).encode(),
                          {'basic:gender'.encode(): json.dumps(row.gender).encode()})
                table.put('user:{}'.format(row.user_id).encode(),
                          {'basic:birthday'.encode(): json.dumps(age).encode()})
                conn.close()

    user_basic.foreachPartition(_udapte_user_basic)
    logger.info(
        &quot;{} INFO completely update infomation of basic&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
        
# hbase(main):016:0&gt; get 'user_profile', 'user:2'
</code></pre>
<h3 id="ç”¨æˆ·ç”»åƒå¢é‡æ›´æ–°å®šæ—¶å¼€å¯">ç”¨æˆ·ç”»åƒå¢é‡æ›´æ–°å®šæ—¶å¼€å¯:</h3>
<p>ç”¨æˆ·ç”»åƒå¢é‡æ›´æ–°ä»£ç æ•´ç†<br>
æ·»åŠ å®šæ—¶ä»»åŠ¡ä»¥åŠè¿›ç¨‹ç®¡ç†<br>
from offline.update_user import UpdateUserProfile</p>
<pre><code>def update_user_profile():
	&quot;&quot;&quot;
	æ›´æ–°ç”¨æˆ·ç”»åƒ
	&quot;&quot;&quot;
	uup = UpdateUserProfile()
	if uup.update_user_action_basic():
		uup.update_user_label()
		uup.update_user_info()
scheduler.add_job(update_user_profile, trigger='interval', hours=2)</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => Spark mapPartition]]></title>
        <id>https://cythonlin.github.io/post/py-greater-spark-mappartition/</id>
        <link href="https://cythonlin.github.io/post/py-greater-spark-mappartition/">
        </link>
        <updated>2020-09-29T04:20:11.000Z</updated>
        <content type="html"><![CDATA[<h1 id="æ­£è§£">æ­£è§£</h1>
<p>map()ï¼šæ¯æ¬¡å¤„ç†ä¸€æ¡æ•°æ®</p>
<p>mapPartition()ï¼šæ¯æ¬¡å¤„ç†ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®ï¼Œè¿™ä¸ªåˆ†åŒºçš„æ•°æ®å¤„ç†å®Œåï¼ŒåŸRDDä¸­åˆ†åŒºçš„æ•°æ®æ‰èƒ½é‡Šæ”¾ï¼Œå¯èƒ½å¯¼è‡´OOM</p>
<p>å½“å†…å­˜ç©ºé—´è¾ƒå¤§çš„æ—¶å€™å»ºè®®ä½¿ç”¨mapPartition()ï¼Œä»¥æé«˜å¤„ç†æ•ˆç‡</p>
<h1 id="åˆ†éš”">åˆ†éš”</h1>
<p>ä¸‹é¢æ‰€æœ‰æ˜¯è‡ªå·±çš„æ€è·¯ï¼Œæ¨åˆ°æœ€åç»™è‡ªå·±æ¨è’™äº†ï¼Œå¯ä»¥æ è¿‡</p>
<h1 id="æ­£æ–‡å®éªŒå¯å¿½ç•¥ç›´å¥”ç»“æœ">æ­£æ–‡å®éªŒï¼ˆå¯å¿½ç•¥ï¼Œç›´å¥”ç»“æœï¼‰</h1>
<p>ä¸ºäº†æ–¹ä¾¿ï¼Œç”¨æŠŠç»„è£…çš„æ•°æ®ç±»å‹çŒå…¥ map æ¥æ¨¡æ‹Ÿ mapPartitions<br>
åŠŸèƒ½æ˜¯æ¨¡æ‹Ÿè®¡ç®— tf-idf</p>
<pre><code>class Article:
    '''æ–‡ç« ç±»'''
    def __init__(self,id, indexex, tfidfs):
        self.id = id
        self.indexex = indexex   # æ–‡ç« åˆ†è¯åçš„æ‰€æœ‰è¯ç´¢å¼•åˆ—è¡¨
        self.tfidfs = tfidfs     # æ¯ä¸ªè¯å¯¹åº”çš„TF-IDFå€¼ åˆ—è¡¨

def f(partition):
    for row in partition:   # row ä»£è¡¨æ¯ä¸ªæ–‡ç« 
        # row.indexex ä»£è¡¨ æ–‡ç« åˆ†è¯åçš„æ‰€æœ‰è¯ç´¢å¼•åˆ—è¡¨
        # row.tfidfs  ä»£è¡¨ æ¯ä¸ªè¯å¯¹åº”çš„TF-IDFå€¼ åˆ—è¡¨
        word_list = list(zip(row.indexex, row.tfidfs))  
        
        for index, tfidf in word_list:    # éå† &quot;æ¯ä¸ª&quot;è¯è¯­ çš„ indexä¸tfidf
            ########### è¿™é‡Œ yield æ˜¯é‡ç‚¹ ###########
            yield f'æ–‡ç« {row.id}', index, tfidf
            

c = map(f, 
    [   #  &lt;-ä¸ºäº†æ¨¡æ‹Ÿåˆ†åŒºï¼Œè¿™ä¸€å±‚çš„åˆ—è¡¨ä»£è¡¨partition
        [   # &lt;- è¿™ä¸€å±‚æ¨¡æ‹Ÿçš„æ˜¯æ¯ä¸ªåˆ†åŒºé‡Œé¢çš„æ–‡ç« åˆ—è¡¨
            Article(0, [1,2],[0.1,0.4] ),   # &lt;-æ–‡ç« 0
            Article(1, [3,4],[3.4,3.7] )    # &lt;-æ–‡ç« 1
        ] 
    ] 
)
######################## æ‰§è¡Œ ########################
for x in c:              # è§£zip
    print(list(x))       # è§£yield
</code></pre>
<h1 id="ç»“æœ">ç»“æœï¼š</h1>
<p>å¦‚æœä½¿ç”¨ return å…³é”®è¯ï¼Œå¾—å‡ºçš„æœ€ç»ˆæ‰“å°ç»“æœ: ï¼ˆä¸æ»¡è¶³ï¼‰</p>
<pre><code>['æ–‡ç« 0', 1, 0.1]          
</code></pre>
<p>å¦‚æœä½¿ç”¨ yield å…³é”®è¯ï¼Œå¾—å‡ºçš„æœ€ç»ˆæ‰“å°ç»“æœ:  ï¼ˆæ»¡è¶³ï¼‰</p>
<pre><code>[('æ–‡ç« 0', 1, 0.1), ('æ–‡ç« 0', 2, 0.4), ('æ–‡ç« 1', 3, 3.4), ('æ–‡ç« 1', 4, 3.7)]
</code></pre>
<h1 id="è¿™é‡Œå°±å‡ºç°äº†ä¸€ä¸ªé—®é¢˜">è¿™é‡Œå°±å‡ºç°äº†ä¸€ä¸ªé—®é¢˜ï¼š</h1>
<p>æ­£å¸¸ç”¨æ³•éƒ½æ˜¯ç”¨ returnï¼Œæ—¶å¸¸ç”¨ lambdaï¼ˆlambdaé»˜è®¤ä¹Ÿæ˜¯éšå¼ returnã€‚ï¼‰<br>
æ˜¯ä½•åŸå› è®©æˆ‘ä»¬ä¸å¾—ä¸ç”¨ yield?<br>
ä¸€ç‚¹ä¸€ç‚¹å¾€ä¸‹æ¨ï¼š</p>
<pre><code>map: æ ¸å¿ƒæ˜¯ &quot;æŒ‰å•ä¸ªæ•°æ®æ˜ å°„&quot;
mapPartitionï¼š æ ¸å¿ƒæ˜¯&quot;æŠŠæ•°æ®åˆ†ç»„ï¼ŒæŒ‰ç»„æ˜ å°„&quot;
    æŒ‰ç»„æ˜ å°„æ˜¯æ²¡é”™ï¼Œä½†æˆ‘ä»¬çš„ç›®çš„æ˜¯æƒ³æ“ä½œç»„å†…çš„æ¯æ¡æ•°æ®ã€‚
    æ‰€ä»¥æˆ‘ä»¬å¿…é¡»éœ€è¦æ¯æ¬¡å¯¹ç»„å†…æ•°æ® forå¾ªç¯éå†å‡ºæ¥å•ç‹¬å¤„ç†ã€‚         ç„¶å è¿”å›å›å»ã€‚
</code></pre>
<p>é‚£æˆ‘ä»¬å…ˆç”¨æ­£å¸¸çš„ return è¿”å›è¯•è¯•ï¼š</p>
<pre><code>def(partation):
    for x in partition:
        return x.name, x.age
</code></pre>
<p>ä¹Ÿè®¸çœ‹åˆ°è¿™é‡Œä½ è§‰å¾—æ²¡ä»€ä¹ˆé—®é¢˜ã€‚ã€‚ã€‚<br>
ä½†æ˜¯ä¸è¦å¿˜äº†æœ€åŸºç¡€çš„å†…å®¹ï¼Œ return æ˜¯ç›´æ¥è·³å‡º for å¾ªç¯å’Œå‡½æ•°çš„ã€‚<br>
å†æ¬¡å¼ºè°ƒï¼ŒmapPartitionæ˜¯æŒ‰ç»„æ˜ å°„ï¼Œæ‰€ä»¥ä»”ç»†çœ‹ä¸Šé¢ä»£ç :<br>
æœ€ç»ˆçš„mapPartitionæ˜¯æŒ‰ç»„æ˜ å°„ç»“æœå°±æ˜¯ï¼š<br>
æ¯ç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„é›†åˆ ï¼ˆå› ä¸ºforè¢«returnäº†ï¼Œæ¯ç»„çš„å‡½æ•°ä¹Ÿè¢«returnäº†ï¼‰<br>
è§£å†³è¿™ç§é—®é¢˜ï¼Œæœ‰ä¸¤ç§æ–¹å¼ï¼š</p>
<ol>
<li>
<p>æœ€ç®€å•å°†æºä»£ç forå¾ªç¯å†…éƒ¨çš„ return æ”¹ä¸º yield</p>
</li>
<li>
<p>æ–°å»ºä¸´æ—¶åˆ—è¡¨è¿‡æ¸¡ï¼Œreturnæ”¾åœ¨forå¤–é¢ï¼Œå¦‚ä¸‹æ¡ˆä¾‹ï¼š</p>
<p>def f(partition):<br>
_ = []<br>
for x in partition:<br>
_.append(x)<br>
return _<br>
b = [[1,2,3], [4,5,6]]<br>
c = map(f,b)<br>
print(list(c))</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RS => æ¨èç³»ç»Ÿï¼ˆä¸€ï¼‰ç¯å¢ƒé…ç½®+æ•°æ®æ”¶é›†]]></title>
        <id>https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-yi-huan-jing-pei-zhi-shu-ju-shou-ji/</id>
        <link href="https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-yi-huan-jing-pei-zhi-shu-ju-shou-ji/">
        </link>
        <updated>2020-09-29T04:17:17.000Z</updated>
        <content type="html"><![CDATA[<h1 id="pythonç¯å¢ƒ">Pythonç¯å¢ƒ</h1>
<p>minicondaåˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼š<br>
conda create -n reco_sys python=3.6.7<br>
æ¿€æ´»/é€€å‡º è™šæ‹Ÿç¯å¢ƒï¼š<br>
conda activate spider-venv<br>
conda deactivate<br>
2ä¸ªslaveéœ€è¦å®‰è£…ä¾èµ–ï¼š<br>
yum -y install gcc<br>
å®‰è£…æ¨¡å—ï¼š<br>
pip install redis supervisor apscheduler chardet jieba jupyter numpy pandas scipy scikit-learn pyspark findspark happybase pyhdfs -i https://pypi.douban.com/simple</p>
<h1 id="å¤§æ•°æ®ç¯å¢ƒ">å¤§æ•°æ®ç¯å¢ƒ</h1>
<h3 id="lambdaç¯å¢ƒå¯åŠ¨è„šæœ¬é…ç½®">Lambdaç¯å¢ƒå¯åŠ¨è„šæœ¬é…ç½®</h3>
<p>ç¦ç”¨+å…³é—­é˜²ç«å¢™ï¼š<br>
systemctl disable firewalld.service<br>
systemctl stop firewalld.service<br>
åŒæ­¥ç³»ç»Ÿæ—¶é—´ï¼ˆä¸è¿™æ ·åš hbaseå¯èƒ½å¯åŠ¨å¤±è´¥ï¼‰ï¼ˆ3å°éƒ½è¿è¡Œå‘½ä»¤ï¼‰ï¼š<br>
yum install ntpdate -y<br>
ntpdate 0.cn.pool.ntp.org</p>
<p>åˆ›å»ºç»¼åˆå¯åŠ¨è„šæœ¬<br>
vi start.sh<br>
/root/bigdata/hadoop/sbin/start-all.sh<br>
start-hbase.sh<br>
/root/bigdata/spark/sbin/start-all.sh</p>
<pre><code>vi stop.sh
    /root/bigdata/spark/sbin/stop-all.sh 
    stop-hbase.sh
    /root/bigdata/hadoop/sbin/stop-all.sh
</code></pre>
<p>å¼€å¯<br>
/root/scripts/start.sh<br>
åœæ­¢<br>
/root/scripts/stop.sh</p>
<h3 id="uiåœ°å€æŸ¥logä¹Ÿå¯">UIåœ°å€ï¼ˆæŸ¥logä¹Ÿå¯ï¼‰ï¼š</h3>
<p>Hadoop UI:<br>
http://192.168.19.137:8088<br>
YARN UIï¼š<br>
http://192.168.19.137:50070<br>
Hbase UI:<br>
http://192.168.19.137:16010<br>
Spark UIï¼š<br>
http://192.168.19.137:8080/</p>
<h3 id="æ•°æ®åº“è¿è¡Œ">æ•°æ®åº“è¿è¡Œï¼š</h3>
<p>MySQLå¯åŠ¨ï¼š<br>
systemctl start docker<br>
docker start mysql<br>
Hiveå…ƒæ•°æ®æœåŠ¡å¼€å¯:<br>
nohup hive --service metastore &amp;</p>
<h3 id="sparkç›¸å…³é—®é¢˜">sparkç›¸å…³é—®é¢˜</h3>
<p>spark on yarn å¯åŠ¨å·¨æ…¢ï¼Œè§£å†³åŠæ³•ï¼š<br>
hadoop fs -mkdir -p /system/spark-lib<br>
hadoop fs -put /root/bigdata/spark-2.2.2-bin-hadoop2.7/jars/* /system/spark-lib<br>
hadoop fs -chmod -R 755 /system/spark-lib<br>
cd $SPARK_HOME/conf<br>
cp spark-defaults.conf.template spark-defaults.conf<br>
vi spark-defaults.conf<br>
spark.yarn.jars    hdfs://192.168.19.137:9000//system/spark-lib/*</p>
<pre><code>å¦‚æœç”¨çš„æ˜¯ jupyter, è®°å¾—é‡å¯ jupyteræœåŠ¡
jupyter notebook --allow-root --ip 0.0.0.0
</code></pre>
<h3 id="hdfs-hiveç›¸å…³é—®é¢˜">HDFS-Hiveç›¸å…³é—®é¢˜</h3>
<p>å†…éƒ¨è¡¨ä¿®æ”¹ä¸ºå¤–éƒ¨è¡¨ï¼š<br>
alter table user_profile SET TBLPROPERTIES('EXTERNAL'='TRUE');</p>
<h1 id="æ•°æ®æ„æˆ">æ•°æ®æ„æˆ</h1>
<h3 id="æ•°æ®åº“1-toutiao">æ•°æ®åº“1ï¼š toutiao</h3>
<pre><code>news_article_basic   # æ–‡ç« æ ‡é¢˜
news_article_content  # æ–‡ç« å†…å®¹  
news_channel       # æ–‡ç« é¢‘é“ï¼ˆç±»åˆ«ï¼‰
user_basic			  # ç”¨æˆ·ä¸šåŠ¡æ•°æ® 
user_profile       # ç”¨æˆ·ç§äººä¿¡æ¯
</code></pre>
<h3 id="æ•°æ®åº“2-profile">æ•°æ®åº“2ï¼š profile</h3>
<pre><code>user_action			  # ç”¨æˆ·è¡Œä¸ºæ—¥å¿—
</code></pre>
<h3 id="æ•°æ®åº“2-article">æ•°æ®åº“2ï¼š article</h3>
<pre><code>article_data       # åˆå¹¶æ–‡ç« æ ‡é¢˜+å†…å®¹+é¢‘é“åçš„å­˜å‚¨ç»“æœ
...
</code></pre>
<h1 id="æ•°æ®è¿ç§»sqoop">æ•°æ®è¿ç§»ï¼ˆSqoopï¼‰</h1>
<h3 id="è€—æ—¶">è€—æ—¶</h3>
<pre><code>4000w (10+g): 30+ min
</code></pre>
<h3 id="æ£€æµ‹sqoopæ˜¯å¦èƒ½è¿é€šmysql-å¹¶åˆ—å‡ºmysqlæ‰€æœ‰æ•°æ®åº“">æ£€æµ‹Sqoopæ˜¯å¦èƒ½è¿é€šMySQL, å¹¶åˆ—å‡ºMySQLæ‰€æœ‰æ•°æ®åº“:</h3>
<pre><code>sqoop list-databases --connect jdbc:mysql://192.168.19.137:3306/ --username root -P
</code></pre>
<h3 id="å…¨é‡å¯¼å…¥æ–¹å¼ä¸æ¨è">å…¨é‡å¯¼å…¥æ–¹å¼ï¼ˆä¸æ¨èï¼‰ï¼š</h3>
<pre><code>#!/bin/bash
array=(user_profile user_basic news_user_channel news_channel user_follow user_blacklist user_search news_collection news_article_basic news_article_content news_read news_article_statistic user_material)

for table_name in ${array[@]};
do
    sqoop import \
        --connect jdbc:mysql://192.168.19.137/toutiao \
        --username root \
        --password password \
        --table $table_name \
        --m 5 \
        --hive-home /root/bigdata/hive \
        --hive-import \
        --create-hive-table  \
        --hive-drop-import-delims \
        --warehouse-dir /user/hive/warehouse/toutiao.db \
        --hive-table toutiao.$table_name
done
</code></pre>
<h3 id="å¢é‡å¯¼å…¥æ–¹å¼">å¢é‡å¯¼å…¥æ–¹å¼</h3>
<p>æ–¹å¼1ï¼š é€šè¿‡æŒ‡å®šé€’å¢çš„å­—æ®µæ¥å¯¼å…¥ï¼ˆä¸æ¨èï¼Œå› ä¸ºæŸäº›å­—æ®µçš„å€¼ä¸æ˜¯é€’å¢çš„ï¼‰<br>
appendï¼šå³é€šè¿‡æŒ‡å®šä¸€ä¸ªé€’å¢çš„åˆ—ï¼Œå¦‚ï¼š--incremental append --check-column num_iid --last-value 0<br>
æ–¹å¼2ï¼šincrementalï¼š æ—¶é—´æˆ³<br>
--incremental lastmodified <br>
--check-column column <br>
--merge-key key <br>
--last-value '2012-02-01 11:0:00'</p>
<pre><code>å°±æ˜¯åªå¯¼å…¥check-columnçš„åˆ—æ¯”'2012-02-01 11:0:00'æ›´å¤§ï¼ˆæ–°ï¼‰çš„æ•°æ®,æŒ‰ç…§keyåˆå¹¶
</code></pre>
<h3 id="å¢é‡å¯¼å…¥ä½ç½®">å¢é‡å¯¼å…¥ä½ç½®</h3>
<ol>
<li>ç›´æ¥sqoopå¯¼å…¥åˆ°hive(â€“incremental lastmodified æ¨¡å¼ä¸æ”¯æŒå¯¼å…¥ Hive )</li>
<li>sqoopå¯¼å…¥åˆ°hdfsï¼Œç„¶åå»ºç«‹hiveè¡¨å…³è”<br>
--target-dir /user/hive/warehouse/toutiao.db/</li>
</ol>
<h3 id="sqoopå¯¼å…¥åˆ°hdfshiveè¡¨å…³è”åˆ°hdfså¡«å‘">sqoopå¯¼å…¥åˆ°hdfsï¼Œhiveè¡¨å…³è”åˆ°hdfså¡«å‘</h3>
<p>ç°è±¡ï¼š<br>
æŸ¥å‡ºä¸€å † null<br>
åŸå› ï¼š<br>
sqoop å¯¼å‡ºçš„ hdfs åˆ†ç‰‡æ•°æ®ï¼Œéƒ½æ˜¯ä½¿ç”¨é€—å· , åˆ†å‰²çš„ã€‚<br>
ç”±äº hive é»˜è®¤çš„åˆ†éš”ç¬¦æ˜¯ /u0001ï¼ˆCtrl+Aï¼‰,ä¸ºäº†å¹³æ»‘è¿ç§»ï¼Œéœ€è¦åœ¨åˆ›å»ºè¡¨æ ¼æ—¶æŒ‡å®šæ•°æ®çš„åˆ†å‰²ç¬¦å·ã€‚<br>
è§£å†³æ–¹å¼ï¼š<br>
å¯¼å…¥æ•°æ®åˆ°hiveä¸­ï¼Œéœ€è¦åœ¨åˆ›å»ºHIVEè¡¨åŠ å…¥ row format delimited fields terminated by ','</p>
<h3 id="sqoopè¿ç§»åˆ°hdfsåhiveåˆ›å»ºè¡¨å¹¶æŒ‡å®šå…³è”ä½ç½®å®ä¾‹">sqoopè¿ç§»åˆ°hdfsåï¼ŒHiveåˆ›å»ºè¡¨å¹¶æŒ‡å®šå…³è”ä½ç½®å®ä¾‹</h3>
<pre><code>create table user_profile(
	user_id BIGINT comment &quot;userID&quot;,
	gender BOOLEAN comment &quot;gender&quot;)
COMMENT &quot;toutiao user profile&quot;
row format delimited fields terminated by ','
LOCATION '/user/hive/warehouse/toutiao.db/user_profile';
</code></pre>
<p>æ³¨ï¼š<br>
5ä¸ªè¡¨ä¸­ï¼Œåªæœ‰ news_article_content<br>
ï¼ˆå› ä¸ºè¿™ä¸ªè¡¨å¥‡æ€ªå­—ç¬¦å¤ªå¤šï¼Œæ˜¯å…¨é‡å¯¼å…¥çš„ï¼Œhiveä¸éœ€è¦æ‰‹åŠ¨åˆ›å»ºè¡¨ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºçš„ï¼‰<br>
è€Œè¿™ä¸ªè¡¨ä» hdfs æ‹¿åˆ°çš„æ•°æ®æ˜¯å·²ç»åšè¿‡è¿‡æ»¤çš„ï¼Œæ‰€ä»¥ä¸éœ€è¦ åŠ åˆ†éš”ç¬¦äº†ï¼Œä¹Ÿå°±æ˜¯ä¸éœ€è¦ä¸‹é¢è¿™è¡Œä»£ç ï¼š<br>
row format delimited fields terminated by ','</p>
<h1 id="flumeæ—¥å¿—æ”¶é›†åˆ°hiveä¸­">Flumeæ—¥å¿—æ”¶é›†åˆ°Hiveä¸­</h1>
<h3 id="æ–°å»ºæ•°æ®åº“">æ–°å»ºæ•°æ®åº“</h3>
<pre><code>create database if not exists profile comment &quot;use action&quot; location '/user/hive/warehouse/profile.db/';
</code></pre>
<h3 id="åˆ›å»ºè¡¨çš„æ ¼å¼è¯­æ³•å®ä¾‹å¦‚ä¸‹">åˆ›å»ºè¡¨çš„æ ¼å¼è¯­æ³•å®ä¾‹å¦‚ä¸‹ï¼š</h3>
<pre><code>create table user_action(
actionTime STRING comment &quot;user actions time&quot;,
readTime STRING comment &quot;user reading time&quot;,
channelId INT comment &quot;article channel id&quot;,
param map&lt;string, string&gt; comment &quot;action parameter&quot;)
COMMENT &quot;user primitive action&quot;
PARTITIONED BY(dt STRING)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
LOCATION '/user/hive/warehouse/profile.db/user_action';
</code></pre>
<p>æ–‡æ¡£ä¸­ï¼šHiveå»ºè¡¨ï¼Œæœ‰ä¸ªé—®é¢˜ map éœ€è¦æŒ‡å®šæ•°æ®ç±»å‹:<br>
param map&lt;string, string&gt; comment &quot;action parameter&quot;)</p>
<pre><code>map è¦å‘ä¸Šé¢ä¸€æ ·æŒ‡å®š &lt;string, string&gt; æ‰å¯ä»¥ï¼Œ ä¸ç„¶ä¼šæŠ¥å¦‚ä¸‹é”™è¯¯ï¼š
'''mismatched input 'comment' expecting &lt; near 'map' in map type'''
</code></pre>
<p>ç–‘éš¾å‚æ•°è§£è¯»ï¼š<br>
PARTITIONED BY(dt STRING)ï¼š  hiveæŒ‰ç…§ dt å­—æ®µåˆ†åŒº<br>
ä¸ºä»€ä¹ˆè¦åˆ†åŒºï¼š<br>
Hiveé€‚åˆå¤„ç†å¤§çš„æ–‡ä»¶å†…å®¹é‡ï¼Œå°‘çš„æ–‡ä»¶æ•°é‡ã€‚<br>
Flumeæ”¶é›†æ—¥å¿—å¯èƒ½æ¥ä¸€ç‚¹æ—¥å¿—å°±åŠ åˆ°ä¸€ä¸ªæ–°æ–‡ä»¶ä¸­ã€‚<br>
å¦‚æ­¤ä¸€æ¥ï¼Œæ–‡ä»¶é›¶æ•£çš„ç‰¹åˆ«å¤šã€‚ Hiveå¤„ç†çš„ä¼šå¾ˆæ…¢ã€‚</p>
<pre><code>		æ‰€ä»¥ï¼ŒHiveæŒ‡å®šä¸ªåˆ†åŒºï¼Œæ¥æŠŠå°æ–‡ä»¶ä»¬åˆ†æˆå‡ å¤§å—ï¼ˆå°±æ˜¯å‡ ä¸ªåˆ†åŒºï¼‰ï¼Œè¿™æ ·å¤„ç†ä¼šæ›´å¿«
        
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'ï¼š å¤„ç†Jsonæ ¼å¼æ•°æ®
</code></pre>
<p>æ•°æ®å¯¼å…¥æ­¥éª¤å¦‚ä¸‹ï¼ˆè™šä»£æ›¿Flumeï¼‰ï¼ˆæ“ä½œå®Œæˆåæ˜¯æŸ¥ä¸åˆ°æ•°æ®çš„ï¼Œéœ€è¦å…³è”ï¼Œä¸‹é¢ä¼šè§£é‡Šï¼‰ï¼š<br>
hadoop fs -put /root/data/backup/profile.db/user_action/*  /user/hive/warehouse/profile.db/user_action/</p>
<pre><code># åˆ é™¤: hadoop fs -rmr /user/hive/warehouse/profile.db/*
</code></pre>
<h3 id="flume-æ”¶é›†é…ç½®">Flume æ”¶é›†é…ç½®</h3>
<p>è¿›å…¥flume/confç›®å½•<br>
åˆ›å»ºä¸€ä¸ªcollect _ click.confçš„æ–‡ä»¶ï¼Œå†™å…¥flumeçš„é…ç½®ï¼š<br>
a1.sources = s1<br>
a1.sinks = k1<br>
a1.channels = c1</p>
<pre><code>a1.sources.s1.channels= c1
a1.sources.s1.type = exec
a1.sources.s1.command = tail -F /root/logs/userClick.log
a1.sources.s1.interceptors=i1 i2
a1.sources.s1.interceptors.i1.type=regex_filter
a1.sources.s1.interceptors.i1.regex=\\{.*\\}
a1.sources.s1.interceptors.i2.type=timestamp

# channel1
a1.channels.c1.type=memory
a1.channels.c1.capacity=30000
a1.channels.c1.transactionCapacity=1000

# k1
a1.sinks.k1.type=hdfs
a1.sinks.k1.channel=c1
a1.sinks.k1.hdfs.path=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d
a1.sinks.k1.hdfs.useLocalTimeStamp = true
a1.sinks.k1.hdfs.fileType=DataStream
a1.sinks.k1.hdfs.writeFormat=Text
a1.sinks.k1.hdfs.rollInterval=0
a1.sinks.k1.hdfs.rollSize=10240
a1.sinks.k1.hdfs.rollCount=0
a1.sinks.k1.hdfs.idleTimeout=60
</code></pre>
<p>å‚æ•°è¯´æ˜ï¼š<br>
sourcesï¼šä¸ºå®æ—¶æŸ¥çœ‹æ–‡ä»¶æœ«å°¾ï¼Œinterceptorsè§£æjsonæ–‡ä»¶<br>
channelsï¼šæŒ‡å®šå†…å­˜å­˜å‚¨ï¼Œå¹¶ä¸”åˆ¶å®šbatchDataçš„å¤§å°ï¼ŒPutListå’ŒTakeListçš„å¤§å°è§å‚æ•°ï¼ŒChannelæ€»å®¹é‡å¤§å°è§å‚æ•°<br>
æŒ‡å®šsinkï¼šå½¢å¼ç›´æ¥åˆ°hdfsï¼Œä»¥åŠè·¯å¾„ï¼Œæ–‡ä»¶å¤§å°ç­–ç•¥é»˜è®¤1024ã€eventæ•°é‡ç­–ç•¥ã€æ–‡ä»¶é—²ç½®æ—¶é—´<br>
å¼€å§‹æ”¶é›†ï¼š<br>
/root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1</p>
<h3 id="hive-å…³è”åˆ†åŒº">Hive å…³è”åˆ†åŒºï¼š</h3>
<p>å¦‚æœä¸å…³è”åˆ†åŒºï¼Œæ— è®ºæ˜¯ Flumeæ”¶é›†åˆ°HDFSçš„åˆ†åŒºæ•°æ®ï¼Œè¿˜æ˜¯æˆ‘ä»¬ä¼ è¿›å»HDFSæ¨¡æ‹Ÿçš„åˆ†åŒºæ•°æ® é€šè¿‡Hiveæ˜¯æŸ¥ä¸åˆ°çš„<br>
å…³è”åˆ†åŒºå¦‚ä¸‹æ“ä½œï¼š<br>
alter table user_action add partition (dt='2018-12-11') location &quot;/user/hive/warehouse/profile.db/user_action/2018-12-11/&quot;</p>
<h1 id="è¿›ç¨‹ç®¡ç†-supervisor">è¿›ç¨‹ç®¡ç† Supervisor</h1>
<h3 id="æ­£å¸¸é…ç½®æµç¨‹">æ­£å¸¸é…ç½®æµç¨‹</h3>
<p>å®‰è£…:<br>
pip install supervisor<br>
åˆ›å»ºé…ç½®æ–‡ä»¶ï¼ˆå½“å‰ç›®å½•æ‰§è¡Œï¼Œæˆ–è€…ä¸»ç›®å½•æ‰§è¡Œéƒ½å¯ï¼Œæ€»é…ç½®æ–‡ä»¶å°±ä¼šç”Ÿæˆåˆ°å½“å‰ç›®å½•ä¸‹ï¼‰ï¼š<br>
echo_supervisord_conf &gt; supervisord.conf<br>
åˆ›å»ºè‡ªå®šä¹‰é…ç½®æ–‡ä»¶ç›®å½•ï¼š<br>
mkdir /etc/supervisor<br>
vim æ‰“å¼€ç¼–è¾‘supervisord.confæ–‡ä»¶ï¼Œä¿®æ”¹æœ€å1è¡Œï¼š<br>
[include]<br>
files = relative/directory/<em>.ini<br>
ä¸º<br>
[include]<br>
files = /etc/supervisor/</em>.conf<br>
å°†æœ€å¼€å§‹ç”Ÿæˆçš„ supervisord.conf å¤åˆ¶åˆ° /etc/ ä¸‹ï¼Œ ç„¶åä¸»æ–‡ä»¶å°±ä¸ç”¨åŠ¨äº†ï¼š<br>
cp supervisord.conf /etc/<br>
æœ€ååœ¨  /etc/supervisor è¿™ä¸ªç›®å½•ä¸­ï¼Œè‡ªå®šä¹‰æˆ‘ä»¬è‡ªå·±éœ€è¦çš„ å¯åŠ¨ç¨‹åºçš„é…ç½®æ–‡ä»¶æ¨¡æ¿ï¼Œè¿™é‡Œä¸º vi reco.confï¼š<br>
è§ä¸‹é¢Flumeæ¡ˆä¾‹</p>
<h3 id="flumesupervisor-é…ç½®æ¡ˆä¾‹">Flume+Supervisor é…ç½®æ¡ˆä¾‹</h3>
<p>flumeå¯åŠ¨éœ€è¦ç›¸å…³hadoop,javaç¯å¢ƒï¼Œå¯ä»¥åœ¨shellè„šæœ¬æ±‡æ€»æ·»åŠ :<br>
å…ˆåˆ›å»ºä¸€ä¸ªå­˜æ”¾æ­¤shellè„šæœ¬çš„ç›®å½•ï¼š<br>
mkdir /root/toutiao_project/scripts<br>
æ‰“å¼€æ–‡ä»¶:<br>
vi /root/toutiao_project/scripts/collect_click.sh<br>
å¹¶å†™å…¥:<br>
#!/usr/bin/env bash</p>
<pre><code>export JAVA_HOME=/root/bigdata/jdk
export HADOOP_HOME=/root/bigdata/hadoop
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin

/root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1
</code></pre>
<p>å¹¶åœ¨	/etc/supervisor çš„reco.confæ·»åŠ :<br>
[program:flume]<br>
command=/bin/bash /root/toutiao_project/scripts/collect_click.sh<br>
user=root<br>
autorestart=true<br>
redirect_stderr=true<br>
stdout_logfile=/root/logs/collect.log<br>
loglevel=info<br>
stopsignal=KILL<br>
stopasgroup=true<br>
killasgroup=true</p>
<p>å¯åŠ¨ supervisoræœåŠ¡:<br>
supervisord -c /etc/supervisord.conf<br>
æŸ¥çœ‹ supervisoræ˜¯å¦è¿è¡Œï¼š<br>
ps aux | grep supervisord<br>
ç®¡ç† supervisorè¿›ç¨‹ç®¡ç†ç•Œé¢ï¼Œè¾“å…¥å‘½ä»¤ï¼š<br>
supervisorctl<br>
ç®¡ç†ç•Œé¢å¯é€šè¿‡å¦‚ä¸‹ å‘½ä»¤+è¿›ç¨‹å æ¥ç®¡ç†è¿›ç¨‹ï¼š<br>
start flume<br>
stop flume<br>
restart flume</p>
<pre><code>status		# æŸ¥çœ‹æ‰€æœ‰è¿›ç¨‹çŠ¶æ€
update		# é‡å¯é…ç½®æ–‡ä»¶ä¿®æ”¹è¿‡çš„ç¨‹åº
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => PySpark-Spark SQL]]></title>
        <id>https://cythonlin.github.io/post/py-greater-pyspark-spark-sql/</id>
        <link href="https://cythonlin.github.io/post/py-greater-pyspark-spark-sql/">
        </link>
        <updated>2020-09-29T04:16:24.000Z</updated>
        <content type="html"><![CDATA[<h1 id="spark-sql">Spark SQL</h1>
<p>Spark SQL åˆ†ä¸ºä¸‰ç±»ï¼š</p>
<ol>
<li>SQL</li>
<li>DataFrame (å‚è€ƒpandasï¼Œä½†ç•¥æœ‰ä¸åŒ)</li>
<li>Datasets (ç”±äºpythonæ˜¯åŠ¨æ€çš„ï¼Œæ‰€ä»¥ä¸æ”¯æŒpython)</li>
</ol>
<h3 id="åˆå§‹ç¯å¢ƒ">åˆå§‹ç¯å¢ƒï¼š</h3>
<pre><code>import findspark
findspark.init()

from pyspark.sql import SparkSession        
spark = SparkSession.builder.appName('myspark').getOrCreate()    # åˆå§‹åŒ–session
# spark.sparkContext.parallelize([1,2,3,4]).collect()  # é‡Œé¢åŒ…å«ä¹‹å‰è¯´è¿‡çš„sparkContext
...
ä¸­é—´è¿™éƒ¨åˆ†ç•™ç»™ä¸‹é¢å†™
...
spark.stop()         # å…³é—­ session
</code></pre>
<p>ä»jsonå¯¼å…¥ä¸ºdf:<br>
df = spark.read.json(&quot;file:///home/lin/data/user.json&quot;,multiLine=True)<br>
æ‰“å°DFå­—æ®µä¿¡æ¯ï¼š<br>
df.printSchema()</p>
<pre><code>root
 |-- age: long (nullable = true)
 |-- gender: string (nullable = true)
 |-- name: string (nullable = true)
</code></pre>
<h3 id="crud">CRUD</h3>
<p>å¢<br>
from pyspark.sql import functions as f</p>
<pre><code># schemaå°±ç›¸å½“äº pandas æŒ‡å®šçš„ columns, åŒå±‚åºåˆ—ï¼Œ  [2x4] çš„æ ·æœ¬
df1 = spark.createDataFrame([[1,2,3,4],[5,6,7,8]],schema=['1_c','2_c', '3_c', '4_c'])
+-----+-----+-----+-----+
|1\_col|2\_col|3\_col|4\_col|
+-----+-----+-----+-----+
|    1|    2|    3|    4|
|    5|    6|    7|    8|
+-----+-----+-----+-----+

# litå¯ä»¥åœ¨æŒ‡å®šç©ºåˆ—çš„æ—¶å€™ï¼ŒæŒ‡å®š nullå€¼ï¼Œ æˆ–è€… intå‹ï¼ˆé‡Œé¢æœ‰å¾ˆå¤šç±»å‹ï¼Œå¯ä»¥å‘ç°ï¼‰
# df2 = df1.withColumn('null_col', f.lit(None)).withColumn('digit_col', f.lit(2))
df2 = df1.withColumn('5_col', df1['4_col']+1)   # åœ¨åŸæ¥åˆ—å­—æ®µåŸºç¡€ä¸Šã€‚
df2.show()
</code></pre>
<p>åˆ <br>
df2 = df1.drop('age')        # åˆ é™¤ ageåˆ—<br>
df2 = df1.dropna()           # åˆ é™¤ç©ºè¡Œ<br>
df2 = df1.drop_duplicates()  # åˆ é™¤é‡å¤-è¡Œ<br>
æ”¹<br>
å’Œ&quot;å¢&quot;ï¼Œå·®ä¸å¤šï¼Œåªä¸è¿‡å­—æ®µï¼ŒæŒ‡å®šä¸ºåŸæœ‰å­—æ®µå­—ç¬¦ä¸²å³å¯ã€‚</p>
<p>æŸ¥<br>
ä¸‹é¢çš„è®²çš„ï¼ˆæŠ•å½±ã€è¿‡æ»¤ã€æ’åºã€åˆ†ç»„ï¼‰ï¼Œå‡ ä¹éƒ½æ˜¯æŸ¥ã€‚</p>
<h3 id="æŠ•å½±">æŠ•å½±</h3>
<p>æŠ•å½±æ‰€æœ‰ï¼š<br>
df.show(n=20)        # é»˜è®¤å°±æ˜¯ n=20 åªè¿”å› å‰20æ¡è®°å½•</p>
<pre><code>+---+------+--------+
|age|gender|    name|
+---+------+--------+
| 18|   man|zhangsan|
+---+------+--------+
</code></pre>
<p>é€‰ä¸­æŸåˆ—æŠ•å½±ï¼š<br>
df.select('name','age').show()        # è‹¥ç›´æ¥å†™ '*', å’Œç›´æ¥ df.show()æ˜¯ä¸€ä¸ªæ•ˆæœ</p>
<pre><code>+--------+---+
|    name|age|
+--------+---+
|zhangsan| 18|
+--------+---+
</code></pre>
<p>æˆ–è€…ç”¨å¦ä¸¤ç§æ–¹å¼æŠ•å½±ï¼ˆæŠ•å½±è¿‡ç¨‹å¯è®¡ç®—ï¼‰ï¼š<br>
df.select(df['name'],df['age']+20).show() # åŒä¸Šï¼Œè¿™æ˜¯å¦ä¸€ç§å†™æ³•ï¼Œæ³¨æ„ä¸€ä¸‹åˆ—å<br>
df.select(df.name, df.age+20).show()      # åŒä¸Šï¼Œè¿™æ˜¯å¦äºŒç§å†™æ³•ï¼Œæ³¨æ„ä¸€ä¸‹åˆ—å</p>
<pre><code>+--------+----------+
|    name|(age + 20)|
+--------+----------+
|zhangsan|        38|
+--------+----------+
</code></pre>
<p>å–å‡ºå‰Næ¡DFï¼Œå¹¶è½¬åŒ–ä¸º [ {} , {} ] æ ¼å¼<br>
df_user.take(1)  # [Row(age=18, name='å¼ ä¸‰')]<br>
df_user.head(1)  # [Row(age=18, name='å¼ ä¸‰')]<br>
df_user.first()  # Row(age=18, name='å¼ ä¸‰')    # æ³¨æ„,æ— åˆ—è¡¨</p>
<h3 id="æ’åº">æ’åº</h3>
<pre><code>df_user.head(1)
df_user.sort(df_user.name.desc()).show()

# å¦å¤–è¯´æ˜ä¸€ç‚¹, dfçš„æ¯ä¸ªç†Ÿæ‚‰éƒ½æœ‰,ä¸€äº›æ“ä½œç¬¦å‡½æ•°, desc()å°±æ˜¯ä¸€ç§æ“ä½œç¬¦å‡½æ•°
</code></pre>
<h3 id="è¿‡æ»¤">è¿‡æ»¤ï¼š</h3>
<pre><code>df.filter( df['age'] &gt; 15).show()

+---+------+----+
|age|gender|name|
+---+------+----+
+---+------+----+
</code></pre>
<h3 id="åˆ†ç»„">åˆ†ç»„ï¼š</h3>
<pre><code>df.groupBy('name').count().show()

+--------+-----+
|    name|count|
+--------+-----+
|zhangsan|    1|
+--------+-----+
</code></pre>
<h3 id="join">Join</h3>
<pre><code>df_user.join(df_user, on=df_user.name==df_user.name, how='inner').show()

+----+---+----+---+
|name|age|name|age|
+----+---+----+---+
|æå››| 20|æå››| 20 |
|å¼ ä¸‰| 18|å¼ ä¸‰| 18 |
+----+---+----+---+
# ç‰¹åˆ«æé†’ï¼Œ æ­¤ Joinï¼Œ åªè¦éƒ½è¿›æ¥æ˜¯ DFæ ¼å¼çš„ä»»ä½•æ•°æ®åº“ï¼Œéƒ½å¯ Join
# æ¯”å¦‚ï¼š MySQL å’Œ Hive  ,  Json ä¹Ÿå¯ã€‚
</code></pre>
<h3 id="å‚¨å­˜ä¸ºä¸´æ—¶è§†å›¾è¡¨-å¹¶è°ƒç”¨sqlè¯­å¥">å‚¨å­˜ä¸ºä¸´æ—¶è§†å›¾ï¼ˆè¡¨), å¹¶è°ƒç”¨sqlè¯­å¥ï¼š</h3>
<pre><code>df.createOrReplaceTempView('user')                  # åˆ›å»ºä¸º userä¸´æ—¶è§†å›¾
df_sql = spark.sql('select * from user').show()     # spark.sqlè¿”å›çš„è¿˜æ˜¯df, æ‰€ä»¥è¦show()

+---+------+--------+
|age|gender|    name|
+---+------+--------+
| 18|   man|zhangsan|
+---+------+--------+
</code></pre>
<h3 id="rdd-ä¸-dfäº’è½¬">RDD ä¸ DFäº’è½¬</h3>
<p>RDD -&gt; DF<br>
### RDD -&gt; DF éœ€è¦æŠŠRDDåšæˆä¸¤ç§æ ¼å¼(ä»»é€‰å…¶ä¸€)<br>
### ç¬¬ä¸€ç§ Row æ ¼å¼<br>
from pyspark import Row<br>
rdd_user = spark.sparkContext.parallelize( [('å¼ ä¸‰',18), ('æå››',20)] )<br>
rdd_user_row = rdd_user.map(lambda x:Row(name=x[0], age=x[1]))<br>
print(rdd_user_row.collect()) # [Row(age=18, name='å¼ ä¸‰'), Row(age=20, name='æå››')]<br>
df_user = spark.createDataFrame(rdd_user_row)</p>
<pre><code>### ç¬¬äºŒç§ [('å¼ ä¸‰', 18),('æå››', 20)]
    rdd_user = spark.sparkContext.parallelize( [('å¼ ä¸‰',18), ('æå››',20)] )
    df_user = rdd_user.toDF(['name', 'age'])        # ç»™å®šåˆ—å
df_user.show()
</code></pre>
<p>DF -&gt; RDD<br>
rdd_row = df_user.rdd.map(lambda x: x.asDict())  # æˆ–è€… x.name, x.ageå–å€¼<br>
rdd_row.collect()  # [{'age': 18, 'name': 'å¼ ä¸‰'}, {'age': 20, 'name': 'æå››'}]</p>
<h3 id="csvè¯»å†™">CSVè¯»å†™</h3>
<p>ä»HDFSä¸­è¯»å–(æˆ‘ä»¬å…ˆæ–°å»ºä¸€ä¸ªCSVå¹¶æ‰”åˆ°HDFSä¸­),<br>
vi mydata.csv:<br>
name,age<br>
zhangsan,18<br>
lisi, 20</p>
<pre><code>hadoop fs -mkdir /data                 # åœ¨HDFSä¸­æ–°å»ºä¸€ä¸ªç›®å½• /data
hadoop fs -put mydata.csv /data        # å¹¶æŠŠæœ¬åœ° mydata.csvæ‰”è¿›å» (-getå¯æ‹¿å‡ºæ¥)
</code></pre>
<p>åœ¨ä»£ç ä¸­è¯»å– HDFSæ•°æ®:<br>
df = spark.read.csv(&quot;hdfs:///data/mydata.csv&quot;, header=True)<br>
df.show()<br>
# header=True ä»£è¡¨, csvæ–‡ä»¶çš„ç¬¬ä¸€è¡Œä½œä¸ºcsvçš„æŠ¬å¤´(åˆ—å)<br>
# df.write.csv(&quot;hdfs:///data/mydata.csv&quot;, header=True)   # readæ”¹ä¸ºwriteå°±å˜æˆäº†å†™</p>
<h3 id="hiveè¯»å†™">Hiveè¯»å†™</h3>
<p>Hiveçš„é…ç½®ä¸ä¾èµ–ä¹‹å‰è®²è¿‡äº†ï¼ˆæœ€å€¼å¾—æ³¨æ„çš„æ˜¯éœ€è¦å…ˆå¯åŠ¨ä¸€ä¸ª metadataçš„æœåŠ¡ï¼‰<br>
å…ˆéªŒä¼ é€é—¨ï¼š<a href="https://segmentfault.com/a/1190000020841646">https://segmentfault.com/a/1190000020841646</a><br>
import findspark<br>
findspark.init()</p>
<pre><code>from pyspark.sql import SparkSession   

spark = SparkSession.builder\
    .appName(&quot;Spark Hive Example&quot;)\
    .master(&quot;local[*]&quot;)\
    .config(&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;)\
    .enableHiveSupport()\
    .getOrCreate()

spark.sql(&quot;use mydatabase&quot;)        # æ‰§è¡ŒHive çš„ SQL, åˆ‡æ¢æ•°æ®åº“ï¼ˆå‰æä½ å¾—æœ‰ï¼‰
</code></pre>
<p>è¯»ï¼š<br>
df = spark.table('person').show()  # ç›´æ¥å¯¹è¡¨æ“ä½œ (æ³¨æ„ï¼Œsqlè¯­å¥ä¹Ÿå¯)<br>
å†™ï¼š<br>
df = spark.table('person')<br>
df2 = df.withColumn('nickname', df.name)  # ç¨å¾®å˜åŠ¨ä¸€ä¸‹ï¼Œæ·»ä¸€ä¸ªå­—æ®µ<br>
df2.write.saveAsTable(&quot;new_person&quot;)       # å†™å…¥æ–°è¡¨</p>
<h3 id="mysqlè¯»å†™">MySQLè¯»å†™</h3>
<p>è¯»ï¼š<br>
# æ³¨æ„0ï¼šæœ‰å¥½å‡ ç§æ–¹å¼ï¼Œæˆ‘åªåˆ—ä¸¾ä¸€ä¸ª æˆå¯¹çš„è¯»å†™é…ç½®ã€‚<br>
# æ³¨æ„1: urlä¸­ &quot;hive&quot;æ˜¯æ•°æ®åº“å. ä½ ä¹Ÿå¯ä»¥èµ·ä¸ºåˆ«çš„å<br>
# æ³¨æ„2ï¼štableçš„å€¼--&quot;TBLS&quot;,  å®ƒæ˜¯ MySQLä¸­&quot;hiveåº“&quot;ä¸­çš„ä¸€ä¸ªè¡¨ã€‚<br>
# æ³¨æ„3ï¼šç‰¹åˆ«æ³¨æ„ï¼ TBLSä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„è¡¨ã€‚ä»–åªæ˜¯ä¸€ä¸ªå¤§è¡¨ï¼Œç®¡ç†äº†æˆ‘ä»¬hiveçš„ä¿¡æ¯<br>
#        TBLSä¸­çš„ ä¸€ä¸ªåˆ—å±æ€§ &quot;TBL_NAME&quot; æ‰æ˜¯çœŸæ­£æˆ‘ä»¬éœ€è¦çš„è¡¨ï¼ï¼</p>
<pre><code>df = spark.read.jdbc(
    url='jdbc:mysql://localhost:3306/hive?user=root&amp;password=123',
    table=&quot;TBLS&quot;,
    properties={&quot;driver&quot;:&quot;com.mysql.jdbc.Driver&quot;},
)

df.show()
df.select(&quot;TBL_NAME&quot;).show()
</code></pre>
<p>å†™ï¼š<br>
df.write.jdbc(<br>
url='jdbc:mysql://localhost:3306/hive?user=root&amp;password=123',<br>
table=&quot;new_table&quot;,<br>
mode='append',<br>
properties={&quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;}<br>
)</p>
<pre><code># åŒæ˜¯ç‰¹åˆ«æ³¨æ„ï¼š å’Œè¯»ä¸€æ ·ï¼Œ å®ƒå†™å…¥çš„æ–°è¡¨ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæ•´ä½“çš„è¡¨ç»“æ„ã€‚
#     æ­¤è¡¨çš„ä¸€ä¸ªåˆ—&quot;TBL_NAME&quot;ï¼Œå®ƒæ‰å¯¹åº”äº†æˆ‘ä»¬çœŸæ­£è¦æ“ä½œçš„è¡¨</code></pre>
]]></content>
    </entry>
</feed>