<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://cythonlin.github.io</id>
    <title>Cython_lin</title>
    <updated>2021-01-29T06:36:50.707Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://cythonlin.github.io"/>
    <link rel="self" href="https://cythonlin.github.io/atom.xml"/>
    <logo>https://cythonlin.github.io/images/avatar.png</logo>
    <icon>https://cythonlin.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Cython_lin</rights>
    <entry>
        <title type="html"><![CDATA[PR => MusicBee & Winamp & VST]]></title>
        <id>https://cythonlin.github.io/post/pr-greater-musicbee-and-winamp-and-vst/</id>
        <link href="https://cythonlin.github.io/post/pr-greater-musicbee-and-winamp-and-vst/">
        </link>
        <updated>2021-01-29T04:58:49.000Z</updated>
        <content type="html"><![CDATA[<h1 id="musicbee">MusicBee</h1>
<h3 id="musicbee-皮肤">MusicBee 皮肤</h3>
<pre><code>Dark -&gt; Bee78
</code></pre>
<h3 id="musicbee-2">MusicBee</h3>
<h1 id="musicbee安装vst插槽">MusicBee安装VST插槽</h1>
<p>注：MusicBee不能直接安装VST插件， 需要官网安装一个VST插槽。用此插槽才可以安装其他VST插件<br>
<a href="https://getmusicbee.com/addons/plugins/16/vst-effects-support/">VST插槽下载地址</a></p>
<h3 id="安装插槽">安装插槽</h3>
<pre><code>Preference -&gt; Plugins -&gt; Add Plugins -&gt; 插槽下载的位置
</code></pre>
<h1 id="使用vst插槽安装vst插件">使用VST插槽安装VST插件</h1>
<h2 id="正常dll文件安装方式">正常DLL文件安装方式</h2>
<p><a href="https://freevstplugins.net/?s=NT+Pitch">VST插件下载地址</a><br>
注：MusicBee对VST插件支持很不友好，看了一大堆选了几个适合我这种初级选手能用的：</p>
<pre><code>3D_Panner_2.0
wL_niceNwide
Ceres (1)
cs12-156
</code></pre>
<p>下载后直接解压，将解压文件夹直接放到 music的plugins目录下即可（需重启MusicBee）</p>
<h2 id="痛苦安装方式只用通过这种方式才能安装到pitch-shift类插件">痛苦安装方式（只用通过这种方式才能安装到Pitch Shift类插件）</h2>
<p>这里提到一个超级老牌player -&gt;  Winamp...   ，现在应该没人用了。 或者都用Foobar2000了。<br>
Winamp 我看论坛都是 2000年左右的评论。。。可知它的插件也特别旧了。  但是它的插件特别多。<br>
可惜大多数放在 MusicBee上不好用或者崩溃。但没办法，还得用。</p>
<h3 id="为什么这里要提到winamp">为什么这里要提到Winamp</h3>
<p>因为MusicBee有个选项（从Winamp导入插件）<br>
而 Winamp的插件大多数都是 EXE格式的， 需要识别 Winamp的安装路径，并且安装到Winamp路径下。<br>
（其实EXE安装完也就是 DLL文件， 只不过它只能是这种方式。）<br>
所以 Winamp 只是个过渡的工具人。。。</p>
<h3 id="winamp插件地址">Winamp插件地址</h3>
<p><a href="https://winampheritage.com/plugins/DSP-Effect-5">Winamp插件</a><br>
拾到可用的插件 take it easy 可以改变 Pitch<br>
但是试过一段时间很遗憾，会让 MusicBee 宕掉。</p>
<h1 id="最后一根救命稻草-pitchshifterv101">最后一根救命稻草 pitchShifter.V1.01</h1>
<p><a href="https://github.com/kawaCat/pitchShifter-Vst/releases">pitchShifter地址</a><br>
这个就用不到 Winmap了， 直接移动到 MusicBee 的 Plugins中即可</p>
<h1 id="总结">总结</h1>
<p>找的很辛苦，普通的VST插件很好找， PITCH shift 这种的真的难找。<br>
即使找到了，MusicBee有些也各种不支持，不兼容，宕掉。</p>
<h2 id="最终插件合集">最终插件合集：</h2>
<pre><code>3D_Panner_2.0
wL_niceNwide
Ceres (1)
cs12-156

pitchShifter       （灵魂）
</code></pre>
<p>为何我如此执着音乐的变调，因为对于我来说，每一个Key都是一首新音乐！</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PR => U&P]]></title>
        <id>https://cythonlin.github.io/post/pr-greater-uandp/</id>
        <link href="https://cythonlin.github.io/post/pr-greater-uandp/">
        </link>
        <updated>2021-01-19T08:45:11.000Z</updated>
        <content type="html"><![CDATA[<h1 id="google">Google</h1>
<pre><code>96@
</code></pre>
<h1 id="github">Github</h1>
<pre><code>Cy
ha-
http://cythonlin.github.io/
</code></pre>
<h1 id="vultr">Vultr</h1>
<pre><code>Cy
</code></pre>
<h1 id="kaggle">Kaggle</h1>
<pre><code>cy
</code></pre>
<h1 id="microsoft">MicroSoft</h1>
<pre><code>96@ / si
</code></pre>
<h1 id="nvidia">Nvidia</h1>
<pre><code>96@
</code></pre>
<h1 id="anaconda">Anaconda</h1>
<pre><code>ha_
</code></pre>
<h1 id="七牛">七牛</h1>
<pre><code>96@
</code></pre>
<h1 id="ynote">YNote</h1>
<pre><code>96@
</code></pre>
<h1 id="coding-pages">Coding Pages</h1>
<pre><code>96@ / Jxxxxxxxxxxxxxx.       （末尾多个英文标点 句号）
https://cythonlin.coding.net
</code></pre>
<h1 id="postman">Postman</h1>
<pre><code>96@
</code></pre>
<h1 id="bd">BD</h1>
<pre><code>18/13
</code></pre>
<h1 id="docker">Docker</h1>
<pre><code>ha
</code></pre>
<h1 id="极算">极算</h1>
<pre><code>18
</code></pre>
<h1 id="neo4j">Neo4j</h1>
<pre><code>neo4j / zxc
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PR => Windows Terminal]]></title>
        <id>https://cythonlin.github.io/post/pr-greater-windows-terminal-ge-xing-hua-pei-zhi/</id>
        <link href="https://cythonlin.github.io/post/pr-greater-windows-terminal-ge-xing-hua-pei-zhi/">
        </link>
        <updated>2021-01-18T09:41:15.000Z</updated>
        <content type="html"><![CDATA[<h1 id="快捷键">快捷键</h1>
<p>搜索功能</p>
<pre><code>ctrl + shift + p
</code></pre>
<p>搜索文本</p>
<pre><code>ctrl + shift + f
</code></pre>
<p>編輯配置文件</p>
<pre><code>ctrl + '       (回車旁邊的符號 ')
</code></pre>
<p>橫向拆分窗口（下面action中，自定義配置）</p>
<pre><code>ctrl + +號
</code></pre>
<p>垂直拆分窗口（下面actrion中，自定義配置）</p>
<pre><code>ctrl + -號
</code></pre>
<p>關閉拆分的窗口（下面action中，自定義配置）</p>
<pre><code>ctrl + w
</code></pre>
<p>配置文件（ctrl + ' 后，全部内容整體替換即可）</p>
<pre><code>    // To view the default settings, hold &quot;alt&quot; while clicking on the &quot;Settings&quot; button.
    // For documentation on these settings, see: https://aka.ms/terminal-documentation

    {
        // 官方设置指南?
        &quot;$schema&quot;: &quot;https://aka.ms/terminal-profiles-schema&quot;,

        // 一些globals设置
        &quot;theme&quot;: &quot;dark&quot;, // 窗口主题
        &quot;initialRows&quot;: 25,
        &quot;initialCols&quot;: 100,

        &quot;defaultProfile&quot;: &quot;{61c54bbd-c2c6-5271-96e7-009a87ff44bf}&quot;,

        &quot;profiles&quot;: {

            &quot;defaults&quot;: {
                // Put settings here that you want to apply to all profiles
                &quot;colorScheme&quot;: &quot;Seafoam Pastel&quot;,
                &quot;useAcrylic&quot;: true,
                &quot;acrylicOpacity&quot;: 0.55,
                &quot;cursorShape&quot;: &quot;vintage&quot;,
                &quot;cursorHeight&quot;: 60,
                &quot;cursorColor&quot;: &quot;#B00C11&quot;,
                &quot;fontFace&quot;: &quot;YaHei Consolas Hybrid&quot;,
                &quot;fontSize&quot;: 18
            },

            &quot;list&quot;: [{
                    // Make changes here to the powershell.exe profile
                    &quot;guid&quot;: &quot;{61c54bbd-c2c6-5271-96e7-009a87ff44bf}&quot;,
                    &quot;name&quot;: &quot;Windows PowerShell&quot;,
                    &quot;commandline&quot;: &quot;powershell.exe&quot;,
                    &quot;hidden&quot;: false
                },
                {
                    // Make changes here to the cmd.exe profile
                    &quot;guid&quot;: &quot;{0caa0dad-35be-5f56-a8ff-afceeeaa6101}&quot;,
                    &quot;name&quot;: &quot;cmd&quot;,
                    &quot;commandline&quot;: &quot;cmd.exe&quot;,
                    &quot;hidden&quot;: false
                },
                {
                    &quot;guid&quot;: &quot;{b453ae62-4e3d-5e58-b989-0a998ec441b8}&quot;,
                    &quot;hidden&quot;: false,
                    &quot;name&quot;: &quot;Azure Cloud Shell&quot;,
                    &quot;source&quot;: &quot;Windows.Terminal.Azure&quot;
                }
            ]
        },

        // Add custom color schemes to this array
        &quot;schemes&quot;: [
    {
    &quot;name&quot;: &quot;Seafoam Pastel&quot;,
    &quot;black&quot;: &quot;#000000&quot;,
    &quot;red&quot;: &quot;#ff7092&quot;,
    &quot;green&quot;: &quot;#00fbac&quot;,
    &quot;yellow&quot;: &quot;#fffa6a&quot;,
    &quot;blue&quot;: &quot;#00bfff&quot;,
    &quot;purple&quot;: &quot;#df95ff&quot;,
    &quot;cyan&quot;: &quot;#86cbfe&quot;,
    &quot;white&quot;: &quot;#ffffff&quot;,
    &quot;brightBlack&quot;: &quot;#000000&quot;,
    &quot;brightRed&quot;: &quot;#ff8aa4&quot;,
    &quot;brightGreen&quot;: &quot;#21f6bc&quot;,
    &quot;brightYellow&quot;: &quot;#fff787&quot;,
    &quot;brightBlue&quot;: &quot;#1bccfd&quot;,
    &quot;brightPurple&quot;: &quot;#e6aefe&quot;,
    &quot;brightCyan&quot;: &quot;#99d6fc&quot;,
    &quot;brightWhite&quot;: &quot;#ffffff&quot;,
    &quot;background&quot;: &quot;#332a57&quot;,
    &quot;foreground&quot;: &quot;#e5e5e5&quot;
    }
    ],

        // Add any keybinding overrides to this array.
        // To unbind a default keybinding, set the command to &quot;unbound&quot;
        &quot;keybindings&quot;: [],
        &quot;actions&quot;:
        [
            // Copy and paste are bound to Ctrl+Shift+C and Ctrl+Shift+V in your defaults.json.
            // These two lines additionally bind them to Ctrl+C and Ctrl+V.
            // To learn more about selection, visit https://aka.ms/terminal-selection
            { &quot;command&quot;: {&quot;action&quot;: &quot;copy&quot;, &quot;singleLine&quot;: false }, &quot;keys&quot;: &quot;ctrl+c&quot; },
            { &quot;command&quot;: &quot;paste&quot;, &quot;keys&quot;: &quot;ctrl+v&quot; },

            // Press Ctrl+Shift+F to open the search box
            { &quot;command&quot;: &quot;find&quot;, &quot;keys&quot;: &quot;ctrl+f&quot; },
            // settings
            { &quot;command&quot;: &quot;openSettings&quot;, &quot;keys&quot;: &quot;ctrl+'&quot; },
            // Press Alt+Shift+D to open a new pane.
            // - &quot;split&quot;: &quot;auto&quot; makes this pane open in the direction that provides the most surface area.
            // - &quot;splitMode&quot;: &quot;duplicate&quot; makes the new pane use the focused pane's profile.
            // To learn more about panes, visit https://aka.ms/terminal-panes
            { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;auto&quot;, &quot;splitMode&quot;: &quot;duplicate&quot; }, &quot;keys&quot;: &quot;alt+shift+d&quot; },
            { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;vertical&quot; }, &quot;keys&quot;: &quot;ctrl+plus&quot; },
            { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;horizontal&quot; }, &quot;keys&quot;: &quot;ctrl+-&quot; },
            { &quot;command&quot;: &quot;closePane&quot;, &quot;keys&quot;: &quot;ctrl+w&quot; }
        ]
    }
</code></pre>
<h1 id="wsl2附着在选项卡">WSL2(附着在选项卡)</h1>
<ol>
<li>
<p>启用WSL(用PowerShell替代手动启用),（可稍后重启，继续下面）</p>
<pre><code> dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart
</code></pre>
</li>
<li>
<p>启用虚拟机功能（这部结束后，需要重启）</p>
<pre><code> dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart
</code></pre>
</li>
<li>
<p>下载 X64 WSL2 更新包，并双击安装<br>
<a href="https://docs.microsoft.com/zh-cn/windows/wsl/install-win10#step-4---download-the-linux-kernel-update-package">WSL2更新包地址</a></p>
</li>
<li>
<p>将WSL2设置为默认版本</p>
<pre><code> wsl --set-default-version 2
</code></pre>
</li>
<li>
<p>MIcrosoft Store 安装 Ubuntu20 LTS</p>
</li>
<li>
<p>可以在 Windows Terminal 选项卡打开，也可直接打开Ubuntu</p>
</li>
<li>
<p>Win Terminal 输入 wsl 即可进入 Ubuntu,</p>
</li>
<li>
<p>换源</p>
<pre><code>cp -a /etc/apt/sources.list /etc/apt/sources.list.bak

 sudo sed -i &quot;s@http://.*archive.ubuntu.com@http://mirrors.huaweicloud.com@g&quot; /etc/apt/sources.list
 sudo sed -i &quot;s@http://.*security.ubuntu.com@http://mirrors.huaweicloud.com@g&quot; /etc/apt/sources.list
 
 sudo apt-get update
</code></pre>
</li>
</ol>
<h2 id="查看wsl版本信息与运行状态">查看WSL版本信息与运行状态</h2>
<pre><code>wsl -l -v
</code></pre>
<h1 id="安装scoop并非sqoop">安装Scoop(并非Sqoop...)</h1>
<h3 id="作用">作用</h3>
<pre><code>主要用在 单个纯 Windows Terminal  去安装 Linux工具使用。
但我发现 Win Terminal 的一些基础命令 还是没Linux好用（比如 ls无选项看不到隐藏文件）

所以我最后选择了 Win Terminal + WSL2 并用
所以Scoop这项就可以不用了
</code></pre>
<h3 id="流程">流程</h3>
<p>因为用Powershell命令，会访问到raw.githubusercontent.com，所以先修改DNS</p>
<pre><code>C:\Windows\System32\drivers\etc
    199.232.68.133 raw.githubusercontent.com
ipconfig /flushdns
</code></pre>
<p>用Powershell命令正式安装Scoop:</p>
<pre><code>Set-ExecutionPolicy RemoteSigned -scope CurrentUser
iwr -useb get.scoop.sh | iex
</code></pre>
<p>静待Scoop安装完成， 用Scoop安装 Unix工具：</p>
<pre><code>scoop install sudo
</code></pre>
<h1 id="三方桌面程序">三方桌面程序</h1>
<p>Winstep Xtreme 18.0+ (含 workshelf + NextStart)</p>
<h1 id="三方-file-explorer">三方 File Explorer</h1>
<p>RX 文件管理器</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => 音声合成]]></title>
        <id>https://cythonlin.github.io/post/py-greater-yin-sheng-he-cheng/</id>
        <link href="https://cythonlin.github.io/post/py-greater-yin-sheng-he-cheng/">
        </link>
        <updated>2020-10-12T12:20:17.000Z</updated>
        <content type="html"><![CDATA[<h1 id="背景">背景</h1>
<p>音声合成 基于 很久之前写的文章 <a href="https://www.cklin.top/post/py-greater-yin-sheng-fen-chi/">音声分离</a><br>
一些 Light Music 的 Electronic Drum 太吵了。<br>
于是突发奇想，如何  N v 1 分离出 Drum 并且 Drop</p>
<h1 id="音声分离更新为5stems-16khz-model">音声分离（更新为5stems-16kHz Model）</h1>
<p>2stems (vocals / accompaniment)</p>
<pre><code>spleeter separate  -o audio_output -i audio_example.mp3
</code></pre>
<p>4stems (vocals / bass / drums / other )</p>
<pre><code>spleeter separate -o audio_output -p spleeter:4stems  -i audio_example.mp3
</code></pre>
<p>5stems (vocals / bass / drums / piano / other)</p>
<pre><code>spleeter separate -o audio_output -p spleeter:5stems-16kHz -i audio_example.mp3
</code></pre>
<p>这次用的是 5stems预训练模型， 得到了如下5个文件：</p>
<pre><code>bass.wav
drums.wav
other.wav
piano.wav
vocals.wav
</code></pre>
<h1 id="寻找解决方案">寻找解决方案</h1>
<p>最开始不知道从何搜起，后来直接索性Github贴了一个 <a href="https://github.com/deezer/spleeter/issues/506">Question Issues</a>。<br>
有人给出stack的<a href="https://stackoverflow.com/questions/14498539/how-to-overlay-downmix-two-audio-files-using-ffmpeg">解决方案</a>， 个人简化使用如下：</p>
<pre><code>ffmpeg -i other.wav -i vocals.wav -i bass.wav -i piano.wav -filter_complex amix=inputs=4:duration=longest output.mp3
</code></pre>
<h1 id="结果">结果</h1>
<p>最终成功把 Electronic Drum 声 Drop。<br>
唯一美中不足的就是，5个Stems预训练模型分的不够细致, Github Wiki最新方案的就是5stems~</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => K8S（未完待续）]]></title>
        <id>https://cythonlin.github.io/post/py-greater-k8s/</id>
        <link href="https://cythonlin.github.io/post/py-greater-k8s/">
        </link>
        <updated>2020-10-06T22:18:23.000Z</updated>
        <content type="html"><![CDATA[<h1 id="安装">安装</h1>
<p>官档： <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a><br>
官档中的镜像时Google的，需要换成阿里源。</p>
<h3 id="ubuntu">Ubuntu</h3>
<pre><code>apt-get update &amp;&amp; apt-get install -y apt-transport-https

curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 

cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF

apt-get update
apt-get install -y kubelet kubeadm kubectl
</code></pre>
<h3 id="centos">CentOS</h3>
<pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

setenforce 0
yum install -y kubelet kubeadm kubectl
systemctl enable kubelet &amp;&amp; systemctl start kubelet</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => Github-Cli]]></title>
        <id>https://cythonlin.github.io/post/py-greater-github-cli-quan-tao/</id>
        <link href="https://cythonlin.github.io/post/py-greater-github-cli-quan-tao/">
        </link>
        <updated>2020-10-03T01:53:58.000Z</updated>
        <content type="html"><![CDATA[<h1 id="下载">下载</h1>
<p>选个OS版本（我用的Win）：<a href="https://github.com/cli/cli/releases">https://github.com/cli/cli/releases  </a></p>
<h1 id="列出配置">列出配置</h1>
<pre><code>git config --list
</code></pre>
<p>由于我github2个号切换，导致，push的时候有403错误混淆，<br>
所以删除了家目录的 .gitconfig(应该是这个有些记不清了)</p>
<h1 id="设置代理">设置代理</h1>
<p>为了加速clone，这里先设置，若没有PROXY, 那此步可略过</p>
<pre><code>git config --global http.proxy &quot;socks5://127.0.0.1:7890&quot;
git config --global https.proxy &quot;socks5://127.0.0.1:7890&quot;
</code></pre>
<p>清除代理也很简单</p>
<pre><code>git config --global --unset http.proxy
git config --global --unset https.proxy
</code></pre>
<h1 id="设置ssh-key">设置SSH Key</h1>
<h3 id="说明">说明</h3>
<pre><code>如果你不习惯用SSH，而是习惯用HTTP的方式，那这步可省
</code></pre>
<h3 id="生成密钥命令">生成密钥命令</h3>
<pre><code>ssh-keygen
</code></pre>
<p>进入用户家目录，把id_rsa.pub公钥复制出来<br>
粘贴到-&gt; <a href="https://github.com/settings/ssh/">https://github.com/settings/ssh/new</a></p>
<h1 id="登录">登录</h1>
<pre><code>gh auth login
</code></pre>
<p>提前声明，遇到选项，都是用上下箭头选择<br>
第1个选项： 选择Github.com（也就是个人用户）<br>
第2个选项：选择 Login with a web browser<br>
Command中会给一串代码，复制代码-&gt;CMD回车-&gt;自动跳转到Web-&gt;粘贴代码-&gt;确认-&gt;确认授权</p>
<pre><code>这里也可以选择使用 Token 代替 web browser。但是这种方式需要生成一个Token
生成URL如下：
:  -&gt;   https://github.com/settings/tokens

点击 Generate bew token ，新建一个新 token：
    注意： 需要把 repo的所有权限勾上
               外加一个admin:org下面的  read:org  选项
    温馨提示：  read:org  必须勾上，不然创建失败。
</code></pre>
<p>第3个选项：选择SSH（HTTPS也可以）</p>
<h3 id="查看登录状态">查看登录状态</h3>
<pre><code>gh auth status
</code></pre>
<h3 id="退出登录">退出登录</h3>
<pre><code>gh auth logout
</code></pre>
<h1 id="仓库">仓库</h1>
<h3 id="创建仓库">创建仓库</h3>
<pre><code>gh repo create my-gh
    -&gt; Public
    -&gt; xxx  in your current directory（Y/N） y   (回车默认就是yes，下同)
    -&gt; Create a local project directory for xxx （Y/N）y
</code></pre>
<h3 id="查看远程权限">查看远程权限</h3>
<pre><code>git remote -v
    origin  https://github.com/Cythonlin/my-gh.git (fetch)
    origin  https://github.com/Cythonlin/my-gh.git (push)
</code></pre>
<h3 id="克隆">克隆</h3>
<pre><code>gh repo clone gin-gonic/gin
cd gin
git remote -v
    origin  https://github.com/gin-gonic/gin.git (fetch)
    origin  https://github.com/gin-gonic/gin.git (push)
    # 我们可以发现，这分支并不是我们的自己的
    # 所以我们可以 fork 下来
</code></pre>
<h3 id="fork">fork</h3>
<p>fork 指的是，把克隆到自己的仓库，作为上游（upstream）项目，然后自己就可自由同步它</p>
<pre><code>cd gin  # 上面已经进此路径，这步可省
gh repo fork
    -&gt; Would you like to add a remote for the fork? (Y/n) 回车yes
</code></pre>
<p>上面是先clone,然后进入路径，再fork<br>
如果事先未clone， 也可以用gh repo fork + 用户名/仓库名， 直接 fork+clone一步到位</p>
<pre><code># 这就不需要像上面先 cd进入clone的目录下再fork了，这种方式直接fork即可
gh repo fork pytorch/pytorch
</code></pre>
<h1 id="gist">Gist</h1>
<p>gist是github分享数据内容的平台 -&gt; <a href="https://gist.github.com/">https://gist.github.com/</a><br>
上面的地址可以分享公有/私有的文件，创建上传后，Github会跳转生成一个链接，来给我们使用</p>
<h3 id="github-cli实现gist">github-cli实现Gist</h3>
<p>默认是私有的，命令如下：</p>
<pre><code>gh gist create 1.txt
</code></pre>
<p>公有命令如下：</p>
<pre><code>gh gist create 1.txt --public
</code></pre>
<p>二者都会生成个URL，即可访问。<br>
也可以一个gist中存2个文件：</p>
<pre><code>gh gist create 1.txt 2.txt
</code></pre>
<h3 id="修改-gist共享的文件参数为生成url的尾部路径参数">修改 Gist共享的文件，参数为生成url的尾部路径参数</h3>
<pre><code>gh gist edit 5ff497631f0cc1e0a4463079a6a9eeff
</code></pre>
<h3 id="列出-上传过的gist文件-public代表公有文件-secret私有-不加参数代表所有">列出 上传过的Gist文件, --public代表公有文件，--secret私有, 不加参数代表所有</h3>
<pre><code>gh gist list
gh gist list --secret  
gh gist list --public
</code></pre>
<h1 id="pr-pull-request">PR (Pull Request）</h1>
<h2 id="pr概念">PR概念</h2>
<p>&quot;我fork了你们的代码，现在我发送一个请求，请你们回收我的代码&quot;😂</p>
<h2 id="pr流程">PR流程</h2>
<ol>
<li>fork别人仓库（先fork在clone， 前面已经提到 gh 可以直接fork一步到位了）</li>
<li>切换分支（也可以在 master 下），add,commit,push 修改代码。</li>
<li>在你fork后的仓库主页点击右上角的 Compare &amp; pull request 提交合并申请</li>
<li>等待别人合并你的请求</li>
</ol>
<h2 id="实验流程">实验流程</h2>
<h3 id="一-用当前的号去-fork另一个号另外那个也是自己的号方便做实验的仓库">一、用当前的号，去 fork另一个号（另外那个也是自己的号方便做实验）的仓库。</h3>
<pre><code>gh repo fork hacker-lin/bio2bioes
cd bio2bioes
</code></pre>
<h3 id="二-创建切换分支addcommitpushadd">二、 创建+切换分支+add+commit+push+add</h3>
<pre><code>git checkout -b dev
echo 111 &gt; 1.txt
git add . &amp;&amp; git commit -m &quot;my_test&quot; 
到这里即可，先不要push（下面PR创建的过程，会自动帮我们push，这里push我试了会出错）
</code></pre>
<h3 id="三-用github-cli命令-代替-点击compare-pull-request-按钮提交合并申请">三、用GitHUB-Cli命令 代替 点击Compare &amp; pull request 按钮提交合并申请</h3>
<p>创建PR</p>
<pre><code>gh pr create
     -&gt; Where should we push the 'dev' branch # 选第一个
     -&gt; Title  # 随便写个  some update
     -&gt; Body # 直接 回车 跳过就行。
     -&gt; What's next? # 选 Submit  提交即可 
</code></pre>
<p>列出提交的PR</p>
<pre><code>gh pr list 
# 执行后，你会发现 PR信息， # 后面的数字记住下面Closed和diff用得到
</code></pre>
<p>Merge PR</p>
<pre><code>gh pr merge
    -&gt; What merge method would you like to use # Create a merge commit即可
</code></pre>
<p>Closed PR</p>
<pre><code>gh pr close 3
# 这个3 就是上面 gh pr list 的结果
</code></pre>
<p>比较 PR 信息</p>
<pre><code>gh pr diff 3
# 最下面是最新的
</code></pre>
<p>查看 PR 详细信息</p>
<pre><code>gh pr status
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[GO => 交换元素的四种方式]]></title>
        <id>https://cythonlin.github.io/post/go-greater-jiao-huan-yuan-su-de-si-chong-fang-shi/</id>
        <link href="https://cythonlin.github.io/post/go-greater-jiao-huan-yuan-su-de-si-chong-fang-shi/">
        </link>
        <updated>2020-09-29T04:22:34.000Z</updated>
        <content type="html"><![CDATA[<h1 id="python-and-go">Python and Go</h1>
<p>下面代码，除了数据声明与定义，其他Py 和 Go 的语法都是一摸一样的（主要强调 异或方式）<br>
a := 1<br>
b := 3</p>
<h3 id="方式0">方式0</h3>
<pre><code>c := 0
c = a
a = b
b = c
</code></pre>
<h3 id="方式1">方式1</h3>
<pre><code>a,b = b,a
</code></pre>
<h3 id="方式2">方式2</h3>
<pre><code>a = a + b
b = a - b
a = a - b
</code></pre>
<h3 id="方式3-注必须是整形-py也一样">方式3 （注：必须是整形， Py也一样）</h3>
<pre><code>a = a ^ b
b = a ^ b
a = a ^ b

fmt.Println(a)
fmt.Println(b)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RS => 推荐系统（三）离线召回与排序 ]]></title>
        <id>https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-san-chi-xian-zhao-hui-yu-pai-xu/</id>
        <link href="https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-san-chi-xian-zhao-hui-yu-pai-xu/">
        </link>
        <updated>2020-09-29T04:21:44.000Z</updated>
        <content type="html"><![CDATA[<h1 id="召回设计">召回设计</h1>
<h3 id="召回排序流程">召回排序流程</h3>
<h4 id="匿名用户">匿名用户：</h4>
<pre><code>通常使用用户冷启动方案，区别在于user_id为匿名用户手机识别号(黑马头条不允许匿名用户)
所有只正针对于登录用户：
</code></pre>
<h4 id="用户冷启动前期点击行为较少情况">用户冷启动（前期点击行为较少情况）</h4>
<p>非个性化推荐<br>
热门召回：自定义热门规则，根据当前时间段热点定期更新维护人点文章库<br>
新文章召回：为了提高新文章的曝光率，建立新文章库，进行推荐<br>
个性化推荐：<br>
基于内容的协同过滤在线召回：基于用户实时兴趣画像相似的召回结果用于首页的个性化推荐</p>
<h4 id="后期离线部分用户点击行为较多用户画像完善">后期离线部分（用户点击行为较多，用户画像完善）</h4>
<p>建立用户长期兴趣画像（详细）：包括用户各个维度的兴趣特征<br>
训练排序模型<br>
LR模型、FTRL、Wide&amp;Deep<br>
离线部分的召回：<br>
基于模型协同过滤推荐离线召回：ALS<br>
基于内容的离线召回：或者称基于用户画像的召回</p>
<h3 id="召回表设计与模型召回">召回表设计与模型召回</h3>
<h4 id="召回表设计">召回表设计</h4>
<p>我们的召回方式有很多种。<br>
多路召回结果存储模型召回 与 内容召回的结果 需要进行相应频道推荐合并。<br>
方案：基于模型与基于内容的召回结果存入同一张表，避免多张表进行读取处理<br>
由于HBASE有多个版本数据功能存在的支持<br>
TTL=&gt;7776000, VERSIONS=&gt;999999<br>
如下：<br>
create 'cb_recall', {NAME=&gt;'als', TTL=&gt;7776000, VERSIONS=&gt;999999}<br>
alter 'cb_recall', {NAME=&gt;'content', TTL=&gt;7776000, VERSIONS=&gt;999999}<br>
alter 'cb_recall', {NAME=&gt;'online', TTL=&gt;7776000, VERSIONS=&gt;999999}</p>
<pre><code># 例子（多版本）：
put 'cb_recall', 'recall:user:5', 'als:1',[45,3,5,10]
put 'cb_recall', 'recall:user:5', 'als:1',[289,11,65,52,109,8]
put 'cb_recall', 'recall:user:5', 'als:2',[1,2,3,4,5,6,7,8,9,10]
put 'cb_recall', 'recall:user:2', 'content:1',[45,3,5,10,289,11,65,52,109,8]
put 'cb_recall', 'recall:user:2', 'content:2',[1,2,3,4,5,6,7,8,9,10]


hbase(main):084:0&gt; desc 'cb_recall'
Table cb_recall is ENABLED                                                                             
cb_recall                                                                                              
COLUMN FAMILIES DESCRIPTION                                                                            
{NAME =&gt; 'als', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false'
, KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 
'7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE
_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_
OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                    
{NAME =&gt; 'content', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'fa
lse', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL
          =&gt; '7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', C
ACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS
_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                
{NAME =&gt; 'online', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'fal
se', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL 
=&gt; '7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CA
CHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_
ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                 
3 row(s)	
</code></pre>
<p>（几乎不用）在HIVE用户数据数据库下建立HIVE外部表,若hbase表有修改，则进行HIVE 表删除更新<br>
create external table cb_recall_hbase(<br>
user_id STRING comment &quot;userID&quot;,<br>
als map&lt;string, ARRAY<BIGINT>&gt; comment &quot;als recall&quot;,<br>
content map&lt;string, ARRAY<BIGINT>&gt; comment &quot;content recall&quot;,<br>
online map&lt;string, ARRAY<BIGINT>&gt; comment &quot;online recall&quot;)<br>
COMMENT &quot;user recall table&quot;<br>
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'<br>
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,als:,content:,online:&quot;)<br>
TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;cb_recall&quot;);</p>
<h4 id="增加一个历史召回结果表">增加一个历史召回结果表</h4>
<pre><code>create 'history_recall', {NAME=&gt;'channel', TTL=&gt;7776000, VERSIONS=&gt;999999}

put 'history_recall', 'recall:user:5', 'als:1',[1,2,3]
put 'history_recall', 'recall:user:5', 'als:1',[4,5,6,7]
put 'history_recall', 'recall:user:5', 'als:1',[8,9,10]
</code></pre>
<p>为什么增加历史召回表？<br>
1、直接在存储召回结果部分进行过滤，比之后排序过滤，节省排序时间<br>
2、防止Redis缓存没有消耗完，造成重复推荐，从源头进行过滤</p>
<h3 id="基于模型召回集合计算">基于模型召回集合计算</h3>
<h4 id="als模型推荐实现">ALS模型推荐实现</h4>
<p>步骤：<br>
1、数据类型转换,clicked以及用户ID与文章ID处理<br>
2、ALS模型训练以及推荐<br>
3、推荐结果解析处理<br>
4、推荐结果存储<br>
数据类型转换,clicked( bool 转 int)<br>
ur.spark.sql(&quot;use profile&quot;)<br>
user_article_click = ur.spark.sql(&quot;select * from user_article_basic&quot;).<br>
select(['user_id', 'article_id', 'clicked'])<br>
# 更换类型<br>
def change_types(row):<br>
return row.user_id, row.article_id, int(row.clicked)</p>
<pre><code>user_article_click = user_article_click.rdd.map(change_types).toDF(['user_id', 'article_id', 'clicked'])
</code></pre>
<p>这步处理结果格式如下：<br>
user_id	article_id	clicked<br>
0<br>
1<br>
用户ID与文章ID处理，编程ID索引（原用户ID和文章ID是长字符串，ALS模型不能处理，要重新编排ID索引）<br>
from pyspark.ml.feature import StringIndexer<br>
from pyspark.ml import Pipeline<br>
# 用户和文章ID超过ALS最大整数值，需要使用StringIndexer进行转换<br>
user_id_indexer = StringIndexer(inputCol='user_id', outputCol='als_user_id')<br>
article_id_indexer = StringIndexer(inputCol='article_id', outputCol='als_article_id')<br>
pip = Pipeline(stages=[user_id_indexer, article_id_indexer])<br>
pip_fit = pip.fit(user_article_click)<br>
als_user_article_click = pip_fit.transform(user_article_click)<br>
ALS 模型训练与推荐（ALS模型需要输出用户ID列，文章ID列以及点击列）<br>
from pyspark.ml.recommendation import ALS<br>
# 模型训练和推荐默认每个用户固定文章个数<br>
als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1)<br>
model = als.fit(als_user_article_click)<br>
recall_res = model.recommendForAllUsers(100)<br>
结果：<br>
als_user_id	recommendations<br>
1			[[article_id, 分数]]</p>
<h4 id="推荐结果处理">推荐结果处理</h4>
<p>通过StringIndexer变换后的下标知道原来的和用户ID<br>
# recall_res得到需要使用StringIndexer变换后的下标<br>
# 保存原来的下表映射关系<br>
refection_user = als_user_article_click.groupBy(['user_id']).max('als_user_id').withColumnRenamed(<br>
'max(als_user_id)', 'als_user_id')<br>
refection_article = als_user_article_click.groupBy(['article_id']).max('als_article_id').withColumnRenamed(<br>
'max(als_article_id)', 'als_article_id')</p>
<pre><code># Join推荐结果与 refection_user映射关系表
# +-----------+--------------------+-------------------+
# | als_user_id | recommendations | user_id |
# +-----------+--------------------+-------------------+
# | 8 | [[163, 0.91328144]... | 2 |
 # | 0 | [[145, 0.653115], ... | 1106476833370537984 |
 
recall_res = recall_res.join(refection_user, on=['als_user_id'], how='left').select(
['als_user_id', 'recommendations', 'user_id'])
</code></pre>
<p>对推荐文章ID后处理：得到推荐列表,获取推荐列表中的ID索引<br>
# Join推荐结果与 refection_article映射关系表<br>
# +-----------+-------+----------------+<br>
# | als_user_id | user_id | als_article_id |<br>
# +-----------+-------+----------------+<br>
# | 8 | 2 | [163, 0.91328144] |<br>
# | 8 | 2 | [132, 0.91328144] |<br>
import pyspark.sql.functions as F<br>
recall_res = recall_res.withColumn('als_article_id', F.explode('recommendations')).drop('recommendations')</p>
<pre><code># +-----------+-------+--------------+
# | als_user_id | user_id | als_article_id |
# +-----------+-------+--------------+
# | 8 | 2 | 163 |
# | 8 | 2 | 132 |
def _article_id(row):
	return row.als_user_id, row.user_id, row.als_article_id[0]
</code></pre>
<p>进行索引对应文章ID获取<br>
als_recall = recall_res.rdd.map(_article_id).toDF(['als_user_id', 'user_id', 'als_article_id'])<br>
als_recall = als_recall.join(refection_article, on=['als_article_id'], how='left').select(<br>
['user_id', 'article_id'])<br>
# 得到每个用户ID 对应推荐文章<br>
# +-------------------+----------+<br>
# | user_id |				 article_id |<br>
# +-------------------+----------+<br>
# | 1106476833370537984 |   44075 |<br>
# | 1 | 					 44075 |<br>
获取每个文章对应的频道，推荐给用户时按照频道存储:<br>
ur.spark.sql(&quot;use toutiao&quot;)<br>
news_article_basic = ur.spark.sql(&quot;select article_id, channel_id from news_article_basic&quot;)</p>
<pre><code>als_recall = als_recall.join(news_article_basic, on=['article_id'], how='left')
als_recall = als_recall.groupBy(['user_id', 'channel_id']).agg(F.collect_list('article_id')).withColumnRenamed(
                    'collect_list(article_id)', 'article_list')

als_recall = als_recall.dropna()
</code></pre>
<h4 id="召回结果存储">召回结果存储</h4>
<p>HBASE表设计概览：<br>
put 'cb_recall', 'recall:user:5', 'als:1',[45,3,5,10,289,11,65,52,109,8]<br>
put 'cb_recall', 'recall:user:5', 'als:2',[1,2,3,4,5,6,7,8,9,10]<br>
存储代码如下：<br>
def save_offline_recall_hbase(partition):<br>
&quot;&quot;&quot;离线模型召回结果存储<br>
&quot;&quot;&quot;<br>
import happybase<br>
pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)<br>
for row in partition:<br>
with pool.connection() as conn:<br>
# 获取历史看过的该频道文章<br>
history_table = conn.table('history_recall')<br>
# 多个版本<br>
data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),<br>
'channel:{}'.format(row.channel_id).encode())</p>
<pre><code>            history = []
            if len(data) &gt;= 2:
                for l in data[:-1]:
                    history.extend(eval(l))
            else:
                history = []

            # 过滤reco_article与history
            reco_res = list(set(row.article_list) - set(history))

            if reco_res:

                table = conn.table('cb_recall')
                # 默认放在推荐频道
                table.put('recall:user:{}'.format(row.user_id).encode(),
                          {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                # 放入历史推荐过文章
                history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                  {'channel:{}'.format(row.channel_id): str(reco_res).encode()})
            conn.close()

als_recall.foreachPartition(save_offline_recall_hbase)
</code></pre>
<h3 id="离线用户基于内容召回集">离线用户基于内容召回集</h3>
<p>目标<br>
知道离线内容召回的概念<br>
知道如何进行内容召回计算存储规则<br>
应用<br>
应用spark完成离线用户基于内容的协同过滤推荐</p>
<h4 id="基于内容召回实现文章向量之前已经弄好了">基于内容召回实现（文章向量之前已经弄好了）</h4>
<p>过滤用户点击的文章<br>
# 基于内容相似召回（画像召回）<br>
ur.spark.sql(&quot;use profile&quot;)<br>
user_article_basic = self.spark.sql(&quot;select * from user_article_basic&quot;)<br>
user_article_basic = user_article_basic.filter(&quot;clicked=True&quot;)</p>
<pre><code>def save_content_filter_history_to__recall(partition):
    &quot;&quot;&quot;计算每个用户的每个操作文章的相似文章，过滤之后，写入content召回表当中（支持不同时间戳版本）
    &quot;&quot;&quot;
    import happybase
    pool = happybase.ConnectionPool(size=10, host='hadoop-master')

    # 进行为相似文章获取
    with pool.connection() as conn:

        # key:   article_id,    column:  similar:article_id
        similar_table = conn.table('article_similar')
        # 循环partition
        for row in partition:
            # 获取相似文章结果表
            similar_article = similar_table.row(str(row.article_id).encode(),
                                                columns=[b'similar'])
            # 相似文章相似度排序过滤，召回不需要太大的数据， 百个，千
            _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True)
            if _srt:
                # 每次行为推荐10篇文章
                reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10]

                # 获取历史看过的该频道文章
                history_table = conn.table('history_recall')
                # 多个版本
                data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                           'channel:{}'.format(row.channel_id).encode())

                history = []
                if len(data) &gt;= 2:
                    for l in data[:-1]:
                        history.extend(eval(l))
                else:
                    history = []

                # 过滤reco_article与history
                reco_res = list(set(reco_article) - set(history))

                # 进行推荐，放入基于内容的召回表当中以及历史看过的文章表当中
                if reco_res:
                    # content_table = conn.table('cb_content_recall')
                    content_table = conn.table('cb_recall')
                    content_table.put(&quot;recall:user:{}&quot;.format(row.user_id).encode(),
                                      {'content:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                    # 放入历史推荐过文章
                    history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                      {'channel:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

        conn.close()

user_article_basic.foreachPartition(save_content_filter_history_to__recall)
</code></pre>
<h3 id="离线用户召回定时更新">离线用户召回定时更新</h3>
<h4 id="定时更新代码">定时更新代码</h4>
<pre><code>import os
import sys
# 如果当前代码文件运行测试需要加入修改路径，否则后面的导包出现问题
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(BASE_DIR))
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline

from pyspark.ml.recommendation import ALS
from offline import SparkSessionBase
from datetime import datetime
import time
import numpy as np


class UpdateRecall(SparkSessionBase):

    SPARK_APP_NAME = &quot;updateRecall&quot;
    ENABLE_HIVE_SUPPORT = True

    def __init__(self, number):

        self.spark = self._create_spark_session()
        self.N = number

    def update_als_recall(self):
        &quot;&quot;&quot;
        更新基于模型（ALS）的协同过滤召回集
        :return:
        &quot;&quot;&quot;
        # 读取用户行为基本表
        self.spark.sql(&quot;use profile&quot;)
        user_article_click = self.spark.sql(&quot;select * from user_article_basic&quot;).\
            select(['user_id', 'article_id', 'clicked'])

        # 更换类型
        def change_types(row):
            return row.user_id, row.article_id, int(row.clicked)

        user_article_click = user_article_click.rdd.map(change_types).toDF(['user_id', 'article_id', 'clicked'])
        # 用户和文章ID超过ALS最大整数值，需要使用StringIndexer进行转换
        user_id_indexer = StringIndexer(inputCol='user_id', outputCol='als_user_id')
        article_id_indexer = StringIndexer(inputCol='article_id', outputCol='als_article_id')
        pip = Pipeline(stages=[user_id_indexer, article_id_indexer])
        pip_fit = pip.fit(user_article_click)
        als_user_article_click = pip_fit.transform(user_article_click)

        # 模型训练和推荐默认每个用户固定文章个数
        als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1)
        model = als.fit(als_user_article_click)
        recall_res = model.recommendForAllUsers(self.N)

        # recall_res得到需要使用StringIndexer变换后的下标
        # 保存原来的下表映射关系
        refection_user = als_user_article_click.groupBy(['user_id']).max('als_user_id').withColumnRenamed(
            'max(als_user_id)', 'als_user_id')
        refection_article = als_user_article_click.groupBy(['article_id']).max('als_article_id').withColumnRenamed(
            'max(als_article_id)', 'als_article_id')

        # Join推荐结果与 refection_user映射关系表
        # +-----------+--------------------+-------------------+
        # | als_user_id | recommendations | user_id |
        # +-----------+--------------------+-------------------+
        # | 8 | [[163, 0.91328144]... | 2 |
        #        | 0 | [[145, 0.653115], ... | 1106476833370537984 |
        recall_res = recall_res.join(refection_user, on=['als_user_id'], how='left').select(
            ['als_user_id', 'recommendations', 'user_id'])

        # Join推荐结果与 refection_article映射关系表
        # +-----------+-------+----------------+
        # | als_user_id | user_id | als_article_id |
        # +-----------+-------+----------------+
        # | 8 | 2 | [163, 0.91328144] |
        # | 8 | 2 | [132, 0.91328144] |
        import pyspark.sql.functions as F
        recall_res = recall_res.withColumn('als_article_id', F.explode('recommendations')).drop('recommendations')

        # +-----------+-------+--------------+
        # | als_user_id | user_id | als_article_id |
        # +-----------+-------+--------------+
        # | 8 | 2 | 163 |
        # | 8 | 2 | 132 |
        def _article_id(row):
            return row.als_user_id, row.user_id, row.als_article_id[0]

        als_recall = recall_res.rdd.map(_article_id).toDF(['als_user_id', 'user_id', 'als_article_id'])
        als_recall = als_recall.join(refection_article, on=['als_article_id'], how='left').select(
            ['user_id', 'article_id'])
        # 得到每个用户ID 对应推荐文章
        # +-------------------+----------+
        # | user_id | article_id |
        # +-------------------+----------+
        # | 1106476833370537984 | 44075 |
        # | 1 | 44075 |
        # 分组统计每个用户，推荐列表
        # als_recall = als_recall.groupby('user_id').agg(F.collect_list('article_id')).withColumnRenamed(
        #     'collect_list(article_id)', 'article_list')
        self.spark.sql(&quot;use toutiao&quot;)
        news_article_basic = self.spark.sql(&quot;select article_id, channel_id from news_article_basic&quot;)
        als_recall = als_recall.join(news_article_basic, on=['article_id'], how='left')
        als_recall = als_recall.groupBy(['user_id', 'channel_id']).agg(F.collect_list('article_id')).withColumnRenamed(
            'collect_list(article_id)', 'article_list')
        als_recall = als_recall.dropna()

        # 存储
        def save_offline_recall_hbase(partition):
            &quot;&quot;&quot;离线模型召回结果存储
            &quot;&quot;&quot;
            import happybase
            pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)
            for row in partition:
                with pool.connection() as conn:
                    # 获取历史看过的该频道文章
                    history_table = conn.table('history_recall')
                    # 多个版本
                    data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                               'channel:{}'.format(row.channel_id).encode())

                    history = []
                    if len(data) &gt;= 2:
                        for l in data[:-1]:
                            history.extend(eval(l))
                    else:
                        history = []

                    # 过滤reco_article与history
                    reco_res = list(set(row.article_list) - set(history))

                    if reco_res:

                        table = conn.table('cb_recall')
                        # 默认放在推荐频道
                        table.put('recall:user:{}'.format(row.user_id).encode(),
                                  {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()})
                        conn.close()

                        # 放入历史推荐过文章
                        history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                          {'channel:{}'.format(row.channel_id): str(reco_res).encode()})
                    conn.close()

        als_recall.foreachPartition(save_offline_recall_hbase)

    def update_content_recall(self):
        &quot;&quot;&quot;
        更新基于内容（画像）的推荐召回集, word2vec相似
        :return:
        &quot;&quot;&quot;
        # 基于内容相似召回（画像召回）
        ur.spark.sql(&quot;use profile&quot;)
        user_article_basic = self.spark.sql(&quot;select * from user_article_basic&quot;)
        user_article_basic = user_article_basic.filter(&quot;clicked=True&quot;)

        def save_content_filter_history_to__recall(partition):
            &quot;&quot;&quot;计算每个用户的每个操作文章的相似文章，过滤之后，写入content召回表当中（支持不同时间戳版本）
            &quot;&quot;&quot;
            import happybase
            pool = happybase.ConnectionPool(size=10, host='hadoop-master')

            # 进行为相似文章获取
            with pool.connection() as conn:

                # key:   article_id,    column:  similar:article_id
                similar_table = conn.table('article_similar')
                # 循环partition
                for row in partition:
                    # 获取相似文章结果表
                    similar_article = similar_table.row(str(row.article_id).encode(),
                                                        columns=[b'similar'])
                    # 相似文章相似度排序过滤，召回不需要太大的数据， 百个，千
                    _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True)
                    if _srt:
                        # 每次行为推荐10篇文章
                        reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10]

                        # 获取历史看过的该频道文章
                        history_table = conn.table('history_recall')
                        # 多个版本
                        data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                                   'channel:{}'.format(row.channel_id).encode())

                        history = []
                            if len(_history_data) &gt; 1:
                                for l in _history_data:
                                    history.extend(l)

                        # 过滤reco_article与history
                        reco_res = list(set(reco_article) - set(history))

                        # 进行推荐，放入基于内容的召回表当中以及历史看过的文章表当中
                        if reco_res:
                            # content_table = conn.table('cb_content_recall')
                            content_table = conn.table('cb_recall')
                            content_table.put(&quot;recall:user:{}&quot;.format(row.user_id).encode(),
                                              {'content:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                            # 放入历史推荐过文章
                            history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                              {'channel:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                conn.close()

        user_article_basic.foreachPartition(save_content_filter_history_to__recall)


if __name__ == '__main__':
    ur = UpdateRecall(500)
    ur.update_als_recall()
    ur.update_content_recall()
</code></pre>
<p>定时更新代码，在main.py和update.py中添加以下代码：<br>
from offline.update_recall import UpdateRecall<br>
from schedule.update_profile import update_user_profile, update_article_profile, update_recall</p>
<pre><code>def update_recall():
    &quot;&quot;&quot;
    更新用户的召回集
    :return:
    &quot;&quot;&quot;
    udp = UpdateRecall(200)
    udp.update_als_recall()
    udp.update_content_recall()
</code></pre>
<p>main中添加<br>
scheduler.add_job(update_recall, trigger='interval', hour=3)</p>
<h1 id="排序设计">排序设计</h1>
<h3 id="排序模型">排序模型</h3>
<p>宽模型 + 特征⼯程<br>
LR/MLR + 非ID类特征(⼈⼯离散/GBDT/FM)<br>
spark 中可以直接使用<br>
宽模型 + 深模型<br>
wide&amp;deep,DeepFM<br>
使用TensorFlow进行训练<br>
深模型：<br>
DNN + 特征embedding<br>
使用TensorFlow进行训练</p>
<h3 id="特征处理原则">特征处理原则</h3>
<p>离散数据<br>
one-hot编码<br>
连续数据<br>
归一化<br>
图片/文本<br>
文章标签/关键词提取<br>
embedding</p>
<h3 id="优化训练方式">优化训练方式</h3>
<p>使用Batch SGD优化<br>
加入正则化防止过拟合</p>
<h3 id="spark-lr-进行预估">spark LR 进行预估</h3>
<p>目的：通过LR模型进行CTR预估<br>
步骤：<br>
1、需要通过spark读取HIVE外部表，需要新的sparksession配置<br>
增加HBASE配置<br>
2、读取用户点击行为表，与用户画像和文章画像，构造训练样本<br>
3、LR模型进行训练<br>
4、LR模型预测、结果评估</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => HBase]]></title>
        <id>https://cythonlin.github.io/post/py-greater-hbase/</id>
        <link href="https://cythonlin.github.io/post/py-greater-hbase/">
        </link>
        <updated>2020-09-29T04:21:09.000Z</updated>
        <content type="html"><![CDATA[<h1 id="报错">报错</h1>
<p>若list 或其他命令 有如下错误：<br>
ERROR: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing<br>
则使用如下命令：<br>
cd $HBASE_HOME/bin<br>
./hbase zkcli<br>
ls /<br>
rmr /hbase<br>
ls /<br>
退出 zookeeper cli， 删除hdfs中的 /hbase<br>
hdfs dfs -rm -r /hbase<br>
然后重启hbase:<br>
cd $HBASE_HOME/bin<br>
./stop-hbase.sh<br>
./start-hbase.sh<br>
若stop hbase的时候出现 ..... 停止不掉， 则：<br>
cd $HBASE_HOME/bin<br>
./hbase-daemons.sh stop regionserver</p>
<pre><code># ./start-hbase.sh
# kill -9 pid来终止hbase的进程
</code></pre>
<h1 id="hbase命令">HBase命令</h1>
<p>https://blog.csdn.net/vbirdbest/article/details/88236575</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RS => 推荐系统（二）离线画像构建 ]]></title>
        <id>https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-er-chi-xian-hua-xiang-gou-jian/</id>
        <link href="https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-er-chi-xian-hua-xiang-gou-jian/">
        </link>
        <updated>2020-09-29T04:20:35.000Z</updated>
        <content type="html"><![CDATA[<h1 id="文章离线画像构建">文章离线画像构建</h1>
<h3 id="spark配置基类抽取">Spark配置基类抽取</h3>
<pre><code>from pyspark import SparkConf
from pyspark.sql import SparkSession
import os

class SparkSessionBase(object):

	SPARK_APP_NAME = None
	SPARK_URL = &quot;yarn&quot;

	SPARK_EXECUTOR_MEMORY = &quot;2g&quot;
	SPARK_EXECUTOR_CORES = 2
	SPARK_EXECUTOR_INSTANCES = 2

	ENABLE_HIVE_SUPPORT = False

	def _create_spark_session(self):
		conf = SparkConf()  # 创建spark config对象
		config = (
			(&quot;spark.app.name&quot;, self.SPARK_APP_NAME),  # 设置启动的spark的app名称，没有提供，将随机产生一个名称
			(&quot;spark.executor.memory&quot;, self.SPARK_EXECUTOR_MEMORY),  # 设置该app启动时占用的内存用量，默认2g
	 		(&quot;spark.master&quot;, self.SPARK_URL),  # spark master的地址
			(&quot;spark.executor.cores&quot;, self.SPARK_EXECUTOR_CORES),  # 设置spark executor使用的CPU核心数，默认是1核心
			(&quot;spark.executor.instances&quot;, self.SPARK_EXECUTOR_INSTANCES),
			(&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;),
		)

	conf.setAll(config)

	# 利用config对象，创建spark session
	if self.ENABLE_HIVE_SUPPORT:
		return SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()
	else:
		return SparkSession.builder.config(conf=conf).getOrCreate()
</code></pre>
<h3 id="主应用导入基类">主应用导入基类</h3>
<pre><code># pip install pyspark
# pip install findspark

import findspark
findspark.init()

import os
import sys
# 如果当前代码文件运行测试需要加入修改路径，避免出现后导包问题
BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))
sys.path.insert(0, os.path.join(BASE_DIR))
print(BASE_DIR)
PYSPARK_PYTHON = &quot;/miniconda2/envs/reco_sys/bin/python&quot;
# 当存在多个版本时，不指定很可能会导致出错
os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHON
os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHON
from offline import SparkSessionBase

class OriginArticleData(SparkSessionBase):

	SPARK_APP_NAME = &quot;mergeArticle&quot;
	SPARK_URL = &quot;yarn&quot;

	ENABLE_HIVE_SUPPORT = True

	def __init__(self):
		self.spark = self._create_spark_session()

oa = OriginArticleData()   # oa就是带有配置的 sparkSession的实例化对象
</code></pre>
<h3 id="文章-表-合并">文章 表 合并</h3>
<p>文章基本信息表+文章内容表+频道表：<br>
titlce_content = oa.spark.sql(&quot;select a.article_id, a.channel_id, a.title, b.content from news_article_basic a inner join news_article_content b on a.article_id=b.article_id)<br>
因为得到的是 DF类型，想要用SQL，可以把DF注册为临时表<br>
titlce_content.registerTempTable('temptable')<br>
再把 频道表 的 频道名 合并进来<br>
channel_title_content = oa.spark.sql(&quot;select t.*, n.channel_name from temptable t left join news_channel n on t.channel_id=n.channel_id&quot;)</p>
<h3 id="文章-字段-合并">文章 字段 合并</h3>
<p>将 文章标题+文章内容+文章频道 的列，拼接成一个大字符串<br>
import pyspark.sql.functions as F<br>
import gc</p>
<pre><code># 增加channel的名字，后面会使用
basic_content.registerTempTable(&quot;temparticle&quot;)
channel_basic_content = oa.spark.sql(
	&quot;select t.*, n.channel_name from temparticle t left join news_channel n on t.channel_id=n.channel_id&quot;
)

# 利用concat_ws方法，将多列数据合并为一个长文本内容（频道，标题以及内容合并）
oa.spark.sql(&quot;use article&quot;)
sentence_df = channel_basic_content.select(&quot;article_id&quot;, &quot;channel_id&quot;, &quot;channel_name&quot;, &quot;title&quot;, &quot;content&quot;, \
	F.concat_ws(
		&quot;,&quot;,											# 指定大字符串分隔符
		channel_basic_content.channel_name,         
		channel_basic_content.title,
		channel_basic_content.content					
		).alias(&quot;sentence&quot;)                    # 新列 大字符串 取名
	)
del basic_content
del channel_basic_content
gc.collect()

# sentence_df.write.insertInto(&quot;article_data&quot;)       # 写入提前创建好的Hive表中
</code></pre>
<h3 id="分词">分词</h3>
<pre><code>def segmentation(partition):          # 就这一行的缩进需要调整下
import os
import re

import jieba
import jieba.analyse
import jieba.posseg as pseg
import codecs

abspath = &quot;/root/words&quot;

# 结巴加载用户词典
userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;)
jieba.load_userdict(userDict_path)

# 停用词文本
stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;)

def get_stopwords_list():
    &quot;&quot;&quot;返回stopwords列表&quot;&quot;&quot;
    stopwords_list = [i.strip()
                      for i in codecs.open(stopwords_path).readlines()]
    return stopwords_list

# 所有的停用词列表
stopwords_list = get_stopwords_list()

# 分词
def cut_sentence(sentence):
    &quot;&quot;&quot;对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词&quot;&quot;&quot;
    # print(sentence,&quot;*&quot;*100)
    # eg:[pair('今天', 't'), pair('有', 'd'), pair('雾', 'n'), pair('霾', 'g')]
    seg_list = pseg.lcut(sentence)
    seg_list = [i for i in seg_list if i.flag not in stopwords_list]
    filtered_words_list = []
    for seg in seg_list:
        # print(seg)
        if len(seg.word) &lt;= 1:
            continue
        elif seg.flag == &quot;eng&quot;:
            if len(seg.word) &lt;= 2:
                continue
            else:
                filtered_words_list.append(seg.word)
        elif seg.flag.startswith(&quot;n&quot;):
            filtered_words_list.append(seg.word)
        elif seg.flag in [&quot;x&quot;, &quot;eng&quot;]:  # 是自定一个词语或者是英文单词
            filtered_words_list.append(seg.word)
    return filtered_words_list

for row in partition:
    sentence = re.sub(&quot;&lt;.*?&gt;&quot;, &quot;&quot;, row.sentence)    # 替换掉标签数据
    words = cut_sentence(sentence)
    yield row.article_id, row.channel_id, words
</code></pre>
<h3 id="计算-tf-idf">计算 TF-IDF</h3>
<p>TF:<br>
ktt.spark.sql(&quot;use article&quot;)<br>
article_dataframe = ktt.spark.sql(&quot;select * from article_data limit 20&quot;)<br>
words_df = article_dataframe.rdd.mapPartitions(segmentation).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;words&quot;])</p>
<pre><code>from pyspark.ml.feature import CountVectorizer
# 总词汇的大小，文本中必须出现的次数
cv = CountVectorizer(inputCol=&quot;words&quot;, outputCol=&quot;countFeatures&quot;, vocabSize=200*10000, minDF=1.0)
# 训练词频统计模型
cv_model = cv.fit(words_df)
cv_model.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;)

# cv_model.vocabulary 查看统计词表（相当于groupby结果的 key,  但不包括value）
</code></pre>
<p>训练TF-IDF:<br>
# 词语与词频统计<br>
from pyspark.ml.feature import CountVectorizerModel<br>
cv_model = CountVectorizerModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;)<br>
# 得出词频向量结果<br>
cv_result = cv_model.transform(words_df)<br>
# 训练IDF模型 (把 tf结果传进去，其实说是 IDF模型，计算结果得出的就是 TF-IDF)<br>
from pyspark.ml.feature import IDF<br>
idf = IDF(inputCol=&quot;countFeatures&quot;, outputCol=&quot;idfFeatures&quot;)<br>
idfModel = idf.fit(cv_result)<br>
idfModel.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/IDF.model&quot;)</p>
<pre><code># idfModel.idf.toArray()[:20]    查看逆文档频率矩阵
</code></pre>
<p>TF-IDF结果数据格式：<br>
列1， 列...， 列 TF-IDF<br>
(1000,[804,1032],[6.349777077,7.0761797]) 。。。<br>
使用TF-IDF模型，取Top-K个词：<br>
from pyspark.ml.feature import CountVectorizerModel<br>
cv_model = CountVectorizerModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model&quot;)<br>
from pyspark.ml.feature import IDFModel<br>
idf_model = IDFModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model&quot;)<br>
cv_result = cv_model.transform(words_df)<br>
tfidf_result = idf_model.transform(cv_result)</p>
<pre><code>def func(partition):            
	TOPK = 20
	for row in partition:
		# 找到索引与IDF值并进行排序
		_ = list(zip(row.idfFeatures.indices, row.idfFeatures.values))   # [ (indexes,values)，  (indexes,values)]
		_ = sorted(_, key=lambda x: x[1], reverse=True)
		result = _[:TOPK]
		for word_index, tfidf in result:
			yield row.article_id, row.channel_id, int(word_index), round(float(tfidf), 4)
			# yield这句注定了返回结果格式 (多层for循环 yield， 原本一行数据按每个单词爆炸展开)
			# article_id,   channel_id,   word_index,   tfidf
			# 1				 100         40         15.5
			# 1				 100         14         10.3         
			# 1				 100         23         13.2
            
            
_keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;index&quot;, &quot;tfidf&quot;])
</code></pre>
<p>我们的目标是： 构成  词+TFIDF值, 而不是索引+TFIDF<br>
cv_model.vocabulary 结果是所有单词的列表。 上面的 index就是对应这个列表的索引<br>
最终构建一个词典+索引表：<br>
index	word<br>
..		..	<br>
然后将 主表（文章id,频道id,索引，tfidf）与 词典表（index+word） 合并<br>
得到  （文章id,频道id, 词， tfidf）</p>
<h3 id="计算-textrank">计算 TextRank</h3>
<p>TextRank和核心就是设定一个固定窗口来滑动<br>
把每个窗口内的每个词， 设为字典的Key, value就是他附近的n个词的列表<br>
然后每个词都这样做， 遇到相同的词就追加到字典的value 列表中<br>
# 分词<br>
def textrank(partition):<br>
import os</p>
<pre><code>import jieba
import jieba.analyse
import jieba.posseg as pseg
import codecs

abspath = &quot;/root/words&quot;

# 结巴加载用户词典
userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;)
jieba.load_userdict(userDict_path)

# 停用词文本
stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;)

def get_stopwords_list():
    &quot;&quot;&quot;返回stopwords列表&quot;&quot;&quot;
    stopwords_list = [i.strip()
                      for i in codecs.open(stopwords_path).readlines()]
    return stopwords_list

# 所有的停用词列表
stopwords_list = get_stopwords_list()

class TextRank(jieba.analyse.TextRank):
    def __init__(self, window=20, word_min_len=2):
        super(TextRank, self).__init__()
        self.span = window  # 窗口大小
        self.word_min_len = word_min_len  # 单词的最小长度
        # 要保留的词性，根据jieba github ，具体参见https://github.com/baidu/lac
        self.pos_filt = frozenset(
            ('n', 'x', 'eng', 'f', 's', 't', 'nr', 'ns', 'nt', &quot;nw&quot;, &quot;nz&quot;, &quot;PER&quot;, &quot;LOC&quot;, &quot;ORG&quot;))

    def pairfilter(self, wp):
        &quot;&quot;&quot;过滤条件，返回True或者False&quot;&quot;&quot;

        if wp.flag == &quot;eng&quot;:
            if len(wp.word) &lt;= 2:
                return False

        if wp.flag in self.pos_filt and len(wp.word.strip()) &gt;= self.word_min_len \
                and wp.word.lower() not in stopwords_list:
            return True
# TextRank过滤窗口大小为5，单词最小为2
textrank_model = TextRank(window=5, word_min_len=2)
allowPOS = ('n', &quot;x&quot;, 'eng', 'nr', 'ns', 'nt', &quot;nw&quot;, &quot;nz&quot;, &quot;c&quot;)

for row in partition:
    tags = textrank_model.textrank(row.sentence, topK=20, withWeight=True, allowPOS=allowPOS, withFlag=False)
    for tag in tags:
        yield row.article_id, row.channel_id, tag[0], tag[1]

# 计算textrank
textrank_keywords_df = article_dataframe.rdd.mapPartitions(textrank).toDF(
	[&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;textrank&quot;]
)

# textrank_keywords_df.write.insertInto(&quot;textrank_keywords_values&quot;)
</code></pre>
<p>textrank运行结果如下：<br>
hive&gt; select * from textrank_keywords_values limit 20;<br>
OK<br>
文章ID  channel   word    textrank<br>
98319   17      var     20.6079<br>
98323   17      var     7.4938<br>
98326   17      var     104.9128<br>
然后和 tfidf一样 根据 textrank值，  取TOP-K个词</p>
<h3 id="计算-主题词-和-关键词">计算 主题词 和 关键词</h3>
<p>关键词：TEXTRANK计算出的结果TOPK个词以及权重<br>
主题词：TEXTRANK的TOPK词 与 ITFDF计算的TOPK个词的交集<br>
格式如下：<br>
hive&gt; desc article_profile;<br>
OK<br>
article_id              int                     article_id<br>
channel_id              int                     channel_id<br>
keywords               map								 keywords<br>
topics						  array								topics</p>
<pre><code>hive&gt; select * from article_profile limit 1;    
# 这里把结果按行排列开方便观看
article_id			26
channel_id		   17            
关键词字典		  {&quot;策略&quot;:0.3973770571351729,&quot;jpg&quot;:0.9806348975390871,&quot;用户&quot;:1.2794959063944176,&quot;strong&quot;:1.6488457985625076,&quot;文件&quot;:0.28144603583387057,&quot;逻辑&quot;:0.45256526469610714,&quot;形式&quot;:0.4123994242601279,&quot;全自&quot;:0.9594604850547191,&quot;h2&quot;:0.6244481634710125,&quot;版本&quot;:0.44280276959510817,&quot;Adobe&quot;:0.8553618185108718,&quot;安装&quot;:0.8305037437573172,&quot;检查更新&quot;:1.8088946300014435,&quot;产品&quot;:0.774842382276899,&quot;下载页&quot;:1.4256311032544344,&quot;过程&quot;:0.19827163395829256,&quot;json&quot;:0.6423301791599972,&quot;方式&quot;:0.582762869780791,&quot;退出应用&quot;:1.2338671268242603,&quot;Setup&quot;:1.004399549339134} 

主题词列表				[&quot;Electron&quot;,&quot;全自动&quot;,&quot;产品&quot;,&quot;版本号&quot;,&quot;安装包&quot;,&quot;检查更新&quot;,&quot;方案&quot;,&quot;版本&quot;,&quot;退出应用&quot;,&quot;逻辑&quot;,&quot;安装过程&quot;,&quot;方式&quot;,&quot;定性&quot;,&quot;新版本&quot;,&quot;Setup&quot;,&quot;静默&quot;,&quot;用户&quot;]
</code></pre>
<h3 id="增量更新-离线文章画像">增量更新 离线文章画像</h3>
<p>更新流程：<br>
1、toutiao 数据库中，news_article_content 与news_article_basic—&gt;更新到article数据库中article_data表，方便操作<br>
2. 第一次：所有更新，后面增量每天的数据更新26日：1：00<sub>2：00，2：00</sub>3：00，左闭右开,一个小时更新一次<br>
3、刚才新更新的文章，通过已有的idf计算出tfidf值以及hive 的textrank_keywords_values<br>
4、更新hive的article_profile</p>
<p>离线更新文章画像 代码组装：Pycharm<br>
注意在Pycharm中运行要设置环境：</p>
<pre><code>PYTHONUNBUFFERED=1
JAVA_HOME=/root/bigdata/jdk
SPARK_HOME=/root/bigdata/spark
HADOOP_HOME=/root/bigdata/hadoop
PYSPARK_PYTHON=/root/anaconda3/envs/reco_sys/bin/python
PYSPARK_DRIVER_PYTHON=/root/anaconda3/envs/reco_sys/bin/python
</code></pre>
<p>具体代码如下：<br>
import os<br>
import sys<br>
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(<strong>file</strong>)))<br>
sys.path.insert(0, os.path.join(BASE_DIR))<br>
from offline import SparkSessionBase<br>
from datetime import datetime<br>
from datetime import timedelta<br>
import pyspark.sql.functions as F<br>
import pyspark<br>
import gc</p>
<pre><code>class UpdateArticle(SparkSessionBase):
&quot;&quot;&quot;
更新文章画像
&quot;&quot;&quot;
SPARK_APP_NAME = &quot;updateArticle&quot;
ENABLE_HIVE_SUPPORT = True

SPARK_EXECUTOR_MEMORY = &quot;7g&quot;

def __init__(self):
    self.spark = self._create_spark_session()

    self.cv_path = &quot;hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model&quot;
    self.idf_path = &quot;hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model&quot;

def get_cv_model(self):
    # 词语与词频统计
    from pyspark.ml.feature import CountVectorizerModel
    cv_model = CountVectorizerModel.load(self.cv_path)
    return cv_model

def get_idf_model(self):
    from pyspark.ml.feature import IDFModel
    idf_model = IDFModel.load(self.idf_path)
    return idf_model

@staticmethod
def compute_keywords_tfidf_topk(words_df, cv_model, idf_model):
    &quot;&quot;&quot;保存tfidf值高的20个关键词
    :param spark:
    :param words_df:
    :return:
    &quot;&quot;&quot;
    cv_result = cv_model.transform(words_df)
    tfidf_result = idf_model.transform(cv_result)
    # print(&quot;transform compelete&quot;)

    # 取TOP-N的TFIDF值高的结果
    def func(partition):
        TOPK = 20
        for row in partition:
            _ = list(zip(row.idfFeatures.indices, row.idfFeatures.values))
            _ = sorted(_, key=lambda x: x[1], reverse=True)
            result = _[:TOPK]
            #         words_index = [int(i[0]) for i in result]
            #         yield row.article_id, row.channel_id, words_index

            for word_index, tfidf in result:
                yield row.article_id, row.channel_id, int(word_index), round(float(tfidf), 4)

    _keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;index&quot;, &quot;tfidf&quot;])

    return _keywordsByTFIDF

def merge_article_data(self):
    &quot;&quot;&quot;
    合并业务中增量更新的文章数据
    :return:
    &quot;&quot;&quot;
    # 获取文章相关数据, 指定过去一个小时整点到整点的更新数据
    # 如：26日：1：00~2：00，2：00~3：00，左闭右开
    self.spark.sql(&quot;use toutiao&quot;)
    _yester = datetime.today().replace(minute=0, second=0, microsecond=0)
    start = datetime.strftime(_yester + timedelta(days=0, hours=-1, minutes=0), &quot;%Y-%m-%d %H:%M:%S&quot;)
    end = datetime.strftime(_yester, &quot;%Y-%m-%d %H:%M:%S&quot;)

    # 合并后保留：article_id、channel_id、channel_name、title、content
    # +----------+----------+--------------------+--------------------+
    # | article_id | channel_id | title | content |
    # +----------+----------+--------------------+--------------------+
    # | 141462 | 3 | test - 20190316 - 115123 | 今天天气不错，心情很美丽！！！ |
    basic_content = self.spark.sql(
        &quot;select a.article_id, a.channel_id, a.title, b.content from news_article_basic a &quot;
        &quot;inner join news_article_content b on a.article_id=b.article_id where a.review_time &gt;= '{}' &quot;
        &quot;and a.review_time &lt; '{}' and a.status = 2&quot;.format(start, end))
    # 增加channel的名字，后面会使用
    basic_content.registerTempTable(&quot;temparticle&quot;)
    channel_basic_content = self.spark.sql(
        &quot;select t.*, n.channel_name from temparticle t left join news_channel n on t.channel_id=n.channel_id&quot;)

    # 利用concat_ws方法，将多列数据合并为一个长文本内容（频道，标题以及内容合并）
    self.spark.sql(&quot;use article&quot;)
    sentence_df = channel_basic_content.select(&quot;article_id&quot;, &quot;channel_id&quot;, &quot;channel_name&quot;, &quot;title&quot;, &quot;content&quot;, \
                                               F.concat_ws(
                                                   &quot;,&quot;,
                                                   channel_basic_content.channel_name,
                                                   channel_basic_content.title,
                                                   channel_basic_content.content
                                               ).alias(&quot;sentence&quot;)
                                               )
    del basic_content
    del channel_basic_content
    gc.collect()

    sentence_df.write.insertInto(&quot;article_data&quot;)
    return sentence_df

def generate_article_label(self, sentence_df):
    &quot;&quot;&quot;
    生成文章标签  tfidf, textrank
    :param sentence_df: 增量的文章内容
    :return:
    &quot;&quot;&quot;
    # 进行分词
    words_df = sentence_df.rdd.mapPartitions(segmentation).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;words&quot;])
    cv_model = self.get_cv_model()
    idf_model = self.get_idf_model()

    # 1、保存所有的词的idf的值，利用idf中的词的标签索引
    # 工具与业务隔离
    _keywordsByTFIDF = UpdateArticle.compute_keywords_tfidf_topk(words_df, cv_model, idf_model)

    keywordsIndex = self.spark.sql(&quot;select keyword, index idx from idf_keywords_values&quot;)

    keywordsByTFIDF = _keywordsByTFIDF.join(keywordsIndex, keywordsIndex.idx == _keywordsByTFIDF.index).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;tfidf&quot;])

    keywordsByTFIDF.write.insertInto(&quot;tfidf_keywords_values&quot;)

    del cv_model
    del idf_model
    del words_df
    del _keywordsByTFIDF
    gc.collect()

    # 计算textrank
    textrank_keywords_df = sentence_df.rdd.mapPartitions(textrank).toDF(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;textrank&quot;])
    textrank_keywords_df.write.insertInto(&quot;textrank_keywords_values&quot;)

    return textrank_keywords_df, keywordsIndex

def get_article_profile(self, textrank, keywordsIndex):
    &quot;&quot;&quot;
    文章画像主题词建立
    :param idf: 所有词的idf值
    :param textrank: 每个文章的textrank值
    :return: 返回建立号增量文章画像
    &quot;&quot;&quot;
    keywordsIndex = keywordsIndex.withColumnRenamed(&quot;keyword&quot;, &quot;keyword1&quot;)
    result = textrank.join(keywordsIndex, textrank.keyword == keywordsIndex.keyword1)

    # 1、关键词（词，权重）
    # 计算关键词权重
    _articleKeywordsWeights = result.withColumn(&quot;weights&quot;, result.textrank * result.idf).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;weights&quot;])

    # 合并关键词权重到字典
    _articleKeywordsWeights.registerTempTable(&quot;temptable&quot;)
    articleKeywordsWeights = self.spark.sql(
        &quot;select article_id, min(channel_id) channel_id, collect_list(keyword) keyword_list, collect_list(weights) weights_list from temptable group by article_id&quot;)
    def _func(row):
        return row.article_id, row.channel_id, dict(zip(row.keyword_list, row.weights_list))
    articleKeywords = articleKeywordsWeights.rdd.map(_func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keywords&quot;])

    # 2、主题词
    # 将tfidf和textrank共现的词作为主题词
    topic_sql = &quot;&quot;&quot;
            select t.article_id article_id2, collect_set(t.keyword) topics from tfidf_keywords_values t
            inner join 
            textrank_keywords_values r
            where t.keyword=r.keyword
            group by article_id2
            &quot;&quot;&quot;
    articleTopics = self.spark.sql(topic_sql)

    # 3、将主题词表和关键词表进行合并，插入表
    articleProfile = articleKeywords.join(articleTopics,
                                          articleKeywords.article_id == articleTopics.article_id2).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keywords&quot;, &quot;topics&quot;])
    articleProfile.write.insertInto(&quot;article_profile&quot;)

    del keywordsIndex
    del _articleKeywordsWeights
    del articleKeywords
    del articleTopics
    gc.collect()

    return articleProfile


if __name__ == '__main__':
ua = UpdateArticle()
sentence_df = ua.merge_article_data()
if sentence_df.rdd.collect():
    rank, idf = ua.generate_article_label(sentence_df)
    articleProfile = ua.get_article_profile(rank, idf)
</code></pre>
<p>使用工具：Supervisor+Apscheduler<br>
# pip install APScheduler<br>
from apscheduler.schedulers.blocking import BlockingScheduler<br>
from apscheduler.executors.pool import ProcessPoolExecutor</p>
<pre><code>from scheduler.update import update_article_profile

# 创建scheduler，多进程执行
executors = {
	'default': ProcessPoolExecutor(3)
}
scheduler = BlockingScheduler(executors=executors)
# 添加定时更新任务更新文章画像,每隔一小时更新， trigger还有其他定时方式
scheduler.add_job(update_article_profile, trigger='interval', hours=1)
scheduler.start()
</code></pre>
<p>自定义Logger:<br>
import logging<br>
import logging.handlers<br>
import os</p>
<pre><code>logging_file_dir = '/root/logs/'
def create_logger():
	# 离线处理更新打印日志
	 log_trace = logging.getLogger('offline')
     
	trace_file_handler = logging.FileHandler(
        os.path.join(logging_file_dir, 'offline.log')
    )
	 trace_file_handler.setFormatter(logging.Formatter('%(message)s'))
    
    log_trace.addHandler(trace_file_handler)
    log_trace.setLevel(logging.INFO)
</code></pre>
<p>supervisor管理apscheduler:<br>
[program:offline]<br>
environment=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python<br>
command=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/scheduler/main.py<br>
directory=/root/toutiao_project/scheduler<br>
user=root<br>
autorestart=true<br>
redirect_stderr=true<br>
stdout_logfile=/root/logs/offlinesuper.log<br>
loglevel=info<br>
stopsignal=KILL<br>
stopasgroup=true<br>
killasgroup=true</p>
<h3 id="word2vec与文章相似度">Word2Vec与文章相似度</h3>
<pre><code>w2v.spark.sql(&quot;use article&quot;)
article = w2v.spark.sql(&quot;select * from article_data where channel_id=18 limit 2&quot;)
words_df = article.rdd.mapPartitions(segmentation).toDF(['article_id', 'channel_id', 'words'])
</code></pre>
<p>Spark Word2Vec API介绍：<br>
模块：from pyspark.ml.feature import Word2Vec</p>
<pre><code>API：class pyspark.ml.feature.Word2Vec(vectorSize=100, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, seed=None, inputCol=None, outputCol=None, windowSize=5, maxSentenceLength=1000)
参数说明：
	vectorSize=100: 词向量长度
	minCount：过滤次数小于默认5次的词
	windowSize=5：训练时候的窗口大小
	inputCol=None：输入列名
	outputCol=None：输出列名
</code></pre>
<p>Spark Word2Vec训练保存模型：<br>
new_word2Vec = Word2Vec(vectorSize=100, inputCol=&quot;words&quot;, outputCol=&quot;model&quot;, minCount=3)<br>
new_model = new_word2Vec.fit(words_df)<br>
new_model.save(&quot;hdfs://hadoop-master:9000/headlines/models/test.word2vec&quot;)<br>
上传历史数据训练的模型：<br>
hadoop dfs -put ./word2vec_model /headlines/models/</p>
<h3 id="增量更新-文章向量计算">增量更新-文章向量计算</h3>
<p>有了词向量之后，我们就可以得到一篇文章的向量了，为了后面快速使用文章的向量，我们会将每个频道所有的文章向量保存起来。</p>
<p>目的：保存所有历史训练的文章向量<br>
步骤：<br>
1、加载某个频道模型，得到每个词的向量<br>
2、获取频道的文章画像，得到文章画像的关键词(接着之前增量更新的文章article_profile)<br>
3、计算得到文章每个词的向量<br>
4、计算得到文章的平均词向量即文章的向量<br>
加载某个频道模型，得到每个词的向量<br>
from pyspark.ml.feature import Word2VecModel<br>
channel_id = 18<br>
channel = &quot;python&quot;<br>
wv_model = Word2VecModel.load(<br>
&quot;hdfs://hadoop-master:9000/headlines/models/word2vec_model/channel_%d_%s.word2vec&quot; % (channel_id, channel))<br>
vectors = wv_model.getVectors()<br>
获取新增的文章画像，得到文章画像的关键词：<br>
# 选出新增的文章的画像做测试，上节计算的画像中有不同频道的，我们选取Python频道的进行计算测试<br>
# 新增的文章画像获取部分<br>
profile = w2v.spark.sql(&quot;select * from article_profile where channel_id=18 limit 10&quot;)<br>
# profile = articleProfile.filter('channel_id = {}'.format(channel_id))</p>
<pre><code>profile.registerTempTable(&quot;incremental&quot;)
articleKeywordsWeights = w2v.spark.sql(
                &quot;select article_id, channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight&quot;)
_article_profile = articleKeywordsWeights.join(vectors, vectors.word==articleKeywordsWeights.keyword, &quot;inner&quot;)
</code></pre>
<p>计算得到文章的平均词向量即文章的向量<br>
def avg(row):<br>
x = 0<br>
for v in row.vectors:<br>
x += v<br>
#  将平均向量作为article的向量<br>
return row.article_id, row.channel_id, x / len(row.vectors)</p>
<pre><code>articleKeywordVectors.registerTempTable(&quot;tempTable&quot;)
articleVector = w2v.spark.sql(
	&quot;select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from tempTable group by article_id&quot;).rdd \
    # 分组之后， 求map的平均之前， 结果是  artile_id, channel_id, vector_list # vector_list 是二维数组。
    .map(avg).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;articleVector&quot;])
    # 求map的平均之后结果是 article_id, channel_id, article_vector # article_vector 代表文章向量
</code></pre>
<h3 id="文章相似度计算">文章相似度计算</h3>
<h4 id="存在的问题以进行某频道全量所有的两两相似度计算-但是事实当文章量达到千万级别或者上亿级别特征也会上亿级别计算量就会很大-以下有两种类型解决方案">存在的问题：以进行某频道全量所有的两两相似度计算。但是事实当文章量达到千万级别或者上亿级别，特征也会上亿级别，计算量就会很大。以下有两种类型解决方案：</h4>
<ol>
<li>每个频道的文章先进行聚类（缺点，（分成几个簇）也是个超参数）</li>
<li>局部敏感哈希LSH(Locality Sensitive Hashing)<br>
基本思想1：LSH算法基于一个假设，如果两个文本在原有的数据空间是相似的，那么分别经过哈希函数转换以后的它们也具有很高的相似度<br>
基本思想2: 经常使用的哈希函数，冲突总是难以避免。LSH却依赖于冲突，在解决NNS(Nearest neighbor search )时，我们期望：<br>
离得越近的对象，发生冲突的概率越高<br>
离得越远的对象，发生冲突的概率越低</li>
</ol>
<h4 id="局部敏感哈希lshlocality-sensitive-hashing-lsh过程">局部敏感哈希LSH(Locality Sensitive Hashing) LSH过程：</h4>
<p>mini hashing(略)	<br>
Random Projection（特征压缩）：<br>
Random Projection是一种随机算法.随机投影的算法有很多，如PCA、Gaussian random projection - 高斯随机投影。<br>
随机桶投影是用于欧几里德距离的 LSH family。其LSH family将x特征向量映射到随机单位矢量v，并将映射结果分为哈希桶中。哈希表中的每个位置表示一个哈希桶。	<br>
使得：<br>
离得越近的对象，发生冲突的概率越高<br>
离得越远的对象，发生冲突的概率越低</p>
<h4 id="代码实现">代码实现：</h4>
<p>读取数据，进行类型处理(数组转换类型为Vector)：<br>
from pyspark.ml.linalg import Vectors<br>
# 选取部分数据做测试<br>
article_vector = w2v.spark.sql(&quot;select article_id, articlevector from article_vector where channel_id=18 limit 10&quot;)<br>
train = articlevector.select(['article_id', 'articleVector'])</p>
<pre><code>def _array_to_vector(row):
	return row.article_id, Vectors.dense(row.articleVector)

train = train.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])
</code></pre>
<p>相似度计算（BRP进行FIT）：<br>
函数参数说明：<br>
class pyspark.ml.feature.BucketedRandomProjectionLSH(inputCol=None, outputCol=None, seed=None, numHashTables=1, bucketLength=None)<br>
inputCol=None：输入特征列<br>
outputCol=None：输出特征列<br>
numHashTables=1：哈希表数量，几个hash function对数据进行hash操作<br>
bucketLength=None：桶的数量，值越大相同数据进入到同一个桶的概率越高<br>
method:<br>
# 计算df1每个文章相似的df2数据集的数据<br>
approxSimilarityJoin(df1, df2, 2.0, distCol='EuclideanDistance')  # 转为向量</p>
<pre><code># 代码调用：
from pyspark.ml.feature import BucketedRandomProjectionLSH

brp = BucketedRandomProjectionLSH(inputCol='articleVector', outputCol='hashes', numHashTables=4.0, bucketLength=10.0)
model = brp.fit(旧文章向量)
</code></pre>
<p>计算相似的文章以及相似度<br>
# 计算文章和文章之间的相似度<br>
similar = model.approxSimilarityJoin(新增文章向量, 新增文章向量, 2.0, distCol='Similarity')  # 输出列名<br>
similar.sort(['EuclideanDistance']).show()<br>
计算结果：<br>
datasetA(新增),    datasetB（旧的）,     Similarity<br>
[2,[文章向量]]		[5,[文章向量]]			0.0051<br>
[1,[文章向量]]		[3,[文章向量]]			0.0054<br>
[2,[文章向量]]		[8,[文章向量]]			0.0053<br>
[1,[文章向量]]		[4,[文章向量]]			0.0052<br>
[2,[文章向量]]		[7,[文章向量]]			0.0055<br>
...</p>
<h3 id="文章相似度存储-hbase">文章相似度存储 HBase</h3>
<h4 id="存储目标存储-文章相似文章-相似度">存储目标：存储 文章，相似文章， 相似度</h4>
<p>调用foreachPartition：<br>
foreachPartition不同于map和mapPartition。<br>
无返回结果，主要用于离线分析之后的数据（数据库存储等）落地<br>
如果想要返回新的一个数据DF，就使用map后者。<br>
我们需要建立一个HBase存储文章相似度的表：<br>
create 'article_similar', 'similar'</p>
<pre><code># 存储格式如下：
		 表           row_key  column_family   value
	put 'article_similar', '1', 	'similar:1',    0.2
	put 'article_similar', '1', 	'similar:2',    0.34
</code></pre>
<p>HBase 开启失败可能的原因的：</p>
<ol>
<li>
<p>时间未同步的解决办法：<br>
ntpdate 0.cn.pool.ntp.org<br>
或<br>
ntpdate ntp1.aliyun.com</p>
</li>
<li>
<p>thrift服务未开启的解决办法：<br>
hbase-daemon.sh start thrift<br>
happybase代码实现：<br>
def save_hbase(partition):<br>
import happybase<br>
pool = happybase.ConnectionPool(size=3, host='hadoop-master')</p>
<p>with pool.connection() as conn:<br>
# 建议表的连接<br>
table = conn.table('article_similar')<br>
for row in partition:<br>
if row.datasetA.article_id == row.datasetB.article_id:<br>
pass<br>
else:<br>
table.put(str(row.datasetA.article_id).encode(),<br>
{&quot;similar:{}&quot;.format(row.datasetB.article_id).encode(): b'%0.4f' % (row.EuclideanDistance)})<br>
# 手动关闭所有的连接<br>
conn.close()</p>
<p>similar.foreachPartition(save_hbase)</p>
</li>
</ol>
<h3 id="文章相似度增量更新代码整理">文章相似度增量更新代码整理</h3>
<pre><code>def compute_article_similar(self, articleProfile):
    &quot;&quot;&quot;
    计算增量文章与历史文章的相似度 word2vec
    :return:
    &quot;&quot;&quot;
    # 得到要更新的新文章通道类别(不采用)
    # all_channel = set(articleProfile.rdd.map(lambda x: x.channel_id).collect())
    def avg(row):
        x = 0
        for v in row.vectors:
            x += v
        #  将平均向量作为article的向量
        return row.article_id, row.channel_id, x / len(row.vectors)

    for channel_id, channel_name in CHANNEL_INFO.items():

        profile = articleProfile.filter('channel_id = {}'.format(channel_id))
        wv_model = Word2VecModel.load(
            &quot;hdfs://hadoop-master:9000/headlines/models/channel_%d_%s.word2vec&quot; % (channel_id, channel_name))
        vectors = wv_model.getVectors()

        # 计算向量
        profile.registerTempTable(&quot;incremental&quot;)
        articleKeywordsWeights = ua.spark.sql(
            &quot;select article_id, channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight where channel_id=%d&quot; % channel_id)

        articleKeywordsWeightsAndVectors = articleKeywordsWeights.join(vectors,
                                                        vectors.word == articleKeywordsWeights.keyword, &quot;inner&quot;)
        articleKeywordVectors = articleKeywordsWeightsAndVectors.rdd.map(
            lambda r: (r.article_id, r.channel_id, r.keyword, r.weight * r.vector)).toDF(
            [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;weightingVector&quot;])

        articleKeywordVectors.registerTempTable(&quot;tempTable&quot;)
        articleVector = self.spark.sql(
            &quot;select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from tempTable group by article_id&quot;).rdd.map(
            avg).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;articleVector&quot;])

        # 写入数据库
        def toArray(row):
            return row.article_id, row.channel_id, [float(i) for i in row.articleVector.toArray()]
        articleVector = articleVector.rdd.map(toArray).toDF(['article_id', 'channel_id', 'articleVector'])
        articleVector.write.insertInto(&quot;article_vector&quot;)

        import gc
        del wv_model
        del vectors
        del articleKeywordsWeights
        del articleKeywordsWeightsAndVectors
        del articleKeywordVectors
        gc.collect()

        # 得到历史数据, 转换成固定格式使用LSH进行求相似
        train = self.spark.sql(&quot;select * from article_vector where channel_id=%d&quot; % channel_id)

        def _array_to_vector(row):
            return row.article_id, Vectors.dense(row.articleVector)
        train = train.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])
        test = articleVector.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])

        brp = BucketedRandomProjectionLSH(inputCol='articleVector', outputCol='hashes', seed=12345,
                                          bucketLength=1.0)
        model = brp.fit(train)
        similar = model.approxSimilarityJoin(test, train, 2.0, distCol='EuclideanDistance')

        def save_hbase(partition):
            import happybase
            for row in partition:
                pool = happybase.ConnectionPool(size=3, host='hadoop-master')
                # article_similar article_id similar:article_id sim
                with pool.connection() as conn:
                    table = connection.table(&quot;article_similar&quot;)
                    for row in partition:
                        if row.datasetA.article_id == row.datasetB.article_id:
                            pass
                        else:
                            table.put(str(row.datasetA.article_id).encode(),
                                      {b&quot;similar:%d&quot; % row.datasetB.article_id: b&quot;%0.4f&quot; % row.EuclideanDistance})
                    conn.close()
        similar.foreachPartition(save_hbase)
</code></pre>
<p>添加函数到主函数中文件中，修改update更新代码：<br>
ua = UpdateArticle()<br>
sentence_df = ua.merge_article_data()<br>
if sentence_df.rdd.collect():<br>
rank, idf = ua.generate_article_label(sentence_df)<br>
articleProfile = ua.get_article_profile(rank, idf)<br>
ua.compute_article_similar(articleProfile)</p>
<h1 id="用户画像构建与更新">用户画像构建与更新</h1>
<h3 id="组成成分">组成成分</h3>
<p>用户基本信息+用户行为(历史+新增)<br>
用户行为包括：<br>
hive&gt; select * from user_action limit 1;<br>
OK<br>
2019-03-05 10:19:40		0		{&quot;action&quot;:&quot;exposure&quot;,&quot;userId&quot;:&quot;2&quot;,&quot;articleId&quot;:&quot;[16000, 44371, 16421, 16181, 17454]&quot;,&quot;algorithmCombine&quot;:&quot;C2&quot;} 2019-03-05<br>
我们需要对用户行为（字典）数据格式平铺处理<br>
user_id	action_time	article_id	share	click	collected	exposure	read_time</p>
<h3 id="步骤">步骤：</h3>
<p>1、创建HIVE基本数据表<br>
2、读取固定时间内的用户行为日志<br>
3、进行用户日志数据处理<br>
4、存储到user_article_basic表中</p>
<h4 id="创建hive基本数据表">创建HIVE基本数据表</h4>
<pre><code>create table user_article_basic(
user_id BIGINT comment &quot;userID&quot;,
action_time STRING comment &quot;user actions time&quot;,
article_id BIGINT comment &quot;articleid&quot;,
channel_id INT comment &quot;channel_id&quot;,
shared BOOLEAN comment &quot;is shared&quot;,
clicked BOOLEAN comment &quot;is clicked&quot;,
collected BOOLEAN comment &quot;is collected&quot;,
exposure BOOLEAN comment &quot;is exposured&quot;,
read_time STRING comment &quot;reading time&quot;)
COMMENT &quot;user_article_basic&quot;
CLUSTERED by (user_id) into 2 buckets
STORED as textfile
LOCATION '/user/hive/warehouse/profile.db/user_article_basic';
</code></pre>
<h4 id="读取增量用户行为数据-固定时间内的用户行为日志">读取增量用户行为数据-固定时间内的用户行为日志</h4>
<p>关联历史日期文件<br>
# 在进行日志信息的处理之前，先将我们之前建立的user_action表之间进行所有日期关联，spark hive不会自动关联<br>
import pandas as pd<br>
from datetime import datetime</p>
<pre><code>def datelist(beginDate, endDate):
	date_list=[datetime.strftime(x,'%Y-%m-%d') for x in list(pd.date_range(start=beginDate, end=endDate))]
	return date_list

dl = datelist(&quot;2019-03-05&quot;, time.strftime(&quot;%Y-%m-%d&quot;, time.localtime()))

fs = pyhdfs.HdfsClient(hosts='hadoop-master:50070')
for d in dl:
	try:
		_localions = '/user/hive/warehouse/profile.db/user_action/' + d
		if fs.exists(_localions):
			uup.spark.sql(&quot;alter table user_action add partition (dt='%s') location '%s'&quot; % (d, _localions))
	except Exception as e:
		# 已经关联过的异常忽略,partition与hdfs文件不直接关联
		pass
sqlDF = uup.spark.sql(
	&quot;select actionTime, readTime, channelId, param.articleId, param.algorithmCombine, param.action, param.userId from user_action where dt={}&quot;.format(time_str)
)
</code></pre>
<h4 id="原始数据格式与目标数据格式">原始数据格式与目标数据格式</h4>
<p>原始数据格式：（行为参数都算在 action列内）<br>
actionTime	readTime	channelID	articleId	算法名称	action		userId<br>
123						1						exposure	1<br>
321						1						click		1<br>
目标数据格式（行为参数1拆4，action列被爆炸为4列，均为bool类型）<br>
user_id	action_time	article_id	shared	clicked	collected	expore	read_time<br>
1					1			false	false	false		true<br>
1					2			false	true	false		true</p>
<h4 id="进行用户日志数据格式处理">进行用户日志数据格式处理</h4>
<pre><code>if sqlDF.collect():
def _compute(row):
    # 进行判断行为类型
    _list = []
    if row.action == &quot;exposure&quot;:
        for article_id in eval(row.articleId):
            _list.append(
                [row.userId, row.actionTime, article_id, row.channelId, False, False, False, True, row.readTime])
        return _list
    else:
        class Temp(object):
            shared = False
            clicked = False
            collected = False
            read_time = &quot;&quot;

        _tp = Temp()
        if row.action == &quot;share&quot;:
            _tp.shared = True
        elif row.action == &quot;click&quot;:
            _tp.clicked = True
        elif row.action == &quot;collect&quot;:
            _tp.collected = True
        elif row.action == &quot;read&quot;:
            _tp.clicked = True
        else:
            pass
        _list.append(
            [row.userId, row.actionTime, int(row.articleId), row.channelId, _tp.shared, _tp.clicked, _tp.collected,
             True,
             row.readTime])
        return _list
# 进行处理
# 查询内容，将原始日志表数据进行处理
_res = sqlDF.rdd.flatMap(_compute)
data = _res.toDF([&quot;user_id&quot;, &quot;action_time&quot;,&quot;article_id&quot;, &quot;channel_id&quot;, &quot;shared&quot;, &quot;clicked&quot;, &quot;collected&quot;, &quot;exposure&quot;, &quot;read_time&quot;])
</code></pre>
<h4 id="将上述目标格式的数据按照-userid-和-articleid-分组">将上述目标格式的数据按照 userid 和 articleid 分组</h4>
<p>先合并历史数据，存储到user-article-basic表中<br>
# 合并历史数据，插入表中<br>
old = uup.spark.sql(&quot;select * from user_article_basic&quot;)<br>
# 由于合并的结果中不是对于user_id和article_id唯一的，一个用户会对文章多种操作<br>
new_old = old.unionAll(data)<br>
HIVE目前支持hive终端操作ACID，不支持python的pyspark原子性操作，并且开启配置中开启原子性相关配置也不行。<br>
new_old.registerTempTable(&quot;temptable&quot;)<br>
# 按照用户，文章分组存放进去<br>
uup.spark.sql(<br>
&quot;insert overwrite table user_article_basic select user_id, max(action_time) as action_time, &quot;<br>
&quot;article_id, max(channel_id) as channel_id, max(shared) as shared, max(clicked) as clicked, &quot;<br>
&quot;max(collected) as collected, max(exposure) as exposure, max(read_time) as read_time from temptable &quot;<br>
&quot;group by user_id, article_id&quot;)</p>
<h3 id="用户画像标签权重计算">用户画像标签权重计算</h3>
<h4 id="如何存储">如何存储</h4>
<p>用户画像，作为特征提供给一些算法排序，方便与快速读取使用<br>
选择存储在Hbase当中。<br>
然后用 Hive 外表关联 hbase<br>
如果离线分析也想要使用我们可以建立HIVE到Hbase的外部表。</p>
<h4 id="hbase表设计">HBase表设计</h4>
<pre><code>		table_name		column1 column2  column3 
create 'user_profile', 'basic','partial','env'

					row_key   column_family					value
put 'user_profile', 'user:2', 'partial:{channel_id}:{topic}': weights
put 'user_profile', 'user:2', 'basic:{info}': value
put 'user_profile', 'user:2', 'env:{info}': value
</code></pre>
<h4 id="hive表设计">Hive表设计</h4>
<pre><code>create external table user_profile_hbase(
user_id STRING comment &quot;userID&quot;,
information map&lt;string, DOUBLE&gt; comment &quot;user basic information&quot;,
article_partial map&lt;string, DOUBLE&gt; comment &quot;article partial&quot;,
env map&lt;string, INT&gt; comment &quot;user env&quot;)
COMMENT &quot;user profile table&quot;
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,basic:,partial:,env:&quot;)
TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;user_profile&quot;);
</code></pre>
<h4 id="spark-sql关联表读取问题">Spark SQL关联表读取问题</h4>
<p>创建关联表之后，离线读取表内容需要一些依赖包。解决办法：<br>
拷贝/root/bigdata/hbase/lib/下面hbase-<em>.jar 到 /root/bigdata/spark/jars/目录下<br>
拷贝/root/bigdata/hive/lib/h</em>.jar 到 /root/bigdata/spark/jars/目录下<br>
上述操作三台虚拟机都执行一遍。</p>
<h4 id="用户画像频道关键词获取与权重计算">用户画像频道关键词获取与权重计算</h4>
<p>目标：获取用户1~25频道(不包括推荐频道)的关键词，并计算权重<br>
1、读取user-article-basic表，合并行为表与文章画像中的主题词<br>
2、进行用户权重计算公式、同时落地存储<br>
# 获取基本用户行为信息，然后进行文章画像的主题词合并<br>
uup.spark.sql(&quot;use profile&quot;)<br>
# 取出日志中的channel_id<br>
user_article_ = uup.spark.sql(&quot;select * from user_article_basic&quot;).drop('channel_id')<br>
uup.spark.sql('use article')<br>
article_label = uup.spark.sql(&quot;select article_id, channel_id, topics from article_profile&quot;)<br>
# 合并使用文章中正确的channel_id<br>
click_article_res = user_article_.join(article_label, how='left', on=['article_id'])<br>
将关键词字段的列表爆炸<br>
import pyspark.sql.functions as F<br>
click_article_res = click_article_res.withColumn('topic', F.explode('topics')).drop('topics')<br>
爆炸后格式如下：<br>
user_id article_id topic ...<br>
1     1			 python<br>
1     1			 golang<br>
1     1			 linux<br>
...    ...		  ...</p>
<h4 id="用户画像之标签权重算法">用户画像之标签权重算法</h4>
<p>用户标签权重 =( 行为类型权重之和) × 时间衰减<br>
行为类型权重 的 分值 的确定需要整体协商<br>
行为					分值<br>
阅读时间&lt;1000ms		  1<br>
阅读时间&gt;=1000ms	  2<br>
收藏					2<br>
分享					3<br>
点击					5<br>
时间衰减: 1/(log(t)+1) ,t为时间发生时间距离当前时间的大小。<br>
# 计算每个用户对每篇文章的标签的权重<br>
def compute_weights(rowpartition):<br>
&quot;&quot;&quot;处理每个用户对文章的点击数据<br>
&quot;&quot;&quot;<br>
weightsOfaction = {<br>
&quot;read_min&quot;: 1,<br>
&quot;read_middle&quot;: 2,<br>
&quot;collect&quot;: 2,<br>
&quot;share&quot;: 3,<br>
&quot;click&quot;: 5<br>
}</p>
<pre><code>import happybase
from datetime import datetime
import numpy as np
#  用于读取hbase缓存结果配置
pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090)

# 读取文章的标签数据
# 计算权重值
# 时间间隔
for row in rowpartition:
    t = datetime.now() - datetime.strptime(row.action_time, '%Y-%m-%d %H:%M:%S')
    # 时间衰减系数
    time_exp = 1 / (np.log(t.days + 1) + 1)

    if row.read_time == '':
        r_t = 0
    else:
        r_t = int(row.read_time)
    # 浏览时间分数
    is_read = weightsOfaction['read_middle'] if r_t &gt; 1000 else weightsOfaction['read_min']

    # 每个词的权重分数
    weigths = time_exp * (
                row.shared * weightsOfaction['share'] + row.collected * weightsOfaction['collect'] + row.
                clicked * weightsOfaction['click'] + is_read)

#        with pool.connection() as conn:
#            table = conn.table('user_profile')
#            table.put('user:{}'.format(row.user_id).encode(),
#                      {'partial:{}:{}'.format(row.channel_id, row.topic).encode(): json.dumps(
#                          weigths).encode()})
#            conn.close()

click_article_res.foreachPartition(compute_weights)
</code></pre>
<p>落地Hbase中之后，在HBASE中查询，happybase或者hbase终端<br>
import happybase<br>
# 用于读取hbase缓存结果配置<br>
pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090)</p>
<pre><code>with pool.connection() as conn:
	table = conn.table('user_profile')
	# 获取每个键 对应的所有列的结果
	data = table.row(b'user:2', columns=[b'partial'])
	conn.close()

# 等价于  hbase(main):015:0&gt; get 'user_profile', 'user:2'
</code></pre>
<h3 id="基础信息画像更新">基础信息画像更新</h3>
<pre><code>def update_user_info(self):
    &quot;&quot;&quot;
    更新用户的基础信息画像
    :return:
    &quot;&quot;&quot;
    self.spark.sql(&quot;use toutiao&quot;)

    user_basic = self.spark.sql(&quot;select user_id, gender, birthday from user_profile&quot;)

    # 更新用户基础信息
    def _udapte_user_basic(partition):
        &quot;&quot;&quot;更新用户基本信息
        &quot;&quot;&quot;
        import happybase
        #  用于读取hbase缓存结果配置
        pool = happybase.ConnectionPool(size=10, host='172.17.0.134', port=9090)
        for row in partition:

            from datetime import date
            age = 0
            if row.birthday != 'null':
                born = datetime.strptime(row.birthday, '%Y-%m-%d')
                today = date.today()
                age = today.year - born.year - ((today.month, today.day) &lt; (born.month, born.day))

            with pool.connection() as conn:
                table = conn.table('user_profile')
                table.put('user:{}'.format(row.user_id).encode(),
                          {'basic:gender'.encode(): json.dumps(row.gender).encode()})
                table.put('user:{}'.format(row.user_id).encode(),
                          {'basic:birthday'.encode(): json.dumps(age).encode()})
                conn.close()

    user_basic.foreachPartition(_udapte_user_basic)
    logger.info(
        &quot;{} INFO completely update infomation of basic&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
        
# hbase(main):016:0&gt; get 'user_profile', 'user:2'
</code></pre>
<h3 id="用户画像增量更新定时开启">用户画像增量更新定时开启:</h3>
<p>用户画像增量更新代码整理<br>
添加定时任务以及进程管理<br>
from offline.update_user import UpdateUserProfile</p>
<pre><code>def update_user_profile():
	&quot;&quot;&quot;
	更新用户画像
	&quot;&quot;&quot;
	uup = UpdateUserProfile()
	if uup.update_user_action_basic():
		uup.update_user_label()
		uup.update_user_info()
scheduler.add_job(update_user_profile, trigger='interval', hours=2)</code></pre>
]]></content>
    </entry>
</feed>