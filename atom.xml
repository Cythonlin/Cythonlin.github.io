<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://cythonlin.github.io</id>
    <title>Cython_lin</title>
    <updated>2020-09-29T04:20:50.593Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://cythonlin.github.io"/>
    <link rel="self" href="https://cythonlin.github.io/atom.xml"/>
    <logo>https://cythonlin.github.io/images/avatar.png</logo>
    <icon>https://cythonlin.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, Cython_lin</rights>
    <entry>
        <title type="html"><![CDATA[RS => 推荐系统（二）离线画像构建 ]]></title>
        <id>https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-er-chi-xian-hua-xiang-gou-jian/</id>
        <link href="https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-er-chi-xian-hua-xiang-gou-jian/">
        </link>
        <updated>2020-09-29T04:20:35.000Z</updated>
        <content type="html"><![CDATA[<h1 id="文章离线画像构建">文章离线画像构建</h1>
<h3 id="spark配置基类抽取">Spark配置基类抽取</h3>
<pre><code>from pyspark import SparkConf
from pyspark.sql import SparkSession
import os

class SparkSessionBase(object):

	SPARK_APP_NAME = None
	SPARK_URL = &quot;yarn&quot;

	SPARK_EXECUTOR_MEMORY = &quot;2g&quot;
	SPARK_EXECUTOR_CORES = 2
	SPARK_EXECUTOR_INSTANCES = 2

	ENABLE_HIVE_SUPPORT = False

	def _create_spark_session(self):
		conf = SparkConf()  # 创建spark config对象
		config = (
			(&quot;spark.app.name&quot;, self.SPARK_APP_NAME),  # 设置启动的spark的app名称，没有提供，将随机产生一个名称
			(&quot;spark.executor.memory&quot;, self.SPARK_EXECUTOR_MEMORY),  # 设置该app启动时占用的内存用量，默认2g
	 		(&quot;spark.master&quot;, self.SPARK_URL),  # spark master的地址
			(&quot;spark.executor.cores&quot;, self.SPARK_EXECUTOR_CORES),  # 设置spark executor使用的CPU核心数，默认是1核心
			(&quot;spark.executor.instances&quot;, self.SPARK_EXECUTOR_INSTANCES),
			(&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;),
		)

	conf.setAll(config)

	# 利用config对象，创建spark session
	if self.ENABLE_HIVE_SUPPORT:
		return SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()
	else:
		return SparkSession.builder.config(conf=conf).getOrCreate()
</code></pre>
<h3 id="主应用导入基类">主应用导入基类</h3>
<pre><code># pip install pyspark
# pip install findspark

import findspark
findspark.init()

import os
import sys
# 如果当前代码文件运行测试需要加入修改路径，避免出现后导包问题
BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))
sys.path.insert(0, os.path.join(BASE_DIR))
print(BASE_DIR)
PYSPARK_PYTHON = &quot;/miniconda2/envs/reco_sys/bin/python&quot;
# 当存在多个版本时，不指定很可能会导致出错
os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHON
os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHON
from offline import SparkSessionBase

class OriginArticleData(SparkSessionBase):

	SPARK_APP_NAME = &quot;mergeArticle&quot;
	SPARK_URL = &quot;yarn&quot;

	ENABLE_HIVE_SUPPORT = True

	def __init__(self):
		self.spark = self._create_spark_session()

oa = OriginArticleData()   # oa就是带有配置的 sparkSession的实例化对象
</code></pre>
<h3 id="文章-表-合并">文章 表 合并</h3>
<p>文章基本信息表+文章内容表+频道表：<br>
titlce_content = oa.spark.sql(&quot;select a.article_id, a.channel_id, a.title, b.content from news_article_basic a inner join news_article_content b on a.article_id=b.article_id)<br>
因为得到的是 DF类型，想要用SQL，可以把DF注册为临时表<br>
titlce_content.registerTempTable('temptable')<br>
再把 频道表 的 频道名 合并进来<br>
channel_title_content = oa.spark.sql(&quot;select t.*, n.channel_name from temptable t left join news_channel n on t.channel_id=n.channel_id&quot;)</p>
<h3 id="文章-字段-合并">文章 字段 合并</h3>
<p>将 文章标题+文章内容+文章频道 的列，拼接成一个大字符串<br>
import pyspark.sql.functions as F<br>
import gc</p>
<pre><code># 增加channel的名字，后面会使用
basic_content.registerTempTable(&quot;temparticle&quot;)
channel_basic_content = oa.spark.sql(
	&quot;select t.*, n.channel_name from temparticle t left join news_channel n on t.channel_id=n.channel_id&quot;
)

# 利用concat_ws方法，将多列数据合并为一个长文本内容（频道，标题以及内容合并）
oa.spark.sql(&quot;use article&quot;)
sentence_df = channel_basic_content.select(&quot;article_id&quot;, &quot;channel_id&quot;, &quot;channel_name&quot;, &quot;title&quot;, &quot;content&quot;, \
	F.concat_ws(
		&quot;,&quot;,											# 指定大字符串分隔符
		channel_basic_content.channel_name,         
		channel_basic_content.title,
		channel_basic_content.content					
		).alias(&quot;sentence&quot;)                    # 新列 大字符串 取名
	)
del basic_content
del channel_basic_content
gc.collect()

# sentence_df.write.insertInto(&quot;article_data&quot;)       # 写入提前创建好的Hive表中
</code></pre>
<h3 id="分词">分词</h3>
<pre><code>def segmentation(partition):          # 就这一行的缩进需要调整下
import os
import re

import jieba
import jieba.analyse
import jieba.posseg as pseg
import codecs

abspath = &quot;/root/words&quot;

# 结巴加载用户词典
userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;)
jieba.load_userdict(userDict_path)

# 停用词文本
stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;)

def get_stopwords_list():
    &quot;&quot;&quot;返回stopwords列表&quot;&quot;&quot;
    stopwords_list = [i.strip()
                      for i in codecs.open(stopwords_path).readlines()]
    return stopwords_list

# 所有的停用词列表
stopwords_list = get_stopwords_list()

# 分词
def cut_sentence(sentence):
    &quot;&quot;&quot;对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词&quot;&quot;&quot;
    # print(sentence,&quot;*&quot;*100)
    # eg:[pair('今天', 't'), pair('有', 'd'), pair('雾', 'n'), pair('霾', 'g')]
    seg_list = pseg.lcut(sentence)
    seg_list = [i for i in seg_list if i.flag not in stopwords_list]
    filtered_words_list = []
    for seg in seg_list:
        # print(seg)
        if len(seg.word) &lt;= 1:
            continue
        elif seg.flag == &quot;eng&quot;:
            if len(seg.word) &lt;= 2:
                continue
            else:
                filtered_words_list.append(seg.word)
        elif seg.flag.startswith(&quot;n&quot;):
            filtered_words_list.append(seg.word)
        elif seg.flag in [&quot;x&quot;, &quot;eng&quot;]:  # 是自定一个词语或者是英文单词
            filtered_words_list.append(seg.word)
    return filtered_words_list

for row in partition:
    sentence = re.sub(&quot;&lt;.*?&gt;&quot;, &quot;&quot;, row.sentence)    # 替换掉标签数据
    words = cut_sentence(sentence)
    yield row.article_id, row.channel_id, words
</code></pre>
<h3 id="计算-tf-idf">计算 TF-IDF</h3>
<p>TF:<br>
ktt.spark.sql(&quot;use article&quot;)<br>
article_dataframe = ktt.spark.sql(&quot;select * from article_data limit 20&quot;)<br>
words_df = article_dataframe.rdd.mapPartitions(segmentation).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;words&quot;])</p>
<pre><code>from pyspark.ml.feature import CountVectorizer
# 总词汇的大小，文本中必须出现的次数
cv = CountVectorizer(inputCol=&quot;words&quot;, outputCol=&quot;countFeatures&quot;, vocabSize=200*10000, minDF=1.0)
# 训练词频统计模型
cv_model = cv.fit(words_df)
cv_model.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;)

# cv_model.vocabulary 查看统计词表（相当于groupby结果的 key,  但不包括value）
</code></pre>
<p>训练TF-IDF:<br>
# 词语与词频统计<br>
from pyspark.ml.feature import CountVectorizerModel<br>
cv_model = CountVectorizerModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;)<br>
# 得出词频向量结果<br>
cv_result = cv_model.transform(words_df)<br>
# 训练IDF模型 (把 tf结果传进去，其实说是 IDF模型，计算结果得出的就是 TF-IDF)<br>
from pyspark.ml.feature import IDF<br>
idf = IDF(inputCol=&quot;countFeatures&quot;, outputCol=&quot;idfFeatures&quot;)<br>
idfModel = idf.fit(cv_result)<br>
idfModel.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/IDF.model&quot;)</p>
<pre><code># idfModel.idf.toArray()[:20]    查看逆文档频率矩阵
</code></pre>
<p>TF-IDF结果数据格式：<br>
列1， 列...， 列 TF-IDF<br>
(1000,[804,1032],[6.349777077,7.0761797]) 。。。<br>
使用TF-IDF模型，取Top-K个词：<br>
from pyspark.ml.feature import CountVectorizerModel<br>
cv_model = CountVectorizerModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model&quot;)<br>
from pyspark.ml.feature import IDFModel<br>
idf_model = IDFModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model&quot;)<br>
cv_result = cv_model.transform(words_df)<br>
tfidf_result = idf_model.transform(cv_result)</p>
<pre><code>def func(partition):            
	TOPK = 20
	for row in partition:
		# 找到索引与IDF值并进行排序
		_ = list(zip(row.idfFeatures.indices, row.idfFeatures.values))   # [ (indexes,values)，  (indexes,values)]
		_ = sorted(_, key=lambda x: x[1], reverse=True)
		result = _[:TOPK]
		for word_index, tfidf in result:
			yield row.article_id, row.channel_id, int(word_index), round(float(tfidf), 4)
			# yield这句注定了返回结果格式 (多层for循环 yield， 原本一行数据按每个单词爆炸展开)
			# article_id,   channel_id,   word_index,   tfidf
			# 1				 100         40         15.5
			# 1				 100         14         10.3         
			# 1				 100         23         13.2
            
            
_keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;index&quot;, &quot;tfidf&quot;])
</code></pre>
<p>我们的目标是： 构成  词+TFIDF值, 而不是索引+TFIDF<br>
cv_model.vocabulary 结果是所有单词的列表。 上面的 index就是对应这个列表的索引<br>
最终构建一个词典+索引表：<br>
index	word<br>
..		..	<br>
然后将 主表（文章id,频道id,索引，tfidf）与 词典表（index+word） 合并<br>
得到  （文章id,频道id, 词， tfidf）</p>
<h3 id="计算-textrank">计算 TextRank</h3>
<p>TextRank和核心就是设定一个固定窗口来滑动<br>
把每个窗口内的每个词， 设为字典的Key, value就是他附近的n个词的列表<br>
然后每个词都这样做， 遇到相同的词就追加到字典的value 列表中<br>
# 分词<br>
def textrank(partition):<br>
import os</p>
<pre><code>import jieba
import jieba.analyse
import jieba.posseg as pseg
import codecs

abspath = &quot;/root/words&quot;

# 结巴加载用户词典
userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;)
jieba.load_userdict(userDict_path)

# 停用词文本
stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;)

def get_stopwords_list():
    &quot;&quot;&quot;返回stopwords列表&quot;&quot;&quot;
    stopwords_list = [i.strip()
                      for i in codecs.open(stopwords_path).readlines()]
    return stopwords_list

# 所有的停用词列表
stopwords_list = get_stopwords_list()

class TextRank(jieba.analyse.TextRank):
    def __init__(self, window=20, word_min_len=2):
        super(TextRank, self).__init__()
        self.span = window  # 窗口大小
        self.word_min_len = word_min_len  # 单词的最小长度
        # 要保留的词性，根据jieba github ，具体参见https://github.com/baidu/lac
        self.pos_filt = frozenset(
            ('n', 'x', 'eng', 'f', 's', 't', 'nr', 'ns', 'nt', &quot;nw&quot;, &quot;nz&quot;, &quot;PER&quot;, &quot;LOC&quot;, &quot;ORG&quot;))

    def pairfilter(self, wp):
        &quot;&quot;&quot;过滤条件，返回True或者False&quot;&quot;&quot;

        if wp.flag == &quot;eng&quot;:
            if len(wp.word) &lt;= 2:
                return False

        if wp.flag in self.pos_filt and len(wp.word.strip()) &gt;= self.word_min_len \
                and wp.word.lower() not in stopwords_list:
            return True
# TextRank过滤窗口大小为5，单词最小为2
textrank_model = TextRank(window=5, word_min_len=2)
allowPOS = ('n', &quot;x&quot;, 'eng', 'nr', 'ns', 'nt', &quot;nw&quot;, &quot;nz&quot;, &quot;c&quot;)

for row in partition:
    tags = textrank_model.textrank(row.sentence, topK=20, withWeight=True, allowPOS=allowPOS, withFlag=False)
    for tag in tags:
        yield row.article_id, row.channel_id, tag[0], tag[1]

# 计算textrank
textrank_keywords_df = article_dataframe.rdd.mapPartitions(textrank).toDF(
	[&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;textrank&quot;]
)

# textrank_keywords_df.write.insertInto(&quot;textrank_keywords_values&quot;)
</code></pre>
<p>textrank运行结果如下：<br>
hive&gt; select * from textrank_keywords_values limit 20;<br>
OK<br>
文章ID  channel   word    textrank<br>
98319   17      var     20.6079<br>
98323   17      var     7.4938<br>
98326   17      var     104.9128<br>
然后和 tfidf一样 根据 textrank值，  取TOP-K个词</p>
<h3 id="计算-主题词-和-关键词">计算 主题词 和 关键词</h3>
<p>关键词：TEXTRANK计算出的结果TOPK个词以及权重<br>
主题词：TEXTRANK的TOPK词 与 ITFDF计算的TOPK个词的交集<br>
格式如下：<br>
hive&gt; desc article_profile;<br>
OK<br>
article_id              int                     article_id<br>
channel_id              int                     channel_id<br>
keywords               map								 keywords<br>
topics						  array								topics</p>
<pre><code>hive&gt; select * from article_profile limit 1;    
# 这里把结果按行排列开方便观看
article_id			26
channel_id		   17            
关键词字典		  {&quot;策略&quot;:0.3973770571351729,&quot;jpg&quot;:0.9806348975390871,&quot;用户&quot;:1.2794959063944176,&quot;strong&quot;:1.6488457985625076,&quot;文件&quot;:0.28144603583387057,&quot;逻辑&quot;:0.45256526469610714,&quot;形式&quot;:0.4123994242601279,&quot;全自&quot;:0.9594604850547191,&quot;h2&quot;:0.6244481634710125,&quot;版本&quot;:0.44280276959510817,&quot;Adobe&quot;:0.8553618185108718,&quot;安装&quot;:0.8305037437573172,&quot;检查更新&quot;:1.8088946300014435,&quot;产品&quot;:0.774842382276899,&quot;下载页&quot;:1.4256311032544344,&quot;过程&quot;:0.19827163395829256,&quot;json&quot;:0.6423301791599972,&quot;方式&quot;:0.582762869780791,&quot;退出应用&quot;:1.2338671268242603,&quot;Setup&quot;:1.004399549339134} 

主题词列表				[&quot;Electron&quot;,&quot;全自动&quot;,&quot;产品&quot;,&quot;版本号&quot;,&quot;安装包&quot;,&quot;检查更新&quot;,&quot;方案&quot;,&quot;版本&quot;,&quot;退出应用&quot;,&quot;逻辑&quot;,&quot;安装过程&quot;,&quot;方式&quot;,&quot;定性&quot;,&quot;新版本&quot;,&quot;Setup&quot;,&quot;静默&quot;,&quot;用户&quot;]
</code></pre>
<h3 id="增量更新-离线文章画像">增量更新 离线文章画像</h3>
<p>更新流程：<br>
1、toutiao 数据库中，news_article_content 与news_article_basic—&gt;更新到article数据库中article_data表，方便操作<br>
2. 第一次：所有更新，后面增量每天的数据更新26日：1：00<sub>2：00，2：00</sub>3：00，左闭右开,一个小时更新一次<br>
3、刚才新更新的文章，通过已有的idf计算出tfidf值以及hive 的textrank_keywords_values<br>
4、更新hive的article_profile</p>
<p>离线更新文章画像 代码组装：Pycharm<br>
注意在Pycharm中运行要设置环境：</p>
<pre><code>PYTHONUNBUFFERED=1
JAVA_HOME=/root/bigdata/jdk
SPARK_HOME=/root/bigdata/spark
HADOOP_HOME=/root/bigdata/hadoop
PYSPARK_PYTHON=/root/anaconda3/envs/reco_sys/bin/python
PYSPARK_DRIVER_PYTHON=/root/anaconda3/envs/reco_sys/bin/python
</code></pre>
<p>具体代码如下：<br>
import os<br>
import sys<br>
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(<strong>file</strong>)))<br>
sys.path.insert(0, os.path.join(BASE_DIR))<br>
from offline import SparkSessionBase<br>
from datetime import datetime<br>
from datetime import timedelta<br>
import pyspark.sql.functions as F<br>
import pyspark<br>
import gc</p>
<pre><code>class UpdateArticle(SparkSessionBase):
&quot;&quot;&quot;
更新文章画像
&quot;&quot;&quot;
SPARK_APP_NAME = &quot;updateArticle&quot;
ENABLE_HIVE_SUPPORT = True

SPARK_EXECUTOR_MEMORY = &quot;7g&quot;

def __init__(self):
    self.spark = self._create_spark_session()

    self.cv_path = &quot;hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model&quot;
    self.idf_path = &quot;hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model&quot;

def get_cv_model(self):
    # 词语与词频统计
    from pyspark.ml.feature import CountVectorizerModel
    cv_model = CountVectorizerModel.load(self.cv_path)
    return cv_model

def get_idf_model(self):
    from pyspark.ml.feature import IDFModel
    idf_model = IDFModel.load(self.idf_path)
    return idf_model

@staticmethod
def compute_keywords_tfidf_topk(words_df, cv_model, idf_model):
    &quot;&quot;&quot;保存tfidf值高的20个关键词
    :param spark:
    :param words_df:
    :return:
    &quot;&quot;&quot;
    cv_result = cv_model.transform(words_df)
    tfidf_result = idf_model.transform(cv_result)
    # print(&quot;transform compelete&quot;)

    # 取TOP-N的TFIDF值高的结果
    def func(partition):
        TOPK = 20
        for row in partition:
            _ = list(zip(row.idfFeatures.indices, row.idfFeatures.values))
            _ = sorted(_, key=lambda x: x[1], reverse=True)
            result = _[:TOPK]
            #         words_index = [int(i[0]) for i in result]
            #         yield row.article_id, row.channel_id, words_index

            for word_index, tfidf in result:
                yield row.article_id, row.channel_id, int(word_index), round(float(tfidf), 4)

    _keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;index&quot;, &quot;tfidf&quot;])

    return _keywordsByTFIDF

def merge_article_data(self):
    &quot;&quot;&quot;
    合并业务中增量更新的文章数据
    :return:
    &quot;&quot;&quot;
    # 获取文章相关数据, 指定过去一个小时整点到整点的更新数据
    # 如：26日：1：00~2：00，2：00~3：00，左闭右开
    self.spark.sql(&quot;use toutiao&quot;)
    _yester = datetime.today().replace(minute=0, second=0, microsecond=0)
    start = datetime.strftime(_yester + timedelta(days=0, hours=-1, minutes=0), &quot;%Y-%m-%d %H:%M:%S&quot;)
    end = datetime.strftime(_yester, &quot;%Y-%m-%d %H:%M:%S&quot;)

    # 合并后保留：article_id、channel_id、channel_name、title、content
    # +----------+----------+--------------------+--------------------+
    # | article_id | channel_id | title | content |
    # +----------+----------+--------------------+--------------------+
    # | 141462 | 3 | test - 20190316 - 115123 | 今天天气不错，心情很美丽！！！ |
    basic_content = self.spark.sql(
        &quot;select a.article_id, a.channel_id, a.title, b.content from news_article_basic a &quot;
        &quot;inner join news_article_content b on a.article_id=b.article_id where a.review_time &gt;= '{}' &quot;
        &quot;and a.review_time &lt; '{}' and a.status = 2&quot;.format(start, end))
    # 增加channel的名字，后面会使用
    basic_content.registerTempTable(&quot;temparticle&quot;)
    channel_basic_content = self.spark.sql(
        &quot;select t.*, n.channel_name from temparticle t left join news_channel n on t.channel_id=n.channel_id&quot;)

    # 利用concat_ws方法，将多列数据合并为一个长文本内容（频道，标题以及内容合并）
    self.spark.sql(&quot;use article&quot;)
    sentence_df = channel_basic_content.select(&quot;article_id&quot;, &quot;channel_id&quot;, &quot;channel_name&quot;, &quot;title&quot;, &quot;content&quot;, \
                                               F.concat_ws(
                                                   &quot;,&quot;,
                                                   channel_basic_content.channel_name,
                                                   channel_basic_content.title,
                                                   channel_basic_content.content
                                               ).alias(&quot;sentence&quot;)
                                               )
    del basic_content
    del channel_basic_content
    gc.collect()

    sentence_df.write.insertInto(&quot;article_data&quot;)
    return sentence_df

def generate_article_label(self, sentence_df):
    &quot;&quot;&quot;
    生成文章标签  tfidf, textrank
    :param sentence_df: 增量的文章内容
    :return:
    &quot;&quot;&quot;
    # 进行分词
    words_df = sentence_df.rdd.mapPartitions(segmentation).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;words&quot;])
    cv_model = self.get_cv_model()
    idf_model = self.get_idf_model()

    # 1、保存所有的词的idf的值，利用idf中的词的标签索引
    # 工具与业务隔离
    _keywordsByTFIDF = UpdateArticle.compute_keywords_tfidf_topk(words_df, cv_model, idf_model)

    keywordsIndex = self.spark.sql(&quot;select keyword, index idx from idf_keywords_values&quot;)

    keywordsByTFIDF = _keywordsByTFIDF.join(keywordsIndex, keywordsIndex.idx == _keywordsByTFIDF.index).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;tfidf&quot;])

    keywordsByTFIDF.write.insertInto(&quot;tfidf_keywords_values&quot;)

    del cv_model
    del idf_model
    del words_df
    del _keywordsByTFIDF
    gc.collect()

    # 计算textrank
    textrank_keywords_df = sentence_df.rdd.mapPartitions(textrank).toDF(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;textrank&quot;])
    textrank_keywords_df.write.insertInto(&quot;textrank_keywords_values&quot;)

    return textrank_keywords_df, keywordsIndex

def get_article_profile(self, textrank, keywordsIndex):
    &quot;&quot;&quot;
    文章画像主题词建立
    :param idf: 所有词的idf值
    :param textrank: 每个文章的textrank值
    :return: 返回建立号增量文章画像
    &quot;&quot;&quot;
    keywordsIndex = keywordsIndex.withColumnRenamed(&quot;keyword&quot;, &quot;keyword1&quot;)
    result = textrank.join(keywordsIndex, textrank.keyword == keywordsIndex.keyword1)

    # 1、关键词（词，权重）
    # 计算关键词权重
    _articleKeywordsWeights = result.withColumn(&quot;weights&quot;, result.textrank * result.idf).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;weights&quot;])

    # 合并关键词权重到字典
    _articleKeywordsWeights.registerTempTable(&quot;temptable&quot;)
    articleKeywordsWeights = self.spark.sql(
        &quot;select article_id, min(channel_id) channel_id, collect_list(keyword) keyword_list, collect_list(weights) weights_list from temptable group by article_id&quot;)
    def _func(row):
        return row.article_id, row.channel_id, dict(zip(row.keyword_list, row.weights_list))
    articleKeywords = articleKeywordsWeights.rdd.map(_func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keywords&quot;])

    # 2、主题词
    # 将tfidf和textrank共现的词作为主题词
    topic_sql = &quot;&quot;&quot;
            select t.article_id article_id2, collect_set(t.keyword) topics from tfidf_keywords_values t
            inner join 
            textrank_keywords_values r
            where t.keyword=r.keyword
            group by article_id2
            &quot;&quot;&quot;
    articleTopics = self.spark.sql(topic_sql)

    # 3、将主题词表和关键词表进行合并，插入表
    articleProfile = articleKeywords.join(articleTopics,
                                          articleKeywords.article_id == articleTopics.article_id2).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keywords&quot;, &quot;topics&quot;])
    articleProfile.write.insertInto(&quot;article_profile&quot;)

    del keywordsIndex
    del _articleKeywordsWeights
    del articleKeywords
    del articleTopics
    gc.collect()

    return articleProfile


if __name__ == '__main__':
ua = UpdateArticle()
sentence_df = ua.merge_article_data()
if sentence_df.rdd.collect():
    rank, idf = ua.generate_article_label(sentence_df)
    articleProfile = ua.get_article_profile(rank, idf)
</code></pre>
<p>使用工具：Supervisor+Apscheduler<br>
# pip install APScheduler<br>
from apscheduler.schedulers.blocking import BlockingScheduler<br>
from apscheduler.executors.pool import ProcessPoolExecutor</p>
<pre><code>from scheduler.update import update_article_profile

# 创建scheduler，多进程执行
executors = {
	'default': ProcessPoolExecutor(3)
}
scheduler = BlockingScheduler(executors=executors)
# 添加定时更新任务更新文章画像,每隔一小时更新， trigger还有其他定时方式
scheduler.add_job(update_article_profile, trigger='interval', hours=1)
scheduler.start()
</code></pre>
<p>自定义Logger:<br>
import logging<br>
import logging.handlers<br>
import os</p>
<pre><code>logging_file_dir = '/root/logs/'
def create_logger():
	# 离线处理更新打印日志
	 log_trace = logging.getLogger('offline')
     
	trace_file_handler = logging.FileHandler(
        os.path.join(logging_file_dir, 'offline.log')
    )
	 trace_file_handler.setFormatter(logging.Formatter('%(message)s'))
    
    log_trace.addHandler(trace_file_handler)
    log_trace.setLevel(logging.INFO)
</code></pre>
<p>supervisor管理apscheduler:<br>
[program:offline]<br>
environment=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python<br>
command=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/scheduler/main.py<br>
directory=/root/toutiao_project/scheduler<br>
user=root<br>
autorestart=true<br>
redirect_stderr=true<br>
stdout_logfile=/root/logs/offlinesuper.log<br>
loglevel=info<br>
stopsignal=KILL<br>
stopasgroup=true<br>
killasgroup=true</p>
<h3 id="word2vec与文章相似度">Word2Vec与文章相似度</h3>
<pre><code>w2v.spark.sql(&quot;use article&quot;)
article = w2v.spark.sql(&quot;select * from article_data where channel_id=18 limit 2&quot;)
words_df = article.rdd.mapPartitions(segmentation).toDF(['article_id', 'channel_id', 'words'])
</code></pre>
<p>Spark Word2Vec API介绍：<br>
模块：from pyspark.ml.feature import Word2Vec</p>
<pre><code>API：class pyspark.ml.feature.Word2Vec(vectorSize=100, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, seed=None, inputCol=None, outputCol=None, windowSize=5, maxSentenceLength=1000)
参数说明：
	vectorSize=100: 词向量长度
	minCount：过滤次数小于默认5次的词
	windowSize=5：训练时候的窗口大小
	inputCol=None：输入列名
	outputCol=None：输出列名
</code></pre>
<p>Spark Word2Vec训练保存模型：<br>
new_word2Vec = Word2Vec(vectorSize=100, inputCol=&quot;words&quot;, outputCol=&quot;model&quot;, minCount=3)<br>
new_model = new_word2Vec.fit(words_df)<br>
new_model.save(&quot;hdfs://hadoop-master:9000/headlines/models/test.word2vec&quot;)<br>
上传历史数据训练的模型：<br>
hadoop dfs -put ./word2vec_model /headlines/models/</p>
<h3 id="增量更新-文章向量计算">增量更新-文章向量计算</h3>
<p>有了词向量之后，我们就可以得到一篇文章的向量了，为了后面快速使用文章的向量，我们会将每个频道所有的文章向量保存起来。</p>
<p>目的：保存所有历史训练的文章向量<br>
步骤：<br>
1、加载某个频道模型，得到每个词的向量<br>
2、获取频道的文章画像，得到文章画像的关键词(接着之前增量更新的文章article_profile)<br>
3、计算得到文章每个词的向量<br>
4、计算得到文章的平均词向量即文章的向量<br>
加载某个频道模型，得到每个词的向量<br>
from pyspark.ml.feature import Word2VecModel<br>
channel_id = 18<br>
channel = &quot;python&quot;<br>
wv_model = Word2VecModel.load(<br>
&quot;hdfs://hadoop-master:9000/headlines/models/word2vec_model/channel_%d_%s.word2vec&quot; % (channel_id, channel))<br>
vectors = wv_model.getVectors()<br>
获取新增的文章画像，得到文章画像的关键词：<br>
# 选出新增的文章的画像做测试，上节计算的画像中有不同频道的，我们选取Python频道的进行计算测试<br>
# 新增的文章画像获取部分<br>
profile = w2v.spark.sql(&quot;select * from article_profile where channel_id=18 limit 10&quot;)<br>
# profile = articleProfile.filter('channel_id = {}'.format(channel_id))</p>
<pre><code>profile.registerTempTable(&quot;incremental&quot;)
articleKeywordsWeights = w2v.spark.sql(
                &quot;select article_id, channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight&quot;)
_article_profile = articleKeywordsWeights.join(vectors, vectors.word==articleKeywordsWeights.keyword, &quot;inner&quot;)
</code></pre>
<p>计算得到文章的平均词向量即文章的向量<br>
def avg(row):<br>
x = 0<br>
for v in row.vectors:<br>
x += v<br>
#  将平均向量作为article的向量<br>
return row.article_id, row.channel_id, x / len(row.vectors)</p>
<pre><code>articleKeywordVectors.registerTempTable(&quot;tempTable&quot;)
articleVector = w2v.spark.sql(
	&quot;select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from tempTable group by article_id&quot;).rdd \
    # 分组之后， 求map的平均之前， 结果是  artile_id, channel_id, vector_list # vector_list 是二维数组。
    .map(avg).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;articleVector&quot;])
    # 求map的平均之后结果是 article_id, channel_id, article_vector # article_vector 代表文章向量
</code></pre>
<h3 id="文章相似度计算">文章相似度计算</h3>
<h4 id="存在的问题以进行某频道全量所有的两两相似度计算-但是事实当文章量达到千万级别或者上亿级别特征也会上亿级别计算量就会很大-以下有两种类型解决方案">存在的问题：以进行某频道全量所有的两两相似度计算。但是事实当文章量达到千万级别或者上亿级别，特征也会上亿级别，计算量就会很大。以下有两种类型解决方案：</h4>
<ol>
<li>每个频道的文章先进行聚类（缺点，（分成几个簇）也是个超参数）</li>
<li>局部敏感哈希LSH(Locality Sensitive Hashing)<br>
基本思想1：LSH算法基于一个假设，如果两个文本在原有的数据空间是相似的，那么分别经过哈希函数转换以后的它们也具有很高的相似度<br>
基本思想2: 经常使用的哈希函数，冲突总是难以避免。LSH却依赖于冲突，在解决NNS(Nearest neighbor search )时，我们期望：<br>
离得越近的对象，发生冲突的概率越高<br>
离得越远的对象，发生冲突的概率越低</li>
</ol>
<h4 id="局部敏感哈希lshlocality-sensitive-hashing-lsh过程">局部敏感哈希LSH(Locality Sensitive Hashing) LSH过程：</h4>
<p>mini hashing(略)	<br>
Random Projection（特征压缩）：<br>
Random Projection是一种随机算法.随机投影的算法有很多，如PCA、Gaussian random projection - 高斯随机投影。<br>
随机桶投影是用于欧几里德距离的 LSH family。其LSH family将x特征向量映射到随机单位矢量v，并将映射结果分为哈希桶中。哈希表中的每个位置表示一个哈希桶。	<br>
使得：<br>
离得越近的对象，发生冲突的概率越高<br>
离得越远的对象，发生冲突的概率越低</p>
<h4 id="代码实现">代码实现：</h4>
<p>读取数据，进行类型处理(数组转换类型为Vector)：<br>
from pyspark.ml.linalg import Vectors<br>
# 选取部分数据做测试<br>
article_vector = w2v.spark.sql(&quot;select article_id, articlevector from article_vector where channel_id=18 limit 10&quot;)<br>
train = articlevector.select(['article_id', 'articleVector'])</p>
<pre><code>def _array_to_vector(row):
	return row.article_id, Vectors.dense(row.articleVector)

train = train.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])
</code></pre>
<p>相似度计算（BRP进行FIT）：<br>
函数参数说明：<br>
class pyspark.ml.feature.BucketedRandomProjectionLSH(inputCol=None, outputCol=None, seed=None, numHashTables=1, bucketLength=None)<br>
inputCol=None：输入特征列<br>
outputCol=None：输出特征列<br>
numHashTables=1：哈希表数量，几个hash function对数据进行hash操作<br>
bucketLength=None：桶的数量，值越大相同数据进入到同一个桶的概率越高<br>
method:<br>
# 计算df1每个文章相似的df2数据集的数据<br>
approxSimilarityJoin(df1, df2, 2.0, distCol='EuclideanDistance')  # 转为向量</p>
<pre><code># 代码调用：
from pyspark.ml.feature import BucketedRandomProjectionLSH

brp = BucketedRandomProjectionLSH(inputCol='articleVector', outputCol='hashes', numHashTables=4.0, bucketLength=10.0)
model = brp.fit(旧文章向量)
</code></pre>
<p>计算相似的文章以及相似度<br>
# 计算文章和文章之间的相似度<br>
similar = model.approxSimilarityJoin(新增文章向量, 新增文章向量, 2.0, distCol='Similarity')  # 输出列名<br>
similar.sort(['EuclideanDistance']).show()<br>
计算结果：<br>
datasetA(新增),    datasetB（旧的）,     Similarity<br>
[2,[文章向量]]		[5,[文章向量]]			0.0051<br>
[1,[文章向量]]		[3,[文章向量]]			0.0054<br>
[2,[文章向量]]		[8,[文章向量]]			0.0053<br>
[1,[文章向量]]		[4,[文章向量]]			0.0052<br>
[2,[文章向量]]		[7,[文章向量]]			0.0055<br>
...</p>
<h3 id="文章相似度存储-hbase">文章相似度存储 HBase</h3>
<h4 id="存储目标存储-文章相似文章-相似度">存储目标：存储 文章，相似文章， 相似度</h4>
<p>调用foreachPartition：<br>
foreachPartition不同于map和mapPartition。<br>
无返回结果，主要用于离线分析之后的数据（数据库存储等）落地<br>
如果想要返回新的一个数据DF，就使用map后者。<br>
我们需要建立一个HBase存储文章相似度的表：<br>
create 'article_similar', 'similar'</p>
<pre><code># 存储格式如下：
		 表           row_key  column_family   value
	put 'article_similar', '1', 	'similar:1',    0.2
	put 'article_similar', '1', 	'similar:2',    0.34
</code></pre>
<p>HBase 开启失败可能的原因的：</p>
<ol>
<li>
<p>时间未同步的解决办法：<br>
ntpdate 0.cn.pool.ntp.org<br>
或<br>
ntpdate ntp1.aliyun.com</p>
</li>
<li>
<p>thrift服务未开启的解决办法：<br>
hbase-daemon.sh start thrift<br>
happybase代码实现：<br>
def save_hbase(partition):<br>
import happybase<br>
pool = happybase.ConnectionPool(size=3, host='hadoop-master')</p>
<p>with pool.connection() as conn:<br>
# 建议表的连接<br>
table = conn.table('article_similar')<br>
for row in partition:<br>
if row.datasetA.article_id == row.datasetB.article_id:<br>
pass<br>
else:<br>
table.put(str(row.datasetA.article_id).encode(),<br>
{&quot;similar:{}&quot;.format(row.datasetB.article_id).encode(): b'%0.4f' % (row.EuclideanDistance)})<br>
# 手动关闭所有的连接<br>
conn.close()</p>
<p>similar.foreachPartition(save_hbase)</p>
</li>
</ol>
<h3 id="文章相似度增量更新代码整理">文章相似度增量更新代码整理</h3>
<pre><code>def compute_article_similar(self, articleProfile):
    &quot;&quot;&quot;
    计算增量文章与历史文章的相似度 word2vec
    :return:
    &quot;&quot;&quot;
    # 得到要更新的新文章通道类别(不采用)
    # all_channel = set(articleProfile.rdd.map(lambda x: x.channel_id).collect())
    def avg(row):
        x = 0
        for v in row.vectors:
            x += v
        #  将平均向量作为article的向量
        return row.article_id, row.channel_id, x / len(row.vectors)

    for channel_id, channel_name in CHANNEL_INFO.items():

        profile = articleProfile.filter('channel_id = {}'.format(channel_id))
        wv_model = Word2VecModel.load(
            &quot;hdfs://hadoop-master:9000/headlines/models/channel_%d_%s.word2vec&quot; % (channel_id, channel_name))
        vectors = wv_model.getVectors()

        # 计算向量
        profile.registerTempTable(&quot;incremental&quot;)
        articleKeywordsWeights = ua.spark.sql(
            &quot;select article_id, channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight where channel_id=%d&quot; % channel_id)

        articleKeywordsWeightsAndVectors = articleKeywordsWeights.join(vectors,
                                                        vectors.word == articleKeywordsWeights.keyword, &quot;inner&quot;)
        articleKeywordVectors = articleKeywordsWeightsAndVectors.rdd.map(
            lambda r: (r.article_id, r.channel_id, r.keyword, r.weight * r.vector)).toDF(
            [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;weightingVector&quot;])

        articleKeywordVectors.registerTempTable(&quot;tempTable&quot;)
        articleVector = self.spark.sql(
            &quot;select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from tempTable group by article_id&quot;).rdd.map(
            avg).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;articleVector&quot;])

        # 写入数据库
        def toArray(row):
            return row.article_id, row.channel_id, [float(i) for i in row.articleVector.toArray()]
        articleVector = articleVector.rdd.map(toArray).toDF(['article_id', 'channel_id', 'articleVector'])
        articleVector.write.insertInto(&quot;article_vector&quot;)

        import gc
        del wv_model
        del vectors
        del articleKeywordsWeights
        del articleKeywordsWeightsAndVectors
        del articleKeywordVectors
        gc.collect()

        # 得到历史数据, 转换成固定格式使用LSH进行求相似
        train = self.spark.sql(&quot;select * from article_vector where channel_id=%d&quot; % channel_id)

        def _array_to_vector(row):
            return row.article_id, Vectors.dense(row.articleVector)
        train = train.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])
        test = articleVector.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])

        brp = BucketedRandomProjectionLSH(inputCol='articleVector', outputCol='hashes', seed=12345,
                                          bucketLength=1.0)
        model = brp.fit(train)
        similar = model.approxSimilarityJoin(test, train, 2.0, distCol='EuclideanDistance')

        def save_hbase(partition):
            import happybase
            for row in partition:
                pool = happybase.ConnectionPool(size=3, host='hadoop-master')
                # article_similar article_id similar:article_id sim
                with pool.connection() as conn:
                    table = connection.table(&quot;article_similar&quot;)
                    for row in partition:
                        if row.datasetA.article_id == row.datasetB.article_id:
                            pass
                        else:
                            table.put(str(row.datasetA.article_id).encode(),
                                      {b&quot;similar:%d&quot; % row.datasetB.article_id: b&quot;%0.4f&quot; % row.EuclideanDistance})
                    conn.close()
        similar.foreachPartition(save_hbase)
</code></pre>
<p>添加函数到主函数中文件中，修改update更新代码：<br>
ua = UpdateArticle()<br>
sentence_df = ua.merge_article_data()<br>
if sentence_df.rdd.collect():<br>
rank, idf = ua.generate_article_label(sentence_df)<br>
articleProfile = ua.get_article_profile(rank, idf)<br>
ua.compute_article_similar(articleProfile)</p>
<h1 id="用户画像构建与更新">用户画像构建与更新</h1>
<h3 id="组成成分">组成成分</h3>
<p>用户基本信息+用户行为(历史+新增)<br>
用户行为包括：<br>
hive&gt; select * from user_action limit 1;<br>
OK<br>
2019-03-05 10:19:40		0		{&quot;action&quot;:&quot;exposure&quot;,&quot;userId&quot;:&quot;2&quot;,&quot;articleId&quot;:&quot;[16000, 44371, 16421, 16181, 17454]&quot;,&quot;algorithmCombine&quot;:&quot;C2&quot;} 2019-03-05<br>
我们需要对用户行为（字典）数据格式平铺处理<br>
user_id	action_time	article_id	share	click	collected	exposure	read_time</p>
<h3 id="步骤">步骤：</h3>
<p>1、创建HIVE基本数据表<br>
2、读取固定时间内的用户行为日志<br>
3、进行用户日志数据处理<br>
4、存储到user_article_basic表中</p>
<h4 id="创建hive基本数据表">创建HIVE基本数据表</h4>
<pre><code>create table user_article_basic(
user_id BIGINT comment &quot;userID&quot;,
action_time STRING comment &quot;user actions time&quot;,
article_id BIGINT comment &quot;articleid&quot;,
channel_id INT comment &quot;channel_id&quot;,
shared BOOLEAN comment &quot;is shared&quot;,
clicked BOOLEAN comment &quot;is clicked&quot;,
collected BOOLEAN comment &quot;is collected&quot;,
exposure BOOLEAN comment &quot;is exposured&quot;,
read_time STRING comment &quot;reading time&quot;)
COMMENT &quot;user_article_basic&quot;
CLUSTERED by (user_id) into 2 buckets
STORED as textfile
LOCATION '/user/hive/warehouse/profile.db/user_article_basic';
</code></pre>
<h4 id="读取增量用户行为数据-固定时间内的用户行为日志">读取增量用户行为数据-固定时间内的用户行为日志</h4>
<p>关联历史日期文件<br>
# 在进行日志信息的处理之前，先将我们之前建立的user_action表之间进行所有日期关联，spark hive不会自动关联<br>
import pandas as pd<br>
from datetime import datetime</p>
<pre><code>def datelist(beginDate, endDate):
	date_list=[datetime.strftime(x,'%Y-%m-%d') for x in list(pd.date_range(start=beginDate, end=endDate))]
	return date_list

dl = datelist(&quot;2019-03-05&quot;, time.strftime(&quot;%Y-%m-%d&quot;, time.localtime()))

fs = pyhdfs.HdfsClient(hosts='hadoop-master:50070')
for d in dl:
	try:
		_localions = '/user/hive/warehouse/profile.db/user_action/' + d
		if fs.exists(_localions):
			uup.spark.sql(&quot;alter table user_action add partition (dt='%s') location '%s'&quot; % (d, _localions))
	except Exception as e:
		# 已经关联过的异常忽略,partition与hdfs文件不直接关联
		pass
sqlDF = uup.spark.sql(
	&quot;select actionTime, readTime, channelId, param.articleId, param.algorithmCombine, param.action, param.userId from user_action where dt={}&quot;.format(time_str)
)
</code></pre>
<h4 id="原始数据格式与目标数据格式">原始数据格式与目标数据格式</h4>
<p>原始数据格式：（行为参数都算在 action列内）<br>
actionTime	readTime	channelID	articleId	算法名称	action		userId<br>
123						1						exposure	1<br>
321						1						click		1<br>
目标数据格式（行为参数1拆4，action列被爆炸为4列，均为bool类型）<br>
user_id	action_time	article_id	shared	clicked	collected	expore	read_time<br>
1					1			false	false	false		true<br>
1					2			false	true	false		true</p>
<h4 id="进行用户日志数据格式处理">进行用户日志数据格式处理</h4>
<pre><code>if sqlDF.collect():
def _compute(row):
    # 进行判断行为类型
    _list = []
    if row.action == &quot;exposure&quot;:
        for article_id in eval(row.articleId):
            _list.append(
                [row.userId, row.actionTime, article_id, row.channelId, False, False, False, True, row.readTime])
        return _list
    else:
        class Temp(object):
            shared = False
            clicked = False
            collected = False
            read_time = &quot;&quot;

        _tp = Temp()
        if row.action == &quot;share&quot;:
            _tp.shared = True
        elif row.action == &quot;click&quot;:
            _tp.clicked = True
        elif row.action == &quot;collect&quot;:
            _tp.collected = True
        elif row.action == &quot;read&quot;:
            _tp.clicked = True
        else:
            pass
        _list.append(
            [row.userId, row.actionTime, int(row.articleId), row.channelId, _tp.shared, _tp.clicked, _tp.collected,
             True,
             row.readTime])
        return _list
# 进行处理
# 查询内容，将原始日志表数据进行处理
_res = sqlDF.rdd.flatMap(_compute)
data = _res.toDF([&quot;user_id&quot;, &quot;action_time&quot;,&quot;article_id&quot;, &quot;channel_id&quot;, &quot;shared&quot;, &quot;clicked&quot;, &quot;collected&quot;, &quot;exposure&quot;, &quot;read_time&quot;])
</code></pre>
<h4 id="将上述目标格式的数据按照-userid-和-articleid-分组">将上述目标格式的数据按照 userid 和 articleid 分组</h4>
<p>先合并历史数据，存储到user-article-basic表中<br>
# 合并历史数据，插入表中<br>
old = uup.spark.sql(&quot;select * from user_article_basic&quot;)<br>
# 由于合并的结果中不是对于user_id和article_id唯一的，一个用户会对文章多种操作<br>
new_old = old.unionAll(data)<br>
HIVE目前支持hive终端操作ACID，不支持python的pyspark原子性操作，并且开启配置中开启原子性相关配置也不行。<br>
new_old.registerTempTable(&quot;temptable&quot;)<br>
# 按照用户，文章分组存放进去<br>
uup.spark.sql(<br>
&quot;insert overwrite table user_article_basic select user_id, max(action_time) as action_time, &quot;<br>
&quot;article_id, max(channel_id) as channel_id, max(shared) as shared, max(clicked) as clicked, &quot;<br>
&quot;max(collected) as collected, max(exposure) as exposure, max(read_time) as read_time from temptable &quot;<br>
&quot;group by user_id, article_id&quot;)</p>
<h3 id="用户画像标签权重计算">用户画像标签权重计算</h3>
<h4 id="如何存储">如何存储</h4>
<p>用户画像，作为特征提供给一些算法排序，方便与快速读取使用<br>
选择存储在Hbase当中。<br>
然后用 Hive 外表关联 hbase<br>
如果离线分析也想要使用我们可以建立HIVE到Hbase的外部表。</p>
<h4 id="hbase表设计">HBase表设计</h4>
<pre><code>		table_name		column1 column2  column3 
create 'user_profile', 'basic','partial','env'

					row_key   column_family					value
put 'user_profile', 'user:2', 'partial:{channel_id}:{topic}': weights
put 'user_profile', 'user:2', 'basic:{info}': value
put 'user_profile', 'user:2', 'env:{info}': value
</code></pre>
<h4 id="hive表设计">Hive表设计</h4>
<pre><code>create external table user_profile_hbase(
user_id STRING comment &quot;userID&quot;,
information map&lt;string, DOUBLE&gt; comment &quot;user basic information&quot;,
article_partial map&lt;string, DOUBLE&gt; comment &quot;article partial&quot;,
env map&lt;string, INT&gt; comment &quot;user env&quot;)
COMMENT &quot;user profile table&quot;
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,basic:,partial:,env:&quot;)
TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;user_profile&quot;);
</code></pre>
<h4 id="spark-sql关联表读取问题">Spark SQL关联表读取问题</h4>
<p>创建关联表之后，离线读取表内容需要一些依赖包。解决办法：<br>
拷贝/root/bigdata/hbase/lib/下面hbase-<em>.jar 到 /root/bigdata/spark/jars/目录下<br>
拷贝/root/bigdata/hive/lib/h</em>.jar 到 /root/bigdata/spark/jars/目录下<br>
上述操作三台虚拟机都执行一遍。</p>
<h4 id="用户画像频道关键词获取与权重计算">用户画像频道关键词获取与权重计算</h4>
<p>目标：获取用户1~25频道(不包括推荐频道)的关键词，并计算权重<br>
1、读取user-article-basic表，合并行为表与文章画像中的主题词<br>
2、进行用户权重计算公式、同时落地存储<br>
# 获取基本用户行为信息，然后进行文章画像的主题词合并<br>
uup.spark.sql(&quot;use profile&quot;)<br>
# 取出日志中的channel_id<br>
user_article_ = uup.spark.sql(&quot;select * from user_article_basic&quot;).drop('channel_id')<br>
uup.spark.sql('use article')<br>
article_label = uup.spark.sql(&quot;select article_id, channel_id, topics from article_profile&quot;)<br>
# 合并使用文章中正确的channel_id<br>
click_article_res = user_article_.join(article_label, how='left', on=['article_id'])<br>
将关键词字段的列表爆炸<br>
import pyspark.sql.functions as F<br>
click_article_res = click_article_res.withColumn('topic', F.explode('topics')).drop('topics')<br>
爆炸后格式如下：<br>
user_id article_id topic ...<br>
1     1			 python<br>
1     1			 golang<br>
1     1			 linux<br>
...    ...		  ...</p>
<h4 id="用户画像之标签权重算法">用户画像之标签权重算法</h4>
<p>用户标签权重 =( 行为类型权重之和) × 时间衰减<br>
行为类型权重 的 分值 的确定需要整体协商<br>
行为					分值<br>
阅读时间&lt;1000ms		  1<br>
阅读时间&gt;=1000ms	  2<br>
收藏					2<br>
分享					3<br>
点击					5<br>
时间衰减: 1/(log(t)+1) ,t为时间发生时间距离当前时间的大小。<br>
# 计算每个用户对每篇文章的标签的权重<br>
def compute_weights(rowpartition):<br>
&quot;&quot;&quot;处理每个用户对文章的点击数据<br>
&quot;&quot;&quot;<br>
weightsOfaction = {<br>
&quot;read_min&quot;: 1,<br>
&quot;read_middle&quot;: 2,<br>
&quot;collect&quot;: 2,<br>
&quot;share&quot;: 3,<br>
&quot;click&quot;: 5<br>
}</p>
<pre><code>import happybase
from datetime import datetime
import numpy as np
#  用于读取hbase缓存结果配置
pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090)

# 读取文章的标签数据
# 计算权重值
# 时间间隔
for row in rowpartition:
    t = datetime.now() - datetime.strptime(row.action_time, '%Y-%m-%d %H:%M:%S')
    # 时间衰减系数
    time_exp = 1 / (np.log(t.days + 1) + 1)

    if row.read_time == '':
        r_t = 0
    else:
        r_t = int(row.read_time)
    # 浏览时间分数
    is_read = weightsOfaction['read_middle'] if r_t &gt; 1000 else weightsOfaction['read_min']

    # 每个词的权重分数
    weigths = time_exp * (
                row.shared * weightsOfaction['share'] + row.collected * weightsOfaction['collect'] + row.
                clicked * weightsOfaction['click'] + is_read)

#        with pool.connection() as conn:
#            table = conn.table('user_profile')
#            table.put('user:{}'.format(row.user_id).encode(),
#                      {'partial:{}:{}'.format(row.channel_id, row.topic).encode(): json.dumps(
#                          weigths).encode()})
#            conn.close()

click_article_res.foreachPartition(compute_weights)
</code></pre>
<p>落地Hbase中之后，在HBASE中查询，happybase或者hbase终端<br>
import happybase<br>
# 用于读取hbase缓存结果配置<br>
pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090)</p>
<pre><code>with pool.connection() as conn:
	table = conn.table('user_profile')
	# 获取每个键 对应的所有列的结果
	data = table.row(b'user:2', columns=[b'partial'])
	conn.close()

# 等价于  hbase(main):015:0&gt; get 'user_profile', 'user:2'
</code></pre>
<h3 id="基础信息画像更新">基础信息画像更新</h3>
<pre><code>def update_user_info(self):
    &quot;&quot;&quot;
    更新用户的基础信息画像
    :return:
    &quot;&quot;&quot;
    self.spark.sql(&quot;use toutiao&quot;)

    user_basic = self.spark.sql(&quot;select user_id, gender, birthday from user_profile&quot;)

    # 更新用户基础信息
    def _udapte_user_basic(partition):
        &quot;&quot;&quot;更新用户基本信息
        &quot;&quot;&quot;
        import happybase
        #  用于读取hbase缓存结果配置
        pool = happybase.ConnectionPool(size=10, host='172.17.0.134', port=9090)
        for row in partition:

            from datetime import date
            age = 0
            if row.birthday != 'null':
                born = datetime.strptime(row.birthday, '%Y-%m-%d')
                today = date.today()
                age = today.year - born.year - ((today.month, today.day) &lt; (born.month, born.day))

            with pool.connection() as conn:
                table = conn.table('user_profile')
                table.put('user:{}'.format(row.user_id).encode(),
                          {'basic:gender'.encode(): json.dumps(row.gender).encode()})
                table.put('user:{}'.format(row.user_id).encode(),
                          {'basic:birthday'.encode(): json.dumps(age).encode()})
                conn.close()

    user_basic.foreachPartition(_udapte_user_basic)
    logger.info(
        &quot;{} INFO completely update infomation of basic&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
        
# hbase(main):016:0&gt; get 'user_profile', 'user:2'
</code></pre>
<h3 id="用户画像增量更新定时开启">用户画像增量更新定时开启:</h3>
<p>用户画像增量更新代码整理<br>
添加定时任务以及进程管理<br>
from offline.update_user import UpdateUserProfile</p>
<pre><code>def update_user_profile():
	&quot;&quot;&quot;
	更新用户画像
	&quot;&quot;&quot;
	uup = UpdateUserProfile()
	if uup.update_user_action_basic():
		uup.update_user_label()
		uup.update_user_info()
scheduler.add_job(update_user_profile, trigger='interval', hours=2)</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => Spark mapPartition]]></title>
        <id>https://cythonlin.github.io/post/py-greater-spark-mappartition/</id>
        <link href="https://cythonlin.github.io/post/py-greater-spark-mappartition/">
        </link>
        <updated>2020-09-29T04:20:11.000Z</updated>
        <content type="html"><![CDATA[<h1 id="正解">正解</h1>
<p>map()：每次处理一条数据</p>
<p>mapPartition()：每次处理一个分区的数据，这个分区的数据处理完后，原RDD中分区的数据才能释放，可能导致OOM</p>
<p>当内存空间较大的时候建议使用mapPartition()，以提高处理效率</p>
<h1 id="分隔">分隔</h1>
<p>下面所有是自己的思路，推到最后给自己推蒙了，可以掠过</p>
<h1 id="正文实验可忽略直奔结果">正文实验（可忽略，直奔结果）</h1>
<p>为了方便，用把组装的数据类型灌入 map 来模拟 mapPartitions<br>
功能是模拟计算 tf-idf</p>
<pre><code>class Article:
    '''文章类'''
    def __init__(self,id, indexex, tfidfs):
        self.id = id
        self.indexex = indexex   # 文章分词后的所有词索引列表
        self.tfidfs = tfidfs     # 每个词对应的TF-IDF值 列表

def f(partition):
    for row in partition:   # row 代表每个文章
        # row.indexex 代表 文章分词后的所有词索引列表
        # row.tfidfs  代表 每个词对应的TF-IDF值 列表
        word_list = list(zip(row.indexex, row.tfidfs))  
        
        for index, tfidf in word_list:    # 遍历 &quot;每个&quot;词语 的 index与tfidf
            ########### 这里 yield 是重点 ###########
            yield f'文章{row.id}', index, tfidf
            

c = map(f, 
    [   #  &lt;-为了模拟分区，这一层的列表代表partition
        [   # &lt;- 这一层模拟的是每个分区里面的文章列表
            Article(0, [1,2],[0.1,0.4] ),   # &lt;-文章0
            Article(1, [3,4],[3.4,3.7] )    # &lt;-文章1
        ] 
    ] 
)
######################## 执行 ########################
for x in c:              # 解zip
    print(list(x))       # 解yield
</code></pre>
<h1 id="结果">结果：</h1>
<p>如果使用 return 关键词，得出的最终打印结果: （不满足）</p>
<pre><code>['文章0', 1, 0.1]          
</code></pre>
<p>如果使用 yield 关键词，得出的最终打印结果:  （满足）</p>
<pre><code>[('文章0', 1, 0.1), ('文章0', 2, 0.4), ('文章1', 3, 3.4), ('文章1', 4, 3.7)]
</code></pre>
<h1 id="这里就出现了一个问题">这里就出现了一个问题：</h1>
<p>正常用法都是用 return，时常用 lambda（lambda默认也是隐式 return。）<br>
是何原因让我们不得不用 yield?<br>
一点一点往下推：</p>
<pre><code>map: 核心是 &quot;按单个数据映射&quot;
mapPartition： 核心是&quot;把数据分组，按组映射&quot;
    按组映射是没错，但我们的目的是想操作组内的每条数据。
    所以我们必须需要每次对组内数据 for循环遍历出来单独处理。         然后 返回回去。
</code></pre>
<p>那我们先用正常的 return 返回试试：</p>
<pre><code>def(partation):
    for x in partition:
        return x.name, x.age
</code></pre>
<p>也许看到这里你觉得没什么问题。。。<br>
但是不要忘了最基础的内容， return 是直接跳出 for 循环和函数的。<br>
再次强调，mapPartition是按组映射，所以仔细看上面代码:<br>
最终的mapPartition是按组映射结果就是：<br>
每组的第一个元素的集合 （因为for被return了，每组的函数也被return了）<br>
解决这种问题，有两种方式：</p>
<ol>
<li>
<p>最简单将源代码for循环内部的 return 改为 yield</p>
</li>
<li>
<p>新建临时列表过渡，return放在for外面，如下案例：</p>
<p>def f(partition):<br>
_ = []<br>
for x in partition:<br>
_.append(x)<br>
return _<br>
b = [[1,2,3], [4,5,6]]<br>
c = map(f,b)<br>
print(list(c))</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RS => 推荐系统（一）环境配置+数据收集]]></title>
        <id>https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-yi-huan-jing-pei-zhi-shu-ju-shou-ji/</id>
        <link href="https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-yi-huan-jing-pei-zhi-shu-ju-shou-ji/">
        </link>
        <updated>2020-09-29T04:17:17.000Z</updated>
        <content type="html"><![CDATA[<h1 id="python环境">Python环境</h1>
<p>miniconda创建虚拟环境：<br>
conda create -n reco_sys python=3.6.7<br>
激活/退出 虚拟环境：<br>
conda activate spider-venv<br>
conda deactivate<br>
2个slave需要安装依赖：<br>
yum -y install gcc<br>
安装模块：<br>
pip install redis supervisor apscheduler chardet jieba jupyter numpy pandas scipy scikit-learn pyspark findspark happybase pyhdfs -i https://pypi.douban.com/simple</p>
<h1 id="大数据环境">大数据环境</h1>
<h3 id="lambda环境启动脚本配置">Lambda环境启动脚本配置</h3>
<p>禁用+关闭防火墙：<br>
systemctl disable firewalld.service<br>
systemctl stop firewalld.service<br>
同步系统时间（不这样做 hbase可能启动失败）（3台都运行命令）：<br>
yum install ntpdate -y<br>
ntpdate 0.cn.pool.ntp.org</p>
<p>创建综合启动脚本<br>
vi start.sh<br>
/root/bigdata/hadoop/sbin/start-all.sh<br>
start-hbase.sh<br>
/root/bigdata/spark/sbin/start-all.sh</p>
<pre><code>vi stop.sh
    /root/bigdata/spark/sbin/stop-all.sh 
    stop-hbase.sh
    /root/bigdata/hadoop/sbin/stop-all.sh
</code></pre>
<p>开启<br>
/root/scripts/start.sh<br>
停止<br>
/root/scripts/stop.sh</p>
<h3 id="ui地址查log也可">UI地址（查log也可）：</h3>
<p>Hadoop UI:<br>
http://192.168.19.137:8088<br>
YARN UI：<br>
http://192.168.19.137:50070<br>
Hbase UI:<br>
http://192.168.19.137:16010<br>
Spark UI：<br>
http://192.168.19.137:8080/</p>
<h3 id="数据库运行">数据库运行：</h3>
<p>MySQL启动：<br>
systemctl start docker<br>
docker start mysql<br>
Hive元数据服务开启:<br>
nohup hive --service metastore &amp;</p>
<h3 id="spark相关问题">spark相关问题</h3>
<p>spark on yarn 启动巨慢，解决办法：<br>
hadoop fs -mkdir -p /system/spark-lib<br>
hadoop fs -put /root/bigdata/spark-2.2.2-bin-hadoop2.7/jars/* /system/spark-lib<br>
hadoop fs -chmod -R 755 /system/spark-lib<br>
cd $SPARK_HOME/conf<br>
cp spark-defaults.conf.template spark-defaults.conf<br>
vi spark-defaults.conf<br>
spark.yarn.jars    hdfs://192.168.19.137:9000//system/spark-lib/*</p>
<pre><code>如果用的是 jupyter, 记得重启 jupyter服务
jupyter notebook --allow-root --ip 0.0.0.0
</code></pre>
<h3 id="hdfs-hive相关问题">HDFS-Hive相关问题</h3>
<p>内部表修改为外部表：<br>
alter table user_profile SET TBLPROPERTIES('EXTERNAL'='TRUE');</p>
<h1 id="数据构成">数据构成</h1>
<h3 id="数据库1-toutiao">数据库1： toutiao</h3>
<pre><code>news_article_basic   # 文章标题
news_article_content  # 文章内容  
news_channel       # 文章频道（类别）
user_basic			  # 用户业务数据 
user_profile       # 用户私人信息
</code></pre>
<h3 id="数据库2-profile">数据库2： profile</h3>
<pre><code>user_action			  # 用户行为日志
</code></pre>
<h3 id="数据库2-article">数据库2： article</h3>
<pre><code>article_data       # 合并文章标题+内容+频道后的存储结果
...
</code></pre>
<h1 id="数据迁移sqoop">数据迁移（Sqoop）</h1>
<h3 id="耗时">耗时</h3>
<pre><code>4000w (10+g): 30+ min
</code></pre>
<h3 id="检测sqoop是否能连通mysql-并列出mysql所有数据库">检测Sqoop是否能连通MySQL, 并列出MySQL所有数据库:</h3>
<pre><code>sqoop list-databases --connect jdbc:mysql://192.168.19.137:3306/ --username root -P
</code></pre>
<h3 id="全量导入方式不推荐">全量导入方式（不推荐）：</h3>
<pre><code>#!/bin/bash
array=(user_profile user_basic news_user_channel news_channel user_follow user_blacklist user_search news_collection news_article_basic news_article_content news_read news_article_statistic user_material)

for table_name in ${array[@]};
do
    sqoop import \
        --connect jdbc:mysql://192.168.19.137/toutiao \
        --username root \
        --password password \
        --table $table_name \
        --m 5 \
        --hive-home /root/bigdata/hive \
        --hive-import \
        --create-hive-table  \
        --hive-drop-import-delims \
        --warehouse-dir /user/hive/warehouse/toutiao.db \
        --hive-table toutiao.$table_name
done
</code></pre>
<h3 id="增量导入方式">增量导入方式</h3>
<p>方式1： 通过指定递增的字段来导入（不推荐，因为某些字段的值不是递增的）<br>
append：即通过指定一个递增的列，如：--incremental append --check-column num_iid --last-value 0<br>
方式2：incremental： 时间戳<br>
--incremental lastmodified <br>
--check-column column <br>
--merge-key key <br>
--last-value '2012-02-01 11:0:00'</p>
<pre><code>就是只导入check-column的列比'2012-02-01 11:0:00'更大（新）的数据,按照key合并
</code></pre>
<h3 id="增量导入位置">增量导入位置</h3>
<ol>
<li>直接sqoop导入到hive(–incremental lastmodified 模式不支持导入 Hive )</li>
<li>sqoop导入到hdfs，然后建立hive表关联<br>
--target-dir /user/hive/warehouse/toutiao.db/</li>
</ol>
<h3 id="sqoop导入到hdfshive表关联到hdfs填坑">sqoop导入到hdfs，hive表关联到hdfs填坑</h3>
<p>现象：<br>
查出一堆 null<br>
原因：<br>
sqoop 导出的 hdfs 分片数据，都是使用逗号 , 分割的。<br>
由于 hive 默认的分隔符是 /u0001（Ctrl+A）,为了平滑迁移，需要在创建表格时指定数据的分割符号。<br>
解决方式：<br>
导入数据到hive中，需要在创建HIVE表加入 row format delimited fields terminated by ','</p>
<h3 id="sqoop迁移到hdfs后hive创建表并指定关联位置实例">sqoop迁移到hdfs后，Hive创建表并指定关联位置实例</h3>
<pre><code>create table user_profile(
	user_id BIGINT comment &quot;userID&quot;,
	gender BOOLEAN comment &quot;gender&quot;)
COMMENT &quot;toutiao user profile&quot;
row format delimited fields terminated by ','
LOCATION '/user/hive/warehouse/toutiao.db/user_profile';
</code></pre>
<p>注：<br>
5个表中，只有 news_article_content<br>
（因为这个表奇怪字符太多，是全量导入的，hive不需要手动创建表，会自动创建的）<br>
而这个表从 hdfs 拿到的数据是已经做过过滤的，所以不需要 加分隔符了，也就是不需要下面这行代码：<br>
row format delimited fields terminated by ','</p>
<h1 id="flume日志收集到hive中">Flume日志收集到Hive中</h1>
<h3 id="新建数据库">新建数据库</h3>
<pre><code>create database if not exists profile comment &quot;use action&quot; location '/user/hive/warehouse/profile.db/';
</code></pre>
<h3 id="创建表的格式语法实例如下">创建表的格式语法实例如下：</h3>
<pre><code>create table user_action(
actionTime STRING comment &quot;user actions time&quot;,
readTime STRING comment &quot;user reading time&quot;,
channelId INT comment &quot;article channel id&quot;,
param map&lt;string, string&gt; comment &quot;action parameter&quot;)
COMMENT &quot;user primitive action&quot;
PARTITIONED BY(dt STRING)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
LOCATION '/user/hive/warehouse/profile.db/user_action';
</code></pre>
<p>文档中：Hive建表，有个问题 map 需要指定数据类型:<br>
param map&lt;string, string&gt; comment &quot;action parameter&quot;)</p>
<pre><code>map 要向上面一样指定 &lt;string, string&gt; 才可以， 不然会报如下错误：
'''mismatched input 'comment' expecting &lt; near 'map' in map type'''
</code></pre>
<p>疑难参数解读：<br>
PARTITIONED BY(dt STRING)：  hive按照 dt 字段分区<br>
为什么要分区：<br>
Hive适合处理大的文件内容量，少的文件数量。<br>
Flume收集日志可能来一点日志就加到一个新文件中。<br>
如此一来，文件零散的特别多。 Hive处理的会很慢。</p>
<pre><code>		所以，Hive指定个分区，来把小文件们分成几大块（就是几个分区），这样处理会更快
        
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'： 处理Json格式数据
</code></pre>
<p>数据导入步骤如下（虚代替Flume）（操作完成后是查不到数据的，需要关联，下面会解释）：<br>
hadoop fs -put /root/data/backup/profile.db/user_action/*  /user/hive/warehouse/profile.db/user_action/</p>
<pre><code># 删除: hadoop fs -rmr /user/hive/warehouse/profile.db/*
</code></pre>
<h3 id="flume-收集配置">Flume 收集配置</h3>
<p>进入flume/conf目录<br>
创建一个collect _ click.conf的文件，写入flume的配置：<br>
a1.sources = s1<br>
a1.sinks = k1<br>
a1.channels = c1</p>
<pre><code>a1.sources.s1.channels= c1
a1.sources.s1.type = exec
a1.sources.s1.command = tail -F /root/logs/userClick.log
a1.sources.s1.interceptors=i1 i2
a1.sources.s1.interceptors.i1.type=regex_filter
a1.sources.s1.interceptors.i1.regex=\\{.*\\}
a1.sources.s1.interceptors.i2.type=timestamp

# channel1
a1.channels.c1.type=memory
a1.channels.c1.capacity=30000
a1.channels.c1.transactionCapacity=1000

# k1
a1.sinks.k1.type=hdfs
a1.sinks.k1.channel=c1
a1.sinks.k1.hdfs.path=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d
a1.sinks.k1.hdfs.useLocalTimeStamp = true
a1.sinks.k1.hdfs.fileType=DataStream
a1.sinks.k1.hdfs.writeFormat=Text
a1.sinks.k1.hdfs.rollInterval=0
a1.sinks.k1.hdfs.rollSize=10240
a1.sinks.k1.hdfs.rollCount=0
a1.sinks.k1.hdfs.idleTimeout=60
</code></pre>
<p>参数说明：<br>
sources：为实时查看文件末尾，interceptors解析json文件<br>
channels：指定内存存储，并且制定batchData的大小，PutList和TakeList的大小见参数，Channel总容量大小见参数<br>
指定sink：形式直接到hdfs，以及路径，文件大小策略默认1024、event数量策略、文件闲置时间<br>
开始收集：<br>
/root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1</p>
<h3 id="hive-关联分区">Hive 关联分区：</h3>
<p>如果不关联分区，无论是 Flume收集到HDFS的分区数据，还是我们传进去HDFS模拟的分区数据 通过Hive是查不到的<br>
关联分区如下操作：<br>
alter table user_action add partition (dt='2018-12-11') location &quot;/user/hive/warehouse/profile.db/user_action/2018-12-11/&quot;</p>
<h1 id="进程管理-supervisor">进程管理 Supervisor</h1>
<h3 id="正常配置流程">正常配置流程</h3>
<p>安装:<br>
pip install supervisor<br>
创建配置文件（当前目录执行，或者主目录执行都可，总配置文件就会生成到当前目录下）：<br>
echo_supervisord_conf &gt; supervisord.conf<br>
创建自定义配置文件目录：<br>
mkdir /etc/supervisor<br>
vim 打开编辑supervisord.conf文件，修改最后1行：<br>
[include]<br>
files = relative/directory/<em>.ini<br>
为<br>
[include]<br>
files = /etc/supervisor/</em>.conf<br>
将最开始生成的 supervisord.conf 复制到 /etc/ 下， 然后主文件就不用动了：<br>
cp supervisord.conf /etc/<br>
最后在  /etc/supervisor 这个目录中，自定义我们自己需要的 启动程序的配置文件模板，这里为 vi reco.conf：<br>
见下面Flume案例</p>
<h3 id="flumesupervisor-配置案例">Flume+Supervisor 配置案例</h3>
<p>flume启动需要相关hadoop,java环境，可以在shell脚本汇总添加:<br>
先创建一个存放此shell脚本的目录：<br>
mkdir /root/toutiao_project/scripts<br>
打开文件:<br>
vi /root/toutiao_project/scripts/collect_click.sh<br>
并写入:<br>
#!/usr/bin/env bash</p>
<pre><code>export JAVA_HOME=/root/bigdata/jdk
export HADOOP_HOME=/root/bigdata/hadoop
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin

/root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1
</code></pre>
<p>并在	/etc/supervisor 的reco.conf添加:<br>
[program:flume]<br>
command=/bin/bash /root/toutiao_project/scripts/collect_click.sh<br>
user=root<br>
autorestart=true<br>
redirect_stderr=true<br>
stdout_logfile=/root/logs/collect.log<br>
loglevel=info<br>
stopsignal=KILL<br>
stopasgroup=true<br>
killasgroup=true</p>
<p>启动 supervisor服务:<br>
supervisord -c /etc/supervisord.conf<br>
查看 supervisor是否运行：<br>
ps aux | grep supervisord<br>
管理 supervisor进程管理界面，输入命令：<br>
supervisorctl<br>
管理界面可通过如下 命令+进程名 来管理进程：<br>
start flume<br>
stop flume<br>
restart flume</p>
<pre><code>status		# 查看所有进程状态
update		# 重启配置文件修改过的程序
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => PySpark-Spark SQL]]></title>
        <id>https://cythonlin.github.io/post/py-greater-pyspark-spark-sql/</id>
        <link href="https://cythonlin.github.io/post/py-greater-pyspark-spark-sql/">
        </link>
        <updated>2020-09-29T04:16:24.000Z</updated>
        <content type="html"><![CDATA[<h1 id="spark-sql">Spark SQL</h1>
<p>Spark SQL 分为三类：</p>
<ol>
<li>SQL</li>
<li>DataFrame (参考pandas，但略有不同)</li>
<li>Datasets (由于python是动态的，所以不支持python)</li>
</ol>
<h3 id="初始环境">初始环境：</h3>
<pre><code>import findspark
findspark.init()

from pyspark.sql import SparkSession        
spark = SparkSession.builder.appName('myspark').getOrCreate()    # 初始化session
# spark.sparkContext.parallelize([1,2,3,4]).collect()  # 里面包含之前说过的sparkContext
...
中间这部分留给下面写
...
spark.stop()         # 关闭 session
</code></pre>
<p>从json导入为df:<br>
df = spark.read.json(&quot;file:///home/lin/data/user.json&quot;,multiLine=True)<br>
打印DF字段信息：<br>
df.printSchema()</p>
<pre><code>root
 |-- age: long (nullable = true)
 |-- gender: string (nullable = true)
 |-- name: string (nullable = true)
</code></pre>
<h3 id="crud">CRUD</h3>
<p>增<br>
from pyspark.sql import functions as f</p>
<pre><code># schema就相当于 pandas 指定的 columns, 双层序列，  [2x4] 的样本
df1 = spark.createDataFrame([[1,2,3,4],[5,6,7,8]],schema=['1_c','2_c', '3_c', '4_c'])
+-----+-----+-----+-----+
|1\_col|2\_col|3\_col|4\_col|
+-----+-----+-----+-----+
|    1|    2|    3|    4|
|    5|    6|    7|    8|
+-----+-----+-----+-----+

# lit可以在指定空列的时候，指定 null值， 或者 int型（里面有很多类型，可以发现）
# df2 = df1.withColumn('null_col', f.lit(None)).withColumn('digit_col', f.lit(2))
df2 = df1.withColumn('5_col', df1['4_col']+1)   # 在原来列字段基础上。
df2.show()
</code></pre>
<p>删<br>
df2 = df1.drop('age')        # 删除 age列<br>
df2 = df1.dropna()           # 删除空行<br>
df2 = df1.drop_duplicates()  # 删除重复-行<br>
改<br>
和&quot;增&quot;，差不多，只不过字段，指定为原有字段字符串即可。</p>
<p>查<br>
下面的讲的（投影、过滤、排序、分组），几乎都是查。</p>
<h3 id="投影">投影</h3>
<p>投影所有：<br>
df.show(n=20)        # 默认就是 n=20 只返回 前20条记录</p>
<pre><code>+---+------+--------+
|age|gender|    name|
+---+------+--------+
| 18|   man|zhangsan|
+---+------+--------+
</code></pre>
<p>选中某列投影：<br>
df.select('name','age').show()        # 若直接写 '*', 和直接 df.show()是一个效果</p>
<pre><code>+--------+---+
|    name|age|
+--------+---+
|zhangsan| 18|
+--------+---+
</code></pre>
<p>或者用另两种方式投影（投影过程可计算）：<br>
df.select(df['name'],df['age']+20).show() # 同上，这是另一种写法，注意一下列名<br>
df.select(df.name, df.age+20).show()      # 同上，这是另二种写法，注意一下列名</p>
<pre><code>+--------+----------+
|    name|(age + 20)|
+--------+----------+
|zhangsan|        38|
+--------+----------+
</code></pre>
<p>取出前N条DF，并转化为 [ {} , {} ] 格式<br>
df_user.take(1)  # [Row(age=18, name='张三')]<br>
df_user.head(1)  # [Row(age=18, name='张三')]<br>
df_user.first()  # Row(age=18, name='张三')    # 注意,无列表</p>
<h3 id="排序">排序</h3>
<pre><code>df_user.head(1)
df_user.sort(df_user.name.desc()).show()

# 另外说明一点, df的每个熟悉都有,一些操作符函数, desc()就是一种操作符函数
</code></pre>
<h3 id="过滤">过滤：</h3>
<pre><code>df.filter( df['age'] &gt; 15).show()

+---+------+----+
|age|gender|name|
+---+------+----+
+---+------+----+
</code></pre>
<h3 id="分组">分组：</h3>
<pre><code>df.groupBy('name').count().show()

+--------+-----+
|    name|count|
+--------+-----+
|zhangsan|    1|
+--------+-----+
</code></pre>
<h3 id="join">Join</h3>
<pre><code>df_user.join(df_user, on=df_user.name==df_user.name, how='inner').show()

+----+---+----+---+
|name|age|name|age|
+----+---+----+---+
|李四| 20|李四| 20 |
|张三| 18|张三| 18 |
+----+---+----+---+
# 特别提醒， 此 Join， 只要都进来是 DF格式的任何数据库，都可 Join
# 比如： MySQL 和 Hive  ,  Json 也可。
</code></pre>
<h3 id="储存为临时视图表-并调用sql语句">储存为临时视图（表), 并调用sql语句：</h3>
<pre><code>df.createOrReplaceTempView('user')                  # 创建为 user临时视图
df_sql = spark.sql('select * from user').show()     # spark.sql返回的还是df, 所以要show()

+---+------+--------+
|age|gender|    name|
+---+------+--------+
| 18|   man|zhangsan|
+---+------+--------+
</code></pre>
<h3 id="rdd-与-df互转">RDD 与 DF互转</h3>
<p>RDD -&gt; DF<br>
### RDD -&gt; DF 需要把RDD做成两种格式(任选其一)<br>
### 第一种 Row 格式<br>
from pyspark import Row<br>
rdd_user = spark.sparkContext.parallelize( [('张三',18), ('李四',20)] )<br>
rdd_user_row = rdd_user.map(lambda x:Row(name=x[0], age=x[1]))<br>
print(rdd_user_row.collect()) # [Row(age=18, name='张三'), Row(age=20, name='李四')]<br>
df_user = spark.createDataFrame(rdd_user_row)</p>
<pre><code>### 第二种 [('张三', 18),('李四', 20)]
    rdd_user = spark.sparkContext.parallelize( [('张三',18), ('李四',20)] )
    df_user = rdd_user.toDF(['name', 'age'])        # 给定列名
df_user.show()
</code></pre>
<p>DF -&gt; RDD<br>
rdd_row = df_user.rdd.map(lambda x: x.asDict())  # 或者 x.name, x.age取值<br>
rdd_row.collect()  # [{'age': 18, 'name': '张三'}, {'age': 20, 'name': '李四'}]</p>
<h3 id="csv读写">CSV读写</h3>
<p>从HDFS中读取(我们先新建一个CSV并扔到HDFS中),<br>
vi mydata.csv:<br>
name,age<br>
zhangsan,18<br>
lisi, 20</p>
<pre><code>hadoop fs -mkdir /data                 # 在HDFS中新建一个目录 /data
hadoop fs -put mydata.csv /data        # 并把本地 mydata.csv扔进去 (-get可拿出来)
</code></pre>
<p>在代码中读取 HDFS数据:<br>
df = spark.read.csv(&quot;hdfs:///data/mydata.csv&quot;, header=True)<br>
df.show()<br>
# header=True 代表, csv文件的第一行作为csv的抬头(列名)<br>
# df.write.csv(&quot;hdfs:///data/mydata.csv&quot;, header=True)   # read改为write就变成了写</p>
<h3 id="hive读写">Hive读写</h3>
<p>Hive的配置与依赖之前讲过了（最值得注意的是需要先启动一个 metadata的服务）<br>
先验传送门：<a href="https://segmentfault.com/a/1190000020841646">https://segmentfault.com/a/1190000020841646</a><br>
import findspark<br>
findspark.init()</p>
<pre><code>from pyspark.sql import SparkSession   

spark = SparkSession.builder\
    .appName(&quot;Spark Hive Example&quot;)\
    .master(&quot;local[*]&quot;)\
    .config(&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;)\
    .enableHiveSupport()\
    .getOrCreate()

spark.sql(&quot;use mydatabase&quot;)        # 执行Hive 的 SQL, 切换数据库（前提你得有）
</code></pre>
<p>读：<br>
df = spark.table('person').show()  # 直接对表操作 (注意，sql语句也可)<br>
写：<br>
df = spark.table('person')<br>
df2 = df.withColumn('nickname', df.name)  # 稍微变动一下，添一个字段<br>
df2.write.saveAsTable(&quot;new_person&quot;)       # 写入新表</p>
<h3 id="mysql读写">MySQL读写</h3>
<p>读：<br>
# 注意0：有好几种方式，我只列举一个 成对的读写配置。<br>
# 注意1: url中 &quot;hive&quot;是数据库名. 你也可以起为别的名<br>
# 注意2：table的值--&quot;TBLS&quot;,  它是 MySQL中&quot;hive库&quot;中的一个表。<br>
# 注意3：特别注意！ TBLS不是我们想要的表。他只是一个大表，管理了我们hive的信息<br>
#        TBLS中的 一个列属性 &quot;TBL_NAME&quot; 才是真正我们需要的表！！</p>
<pre><code>df = spark.read.jdbc(
    url='jdbc:mysql://localhost:3306/hive?user=root&amp;password=123',
    table=&quot;TBLS&quot;,
    properties={&quot;driver&quot;:&quot;com.mysql.jdbc.Driver&quot;},
)

df.show()
df.select(&quot;TBL_NAME&quot;).show()
</code></pre>
<p>写：<br>
df.write.jdbc(<br>
url='jdbc:mysql://localhost:3306/hive?user=root&amp;password=123',<br>
table=&quot;new_table&quot;,<br>
mode='append',<br>
properties={&quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;}<br>
)</p>
<pre><code># 同是特别注意： 和读一样， 它写入的新表，也是一个整体的表结构。
#     此表的一个列&quot;TBL_NAME&quot;，它才对应了我们真正要操作的表</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => PySpark-Spark Core（RDD）]]></title>
        <id>https://cythonlin.github.io/post/py-greater-pyspark-spark-corerdd/</id>
        <link href="https://cythonlin.github.io/post/py-greater-pyspark-spark-corerdd/">
        </link>
        <updated>2020-09-29T04:15:57.000Z</updated>
        <content type="html"><![CDATA[<h1 id="前言">前言</h1>
<p>第一篇传送门：<a href="https://segmentfault.com/a/1190000020841646">https://segmentfault.com/a/1190000020841646</a></p>
<h1 id="rdd认知">RDD认知</h1>
<h3 id="rdd是什么">RDD是什么？</h3>
<p>RDD: 弹性分布式数据集（Resiliennt Distributed Datasets）</p>
<p>转为格式RDD的几种方式：<br>
1. parallelize:<br>
rdd = sc.parallelize([1,2,3,4,5])   # 里面传的就是普通python类型</p>
<pre><code>2. 读文件/读数据库/读ES等各种方式，此处以读文件为例：
    rdd = sc.textFile('file:///home/lin/data/hello.txt')
</code></pre>
<h3 id="rdd核心概念">RDD核心概念</h3>
<p>Application:<br>
application: 一个app 就是一个自定义的 py脚本（被 spark-submit提交的）或一个spark-shell<br>
app = 1个 driver + 多个executors(相当于多个进程)</p>
<pre><code>注意：数据在不同的 app之间 不能被共享， 若想要共享（需要考虑外部存储）
</code></pre>
<p>Driver:<br>
每一个.py脚本中都有一个 sparkcontext，它就是driver<br>
Worker Node:<br>
相当于standalone 的 slave节点<br>
Executor:<br>
Executor(进程)：每个Driver中都有多个 Executors</p>
<pre><code>并且可以运行多个 Tasks
</code></pre>
<p>Job:<br>
job:  对应下面即将要说的 action   : collect() 等</p>
<pre><code>一个 task 对应 一个 job  (一个 transformation 对应 一个 action)
一个 job 对应 多个 task  (多个 transformations链式调用之后，再调用一个action)
</code></pre>
<p>Task:<br>
task: 对应下面即将要说的 transformation   :map() 等<br>
每个task可用一个线程执行。多个task可并行</p>
<p>Stage:<br>
一个job被切分为多份<br>
Cluster Manager:<br>
管理 从 Standalone, YARN, Mesos 中获取的资源<br>
就是 --master 指定的参数<br>
其中 还包括 空间 内存等参数配置<br>
Cache:<br>
缓存： ### persist &amp; cache &amp; unpersist 三种API可供选择<br>
Lineage(依赖,血缘关系)：<br>
依赖：<br>
父              子               孙<br>
RDD1  -&gt; map-&gt;  RDD2 -&gt; filter-&gt; RDD3<br>
服务器1：        part1 -&gt;        part1-&gt;          part1<br>
服务器2：        part2 -&gt;        part2-&gt;          part2<br>
服务器3：        part3 -&gt;        part3-&gt;          part3</p>
<pre><code>    如上图: 假如 RDD3 的 part2 挂了， 那么就会退回到 RDD2的part2再计算一遍。
           而不是回到&quot;最初&quot;的起点。
           
窄依赖（Narrow, 依赖的很少，很窄）：
    重点:  '子part' 只依赖一个 '父part'。
    map, filter 等:  元素被摊分在每一个part中， 子part出错就找&quot;对应&quot;（一个） 父part即可。

宽依赖（Wide, 依赖的很多，很宽）：
    重点:  '子part' 依赖多个 '父part' 同时计算得到。
    shuffle操作: xxBy, join等： 子part出错 找&quot;对应&quot;（多个） 父part 重新共同计算。
</code></pre>
<p>stage:<br>
遇到 1个宽依赖， 就会做 shuffle操作。<br>
然后就会把&quot;之前&quot;的 “所有窄依赖”划分为 &quot;1个stage&quot;。<br>
最后，整体全部，也当作 &quot;1个stage&quot;。</p>
<h3 id="官档图">官档图</h3>
<p>传送门：<a href="http://spark.apache.org/docs/latest/cluster-overview.html">http://spark.apache.org/docs/latest/cluster-overview.html</a><br>
<img src="/img/bVbzEU3" alt="image.png" loading="lazy"></p>
<h1 id="rdd两大算子">RDD两大算子</h1>
<h2 id="transformation-lazy">Transformation （Lazy）</h2>
<p>主要机制：各种操作不会被立刻执行，但这些操作之间的关系会被记录下来，等待下面action调用。<br>
直观理解举例：<br>
1. 像 sqlalchemy 中的 filter(), groupby(), page()等操作<br>
2. 像 tensorflow1.x 中的 sess.run() 之前的各种操作<br>
3. 像 数据库的事务，在提交之前的各种操作<br>
接下来介绍，Transformation 的各种操作。</p>
<h3 id="map">map</h3>
<pre><code>同 python 的 map。
你只需记住RDD类型里面包裹的就是我们熟悉的python类型
所以： 
    python 的 map 怎么用， RDD对象的 map 就怎么用， 下面filter同理

只举一个语法格式例子：（下面同理）
    rdd.map(lambda x:x+1)
</code></pre>
<h3 id="filter">filter</h3>
<pre><code>同上，同python
</code></pre>
<h3 id="flatmap">flatMap</h3>
<pre><code>和 map 几乎差不多。
唯一有一点区别：
    map 每次基于单个元素，返回什么，那最终结果就是什么（最后拼成序列）。
    flatMap 每次基于单个元素，若返回的是序列（列表等），那么会自动被解包，并一字排开返回。
</code></pre>
<h3 id="groupby-和-groupbykey">groupBy 和 groupByKey</h3>
<p>说一下 没有key, 和 带有key的区别（后面同理，就不啰嗦了）：<br>
没有key:<br>
1. 一般必须需要一个 函数句柄 (lambda), 而这个句柄是针对（操作后新形成的key）使用的<br>
2. 针对一层序列   [, , ...]<br>
带有key<br>
1. 一般无参<br>
2. 针对双层序列   [(),(),...]<br>
直接上例子了（对比着看）：<br>
rdd1 = sc.parallelize(['a','b','c','a'])                      # 一层序列<br>
rdd2 = sc.parallelize( [('a',1),('b',2), ('c',3), ('a',4)] )  # 双层序列</p>
<pre><code>group1 = rdd1.groupBy(lambda x:x)   # 针对 一层序列， 注意这里，必须写 函数句柄
group2 = rdd2.groupByKey()          # 针对 双层序列

print( group1.collect() )
print( group2.collect() )

# 可以这样告诉你， 他们俩的最外层结果是一样的: [{key:value}, ...], 
结果如下 ~~~~
[
    ('a', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb7e4384c88&gt;),     
    ('b', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb7e43848d0&gt;),
    ('c', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb7e4384940&gt;)
]
# 如果加了count(), 那么它们的结果就是一样的了，返回统计的个数， 等到 action再说。
</code></pre>
<h3 id="reducebykey">reduceByKey</h3>
<p>照应双层或多层序列，或者 承接 groupByKey()<br>
rdd = sc.parallelize(['Tom', 'Jerry', 'Tom', 'Putch'])<br>
rdd.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).collect()</p>
<pre><code># 结果（可以忽略上面的 collect(), 它属于action，放在这里方便贴个结果）
&gt;&gt; [('Tom', 2), ('Jerry', 1), ('Putch', 1)]
</code></pre>
<h3 id="sortby-和-sortbykey">sortBy 和 sortByKey</h3>
<p>sortBy：根据元素排序（这里的例子是根据key排序， a[1]代表根据value排序）<br>
&gt;&gt;&gt; a = sc.parallelize([['z',1], ['b',4],['h',3]])<br>
&gt;&gt;&gt; a.sortBy(lambda a:a[0]).collect()<br>
[['b', 4], ['h', 3], ['z', 1]]<br>
&gt;&gt;&gt; a.sortBy(lambda a:a[1]).collect()<br>
[['z', 1], ['h', 3], ['b', 4]]<br>
sortByKey：根据key排序<br>
&gt;&gt;&gt; a.sortByKey().collect()<br>
[('b', 4), ('h', 3), ('z', 1)]<br>
可选参数：<br>
ascending=False  (默认为True升序)</p>
<h3 id="union">union</h3>
<pre><code>rdd1.union(rdd2)    # 相当于 python的 &quot;列表加法&quot; 或者 python的 &quot;extend&quot;
</code></pre>
<h3 id="distinct">distinct</h3>
<pre><code>rdd.distinct()      # 去重
</code></pre>
<h3 id="join">join</h3>
<p>前提： （我的理解就是，能转化成 python 字典的列表格式即可）<br>
eg:  [ [1,2], [3,4], [5,6] ]<br>
两层列表<br>
每层列表的每个元素中，  只有2个元素</p>
<pre><code>错误格式示例：
    [['a','b','c'], ['d','e','f']]

也不能说错误吧，不过若是这种3个-多个子元素的格式， join时默认会取前2个元素。其余丢弃。
</code></pre>
<p>内连接（innerJoin）：<br>
左外连接（leftOuterJoin）：<br>
右外连接（rightOuterJoin）：<br>
全外连接（fullOuterJoin）：</p>
<p>完整示例：<br>
rdd1 = sc.parallelize( [['a','b'], ['d','e']] )       # 左<br>
rdd2 = sc.parallelize( [['a','c'], ['e','f']] )       # 右<br>
# 开头说过:能转化成字典的列表格式即可，或者你可以写成这样（但是不能传原生字典进去）：<br>
rdd1 = sc.parallelize( list({'a': 'b', 'd': 'e'}.items()) )<br>
rdd2 = sc.parallelize( list({'a': 'c', 'e': 'f'}.items()) )</p>
<pre><code># 内连接（交集）
print( rdd1.join(rdd2).collect() )             # [('a', ('b', 'c'))]

# 左连接（左并集）
print( rdd1.leftOuterJoin(rdd2).collect() )    # [('d', ('e', None)), ('a', ('b', 'c'))]

# 右连接（右并集）
print( rdd1.rightOuterJoin(rdd2).collect() )   # [('a', ('b', 'c')), ('e', (None, 'f'))]

# 全连接（并集）
print( rdd1.fullOuterJoin(rdd2).collect() )    # [('d', ('e', None)), ('a', ('b', 'c')), ('e', (None, 'f'))]
</code></pre>
<h3 id="persist-cache-unpersist">persist &amp; cache &amp; unpersist</h3>
<p>cache(): 缓存<br>
persist(): 持久化<br>
unpersist(): 清空缓存  （他属于 action-立即触发， 为了方便对比，我就一起放到了这里）<br>
官档：http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence</p>
<h2 id="action-commit">Action (Commit)</h2>
<p>主要机制：拿到 transformation 记录的关系， 用 action的各种操作来真正触发、执行、返回结果。<br>
对应上面，继续直观举例：<br>
1. 像 sqlalchemy 中的 commit()<br>
2. 像 tensorflow1.x 中的 sess.run()<br>
3. 像 数据库的事务的 &quot;提交&quot;<br>
接下来介绍，Action 的各种操作。</p>
<h3 id="collect">collect</h3>
<p>执行transformation记录的关系 并 返回结果， 在Pyspark中就是RDD类型 转 Python数据类型。<br>
(中间你可以链式调用各种 transformation方法，结尾调用一个 collect(), 就可以出结果了)<br>
rdd1.xx().xx().collect()</p>
<h3 id="count">count</h3>
<p>统计元素项的个数，同上语法, 同上理念，触发返回结果<br>
rdd2 = sc.parallelize( [['a','c','d'], ['e','f','g']] )<br>
rdd2.count()  # 无参<br>
&gt;&gt; 2</p>
<h3 id="reduce">reduce</h3>
<pre><code>rdd2 = sc.parallelize( [['a','c','d'], ['e','f','g']] )
rdd2.reduce(lambda x,y:x+y)    # 参数为2个参数的函数句柄，做&quot;累&quot;的操作，（累加，累乘）等
&gt;&gt; ['a', 'c', 'd', 'e', 'f', 'g']
</code></pre>
<h3 id="take">take</h3>
<p>相当于mysql的limit操作，取前n个<br>
rdd2 = sc.parallelize( [['a','c','d'], ['e','f','g']] )<br>
rdd2.take(0)  # []<br>
rdd2.take(1)  # [['a', 'c', 'd']]<br>
rdd2.take(2)  # [['a', 'c', 'd'], ['e', 'f', 'g']]</p>
<pre><code>再次强调： take的参数是，个数的意思，而不是索引，不要混淆额
</code></pre>
<h3 id="top">top</h3>
<p>返回最大的n个元素（会自动给你排序的）<br>
rdd2 = sc.parallelize( [1,2,3,8,5,3,6,8])<br>
rdd2.top(3)<br>
&gt;&gt; [8, 8, 6]</p>
<h3 id="foreach">foreach</h3>
<p>遍历每个元素，对子元素做-对应函数句柄的操作，下面说这个action的两点注意事项：<br>
注意1： 无返回值（返回None）<br>
注意2： 通常用作 print(), 但是它不会在notebook中打印， 而是在你后台开启的spark中打印。<br>
rdd2 = sc.parallelize( [['a','c','d'], ['e','f','g']] )<br>
rdd2.foreach(lambda x:print(x))</p>
<pre><code>&gt;&gt; ['a', 'c', 'd']
   ['e', 'f', 'g']
</code></pre>
<h3 id="saveastextfile">saveAsTextFile</h3>
<pre><code>rdd = sc.textFile('file:///home/lin/data')
rdd.saveAsTextFile('file:///home/lin/mydata')  

# 这里有个注意事项： saveAsTextFile的参数路径不能在都进来的路径范围内。
# 或者说，读是从这个文件夹A（这是最后一级的目录）读进来的， 写就不能写入文件夹A了
# 另外， mydata是目录名， 进去你会看见 part-00000  这样的文件名，这才是真数据文件。
</code></pre>
<h1 id="spark优化相关">Spark优化相关</h1>
<h3 id="序列化">序列化：</h3>
<pre><code>好处1：网络传输必备
好处2：节省内存
两种方式序列化方式：
    1. Java内部序列化（默认，较慢，但兼容性好）
    2. Kryo （较快，但兼容性不太好） 
</code></pre>
<h3 id="内存管理">内存管理：</h3>
<p>可分为 execution（进程执行） 和 storage（存储）<br>
execution相关操作: shuffle, join, sort, aggregation<br>
storage相关操作  :   cache，<br>
特点：<br>
execution 和 storage 共享整体内存：<br>
execution起到 &quot;存霸&quot; 的角色:<br>
1. 若 execution区域内存 不够用了， 它会去抢夺 storage 区域的内存（不归还）<br>
2. 当然，可以为 storage 设置阈值 （必须给 storage留下多少）<br>
具体分配多少：<br>
总内存 = n<br>
execution内存 = (总内存 - 300M) * 50%<br>
storage内存   = (总内存 - 300M) * 50%</p>
<pre><code>说白了，就是留给JVM 300M， 然后 execution 和 storage 各分一半。
</code></pre>
<p>查看内存占用情况<br>
可通过WebUI查看<br>
（序列化后存储，通常会节省内存）</p>
<h3 id="broadcasting-variable">Broadcasting Variable</h3>
<p>情景：正常来说，每个 task（map, filter等） 都会占用1份数据，100个task就会拿100份数据。<br>
这种情况造成了数据的冗余， BroadCasting Variable（广播变量）就是解决这一问题的。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => Ubuntu-Hadoop-YARN-HDFS-Hive-Spark安装配置]]></title>
        <id>https://cythonlin.github.io/post/py-greater-ubuntu-hadoop-yarn-hdfs-hive-spark-an-zhuang-pei-zhi/</id>
        <link href="https://cythonlin.github.io/post/py-greater-ubuntu-hadoop-yarn-hdfs-hive-spark-an-zhuang-pei-zhi/">
        </link>
        <updated>2020-09-29T04:15:19.000Z</updated>
        <content type="html"><![CDATA[<h1 id="环境条件">环境条件</h1>
<p>Java 8<br>
Python 3.7<br>
Scala 2.12.10<br>
Spark 2.4.4<br>
hadoop 2.7.7<br>
hive 2.3.6<br>
mysql 5.7<br>
mysql-connector-java-5.1.48.jar</p>
<p>R 3.1+（可以不安装）</p>
<h1 id="安装java">安装Java</h1>
<p>先验传送门：https://segmentfault.com/a/1190000020746647#articleHeader0</p>
<h1 id="安装python">安装Python</h1>
<p>用Ubuntu自带Python3.7</p>
<h1 id="安装scala">安装Scala</h1>
<p>下载：https://downloads.lightbend.com/scala/2.12.10/scala-2.12.10.tgz<br>
解压：<br>
tar -zxvf 下载好的Scala<br>
配置：<br>
vi ~/.bashrc<br>
export SCALA_HOME=/home/lin/spark/scala-2.12.10<br>
export PATH=<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>S</mi><mi>C</mi><mi>A</mi><mi>L</mi><msub><mi>A</mi><mi>H</mi></msub><mi>O</mi><mi>M</mi><mi>E</mi></mrow><mi mathvariant="normal">/</mi><mi>b</mi><mi>i</mi><mi>n</mi><mo>:</mo></mrow><annotation encoding="application/x-tex">{SCALA_HOME}/bin:</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault">A</span><span class="mord mathdefault">L</span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span><span class="mord">/</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span></span></span>PATH<br>
保存退出<br>
激活配置：<br>
source ~/.bashrc</p>
<h1 id="安装hadoop">安装Hadoop</h1>
<p>提前说明： 若不使用HDFS 和 YARN，整个 Hadoop安装可忽略，可以直接安装spark。<br>
下载：<a href="https://archive.apache.org/dist/hadoop/common/">https://archive.apache.org/dist/hadoop/common/</a><br>
详细地址: https://archive.apache.org/dist/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz<br>
解压：<br>
tar -zxvf 下载好的 hadoop<br>
配置hadoop:<br>
vi ~/.bashrc<br>
export HADOOP_HOME=/home/lin/hadoop/hadoop-2.7.7<br>
export PATH=<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>H</mi><mi>A</mi><mi>D</mi><mi>O</mi><mi>O</mi><msub><mi>P</mi><mi>H</mi></msub><mi>O</mi><mi>M</mi><mi>E</mi></mrow><mi mathvariant="normal">/</mi><mi>b</mi><mi>i</mi><mi>n</mi><mo>:</mo></mrow><annotation encoding="application/x-tex">{HADOOP_HOME}/bin:</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span><span class="mord">/</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span></span></span>PATH<br>
激活配置：<br>
source ~/.bashrc</p>
<h3 id="hdfs配置">HDFS配置：</h3>
<p>进入解压后的 etc/hadoop   (注意这不是根目录的etc， 而是解压后的hadoop目录下的etc)<br>
echo $JAVA_HOME        # 复制打印出的路径</p>
<pre><code>vi hadoop-env.sh:  （找到 export JAVA_HOME 这行，并替换为如下）
    export JAVA_HOME=/home/lin/spark/jdk1.8.0_181  
    
vi core-site.xml:  （hdfs后面为 主机名:端口号）  （主机名就是终端显示的 @后面的~~~）
    &lt;property&gt;
      &lt;name&gt;fs.default.name&lt;/name&gt;
      &lt;value&gt;hdfs://lin:8020&lt;/value&gt;       
    &lt;/property&gt;
    
vi hdfs-site.xml： (同样在 &lt;configuration&gt; 之间加入) (/home/lin/hdfs是我已经有的目录)
    &lt;property&gt;
      &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
      &lt;value&gt;/home/lin/hdfs/dfs/name&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
      &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
      &lt;value&gt;/home/lin/hdfs/dfs/data&lt;/value&gt;
    &lt;/property&gt;
</code></pre>
<p>格式化HDFS：<br>
hadoop namenode -format<br>
# 然后去刚才上面配置的这个路径里面是否有新东西出现： /home/lin/hdfs<br>
开启HDFS (先进入 sbin目录下，   sbin 和 etc bin是同级的， 这里说的都是hadoop里的目录):<br>
./start-dfs.sh</p>
<pre><code># 一路yes, 若让你输入密码， 则输入对应服务器的密码。（我这里都是本机）
# 若提示权限错误，继续往下看（支线）
    sudo passwd root              # 激活ubuntu的root用户，并设置密码
    vi /etc/ssh/sshd_config：
        PermitRootLogin yes       # 任意位置添加这个（此选项之前可能是存在的注释后再添加）
    service ssh restart 
</code></pre>
<p>查看HDFS里面根目录 / 的内容：<br>
hadoop fs -ls /<br>
向HDFS里面根目录 / 中 传入文件：<br>
echo test &gt; test.txt     # 首先，随便建立一个文件</p>
<pre><code>hadoop fs -put test.txt /     # 向HDFS里面根目录 / 中 传入文件

hadoop fs -ls /          # 再次查看，就发现有 test.txt 文件了。
</code></pre>
<p>从HDFS里面根目录 / 中 读取文件test.txt：<br>
hadoop fs -text /test.txt<br>
从Hadoop WebUI 中查看刚才的文件是否存在：<br>
http://192.168.0.108:50070/           # 50070是默认端口</p>
<pre><code>点击右侧下拉框 &quot;Utilities&quot;  -&gt;  &quot;Browser the file system &quot;
清晰可见， 我们的test.txt 躺在那里~
</code></pre>
<h3 id="yarn配置">YARN配置</h3>
<p>还是etc/hadoop这个目录:<br>
cp mapred-site.xml.template mapred-site.xml</p>
<pre><code>vi mapred-site.xml：
    &lt;property&gt;
      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
      &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
    
vi yarn-site.xml:
    &lt;property&gt;
      &lt;name&gt;yarn.nodemanager.services&lt;/name&gt;
      &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;     
</code></pre>
<p>启动 YARN: （还是 sbin目录下）<br>
./start-yarn.sh</p>
<pre><code># 同样，若有密码，输入机器的密码即可
</code></pre>
<p>从Hadoop WebUI 中查看YARN：<br>
http://192.168.0.108:8088/</p>
<h1 id="安装mysql">安装MySQL</h1>
<p>下面要用MySQL, 所以单独提一下MySQL 的 安装与配置:<br>
其实MySQL是不需要单独说的, (但我装的时候出现了和以往的不同经历), 所以还是说一下吧:<br>
apt-get install mysql-server-5.7<br>
安装容易, 不同版本的MySQL配置有些鸡肋 (我用的是 Ubuntu19):<br>
vi /etc/mysql/mysql.conf.d/mysqld.cnf:<br>
bind-address     0.0.0.0               # 找到修改一下即可<br>
修改密码+远程连接权限 (默认无密码):<br>
mysql                # 啥参数也不用加, 直接就能进去<br>
use mysql<br>
update mysql.user set authentication_string=password(&quot;123&quot;) where user=&quot;root&quot;;<br>
update user set plugin=&quot;mysql_native_password&quot;;<br>
flush privileges;</p>
<pre><code>select host from user;
update user set host ='%' where user ='root';
flush privileges;
</code></pre>
<p>重启服务:<br>
systemctl restart mysql<br>
服务端连接测试:<br>
mysql -uroot -p<br>
# 密码 123<br>
远程连接测试 (Navicat):<br>
成功</p>
<h1 id="安装hive">安装Hive</h1>
<p>下载: https://archive.apache.org/dist/hive/hive-2.3.6/apache-hive-2.3.6-bin.tar.gz</p>
<p>解压：<br>
tar -zxvf apache-hive-2.3.6-bin.tar.gz<br>
配置Hive：<br>
vi ~/.bashrc<br>
export HIVE_HOME=/home/lin/hive/apache-hive-2.3.6-bin<br>
export PATH=<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>H</mi><mi>I</mi><mi>V</mi><msub><mi>E</mi><mi>H</mi></msub><mi>O</mi><mi>M</mi><mi>E</mi></mrow><mi mathvariant="normal">/</mi><mi>b</mi><mi>i</mi><mi>n</mi><mo>:</mo></mrow><annotation encoding="application/x-tex">{HIVE_HOME}/bin:</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span><span class="mord">/</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span></span></span>PATH<br>
激活配置：<br>
source ~/.bashrc<br>
Hive其他相关配置(同理进入hive解压目录的 conf目录中):<br>
cp hive-env.sh.template hive-env.sh<br>
vi hive-env.sh:<br>
HADOOP_HOME=/home/lin/hadoop/hadoop-2.7.7<br>
Hive-MySQL相关配置(同是在 conf目录下):<br>
vi hive-site.xml:  (特别注意后两个 <property> 里面的内容, 自己修改一下用户名和密码)<br>
<configuration><br>
<property><br>
<name>javax.jdo.option.ConnectionURL</name><br>
<value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false</value><br>
</property><br>
<property><br>
<name>javax.jdo.option.ConnectionDriverName</name><br>
<value>com.mysql.jdbc.Driver</value><br>
</property><br>
<property><br>
<name>javax.jdo.option.ConnectionUserName</name><br>
<value>root</value><br>
</property><br>
<property><br>
<name>javax.jdo.option.ConnectionPassword</name><br>
<value>123</value><br>
</property><br>
</configuration></p>
<p>下载 jdbc-mysql驱动,并放入Hive中,操作如下  (因为我们上面hive-site.xml 用的是mysql):</p>
<ol>
<li>
<p>首先下载: http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.48/mysql-connector-java-5.1.48.jar</p>
</li>
<li>
<p>将此jar文件放入 hive的lib目录中(和 conf同级):</p>
</li>
<li>
<p>将此jar文件再copy一份放入 spark的 jar目录下（为了后续jupyter直连MySQL(不通过Hive)使用）<br>
初始化(先确保之前的HDFS和MySQL已经启动):<br>
schematool -dbType mysql -initSchema</p>
<h1 id="注意事项1-这个用命令初始化的-步骤是-hive-2-才需要做的">注意事项1: 这个用命令初始化的 步骤是 Hive 2.+ 才需要做的</h1>
<h1 id="注意事项2-初始化一次即可-多次初始化会使得mysql有重复的键-报错">注意事项2: 初始化一次即可, 多次初始化,会使得MySQL有重复的键. 报错.</h1>
</li>
</ol>
<p>开启 metastore 服务：<br>
nohup hive --service metastore &amp;<br>
检测是否初始化(去MySQL表中查看):<br>
use hive<br>
show tables;<br>
# 若有数据,则说明初始化成功<br>
启动hive:<br>
hive<br>
建库, 建表测试 (注意,千万不要用 user这种关键字当作表名等):<br>
HIVE中输入:<br>
create database mydatabase;<br>
use mydatabase;<br>
create table person (name string);</p>
<pre><code>MySQL中查看Hive表的相关信息: 
    select * from TBLS;                 # 查看所有表结构信息
    select * from COLUMNS_V2;           # 查看所有列的信息
</code></pre>
<p>向Hive导入文件:<br>
vi hive_data.txt: (写入如下两行)<br>
tom catch jerry<br>
every one can learn AI</p>
<pre><code>load data local inpath '/home/lin/data/hive_data.txt' into table person;
</code></pre>
<p>查询:<br>
select * from person;<br>
PySpark客户端配置连接代码:<br>
import findspark<br>
findspark.init()</p>
<pre><code>from pyspark.sql import SparkSession    

spark = SparkSession.builder\
    .appName(&quot;Spark Hive Example&quot;)\
    .master(&quot;local[*]&quot;)\
    .config(&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;)\
    .enableHiveSupport()\
    .getOrCreate()
    
spark.sql(&quot;use mydatabase&quot;).show()
spark.sql('show tables').show()
</code></pre>
<h1 id="安装spark">安装Spark</h1>
<p>下载：<a href="https://www.apache.org/dyn/closer.lua/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz">spark-2.4.4-bin-hadoop2.7.tgz</a>：<br>
粗糙传送门：<a href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a><br>
详细传送门：<a href="http://ftp.riken.jp/net/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz"><strong>http://ftp.riken.jp/net/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz</strong></a></p>
<p>解压：<br>
tar -zxvf 下载好的spark-bin-hadoop<br>
配置spark：<br>
vi ~/.bashrc<br>
export SPARK_HOME=home/lin/spark/spark-2.4.4-bin-hadoop2.7<br>
export PATH=<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>S</mi><mi>P</mi><mi>A</mi><mi>R</mi><msub><mi>K</mi><mi>H</mi></msub><mi>O</mi><mi>M</mi><mi>E</mi></mrow><mi mathvariant="normal">/</mi><mi>b</mi><mi>i</mi><mi>n</mi><mo>:</mo></mrow><annotation encoding="application/x-tex">{SPARK_HOME}/bin:</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span><span class="mord">/</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span></span></span>PATH<br>
激活配置：<br>
source ~/.bashrc</p>
<h1 id="最后一步python环境可能出错">最后一步（Python环境可能出错）</h1>
<p>pyspark脚本默认调用的是 &quot;python&quot; 这个名， 而ubuntu默认只有&quot;python&quot; 和 &quot;python3&quot;。<br>
所以我们需要做如下软连接，来使得可以输入python， 直接寻找python3.7命令（不要用alias）<br>
ln -s /usr/bin/python3.7 /usr/bin/python</p>
<h1 id="测试">测试</h1>
<p>服务端直接输入命令：<br>
pyspark<br>
或远程浏览器输入:<br>
http://192.xx.xx.xx:4040/jobs</p>
<h1 id="远程使用jupyter连接">远程使用Jupyter连接</h1>
<p>安装 Jupyter Notebook:</p>
<pre><code>pip3 install jupyter   
# 若新环境，需要安pip:  apt-get install python3-pip
</code></pre>
<p>pip 安装 findspark 和 pyspark</p>
<pre><code>pip install pyspark
pip install findspark                          （Linux服务端）
</code></pre>
<p>启动 Jupyter Notebook 服务（--ip指定 0.0.0.0），(--allow-root若不加上可能会报错)</p>
<pre><code>jupyter notebook --allow-root --ip 0.0.0.0     （Linux服务端）
</code></pre>
<hr>
<p>下面说的是Jupyter Notebook 客户端（Windows10）<br>
下面两行findspark代码必须放在每个py脚本的第一行</p>
<pre><code>import findspark
findspark.init()

PYSPARK_PYTHON = &quot;/usr/bin/python&quot;
os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHON
os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHON
</code></pre>
<p>然后才可正常写其他代码<br>
from pyspark import SparkConf, SparkContext</p>
<pre><code>sc = SparkContext(
    master='local[*]',   # 下面会讲这个参数
    appName='myPyspark', # 随便起名
) 
# 这句话，就把 spark启动起来了，然后才可以通过浏览器访问了。 4040
# 如果你 python魔法玩的6，那么提到上下文，你应该会自动想到 with语句 （__enter__,__exit__）
# 不写参数，本地运行，这样也是可以的，  sc = SparkContext() 

raw_data = [1,2,3]
rdd_data = sc.parallelize(raw_data)  # python列表类型 转 spark的RDD
print(rdd_data)
raw_data = rdd_data.collect()        # spark的RDD 转回到 python列表类型
print(raw_data)

sc.stop()    # 关闭spark, 同理，浏览器也就访问不到了。
</code></pre>
<p>解释 SparkContext 的 master参数：</p>
<ol>
<li>&quot;local&quot; : 表示只用一个线程，本地运行。</li>
<li>&quot;local[*]&quot; : 表示用(cpu的个数)个线程，本地运行。</li>
<li>&quot;local[n]&quot; : 表示用n个线程，本地运行。</li>
<li>&quot;spark://ip:host&quot; : 连其他集群<br>
回顾环境问题 并 解释 &quot;本地&quot; 的概念：</li>
<li>在 Linux 中 安装了 Spark全套环境。</li>
<li>在 Linux 中 安装了 Jupyter， 并启动了 Jupyter Notebook 服务。</li>
<li>在 Win10 中 远程连接 Linux中的 &quot;Jupyter Notebook&quot;服务 写业务代码（相当于客户端连接）<br>
所以， 之前所说的 &quot;本地&quot;, 这个词归根结底是相对于 Linux来说的，我们写代码一直操作的是Linux。</li>
</ol>
<h1 id="通常使用spark-submit">通常使用spark-submit</h1>
<p>首先：我们自己编写一个包含各种 pyspark-API 的 xx.py 脚本<br>
如果：你用了我上面推荐的 Jupyter Notebook，你会发现文件是.ipynb格式，可以轻松转.py<br>
<img src="/img/bVbzB5s" alt="image.png" loading="lazy"><br>
最后提交py脚本：<br>
spark-submit --master local[*] --name myspark /xx/xx/myspark.py</p>
<pre><code># 你会发现 --master 和 --name  就是上面我们代码中配置的选项，对号入座写入即可。
# /xx/xx/myspark.py 就是 py脚本的绝对路径。 喂给spark，让他去执行。即可。
</code></pre>
<h1 id="standalone部署spark">Standalone部署Spark</h1>
<p>介绍：<br>
Standalone部署需要同时启动：<br>
master端<br>
slave 端<br>
按着下面配置，最后一条  ./start-all.sh 即可同时启动。<br>
查看 JAVA_HOME环境变量。<br>
echo $JAVA_HOME</p>
<pre><code># 记住结果，复制出来
</code></pre>
<p>进入conf目录，做一些配置（conf和spark中的bin目录同级）：<br>
cp spark-env.sh.template spark-env.sh<br>
vi spark-env.sh：（里面写）<br>
JAVA_HOME=上面的结果</p>
<pre><code>cp slaves.template slaves
vi slaves: （localhost改成本机机器名）
    lin
</code></pre>
<p>上面配置完毕后，进入sbin目录（和上面的conf在一个目录中）<br>
./start-all.sh          # 启动</p>
<pre><code># 若提示权限错误，继续往下看（支线）
sudo passwd root              # 激活ubuntu的root用户，并设置密码
vi /etc/ssh/sshd_config：
    PermitRootLogin yes       # 任意位置添加这个（此选项之前可能是存在的注释后再添加）
service ssh restart
</code></pre>
<p>启动没报错，会给你弹出一条绝对路径的日志文件 xxx<br>
cat xxx         # 即可看见启动状态 ，各种日志信息</p>
<pre><code>其中有几条信息:
    Successfully started service 'WorkerUI' on port 8082  （浏览器访问 8082端口）
    Successfully registered with master spark://lin:7077  （代码上下文访问）
其中，有些信息可能未打印出来： 建议浏览器中：( 8080-8082 )端口都可以尝试一下。        
</code></pre>
<p>输入命令，查看启动状态：<br>
jps             # 若同时有 worker 和 master 说明启动成功<br>
测试：<br>
pyspark --master spark://lin:7077</p>
<pre><code># WebUI 的 Worker端，就可看见有 一个Job被添加了进来
</code></pre>
<h1 id="yarn部署spark">YARN部署Spark</h1>
<p>配置：<br>
echo $HADOOP_HOME<br>
# 我的是 /home/lin/hadoop/hadoop-2.7.7</p>
<pre><code>进入spark解压包的路径的 conf 目录中:
vi spark-env.sh:   ( etc/hadoop前面就是刚才 echo出来的，  etc/hadoop大家都是一样的)
    HADOOP_CONF_DIR=/home/lin/hadoop/hadoop-2.7.7/etc/hadoop
</code></pre>
<p>启动spark：<br>
spark-submit --master yarn --name myspark  script/myspark.py<br>
# 注意 --master 的值改成了 yarn ， 其他不变。</p>
<pre><code>或者你可以：
     pyspark --master yarn     
看到启动成功，说明配置成功
</code></pre>
<h1 id="spark历史服务配置">Spark历史服务配置</h1>
<p>痛点：有时我们的spark上下文 stop后，WebUI就不可访问了。<br>
若有未完成，或者历史信息, 也就看不到了。<br>
这时我们配置 history 服务就可在 context stop后，仍可查看 未完成job。</p>
<p>预新建一个HDFS目录myhistory (根路径下),下面用得到：<br>
hadoop fs -mkdir /myhistory<br>
首先，进入 spark解压包的 conf目录下：<br>
cp spark-defaults.conf.template spark-defaults.conf</p>
<pre><code>vi spark-defaults.conf: (解开如下注释, lin本机名称, 放在HDFS的根路径下的myhistory)
    spark.eventLog.enabled           true
    spark.eventLog.dir               hdfs://lin:8020/myhistory
    
vi spark-env.sh:  (我们之前 把template 复制过一次，所以这次直接编辑即可)
    SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://lin:8020/myhistory&quot;
</code></pre>
<p>启动（进入 spark解压包的 sbin目录下）：<br>
./start-history-server.sh</p>
<pre><code># cat 输入的信息（日志文件）。 即可查看是否启动成功
# WebUI默认是 ：http://192.168.0.108:18080/
</code></pre>
<p>测试：<br>
浏览器中访问History WebUI： http://192.168.0.108:18080/<br>
发现啥也没有： 这是正常的，因为我们还没运行 spark context主程序脚本。<br>
---------------------------------------------------------------------<br>
运行spark-context主程序脚本：<br>
spark-submit script/myspark.py<br>
# 这个脚本是随便写的，没什么意义。 不过里面有个我们常用的一个注意事项！！！<br>
# 我的这个脚本的 context 用完，被我 stop了<br>
# 所以我们访问不到它的运行状态的 Spark Context 的 WebUI</p>
<pre><code>    # 但是我们刚才辛辛苦苦配置Spark history 服务，并启动了它。
    # 所以 context的信息，被写入了我们刚才配置的 Spark history 中
    # 所以 我们再次访问 Spark history WebUI 即可看到有内容被写入进来。
---------------------------------------------------------------------
再次访问History WebUI： http://192.168.0.108:18080/
你就会发现，里面有内容了(spark history服务已经为我们干活了)~~~~   
</code></pre>
<h1 id="免密码登录">免密码登录</h1>
<p>环境Ubuntu(CentOS应该也可，很少用)<br>
免密码登录设置：<br>
cd ~<br>
ssh-keygen -t rsa -P &quot;&quot;<br>
cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys<br>
chmod 600 .ssh/authorized_keys<br>
注意几种情况：<br>
你如果是root用户，那么你需要切换到 /root/ 执行上面的命令<br>
如果是普通用户， 那么你需要切换到 /home/xxx/ 执行上面的命令</p>
<pre><code>这个要特别注意一下，有时候用 sudo -s ,路径是没有自动切换的。
需要我们自己手动切换一下 &quot;家&quot; 路径
</code></pre>
<h1 id="自定义脚本启动服务">自定义脚本启动服务</h1>
<p>下面内容仅供个人方便， shell不熟，用py脚本了, 你随意。<br>
vi start.py: (此脚本用于启动上面配置的 HDFS,YARN,SparkHistory 和 Jupyter Notebook)<br>
import os<br>
import subprocess as sub</p>
<pre><code>###### 启动  HDFS + YARN ###############
hadoop_path = os.environ['HADOOP_HOME']
hadoop_sbin = os.path.join(hadoop_path, 'sbin')

os.chdir(hadoop_sbin)
sub.run('./start-dfs.sh')
sub.run('./start-yarn.sh')

###### 启动 SparkHistory ##############
spark_path = os.environ['SPARK_HOME']
spark_sbin = os.path.join(spark_path, 'sbin')
os.chdir(spark_sbin)
sub.run('./start-history-server.sh')

###### 启动  Jupyter Notebook ###############
# home_path = os.environ['HOME']
home_path = '/home/lin'

os.chdir(home_path)
sub.run('jupyter notebook --allow-root --ip 0.0.0.0'.split())
</code></pre>
<p>之后每次重启，就不用进入每个目录去启动了。直接一条命令：<br>
sudo python start.py<br>
nohup hive --service metastore &amp;<br>
查看本脚本启动相关的WebUI：<br>
HDFS:            http://192.168.0.108:50070/<br>
YARN:            http://192.168.0.108:8088/<br>
SparkHistory:    http://192.168.0.108:18080/<br>
另附其他 WebUI：<br>
spark:           http://192.168.0.108:4040/<br>
standalone启动指定的端口(如果你使用的 standalone方式，而不是local,可能用到如下端口):<br>
pyspark --master spark://lin:7077</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI => Seq2Seq+Attention+Transformer(简)]]></title>
        <id>https://cythonlin.github.io/post/ai-greater-seq2seqattentiontransformerjian/</id>
        <link href="https://cythonlin.github.io/post/ai-greater-seq2seqattentiontransformerjian/">
        </link>
        <updated>2020-09-29T04:13:05.000Z</updated>
        <content type="html"><![CDATA[<h1 id="数据预处理tf20-keras-preprocessing">数据预处理（TF20-Keras-Preprocessing）</h1>
<h3 id="我们自己的普通数据集常用">我们自己的普通数据集（常用）</h3>
<p>主要使用tensorflow.keras.preprocessing这个库中的（image, text，sequence）这三个模块。<br>
text： 可以用来 （统计词频，分字，word_2_id, id_2_word等操作。）<br>
sequence 可以（给句子做结构化操作（填充0，裁剪长度））<br>
from tensorflow.keras.preprocessing.text import Tokenizer    # 主干，句子编码<br>
from tensorflow.keras.preprocessing.sequence import pad_sequences # 辅助，填充，剪枝</p>
<pre><code>q1 = '欢 迎 你 你 你'
q2 = '我 很 好'
q_list = [q1,q2]    # 需要特别注意，因为此API对英文友好，所以，我们必须把句子用 空格 隔开输入

token = Tokenizer(
    num_words=2, # num_words代表设置过滤num_words-1个词频， 例如num_words=2，
                 # 那么过滤掉2-1=1个词频， 所以一会你会看到下面词频为1的都被过滤掉了
)  # 这里面参数很多，还有标点符号过滤器等
token.fit_on_texts(q_list)  # 把原始句子集合，放进去拟合一下（封装成一个类）

print(token.document_count) # 2    # 句子个数
print(token.word_index)  # {'你': 1, '欢': 2, '迎': 3, '我': 4, '很': 5, '好': 6}   # word_2_id
print(token.index_word)  # {1: '你', 2: '欢', 3: '迎', 4: '我', 5: '很', 6: '好'}   # id_2_word
print(token.word_counts) # OrderedDict([('欢', 1), ('迎', 1), ('你', 3), ('我', 1), ('很', 1), ('好', 1)])  # 统计词频

seq = token.texts_to_sequences(q_list) # 先把所有的输入，变成一一变成编码化
print(seq) # [[1, 1, 1], []]     # 会不会好奇？数据怎么没了？因为我们上面设置了过滤词频为1的都过滤了

pad_seq = pad_sequences(
        seq,                # 输入编码化后的 句子
        maxlen=2,           # 统一句子最大长度
        padding='pre',      # 不足的补0， 从前面补0， （也可以用 post，代表后面）
        truncating='pre'    # 多余的长度裁剪，从前面裁剪
    )
print(pad_seq)     # 打印一下我们填充后的句子形状。
# [
#   [1 1],      # 如你所愿,最大长度为2，[1,1,1] 已经裁剪成了 [1,1]
#   [0 0],      # 如你所愿，之前[] ，已经都填满了0
# ]
</code></pre>
<p>虽然我们用不到 image这个模块数据增强模块，但是我把了解的API也写出来。<br>
train_datagen = keras.preprocessing.image.ImageDataGenerator( # 数据增强生成器（定义）<br>
rescale=1. / 255,         # 数据归一化<br>
rotation_range = 40,      #  -40-40  随机角度 （数据增强）<br>
width_shift_range = 0.2,  # 宽度位移（0-20%随机选个比例去平移） （数据增强）<br>
height_shift_range = 0.2, # 高度位移（同上）   （数据增强）<br>
shear_range=0.2,          # 图片剪切（0.2）    （数据增强）<br>
zoom_range=0.2,           # 图片缩放（0.2）    （数据增强）<br>
horizontal_flip=True,     # 图片随机水平反转    （数据增强）<br>
fill_mode='nearest',      # 图片填充像素（放大后失帧）用附近像素值来填充 （数据增强）<br>
)</p>
<pre><code># train_generator = train_datagen.flow_from_dataframe()  # 如果你用Pandas，你可以选这个
train_generator = train_datagen.flow_from_directory(     # 从文件中读取（Kaggle）
    train_dir,                       # 图片目录
    target_size = (height, width),   # 图片读取进来后缩放大小
    batch_size = batch_size,         # 就是批次
    seed=6,                          # 随机化种子
    shuffle=True,                    # 样本随机打散训练，增强模型泛化能力
    class_mode='categorical',        # label格式，是否需要one_hot， 是
)
...
...
train_num = train_generator.samples  # 打印样本形状

history = model.fit_generator(       # 注意我们上面是用的数据生成器，所以这要用 fit_generator
    train_generator,
    steps_per_epoch=train_num//batch_size, # 每个epoch多少 step(因为数据增强API是生成器方式，所以需要自己手动计算一下)
    epochs=epochs,
    validation_data=valid_generator,  # 如果你有验证集，你也可以用这个。否则可以不用
    validation_steps=valid_num//batch_size # 同上
)
</code></pre>
<h1 id="seq2seq">Seq2Seq</h1>
<h3 id="思想">思想</h3>
<pre><code>语言不同，那么我们可以搭建桥梁。 
即使我们表面上不相同。 但是我们映射到这个桥梁上的结果是几乎类似的。
</code></pre>
<h3 id="样本句子长度统一">样本句子长度统一</h3>
<p>为什么每个句子的长度需要统一？<br>
因为，每个句子for循环操作会很耗算力， 而转化为矩阵/向量化操作，会节约太多算力。<br>
因为矩阵运算严格要求样本的形状，所以每个句子的长度需要一致<br>
如何做到句子长度统一？<br>
填0， 对应TF操作就是padding， 不过TF20 的keras预处理包中已经有 成品的数据统一化操作。<br>
并且还具有 word_2_id，词向量编码操作。</p>
<h3 id="组成">组成</h3>
<ol>
<li>编码器 （输入每条样本句子的每个单词， 编码器的最后一个RNN单元，浓缩了整个句子的信息）</li>
<li>中间向量 （作为中间特征桥梁， 用来保存，输入进来的整个句子）</li>
<li>解码器 （中间向量作为解码器第一个RNN单元的输入，而每个单元的输出y,作为下一个单元的输入）<br>
其中解码器部分的输出y会用 softmax 对 词库（词典）求多分类概率。<br>
然后求损失（MSE或者CrossEntropy）<br>
注意了： softmax求出的概率该如何选择，这是个问题:<br>
假如: 每个单元的输出y的概率都取最大值, 那么可能一步错，步步错。 太极端了（贪心搜索）<br>
接下来，聊一聊一周 集束搜索的算法 BeamSearch</li>
</ol>
<h3 id="beamsearch">BeamSearch</h3>
<p>由于贪心搜索（只取概率的一个最大值，的结果不尽人意。所以 BeamSearch来啦）<br>
BeamSearch的主要思想:<br>
只取一个太冒险了，所以:     BeamSearch 取每个经过softmax输出概率集合的 Top-N个<br>
Top-N: 的 N 代表你保留几个概率   （举一反三理解: 贪心算法就是 Top-1）<br>
假如我们取Top-3个<br>
那么你一个RNN节点的预测y将会保留3个概率值， 并将这3个概率值作为 下一个节点的输入。<br>
具体流程看:下图 (可能有点丑)<br>
然后，我们会选择出:        3 个 &quot;红线&quot; 最优路径。<br>
最终: 我们通过单独的语言模型，来从这 3 个 &quot;红线&quot; 较优路径中，选出一个 最优路径。<br>
<img src="/img/bVbyAZs" alt="clipboard.png" loading="lazy"></p>
<h1 id="attention注意力机制">Attention(注意力机制)</h1>
<h3 id="前情回顾">前情回顾</h3>
<p>Seq2Seq 的 Encoder部分虽然用的是 高效的 LSTM，并且也很好的解决了，记忆的问题。<br>
但是他不能很好的解决每个单词的权重分配问题。<br>
虽然: Encoder的所有单元都会通过LSTM的记忆传递， 输入进“中间桥梁向量”。<br>
但是: 还是有&quot;偏心&quot;成分, 最后一个LSTM单元信息一定是最浓的。 （新鲜的，热乎的）<br>
所以: 你第1个LSTM单元的信息，或者说前面的LSTM单元的信息，这些记忆到最后可能会被稀释。<br>
为了解决上面的问题, Attention就出来帮忙了~~~</p>
<h3 id="attentioin原理">Attentioin原理</h3>
<p>我觉得墨迹半天不如自己画一张图~~~ （只会mspaint画图）<br>
<img src="/img/bVbyBuW" alt="clipboard.png" loading="lazy"><br>
上图中计算权重那里&quot;通过一个函数，可以是求相似度&quot;， 我简写了。 其实有两种常用的方式：<br>
Bahdanau注意力:<br>
weight = FC层( tanh ( FC层(Encoder的每个输出y) + FC层(Decoder的一个H) ) )<br>
luong注意力:<br>
weight = Encoder的每个输出y @ W随机权重矩阵 @ Decoder的一个H    # @是TF20的矩阵乘法操作符<br>
无论使用上面哪种: 都要套一层 Softmax<br>
weight = softmax(weight, axis=1)<br>
注意力向量C = sum( weight * Encoder的每个输出y , axis=1)   # 加权求和，最终得到一个向量<br>
Decoder的下一个输入 = concat( 注意力向量C, 上一个预测y4 )</p>
<h1 id="transformer">Transformer</h1>
<p>第一印象挑明： 他是一种无RNN的一种特殊的 Seq2Seq 模型。</p>
<p>RNN-LSTM-GRU虽然这些NN的主要特色就是&quot;时间序列&quot;。（缺点：慢，记忆弥散）<br>
但是我们上面说了，要想取得好的效果。那么需要加Attention。<br>
于是有人想到了，既然Attention效果这么好，为什么不直接用Attention呢？<br>
Attention效果虽好，关联性强，但是它不能保证时间序列模式。<br>
于是后来出现了 Transformer。（既能保证记忆注意力，又能保证时间序列）。具体如下！</p>
<h3 id="transformer整体结构组成">Transformer整体结构组成</h3>
<figure data-type="image" tabindex="1"><img src="/img/bVbA3uH" alt="image.png" loading="lazy"></figure>
<h3 id="self-attention">Self-Attention</h3>
<p>self-attention原理就是各种链式矩阵乘法（并行计算，可用GPU加速）<br>
self-attention计算过程如下：（假设输入句子切分单词为：矩阵X = [&quot;早&quot;,&quot;上&quot;,&quot;好&quot;]）<br>
矩阵X @ 权重矩阵Q（Q1，Q2，Q3）=&gt; Q矩阵（Q1，Q2，Q3）<br>
矩阵X @ 权重矩阵K（Q1，Q2，Q3）=&gt; K矩阵（Q1，Q2，Q3）<br>
矩阵X @ 权重矩阵V（Q1，Q2，Q3）=&gt; V矩阵（Q1，Q2，Q3）</p>
<pre><code>α = softmax( (Q矩阵 @ K矩阵) / q^0.5 )
self_attention = α @ V矩阵
# 单词1 = Q1*K1*V1 + Q1*K2*V2 + Q1*k3*V3
# 用自己的Q，查别人的KV，加权求和，最终得出的就是自己（自身单词的注意力）
</code></pre>
<h3 id="multi-head-self-attention">Multi-Head Self-Attention</h3>
<p>Multi-Head Attention 对 Self-Attention 对了如下扩展：<br>
self-attention:             一组 Q矩阵，K矩阵，V矩阵<br>
Multi-Head Self-Attention:  多组 Q矩阵，K矩阵，V矩阵<br>
扩张为多头注意力的过程：<br>
Q @ W ====&gt; [Q1, Q2, Q3]<br>
K @ W ====&gt; [K1, K2, K3]<br>
V @ W ====&gt; [V1, V2, V3]</p>
<pre><code>可理解为，多个卷积核的意思能提取不同特征的意思。
</code></pre>
<h3 id="position-encoder">Position Encoder</h3>
<p>上述的self-attention有个问题， 我们没有用到RNN等序列NN，那么矩阵相乘的过程中。<br>
单词的计算顺序可能是不同的。<br>
那么如何保证让他们位置有条不紊？<br>
可以使用位置编码，融入到Embedding，形成带有时间序列性质的模型。<br>
可自行查找计算位置编码的博文。</p>
<h3 id="传送门">传送门</h3>
<p>至于Transformer，现在官方已经有TF20和Pytorch的库了。<br>
传送门如下。<br>
<a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a><br>
Transformer延申的各种模型，像Bert等也有可调用的API<br>
<a href="https://huggingface.co/transformers/">https://huggingface.co/transformers/</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI => Pytorch与Tersorflow2.0简单对比]]></title>
        <id>https://cythonlin.github.io/post/ai-greater-pytorch-yu-tersorflow20-jian-dan-dui-bi/</id>
        <link href="https://cythonlin.github.io/post/ai-greater-pytorch-yu-tersorflow20-jian-dan-dui-bi/">
        </link>
        <updated>2020-09-29T04:12:32.000Z</updated>
        <content type="html"><![CDATA[<h1 id="前言">前言</h1>
<p>目前一些模型API尚未迁移到TF20中。 eg: CRF，Seq2Seq等<br>
如果退回TF10，有些伤。<br>
倒不如转至Torch。<br>
Pytorch的大部分思想和TF20大致相似。</p>
<p>至于安装，GPU我前面说过TF20。这里不赘述。<br>
官档安装：<a href="https://pytorch.org/get-started/locally/#start-locally">https://pytorch.org/get-started/locally/#start-locally</a></p>
<h3 id="注意">注意</h3>
<p>本文几乎通篇以代码案例 和 注释标注 的方式解释API。(模型的训练效果不做考虑。只看语法)<br>
你如果懂Tensorflow2.0（Stable），那么你看本文一定不费劲。<br>
Torch和TF20 很像！！！<br>
因此一些地方，我会列出 TF20 与 Torch的细节对比。</p>
<h1 id="开门案例1-mnist">开门案例1-MNIST</h1>
<h3 id="模块导入">模块导入</h3>
<pre><code>import torch
from torch import nn, optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
</code></pre>
<h3 id="数据预处理">数据预处理</h3>
<pre><code>data_preprocess = transforms.Compose([  # 顶预定数据处理函数，类似map()里的函数句柄
    transforms.Resize(28,28),           # 变形
    transforms.ToTensor(),              # numpy 转 Tensor
])

trian_dataset = datasets.MNIST(         # TF20在keras.datasets中,未归一化（0-255）
    '.',                                # 下载至当前目录， （图片0-1，已经被归一化了）
    train=True,                         # train=True， 代表直接给你切出 训练集
    download=True,                      # True，若未下载，则先下载
    transform=data_preprocess,          # 指定数据预处理函数。第一行我们指定的
)

test_dataset = datasets.MNIST(
    '.', 
    train=False, # False代表测试集       # 就说下这里， False代表 给你切出测试集
    download=True,
    transform=data_preprocess,
)

train = DataLoader(     # 对应TF20中的 tf.data.Dataset对数据二次预处理（分批，乱序）
    trian_dataset,      # 把上面第一次预处理的数据集 加载进来
    batch_size=16,      # mini-batch
    shuffle=True,       # 乱序，增强模型泛化能力
)

test = DataLoader(
    test_dataset,
    batch_size=16,
    shuffle=True,
)
</code></pre>
<h3 id="mnist模型定义-训练代码">MNIST模型（定义-训练代码）</h3>
<pre><code># 模型定义部分
class MyModel(nn.Module):             # TF20是 tk.models.Model
    def __init__(self):               # TF20 也是 __init__()
        super().__init__()

        self.model = nn.Sequential(   # tk.models.Sequential , 并且 TF里面 需要加一个 []
            nn.Linear(28*28, 256),    # tk.layers.Dense(256)
            nn.ReLU(),                # tk.layers.Relu()

            nn.Linear(256, 128),      # tk.layers.Dense(128)
            nn.ReLU(),               

            nn.Linear(128, 10),       # tk.layers.Dense(10)            
        ) 
    def forward(self, x):  # TF20是 __call__()
        x = x.view( x.size(0), 28*28 )      # x.view ==&gt; tf.reshape   x.size ==&gt; x.shape[0]
        y_predict = self.model(x)

        return y_predict
# -------------------------------华丽分割线---------------------------------     
# 模型训练部分
def main():
    vis = visdom.Visdom()
    model = MyModel()
    loss_ = nn.CrossEntropyLoss()     # 会将 y_predict自动加一层 softmax
    optimizer = optim.Adam(model.parameters())     # TF20: model.trainable_variables
    
    # visdom可视化
    # 这步是初始化坐标点，下面loss会用这个直接更新
    vis.line(
        [0],                    # x坐标
        [0],                    # y坐标
        win='loss',             # 窗口名称
        opts={'title': 'loss'}, # 窗口标题
    )
    
    for epoch in range(10):   # epochs
        for step, (x, y_true) in enumerate(train):
            y_predict = model(x)
            loss = loss_(y_predict, y_true)
            optimizer.zero_grad()           #  优化器清零 
            loss.backward()                 #  梯度计算
            optimizer.step()                #  梯度下降更新 tp.gradient(loss, variables)。 
            
            # 在上面的定义的基础上更新追加画点-连成线
            vis.line(
                [loss.item()],
                [step],
                win='loss',
                update='append', # 追加画点，而不是更新覆盖
            )
        print(loss.item())                  #  .item()  =&gt; 相当于 tensorflow 的 numpy()
        
        if epoch % 2 == 0:
            total_correct_samples = 0       # 用于记录（预测正确的样本的 总数量）
            total_samples = 0               # 用于记录（样本的 总数量）

            for x_test, y_test in test:
                y_pred = model(x_test)
                y_final_pred = y_pred.argmax(dim=1)     # TF20的坐标轴参数是 axis
                
                 # 每一批是 batch_size=16，我们要把它们都加在一起
                total_correct_samples += torch.eq(y_final_pred, y_test).float().sum().item()
                
                # 这里提一下 eq() 和 equal() 的返回值的区别， 自己看，我们通常用 eq
                # print( torch.equal( torch.Tensor([[1,2,3]]), torch.Tensor([[4,5,6]] ) ) ) 
                #结果:  False
                # print( torch.eq( torch.Tensor([[1,2,3]]), torch.Tensor([[4,5,6]] ) ) ) 
                #结果:  tensor([[0, 0, 0]], dtype=torch.uint8)

                per_sample = x_test.size(0)   # 再说一次， size(0) 相当于TF xx.shape[0]
                # 获取每批次样本数量, 虽然我们知道是 16
                # 但是最后一个batch_size 可能不是16，所以要准确获取。
                total_samples += per_sample

            acc = total_correct_samples / total_samples
            print(f'epoch: {epoch}, loss: {loss}, acc: {acc}')   
            
            # 测试部分
            vis.line(
                [acc],
                [step],
                win='acc',
                update='append', # 追加画点，而不是更新覆盖
            )

            x, label = iter(test).next()
            target_predict = model(x).argmax(dim=1)
            
            # 画出测试集图片
            viz.images(x, nrow=16, win=&quot;test_x&quot;, opts={'title': &quot;test_x&quot;}) 
            vis.text(    # 显示预测标签文本
                str(target_predict.detach().numpy() ),
                win = 'target_predict',
                opts = {&quot;title&quot;: target_predict}
            )
            vis.text(    # 显示真值文本
                str(label.detach().numpy() ),
                win = 'target_true',
                opts = {&quot;target_true&quot;: target_predict}
            )
main()
</code></pre>
<h3 id="模型可视化visdom">模型可视化（visdom)</h3>
<h5 id="安装-和-运行-和-使用">安装 和 运行 和 使用</h5>
<pre><code>安装
    pip install visdom
运行
    python -m visdom.server  （第一次可能会有点慢）
    
# 语法和Tensorboard很像

使用
    import visdom
    见上代码 vis.xxxxx
</code></pre>
<h1 id="案例2-cifar10cnn">案例2-CIFAR10+CNN</h1>
<h3 id="说明">说明</h3>
<p>模块导入和数据预处理部分和案例1的 MNIST一模一样。<br>
只要稍稍修改 datasets.MNIST ==&gt;  datasets.CIFAR10 即可， 简单的不忍直视~~</p>
<h3 id="代码如下">代码如下：</h3>
<p>模型定义部分：<br>
class MyModel(nn.Module):        # 温馨提示， 这是 Mmodule, 不是model<br>
def <strong>init</strong>(self):<br>
&quot;&quot;&quot;<br>
先注明一下：<br>
TF中输入图片形状为       (样本数, 高，宽，图片通道)<br>
PyTorch中输入图片形状为  (样本数, 图片通道，高，宽)<br>
&quot;&quot;&quot;</p>
<pre><code>        super().__init__()
        self.conv = nn.Sequential( # 再强调一遍，没有 []
            nn.Conv2d(
                in_channels=3,    # 对应TF  图片通道数（或者上一层通道）
                out_channels=8,   # 对应TF  filters, 卷积核数量
                kernel_size=3,    # 卷积核大小
                stride=1,         # 步长, TF 是 strides，  特别注意
                padding=0,        # no padding, 默认
            ),
            nn.ReLU(),
            nn.MaxPool2d(
                kernel_size=3,    # 滑动窗口大小
                stride=None,      # 默认为None， 意为和 kernel_size相同大小
            ),

            nn.Conv2d(
                in_channels=8,    # 对应TF  图片通道数（或者上一层通道）
                out_channels=16,   # 对应TF  filters, 卷积核数量
                kernel_size=3,    # 卷积核大小
                stride=1,         # 步长, TF 是 strides，  特别注意
                padding=0,        # no padding, 默认
            ),
            nn.ReLU(),
            nn.MaxPool2d(
                kernel_size=2,    # 滑动窗口大小
                stride=None,      # 默认为None， 意为和 kernel_size相同大小
            ),
        )
        self.dense = nn.Sequential(
            nn.Linear(16*4*4, 128),    # 对应TF Dense
            nn.Linear(128, 64),
            nn.Linear(64, 10),
        )
    def forward(self, x):
        conv_output = self.conv(x)
        # (16, 16, 4.4)
        conv_output_reshape = conv_output.view(-1, 16*4*4)
        dense_output = self.dense(conv_output_reshape)

        return dense_output
</code></pre>
<p>模型训练（模型调用+模型训练的定义）<br>
def main():<br>
vis = visdom.Visdom()</p>
<pre><code>    epochs = 100

    device = torch.device('cuda')  # 预定义 GPU 槽位（一会往里面塞 模型和数据。）

    model = MyModel().to(device)   # 模型转为 GPU 计算
    
    # CrossEntropyLoss 会自动把下面的 dense_output ，也就是y_predict 加一层 softmax
    loss_ = nn.CrossEntropyLoss().to(device)  
    optimizer = optim.Adam( model.parameters() )

    for epoch in range(epochs):
        for step, (x_train, y_train) in enumerate(train):
            x_train, y_train = x_train.to(device), y_train.to(device)
            dense_output = model(x_train)

            loss = loss_(dense_output, y_train)

            optimizer.zero_grad()   # 上一个例子提到过，梯度清零
            loss.backward()         # 反向传播， 并将梯度累加到 optimizer中
            optimizer.step()        # 相当于做了 w = w - lr * 梯度

        print(loss.item())          # item() 意思就是 tensor转numpy,TF中的 API是 xx.numpy()

        sample_correct_numbers = 0
        sample_total_numbers = 0

        with torch.no_grad():   # 测试部分不需要计算梯度，因此可以包裹在上下文中。
            for x_test, y_test in test:
                x_test, y_test = x_test.to(device), y_test.to(device)
                
                # softmax 的 y_predict  与  y_test的 one-hot做交叉熵
                y_predict = model(x_test).argmax(dim=1) 
                sample_correct_numbers += torch.eq(y_predict, y_test).float().sum().item()
                sample_total_numbers += x_test.size(0)  # 每批样本的总数加在一起
            acc = sample_correct_numbers / sample_total_numbers
            print(acc)

main()
</code></pre>
<h1 id="案例3cifar10resnet-18">案例3：CIFAR10+ResNet-18</h1>
<p>###结构图体系：<br>
<img src="/img/bVbyS1X" alt="image.png" loading="lazy"><br>
上述结构说明：<br>
1conv + (2+2+2+2)*2 + 1 fc = 18层<br>
1conv + (3+4+6+3)*2 + 1 fc = 34层<br>
1conv + (3+4+6+3)*3 + 1 fc = 50层<br>
1conv + (3+4+23+3)*3 + 1 fc = 101层<br>
1conv + (3+8+36+3)*3 + 1 fc = 152层</p>
<h3 id="代码实现">代码实现</h3>
<p>模块导入<br>
import cv2<br>
import torch<br>
from torch import nn, optim<br>
from torchvision import datasets, transforms<br>
from torch.utils.data import DataLoader<br>
import visdom<br>
import torch.nn.functional as F<br>
数据导入预处理<br>
data_preprocess = transforms.Compose([<br>
transforms.Resize(32,32),<br>
transforms.ToTensor(),<br>
transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))<br>
])</p>
<pre><code>train_dataset = datasets.CIFAR10(
    '.',
    train=True,
    download=True,
    transform=data_preprocess,
)


test_dataset = datasets.CIFAR10(
    '.',
    train=False, # False代表测试集
    download=True,
    transform=data_preprocess,
)


train = DataLoader(
    train_dataset,
    batch_size=16,
    shuffle=True,
)

test = DataLoader(
    test_dataset,
    batch_size=16,
    shuffle=True,
)
</code></pre>
<p>基础块定义（BasicBlock）：<br>
class BasicBlock(nn.Module):<br>
&quot;&quot;&quot;单个残差块 2个卷积+2个BN&quot;&quot;&quot;<br>
def <strong>init</strong>(self, input_channel, output_channel, stride=1):<br>
super().<strong>init</strong>()<br>
self.major = nn.Sequential(<br>
# 第一个Conv的步长为指定步长，允许降采样，允许输出输出通道不一致<br>
nn.Conv2d(input_channel,output_channel,kernel_size=3,stride=stride, padding=1),<br>
nn.BatchNorm2d(output_channel),<br>
nn.ReLU(inplace=True),<br>
# 第二个Conv的步长为定长1， 输入输出通道不变（缓冲输出）<br>
nn.Conv2d(output_channel, output_channel, kernel_size=3, stride=1, padding=1),<br>
nn.BatchNorm2d(output_channel),<br>
# 第二个Conv就不用ReLU了， 因为一会需要和 x加在一起，最后最一层大的Relu<br>
)<br>
# 若输入通道==输出通道，且步长为1，意味着图片未被降采样，则残差网络课直接为普通网络</p>
<pre><code>        self.shortcut = nn.Sequential()
        # 若输入输出通道不匹配，这时需要将图片做同样的变换，才能加在一起。
        if input_channel != output_channel or stride != 1:
            self.shortcut = nn.Sequential(
                nn.Conv2d(
                    input_channel,
                    output_channel,
                    kernel_size=(1,1),
                    stride = stride
                ),
                nn.BatchNorm2d(output_channel)
            )            

    def forward(self, x):
        major_out = self.major(x)        # 主干网络的输出
        shotcut_out = self.shortcut(x)   # 残差网络的输出
        # 上面这两个网络是平行的关系，  因为 它们的输出不是链式的， 而是  都是同样的 x。

        # 拼接主干网络+残差网络，F 相当于TF20的 tf.nn 里面单独有各种 loss函数
        return F.relu(major_out + shotcut_out)  # 最后在拼接后的网络外面加一层relu 
</code></pre>
<p>ResNet+ResBlock定义：<br>
class ResNet(nn.Module):<br>
def <strong>init</strong>(self, layers):  # layers用来接受，用户想要指定 ResNet的形状<br>
super().<strong>init</strong>()</p>
<pre><code>        self.conv1 = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
        )        

        self.res_net = nn.Sequential(
            *self.ResBlock(32,64, layers[0],stride=2),    # 16
            *self.ResBlock(64,128, layers[1],stride=2),   # 8
            *self.ResBlock(128,256, layers[2],stride=2),  # 4
            *self.ResBlock(256,512, layers[3],stride=2),  # 2
        )
        # 因为我们一会需要展平，里面填&quot;通道*宽度*高度&quot;, &quot;输出通道&quot;
        self.dense = nn.Linear(512 * 2 * 2, 10)  
        
    def forward(self, x):
        out = self.conv1(x)
        out = self.res_net(out)

        out = out.view(x.size(0), -1)# 卷积展平操作 ， torch中没有flatten所以我们就得手工
        out = self.dense(out)
        return out

    def ResBlock(self, input_channel, output_channel, block_nums=2, stride=2):
        # 自定义规定，第一个block缩小的(对应通道翻倍)，其余block大小不变
        # 通道翻倍，步长*2，特征减半
        all_block = [BasicBlock(input_channel, output_channel,stride=stride)]   

        for x in range(1,block_nums):
            all_block.append(BasicBlock(output_channel, output_channel,stride=1))
        return all_block

# resnet = ResNet(layers=[2,2,2,2])
# out = resnet(torch.randn(4,3,32,32))
# print(out.shape)
</code></pre>
<p>模型训练：<br>
def main():<br>
vis = visdom.Visdom()</p>
<pre><code>    epochs = 5

    device = torch.device('cuda')

    model = ResNet(layers=[2,2,2,2]).to(device) 
    
    # 会自动把下面的 dense_output ，也就是y_predict 加一层 softmax，y_true做one-hot
    loss_ = nn.CrossEntropyLoss().to(device)  
    optimizer = optim.Adam( model.parameters(), lr=0.0001)

    for epoch in range(epochs):
        total_loss = 0.0
        for step, (x_train, y_train) in enumerate(train):
            x_train, y_train = x_train.to(device), y_train.to(device)
            dense_output = model(x_train)

            loss = loss_(dense_output, y_train)

            optimizer.zero_grad()   # 上一个例子提到过，梯度清零
            loss.backward()         # 反向传播， 并将梯度累加到 optimizer中
            optimizer.step()        # 相当于做了 w = w - lr * 梯度
            total_loss += loss.item() # item()就是 tensor转numpy, TF中的 API是 xx.numpy()
            if step % 50 == 49:
                print('epoch:',epoch, 'loss:', total_loss / step) 

        sample_correct_numbers = 0
        sample_total_numbers = 0

        with torch.no_grad():   # 测试部分不需要计算梯度，因此可以包裹在上下文中。
            for x_test, y_test in test:
                x_test, y_test = x_test.to(device), y_test.to(device)
                # softmax 的 y_predict  与  y_test的 one-hot做交叉熵
                y_predict = model(x_test).argmax(dim=1) 
                sample_correct_numbers += torch.eq(y_predict, y_test).float().sum().item()
                sample_total_numbers += x_test.size(0)  # 每批样本的总数加在一起
            acc = sample_correct_numbers / sample_total_numbers
            print(acc)
    torch.save(model, 'model.pkl')  # 保存整个模型
main()
</code></pre>
<p>测试数据预处理(我随便在网上下载下来的 1 张图片)：<br>
# 这是Cifar-10数据的标准标签<br>
label = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']</p>
<pre><code>plane = cv2.imread('plane.jpg')              # 我用的opencv
plane = cv2.cvtColor(plane, cv2.COLOR_BGR2RGB) # opencv读的数据格式是BGR，所以转为RGB

plane = (plane - 127.5) / 127.5    # 二话不说，保持模型输入数据的概率分布，先做归一化
plane = cv2.resize(plane, (32,32)) # 图片缩小到32x32,和模型的输入保持一致
plane = torch.Tensor(plane)        # 转换成 tensor
plane = plane.view(1,32,32,3)      # 增加一个维度
plane = plane.repeat(16,1,1,1)     # 我就用一张图片，为了满足模型的形状16，我复制了16次     
plane = plane.permute([0,3,1,2])   # 虽然torch也有 像TF那样的transpose，但是只能操作2D

device = torch.device('cuda')      # 先定义一个cuda设备对象
plane = plane.to(device)           # 我们训练集用的cuda， 所以预测数据也要转为cuda
</code></pre>
<p>正式输入模型预测：<br>
model = torch.load('model.pkl')    # 读取出 我们训练到最后整个模型<br>
# 说明一下，如果你的预测是另一个脚本中，class ResNet 的代码定义部分也要复制过来</p>
<pre><code>out = model(plane)                 # 预测结果，形状为[16,10] 16个样本，10个预测概率，
label_indexes = out.argmax(dim=1)  # 取10个概率最大值的索引。 （1轴），形状为 [16,1]
print(label_indexes)
for i in label_indexes:            # i为每个样本预测的最大概率值 的 索引位置。
    print(label[i])                # 拿着预测标签的索引  去 真实标签中找到真实标签</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI => Tensorflow2.0语法 - keras_API的使用(三)]]></title>
        <id>https://cythonlin.github.io/post/ai-greater-tensorflow20-yu-fa-keras_api-de-shi-yong-san/</id>
        <link href="https://cythonlin.github.io/post/ai-greater-tensorflow20-yu-fa-keras_api-de-shi-yong-san/">
        </link>
        <updated>2020-09-29T04:12:09.000Z</updated>
        <content type="html"><![CDATA[<h1 id="前言">前言</h1>
<p>keras接口大都实现了 _<em>call</em>_ 方法。<br>
母类 _<em>call</em>_ 调用了 call()。<br>
因此下面说的几乎所有模型/网络层 都可以在定义后，直接像函数一样调用。<br>
eg:<br>
模型对象(参数)<br>
网络层对象(参数)<br>
我们还可以实现继承模板</p>
<h1 id="导入">导入</h1>
<pre><code>from tensorflow import keras
</code></pre>
<h1 id="metrics-统计平均">metrics (统计平均)</h1>
<p>里面有各种度量值的接口<br>
如：二分类、多分类交叉熵损失容器，MSE、MAE的损失值容器， Accuracy精确率容器等。<br>
下面以Accuracy伪码为例：<br>
acc_meter = keras.metrics.Accuracy() # 建立一个容器<br>
for _ in epoches:<br>
for _ in batches:<br>
y = ...<br>
y_predict = ...<br>
acc_meter.update_state(y, y_predict) # 每次扔进去数据，容器都会自动计算accuracy，并储存</p>
<pre><code>        if times % 100 == 0: # 一百次一输出, 设置一个阈值/阀门
            print(acc_meter.result().numpy())   # 取出容器内所有储存的数据的，均值准确率
    acc_meter。reset_states()    # 容器缓存清空， 下一epoch从头计数。
</code></pre>
<h1 id="激活函数损失函数优化器">激活函数+损失函数+优化器</h1>
<p>导入方式：<br>
keras.activations.relu()    # 激活函数：以relu为例，还有很多<br>
keras.losses.categorical_crossentropy() # 损失函数：以交叉熵为例，还有很多<br>
keras.optimizers.SGD()      # 优化器：以随机梯度下降优化器为例<br>
keras.callbacks.EarlyStopping()  # 回调函数： 以‘按指定条件提前暂停训练’回调为例</p>
<h1 id="sequential继承自model属于模型">Sequential(继承自Model)属于模型</h1>
<h3 id="模型定义方式">模型定义方式</h3>
<p>定义方式1：<br>
model = keras.models.Sequential( [首层网络,第二层网络。。。] )<br>
定义方式1：<br>
model = keras.models.Sequential()<br>
model.add(首层网络)<br>
model.add(第二层网络)</p>
<h3 id="模型相关回调配置">模型相关回调配置</h3>
<pre><code>logdir = 'callbacks'
if not os.path.exists(logdir):
    os.mkdir(logdir)
save_model_file = os.path.join(logdir, 'mymodel.h5')

callbacks = [
    keras.callbacks.TensorBoard(logdir),    # 写入tensorboard
    keras.callbacks.ModelCheckpoint(output_model_file, save_best_only=True),  # 模型保存
    keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3)  # 按条件终止模型训练
    # 验证集，每次都会提升，如果提升不动了，提升小于这个min_delta阈值，则会耐心等待5次。
    # 5次过后，要是还提升这么点。就提前结束。
]
# 代码写在这里，如何传递调用， 下面 “模型相关量度配置” 会提到
</code></pre>
<h3 id="模型相关量度配置损失优化器准确率等">模型相关量度配置：(（损失，优化器，准确率等)</h3>
<p>说明，下面的各种量度属性，可通过字符串方式，也可通过上面讲的导入实例化对象方式。<br>
model.compile(<br>
loss=&quot;sparse_categorical_crossentropy&quot;,    # 损失函数，这是字符串方式<br>
optimizer= keras.optimizers.SGD()          # 这是实例化对象的方式，这种方式可以传参<br>
metrics=['accuracy']  # 这项会在fit()时打印出来<br>
)  # compile() 操作，没有真正的训练。<br>
model.fit(<br>
x,y,<br>
epochs=10,                              # 反复训练 10 轮<br>
validation_data = (x_valid,y_valid),    # 把划分好的验证集放进来（fit时打印loss和val）<br>
validation_freq = 5,                    # 训练5次，验证一次。  可不传，默认为1。<br>
callbacks=callbacks,                    # 指定回调函数， 请衔接上面‘模型相关回调配置’</p>
<pre><code>)   # fit()才是真正的训练 
</code></pre>
<h3 id="模型-验证测试">模型 验证&amp;测试</h3>
<p>一般我们会把 数据先分成三部分（如果用相同的数据，起不到测试和验证效果，参考考试作弊思想）：</p>
<ol>
<li>训练集: （大批量，主体）</li>
<li>测试集: （模型所有训练结束后， 才用到）</li>
<li>验证集: （训练的过程种就用到）<br>
说明1：（如何分离？）
<ol>
<li>它们的分离是需要（x,y）组合在一起的，如果手动实现，需要随机打散、zip等操作。</li>
<li>但我们可以通过 scikit-learn库，的 train_test_split() 方法来实现 （2次分隔）</li>
<li>可以使用 tf.split()来手动实现<br>
具体分离案例：参考上一篇文章： https://segmentfault.com/a/1190000020447666</li>
</ol>
</li>
</ol>
<p>说明2：（为什么我们有了测试集，还需要验证集？）</p>
<ol>
<li>测试集是用来在最终，模型训练成型后（参数固定），进行测试，并且返回的是预测的结果值！！！！</li>
<li>验证集是伴随着模型训练过程中而验证）<br>
代码如下：<br>
loss, accuracy = model.evaluate( (x_test, y_test) ) # 度量， 注意，返回的是精度指标等<br>
target = model.predict( (x_test, y_test) )          # 测试， 注意，返回的是 预测的结果！</li>
</ol>
<h3 id="可用参数">可用参数</h3>
<pre><code>model.trainable_variables    # 返回模型中所有可训练的变量
# 使用场景： 就像我们之前说过的 gradient 中用到的 zip(求导结果, model.trainable_variables)
</code></pre>
<h1 id="自定义model">自定义Model</h1>
<p>Model相当于母版， 你继承了它，并实现对应方法，同样也能简便实现模型的定义。</p>
<h1 id="自定义layer">自定义Layer</h1>
<p>同Model， Layer也相当于母版， 你继承了它，并实现对应方法，同样也能简便实现网络层的定义。</p>
<h1 id="模型保存与加载">模型保存与加载</h1>
<p>###方法1：之前callback说的<br>
###方法2：只保存weight(模型不完全一致)<br>
保存：<br>
model = keras.Sequential([...])<br>
...<br>
model.fit()<br>
model.save_weights('weights.ckpt')<br>
加载：<br>
假如在另一个文件中。（当然要把保存的权重要复制到本地目录）<br>
model = keras.Sequential([...])    # 此模型构建必须和保存时候定义结构的一模一样的！<br>
model.load_weights('weights.ckpt')<br>
model.evaluate(...)<br>
model.predict(...)</p>
<p>###方法3：保存整个模型（模型完全一致）<br>
保存：<br>
model = keras.Sequential([...])<br>
...<br>
model.fit()<br>
model.save('model.h5')    # 注意 这里变了，是 save<br>
加载:（直接加载即可，不需要重新复原建模过程）<br>
假如在另一个文件中。（当然要把保存的模型要复制到本地目录）<br>
model = keras.models.load_model('model.h5')  # load_model是在 keras.models下<br>
model.evaluate(...)<br>
model.predict(...)<br>
###方法4：导出可供其他语言使用（工业化）<br>
保存： （使用tf.saved_model模块）<br>
model = keras.Sequential([...])<br>
...<br>
model.fit()<br>
tf.saved_model.save(model, '目录')<br>
加载：（使用tf.saved_model模块）<br>
model = tf.saved_model.load('目录')</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI => 知识图谱之Neo4j-Cypher]]></title>
        <id>https://cythonlin.github.io/post/ai-greater-zhi-shi-tu-pu-zhi-neo4j-cypher/</id>
        <link href="https://cythonlin.github.io/post/ai-greater-zhi-shi-tu-pu-zhi-neo4j-cypher/">
        </link>
        <updated>2020-09-29T04:11:21.000Z</updated>
        <content type="html"><![CDATA[<h1 id="crud">CRUD</h1>
<h3 id="创建">创建</h3>
<p>普通无属性创建（默认给你创建一个ID）<br>
create (p:person)-[:eat]-&gt;(f:food)<br>
带有属性的创建（ {} ）<br>
create (p:person{name:'zhangsan'})-[:eat]-&gt;(f:food{name:'apple'})</p>
<p>给两个孤独的实体创建关系：<br>
match<br>
(a:animal),(c:color)<br>
create    （如果这里改为 merge 则是 “有则查询，无则创建”）<br>
(a)-[h:have]-&gt;(c) return h</p>
<pre><code>对应查询：
	match 
		(a:animal),(c:color) 
	return  a,c
</code></pre>
<h3 id="删除-delete">删除 (delete)</h3>
<pre><code>match
	(a:animal)-[h:have]-&gt;(c:color) 
delete a,h,c
</code></pre>
<h3 id="更新修改set">更新修改（set）</h3>
<pre><code>match
	(f:food) 
set f.age=20
</code></pre>
<h3 id="查询">查询</h3>
<p>主体查询结构<br>
match<br>
(p:)-[:关系名]-[别名2:实体名]<br>
return 别名1,别名2</p>
<p>普通条件查询1（whree）<br>
match<br>
(n:User)<br>
where<br>
n.name='Tom'<br>
return n<br>
普通条件查询2：（ {} ）<br>
match<br>
(p:person{name:'zhangsan'})-[:eat{level:1}]-&gt;(f:food{name:'apple'})<br>
return p,f<br>
正则条件查询(~)<br>
match<br>
(n:User)<br>
where<br>
n.name=~'T.*'<br>
return n<br>
包含条件查询（contains）<br>
match<br>
(n:User)<br>
where<br>
n.name contains 'T'<br>
return n</p>
<p>多度查询<br>
match (t:teacher)-[]-(s:student)-[]-(ss:score) return t,s,ss</p>
<pre><code># 注意1： [] 里面不写，代表所有关系
# 注意2： -  没有箭头，代表任意方向
# 注意3： 别名不可以重复指定， 所以我设置了 ss

多度关系： （通常是基于人脉来讲的）
1度关系：我 -&gt; 你
2度关系：我 -&gt; 你 -&gt; 他

理解技巧： 算几度关系时，把自己（节点）捂住不看， 然后剩下几个人员节点，就是几度关系

特别注意：
    多度查关系时，比如你查 3度关系的结果。
    neo4j的图可能会把， 2度关系也画出来，why? 因为他通过2度关系也可直接得出结果。
    （可理解为 条条大路通罗马。）
    ！！！但是最终有效的返回路径只是你最初想要的 3度。   （2度就不算了）
</code></pre>
<p>查询最短路径：<br>
match (t:teacher), (s:student),<br>
p=shortestpath( (t)-[*..]-(s) )<br>
return p</p>
<pre><code># 注意： p= 之前有个逗号 ，
</code></pre>
<p>查询所有最短路径：<br>
match (t:teacher), (s:student),<br>
p=allshortestpaths( (t)-[*..]-(s) )<br>
return p</p>
<pre><code># 注意1： 前面多个 all ，后面多个s
# 注意2： 所有最短路径的都会列出来。 人人平等~
</code></pre>
<h1 id="索引">索引</h1>
<h3 id="创建索引-create">创建索引 (create)</h3>
<pre><code>create index on  :food(name)      

# food为实体名，name为属性名， 同时注意这个 :    
</code></pre>
<h3 id="删除索引drop">删除索引（drop）</h3>
<pre><code>drop index on  :food(name)
</code></pre>
<h1 id="约束">约束</h1>
<h3 id="创建约束">创建约束</h3>
<pre><code>create constraint on (gf:girlfriend) assert (gf.name) is unique
</code></pre>
<h3 id="删除约束">删除约束</h3>
<pre><code>create constraint on (o:others) assert (o.name) is unique
</code></pre>
<h1 id="聚合">聚合</h1>
<h3 id="统计个数count">统计个数（count）</h3>
<pre><code>match ... return count(别名)        
</code></pre>
<h3 id="限制取多少条-limit">限制取多少条 （limit）</h3>
<pre><code>match ... return 别名 limit 5    # 只取5条
</code></pre>
<h1 id="知识图谱流程">知识图谱流程</h1>
<ol>
<li>数据抓取</li>
<li>知识模型设计</li>
<li>NER （远程监督）</li>
<li>关系抽取（Bootstrap）</li>
<li>知识推理</li>
<li>图谱存储（Neo4j Cypher）</li>
<li>检索/问答/推荐</li>
</ol>
<h3 id="实体抽取">实体抽取</h3>
<p>BILSTM+CRF</p>
<h3 id="关系抽取">关系抽取</h3>
<p>Bootstrap方法：<br>
1. 构建种子实体： &quot;猫&quot;， &quot;老鼠&quot;。</p>
<pre><code>2. 寻找包含 &quot;猫&quot;  &quot;老鼠&quot; 的 句子：
    找到句子：&quot;猫和老鼠是好朋友&quot;  
    可抽取关系: 和...是好朋友

3. 拿着抽取的关系再次寻找新句子：
    找到新句子：&quot;张三和里李四是好朋友&quot;
    提取出新实体: &quot;张三&quot;, &quot;李四&quot;
    
4. 寻找包含 &quot;张三&quot;  &quot;李四&quot; 的 句子：
    找到句子：&quot;张三经常和李四一起玩&quot;  
    可抽取关系:  经常和... 一起玩

5. 拿着抽取的关系再次寻找新句子：
    找到新句子：&quot;王五和赵六是好朋友&quot;
    提取出新实体: &quot;王五&quot;, &quot;赵六&quot;
...
...
循环反复</code></pre>
]]></content>
    </entry>
</feed>