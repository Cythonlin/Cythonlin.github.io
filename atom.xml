<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://cythonlin.github.io</id>
    <title>Cython_lin</title>
    <updated>2021-01-29T06:36:50.707Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://cythonlin.github.io"/>
    <link rel="self" href="https://cythonlin.github.io/atom.xml"/>
    <logo>https://cythonlin.github.io/images/avatar.png</logo>
    <icon>https://cythonlin.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Cython_lin</rights>
    <entry>
        <title type="html"><![CDATA[PR => MusicBee & Winamp & VST]]></title>
        <id>https://cythonlin.github.io/post/pr-greater-musicbee-and-winamp-and-vst/</id>
        <link href="https://cythonlin.github.io/post/pr-greater-musicbee-and-winamp-and-vst/">
        </link>
        <updated>2021-01-29T04:58:49.000Z</updated>
        <content type="html"><![CDATA[<h1 id="musicbee">MusicBee</h1>
<h3 id="musicbee-çš®è‚¤">MusicBee çš®è‚¤</h3>
<pre><code>Dark -&gt; Bee78
</code></pre>
<h3 id="musicbee-2">MusicBee</h3>
<h1 id="musicbeeå®‰è£…vstæ’æ§½">MusicBeeå®‰è£…VSTæ’æ§½</h1>
<p>æ³¨ï¼šMusicBeeä¸èƒ½ç›´æ¥å®‰è£…VSTæ’ä»¶ï¼Œ éœ€è¦å®˜ç½‘å®‰è£…ä¸€ä¸ªVSTæ’æ§½ã€‚ç”¨æ­¤æ’æ§½æ‰å¯ä»¥å®‰è£…å…¶ä»–VSTæ’ä»¶<br>
<a href="https://getmusicbee.com/addons/plugins/16/vst-effects-support/">VSTæ’æ§½ä¸‹è½½åœ°å€</a></p>
<h3 id="å®‰è£…æ’æ§½">å®‰è£…æ’æ§½</h3>
<pre><code>Preference -&gt; Plugins -&gt; Add Plugins -&gt; æ’æ§½ä¸‹è½½çš„ä½ç½®
</code></pre>
<h1 id="ä½¿ç”¨vstæ’æ§½å®‰è£…vstæ’ä»¶">ä½¿ç”¨VSTæ’æ§½å®‰è£…VSTæ’ä»¶</h1>
<h2 id="æ­£å¸¸dllæ–‡ä»¶å®‰è£…æ–¹å¼">æ­£å¸¸DLLæ–‡ä»¶å®‰è£…æ–¹å¼</h2>
<p><a href="https://freevstplugins.net/?s=NT+Pitch">VSTæ’ä»¶ä¸‹è½½åœ°å€</a><br>
æ³¨ï¼šMusicBeeå¯¹VSTæ’ä»¶æ”¯æŒå¾ˆä¸å‹å¥½ï¼Œçœ‹äº†ä¸€å¤§å †é€‰äº†å‡ ä¸ªé€‚åˆæˆ‘è¿™ç§åˆçº§é€‰æ‰‹èƒ½ç”¨çš„ï¼š</p>
<pre><code>3D_Panner_2.0
wL_niceNwide
Ceres (1)
cs12-156
</code></pre>
<p>ä¸‹è½½åç›´æ¥è§£å‹ï¼Œå°†è§£å‹æ–‡ä»¶å¤¹ç›´æ¥æ”¾åˆ° musicçš„pluginsç›®å½•ä¸‹å³å¯ï¼ˆéœ€é‡å¯MusicBeeï¼‰</p>
<h2 id="ç—›è‹¦å®‰è£…æ–¹å¼åªç”¨é€šè¿‡è¿™ç§æ–¹å¼æ‰èƒ½å®‰è£…åˆ°pitch-shiftç±»æ’ä»¶">ç—›è‹¦å®‰è£…æ–¹å¼ï¼ˆåªç”¨é€šè¿‡è¿™ç§æ–¹å¼æ‰èƒ½å®‰è£…åˆ°Pitch Shiftç±»æ’ä»¶ï¼‰</h2>
<p>è¿™é‡Œæåˆ°ä¸€ä¸ªè¶…çº§è€ç‰Œplayer -&gt;  Winamp...   ï¼Œç°åœ¨åº”è¯¥æ²¡äººç”¨äº†ã€‚ æˆ–è€…éƒ½ç”¨Foobar2000äº†ã€‚<br>
Winamp æˆ‘çœ‹è®ºå›éƒ½æ˜¯ 2000å¹´å·¦å³çš„è¯„è®ºã€‚ã€‚ã€‚å¯çŸ¥å®ƒçš„æ’ä»¶ä¹Ÿç‰¹åˆ«æ—§äº†ã€‚  ä½†æ˜¯å®ƒçš„æ’ä»¶ç‰¹åˆ«å¤šã€‚<br>
å¯æƒœå¤§å¤šæ•°æ”¾åœ¨ MusicBeeä¸Šä¸å¥½ç”¨æˆ–è€…å´©æºƒã€‚ä½†æ²¡åŠæ³•ï¼Œè¿˜å¾—ç”¨ã€‚</p>
<h3 id="ä¸ºä»€ä¹ˆè¿™é‡Œè¦æåˆ°winamp">ä¸ºä»€ä¹ˆè¿™é‡Œè¦æåˆ°Winamp</h3>
<p>å› ä¸ºMusicBeeæœ‰ä¸ªé€‰é¡¹ï¼ˆä»Winampå¯¼å…¥æ’ä»¶ï¼‰<br>
è€Œ Winampçš„æ’ä»¶å¤§å¤šæ•°éƒ½æ˜¯ EXEæ ¼å¼çš„ï¼Œ éœ€è¦è¯†åˆ« Winampçš„å®‰è£…è·¯å¾„ï¼Œå¹¶ä¸”å®‰è£…åˆ°Winampè·¯å¾„ä¸‹ã€‚<br>
ï¼ˆå…¶å®EXEå®‰è£…å®Œä¹Ÿå°±æ˜¯ DLLæ–‡ä»¶ï¼Œ åªä¸è¿‡å®ƒåªèƒ½æ˜¯è¿™ç§æ–¹å¼ã€‚ï¼‰<br>
æ‰€ä»¥ Winamp åªæ˜¯ä¸ªè¿‡æ¸¡çš„å·¥å…·äººã€‚ã€‚ã€‚</p>
<h3 id="winampæ’ä»¶åœ°å€">Winampæ’ä»¶åœ°å€</h3>
<p><a href="https://winampheritage.com/plugins/DSP-Effect-5">Winampæ’ä»¶</a><br>
æ‹¾åˆ°å¯ç”¨çš„æ’ä»¶ take it easy å¯ä»¥æ”¹å˜ Pitch<br>
ä½†æ˜¯è¯•è¿‡ä¸€æ®µæ—¶é—´å¾ˆé—æ†¾ï¼Œä¼šè®© MusicBee å®•æ‰ã€‚</p>
<h1 id="æœ€åä¸€æ ¹æ•‘å‘½ç¨»è‰-pitchshifterv101">æœ€åä¸€æ ¹æ•‘å‘½ç¨»è‰ pitchShifter.V1.01</h1>
<p><a href="https://github.com/kawaCat/pitchShifter-Vst/releases">pitchShifteråœ°å€</a><br>
è¿™ä¸ªå°±ç”¨ä¸åˆ° Winmapäº†ï¼Œ ç›´æ¥ç§»åŠ¨åˆ° MusicBee çš„ Pluginsä¸­å³å¯</p>
<h1 id="æ€»ç»“">æ€»ç»“</h1>
<p>æ‰¾çš„å¾ˆè¾›è‹¦ï¼Œæ™®é€šçš„VSTæ’ä»¶å¾ˆå¥½æ‰¾ï¼Œ PITCH shift è¿™ç§çš„çœŸçš„éš¾æ‰¾ã€‚<br>
å³ä½¿æ‰¾åˆ°äº†ï¼ŒMusicBeeæœ‰äº›ä¹Ÿå„ç§ä¸æ”¯æŒï¼Œä¸å…¼å®¹ï¼Œå®•æ‰ã€‚</p>
<h2 id="æœ€ç»ˆæ’ä»¶åˆé›†">æœ€ç»ˆæ’ä»¶åˆé›†ï¼š</h2>
<pre><code>3D_Panner_2.0
wL_niceNwide
Ceres (1)
cs12-156

pitchShifter       ï¼ˆçµé­‚ï¼‰
</code></pre>
<p>ä¸ºä½•æˆ‘å¦‚æ­¤æ‰§ç€éŸ³ä¹çš„å˜è°ƒï¼Œå› ä¸ºå¯¹äºæˆ‘æ¥è¯´ï¼Œæ¯ä¸€ä¸ªKeyéƒ½æ˜¯ä¸€é¦–æ–°éŸ³ä¹ï¼</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PR => U&P]]></title>
        <id>https://cythonlin.github.io/post/pr-greater-uandp/</id>
        <link href="https://cythonlin.github.io/post/pr-greater-uandp/">
        </link>
        <updated>2021-01-19T08:45:11.000Z</updated>
        <content type="html"><![CDATA[<h1 id="google">Google</h1>
<pre><code>96@
</code></pre>
<h1 id="github">Github</h1>
<pre><code>Cy
ha-
http://cythonlin.github.io/
</code></pre>
<h1 id="vultr">Vultr</h1>
<pre><code>Cy
</code></pre>
<h1 id="kaggle">Kaggle</h1>
<pre><code>cy
</code></pre>
<h1 id="microsoft">MicroSoft</h1>
<pre><code>96@ / si
</code></pre>
<h1 id="nvidia">Nvidia</h1>
<pre><code>96@
</code></pre>
<h1 id="anaconda">Anaconda</h1>
<pre><code>ha_
</code></pre>
<h1 id="ä¸ƒç‰›">ä¸ƒç‰›</h1>
<pre><code>96@
</code></pre>
<h1 id="ynote">YNote</h1>
<pre><code>96@
</code></pre>
<h1 id="coding-pages">Coding Pages</h1>
<pre><code>96@ / Jxxxxxxxxxxxxxx.       ï¼ˆæœ«å°¾å¤šä¸ªè‹±æ–‡æ ‡ç‚¹ å¥å·ï¼‰
https://cythonlin.coding.net
</code></pre>
<h1 id="postman">Postman</h1>
<pre><code>96@
</code></pre>
<h1 id="bd">BD</h1>
<pre><code>18/13
</code></pre>
<h1 id="docker">Docker</h1>
<pre><code>ha
</code></pre>
<h1 id="æç®—">æç®—</h1>
<pre><code>18
</code></pre>
<h1 id="neo4j">Neo4j</h1>
<pre><code>neo4j / zxc
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PR => Windows Terminal]]></title>
        <id>https://cythonlin.github.io/post/pr-greater-windows-terminal-ge-xing-hua-pei-zhi/</id>
        <link href="https://cythonlin.github.io/post/pr-greater-windows-terminal-ge-xing-hua-pei-zhi/">
        </link>
        <updated>2021-01-18T09:41:15.000Z</updated>
        <content type="html"><![CDATA[<h1 id="å¿«æ·é”®">å¿«æ·é”®</h1>
<p>æœç´¢åŠŸèƒ½</p>
<pre><code>ctrl + shift + p
</code></pre>
<p>æœç´¢æ–‡æœ¬</p>
<pre><code>ctrl + shift + f
</code></pre>
<p>ç·¨è¼¯é…ç½®æ–‡ä»¶</p>
<pre><code>ctrl + '       (å›è»Šæ—é‚Šçš„ç¬¦è™Ÿ ')
</code></pre>
<p>æ©«å‘æ‹†åˆ†çª—å£ï¼ˆä¸‹é¢actionä¸­ï¼Œè‡ªå®šç¾©é…ç½®ï¼‰</p>
<pre><code>ctrl + +è™Ÿ
</code></pre>
<p>å‚ç›´æ‹†åˆ†çª—å£ï¼ˆä¸‹é¢actrionä¸­ï¼Œè‡ªå®šç¾©é…ç½®ï¼‰</p>
<pre><code>ctrl + -è™Ÿ
</code></pre>
<p>é—œé–‰æ‹†åˆ†çš„çª—å£ï¼ˆä¸‹é¢actionä¸­ï¼Œè‡ªå®šç¾©é…ç½®ï¼‰</p>
<pre><code>ctrl + w
</code></pre>
<p>é…ç½®æ–‡ä»¶ï¼ˆctrl + ' åï¼Œå…¨éƒ¨å†…å®¹æ•´é«”æ›¿æ›å³å¯ï¼‰</p>
<pre><code>    // To view the default settings, hold &quot;alt&quot; while clicking on the &quot;Settings&quot; button.
    // For documentation on these settings, see: https://aka.ms/terminal-documentation

    {
        // å®˜æ–¹è®¾ç½®æŒ‡å—?
        &quot;$schema&quot;: &quot;https://aka.ms/terminal-profiles-schema&quot;,

        // ä¸€äº›globalsè®¾ç½®
        &quot;theme&quot;: &quot;dark&quot;, // çª—å£ä¸»é¢˜
        &quot;initialRows&quot;: 25,
        &quot;initialCols&quot;: 100,

        &quot;defaultProfile&quot;: &quot;{61c54bbd-c2c6-5271-96e7-009a87ff44bf}&quot;,

        &quot;profiles&quot;: {

            &quot;defaults&quot;: {
                // Put settings here that you want to apply to all profiles
                &quot;colorScheme&quot;: &quot;Seafoam Pastel&quot;,
                &quot;useAcrylic&quot;: true,
                &quot;acrylicOpacity&quot;: 0.55,
                &quot;cursorShape&quot;: &quot;vintage&quot;,
                &quot;cursorHeight&quot;: 60,
                &quot;cursorColor&quot;: &quot;#B00C11&quot;,
                &quot;fontFace&quot;: &quot;YaHei Consolas Hybrid&quot;,
                &quot;fontSize&quot;: 18
            },

            &quot;list&quot;: [{
                    // Make changes here to the powershell.exe profile
                    &quot;guid&quot;: &quot;{61c54bbd-c2c6-5271-96e7-009a87ff44bf}&quot;,
                    &quot;name&quot;: &quot;Windows PowerShell&quot;,
                    &quot;commandline&quot;: &quot;powershell.exe&quot;,
                    &quot;hidden&quot;: false
                },
                {
                    // Make changes here to the cmd.exe profile
                    &quot;guid&quot;: &quot;{0caa0dad-35be-5f56-a8ff-afceeeaa6101}&quot;,
                    &quot;name&quot;: &quot;cmd&quot;,
                    &quot;commandline&quot;: &quot;cmd.exe&quot;,
                    &quot;hidden&quot;: false
                },
                {
                    &quot;guid&quot;: &quot;{b453ae62-4e3d-5e58-b989-0a998ec441b8}&quot;,
                    &quot;hidden&quot;: false,
                    &quot;name&quot;: &quot;Azure Cloud Shell&quot;,
                    &quot;source&quot;: &quot;Windows.Terminal.Azure&quot;
                }
            ]
        },

        // Add custom color schemes to this array
        &quot;schemes&quot;: [
    {
    &quot;name&quot;: &quot;Seafoam Pastel&quot;,
    &quot;black&quot;: &quot;#000000&quot;,
    &quot;red&quot;: &quot;#ff7092&quot;,
    &quot;green&quot;: &quot;#00fbac&quot;,
    &quot;yellow&quot;: &quot;#fffa6a&quot;,
    &quot;blue&quot;: &quot;#00bfff&quot;,
    &quot;purple&quot;: &quot;#df95ff&quot;,
    &quot;cyan&quot;: &quot;#86cbfe&quot;,
    &quot;white&quot;: &quot;#ffffff&quot;,
    &quot;brightBlack&quot;: &quot;#000000&quot;,
    &quot;brightRed&quot;: &quot;#ff8aa4&quot;,
    &quot;brightGreen&quot;: &quot;#21f6bc&quot;,
    &quot;brightYellow&quot;: &quot;#fff787&quot;,
    &quot;brightBlue&quot;: &quot;#1bccfd&quot;,
    &quot;brightPurple&quot;: &quot;#e6aefe&quot;,
    &quot;brightCyan&quot;: &quot;#99d6fc&quot;,
    &quot;brightWhite&quot;: &quot;#ffffff&quot;,
    &quot;background&quot;: &quot;#332a57&quot;,
    &quot;foreground&quot;: &quot;#e5e5e5&quot;
    }
    ],

        // Add any keybinding overrides to this array.
        // To unbind a default keybinding, set the command to &quot;unbound&quot;
        &quot;keybindings&quot;: [],
        &quot;actions&quot;:
        [
            // Copy and paste are bound to Ctrl+Shift+C and Ctrl+Shift+V in your defaults.json.
            // These two lines additionally bind them to Ctrl+C and Ctrl+V.
            // To learn more about selection, visit https://aka.ms/terminal-selection
            { &quot;command&quot;: {&quot;action&quot;: &quot;copy&quot;, &quot;singleLine&quot;: false }, &quot;keys&quot;: &quot;ctrl+c&quot; },
            { &quot;command&quot;: &quot;paste&quot;, &quot;keys&quot;: &quot;ctrl+v&quot; },

            // Press Ctrl+Shift+F to open the search box
            { &quot;command&quot;: &quot;find&quot;, &quot;keys&quot;: &quot;ctrl+f&quot; },
            // settings
            { &quot;command&quot;: &quot;openSettings&quot;, &quot;keys&quot;: &quot;ctrl+'&quot; },
            // Press Alt+Shift+D to open a new pane.
            // - &quot;split&quot;: &quot;auto&quot; makes this pane open in the direction that provides the most surface area.
            // - &quot;splitMode&quot;: &quot;duplicate&quot; makes the new pane use the focused pane's profile.
            // To learn more about panes, visit https://aka.ms/terminal-panes
            { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;auto&quot;, &quot;splitMode&quot;: &quot;duplicate&quot; }, &quot;keys&quot;: &quot;alt+shift+d&quot; },
            { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;vertical&quot; }, &quot;keys&quot;: &quot;ctrl+plus&quot; },
            { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;horizontal&quot; }, &quot;keys&quot;: &quot;ctrl+-&quot; },
            { &quot;command&quot;: &quot;closePane&quot;, &quot;keys&quot;: &quot;ctrl+w&quot; }
        ]
    }
</code></pre>
<h1 id="wsl2é™„ç€åœ¨é€‰é¡¹å¡">WSL2(é™„ç€åœ¨é€‰é¡¹å¡)</h1>
<ol>
<li>
<p>å¯ç”¨WSL(ç”¨PowerShellæ›¿ä»£æ‰‹åŠ¨å¯ç”¨),ï¼ˆå¯ç¨åé‡å¯ï¼Œç»§ç»­ä¸‹é¢ï¼‰</p>
<pre><code> dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart
</code></pre>
</li>
<li>
<p>å¯ç”¨è™šæ‹ŸæœºåŠŸèƒ½ï¼ˆè¿™éƒ¨ç»“æŸåï¼Œéœ€è¦é‡å¯ï¼‰</p>
<pre><code> dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart
</code></pre>
</li>
<li>
<p>ä¸‹è½½ X64 WSL2 æ›´æ–°åŒ…ï¼Œå¹¶åŒå‡»å®‰è£…<br>
<a href="https://docs.microsoft.com/zh-cn/windows/wsl/install-win10#step-4---download-the-linux-kernel-update-package">WSL2æ›´æ–°åŒ…åœ°å€</a></p>
</li>
<li>
<p>å°†WSL2è®¾ç½®ä¸ºé»˜è®¤ç‰ˆæœ¬</p>
<pre><code> wsl --set-default-version 2
</code></pre>
</li>
<li>
<p>MIcrosoft Store å®‰è£… Ubuntu20 LTS</p>
</li>
<li>
<p>å¯ä»¥åœ¨ Windows Terminal é€‰é¡¹å¡æ‰“å¼€ï¼Œä¹Ÿå¯ç›´æ¥æ‰“å¼€Ubuntu</p>
</li>
<li>
<p>Win Terminal è¾“å…¥ wsl å³å¯è¿›å…¥ Ubuntu,</p>
</li>
<li>
<p>æ¢æº</p>
<pre><code>cp -a /etc/apt/sources.list /etc/apt/sources.list.bak

 sudo sed -i &quot;s@http://.*archive.ubuntu.com@http://mirrors.huaweicloud.com@g&quot; /etc/apt/sources.list
 sudo sed -i &quot;s@http://.*security.ubuntu.com@http://mirrors.huaweicloud.com@g&quot; /etc/apt/sources.list
 
 sudo apt-get update
</code></pre>
</li>
</ol>
<h2 id="æŸ¥çœ‹wslç‰ˆæœ¬ä¿¡æ¯ä¸è¿è¡ŒçŠ¶æ€">æŸ¥çœ‹WSLç‰ˆæœ¬ä¿¡æ¯ä¸è¿è¡ŒçŠ¶æ€</h2>
<pre><code>wsl -l -v
</code></pre>
<h1 id="å®‰è£…scoopå¹¶ésqoop">å®‰è£…Scoop(å¹¶éSqoop...)</h1>
<h3 id="ä½œç”¨">ä½œç”¨</h3>
<pre><code>ä¸»è¦ç”¨åœ¨ å•ä¸ªçº¯ Windows Terminal  å»å®‰è£… Linuxå·¥å…·ä½¿ç”¨ã€‚
ä½†æˆ‘å‘ç° Win Terminal çš„ä¸€äº›åŸºç¡€å‘½ä»¤ è¿˜æ˜¯æ²¡Linuxå¥½ç”¨ï¼ˆæ¯”å¦‚ lsæ— é€‰é¡¹çœ‹ä¸åˆ°éšè—æ–‡ä»¶ï¼‰

æ‰€ä»¥æˆ‘æœ€åé€‰æ‹©äº† Win Terminal + WSL2 å¹¶ç”¨
æ‰€ä»¥Scoopè¿™é¡¹å°±å¯ä»¥ä¸ç”¨äº†
</code></pre>
<h3 id="æµç¨‹">æµç¨‹</h3>
<p>å› ä¸ºç”¨Powershellå‘½ä»¤ï¼Œä¼šè®¿é—®åˆ°raw.githubusercontent.comï¼Œæ‰€ä»¥å…ˆä¿®æ”¹DNS</p>
<pre><code>C:\Windows\System32\drivers\etc
    199.232.68.133 raw.githubusercontent.com
ipconfig /flushdns
</code></pre>
<p>ç”¨Powershellå‘½ä»¤æ­£å¼å®‰è£…Scoop:</p>
<pre><code>Set-ExecutionPolicy RemoteSigned -scope CurrentUser
iwr -useb get.scoop.sh | iex
</code></pre>
<p>é™å¾…Scoopå®‰è£…å®Œæˆï¼Œ ç”¨Scoopå®‰è£… Unixå·¥å…·ï¼š</p>
<pre><code>scoop install sudo
</code></pre>
<h1 id="ä¸‰æ–¹æ¡Œé¢ç¨‹åº">ä¸‰æ–¹æ¡Œé¢ç¨‹åº</h1>
<p>Winstep Xtreme 18.0+ (å« workshelf + NextStart)</p>
<h1 id="ä¸‰æ–¹-file-explorer">ä¸‰æ–¹ File Explorer</h1>
<p>RX æ–‡ä»¶ç®¡ç†å™¨</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => éŸ³å£°åˆæˆ]]></title>
        <id>https://cythonlin.github.io/post/py-greater-yin-sheng-he-cheng/</id>
        <link href="https://cythonlin.github.io/post/py-greater-yin-sheng-he-cheng/">
        </link>
        <updated>2020-10-12T12:20:17.000Z</updated>
        <content type="html"><![CDATA[<h1 id="èƒŒæ™¯">èƒŒæ™¯</h1>
<p>éŸ³å£°åˆæˆ åŸºäº å¾ˆä¹…ä¹‹å‰å†™çš„æ–‡ç«  <a href="https://www.cklin.top/post/py-greater-yin-sheng-fen-chi/">éŸ³å£°åˆ†ç¦»</a><br>
ä¸€äº› Light Music çš„ Electronic Drum å¤ªåµäº†ã€‚<br>
äºæ˜¯çªå‘å¥‡æƒ³ï¼Œå¦‚ä½•  N v 1 åˆ†ç¦»å‡º Drum å¹¶ä¸” Drop</p>
<h1 id="éŸ³å£°åˆ†ç¦»æ›´æ–°ä¸º5stems-16khz-model">éŸ³å£°åˆ†ç¦»ï¼ˆæ›´æ–°ä¸º5stems-16kHz Modelï¼‰</h1>
<p>2stems (vocals / accompaniment)</p>
<pre><code>spleeter separate  -o audio_output -i audio_example.mp3
</code></pre>
<p>4stems (vocals / bass / drums / other )</p>
<pre><code>spleeter separate -o audio_output -p spleeter:4stems  -i audio_example.mp3
</code></pre>
<p>5stems (vocals / bass / drums / piano / other)</p>
<pre><code>spleeter separate -o audio_output -p spleeter:5stems-16kHz -i audio_example.mp3
</code></pre>
<p>è¿™æ¬¡ç”¨çš„æ˜¯ 5stemsé¢„è®­ç»ƒæ¨¡å‹ï¼Œ å¾—åˆ°äº†å¦‚ä¸‹5ä¸ªæ–‡ä»¶ï¼š</p>
<pre><code>bass.wav
drums.wav
other.wav
piano.wav
vocals.wav
</code></pre>
<h1 id="å¯»æ‰¾è§£å†³æ–¹æ¡ˆ">å¯»æ‰¾è§£å†³æ–¹æ¡ˆ</h1>
<p>æœ€å¼€å§‹ä¸çŸ¥é“ä»ä½•æœèµ·ï¼Œåæ¥ç›´æ¥ç´¢æ€§Githubè´´äº†ä¸€ä¸ª <a href="https://github.com/deezer/spleeter/issues/506">Question Issues</a>ã€‚<br>
æœ‰äººç»™å‡ºstackçš„<a href="https://stackoverflow.com/questions/14498539/how-to-overlay-downmix-two-audio-files-using-ffmpeg">è§£å†³æ–¹æ¡ˆ</a>ï¼Œ ä¸ªäººç®€åŒ–ä½¿ç”¨å¦‚ä¸‹ï¼š</p>
<pre><code>ffmpeg -i other.wav -i vocals.wav -i bass.wav -i piano.wav -filter_complex amix=inputs=4:duration=longest output.mp3
</code></pre>
<h1 id="ç»“æœ">ç»“æœ</h1>
<p>æœ€ç»ˆæˆåŠŸæŠŠ Electronic Drum å£° Dropã€‚<br>
å”¯ä¸€ç¾ä¸­ä¸è¶³çš„å°±æ˜¯ï¼Œ5ä¸ªStemsé¢„è®­ç»ƒæ¨¡å‹åˆ†çš„ä¸å¤Ÿç»†è‡´, Github Wikiæœ€æ–°æ–¹æ¡ˆçš„å°±æ˜¯5stems~</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => K8Sï¼ˆæœªå®Œå¾…ç»­ï¼‰]]></title>
        <id>https://cythonlin.github.io/post/py-greater-k8s/</id>
        <link href="https://cythonlin.github.io/post/py-greater-k8s/">
        </link>
        <updated>2020-10-06T22:18:23.000Z</updated>
        <content type="html"><![CDATA[<h1 id="å®‰è£…">å®‰è£…</h1>
<p>å®˜æ¡£ï¼š <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a><br>
å®˜æ¡£ä¸­çš„é•œåƒæ—¶Googleçš„ï¼Œéœ€è¦æ¢æˆé˜¿é‡Œæºã€‚</p>
<h3 id="ubuntu">Ubuntu</h3>
<pre><code>apt-get update &amp;&amp; apt-get install -y apt-transport-https

curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 

cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF

apt-get update
apt-get install -y kubelet kubeadm kubectl
</code></pre>
<h3 id="centos">CentOS</h3>
<pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

setenforce 0
yum install -y kubelet kubeadm kubectl
systemctl enable kubelet &amp;&amp; systemctl start kubelet</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => Github-Cli]]></title>
        <id>https://cythonlin.github.io/post/py-greater-github-cli-quan-tao/</id>
        <link href="https://cythonlin.github.io/post/py-greater-github-cli-quan-tao/">
        </link>
        <updated>2020-10-03T01:53:58.000Z</updated>
        <content type="html"><![CDATA[<h1 id="ä¸‹è½½">ä¸‹è½½</h1>
<p>é€‰ä¸ªOSç‰ˆæœ¬ï¼ˆæˆ‘ç”¨çš„Winï¼‰ï¼š<a href="https://github.com/cli/cli/releases">https://github.com/cli/cli/releases  </a></p>
<h1 id="åˆ—å‡ºé…ç½®">åˆ—å‡ºé…ç½®</h1>
<pre><code>git config --list
</code></pre>
<p>ç”±äºæˆ‘github2ä¸ªå·åˆ‡æ¢ï¼Œå¯¼è‡´ï¼Œpushçš„æ—¶å€™æœ‰403é”™è¯¯æ··æ·†ï¼Œ<br>
æ‰€ä»¥åˆ é™¤äº†å®¶ç›®å½•çš„ .gitconfig(åº”è¯¥æ˜¯è¿™ä¸ªæœ‰äº›è®°ä¸æ¸…äº†)</p>
<h1 id="è®¾ç½®ä»£ç†">è®¾ç½®ä»£ç†</h1>
<p>ä¸ºäº†åŠ é€Ÿcloneï¼Œè¿™é‡Œå…ˆè®¾ç½®ï¼Œè‹¥æ²¡æœ‰PROXY, é‚£æ­¤æ­¥å¯ç•¥è¿‡</p>
<pre><code>git config --global http.proxy &quot;socks5://127.0.0.1:7890&quot;
git config --global https.proxy &quot;socks5://127.0.0.1:7890&quot;
</code></pre>
<p>æ¸…é™¤ä»£ç†ä¹Ÿå¾ˆç®€å•</p>
<pre><code>git config --global --unset http.proxy
git config --global --unset https.proxy
</code></pre>
<h1 id="è®¾ç½®ssh-key">è®¾ç½®SSH Key</h1>
<h3 id="è¯´æ˜">è¯´æ˜</h3>
<pre><code>å¦‚æœä½ ä¸ä¹ æƒ¯ç”¨SSHï¼Œè€Œæ˜¯ä¹ æƒ¯ç”¨HTTPçš„æ–¹å¼ï¼Œé‚£è¿™æ­¥å¯çœ
</code></pre>
<h3 id="ç”Ÿæˆå¯†é’¥å‘½ä»¤">ç”Ÿæˆå¯†é’¥å‘½ä»¤</h3>
<pre><code>ssh-keygen
</code></pre>
<p>è¿›å…¥ç”¨æˆ·å®¶ç›®å½•ï¼ŒæŠŠid_rsa.pubå…¬é’¥å¤åˆ¶å‡ºæ¥<br>
ç²˜è´´åˆ°-&gt; <a href="https://github.com/settings/ssh/">https://github.com/settings/ssh/new</a></p>
<h1 id="ç™»å½•">ç™»å½•</h1>
<pre><code>gh auth login
</code></pre>
<p>æå‰å£°æ˜ï¼Œé‡åˆ°é€‰é¡¹ï¼Œéƒ½æ˜¯ç”¨ä¸Šä¸‹ç®­å¤´é€‰æ‹©<br>
ç¬¬1ä¸ªé€‰é¡¹ï¼š é€‰æ‹©Github.comï¼ˆä¹Ÿå°±æ˜¯ä¸ªäººç”¨æˆ·ï¼‰<br>
ç¬¬2ä¸ªé€‰é¡¹ï¼šé€‰æ‹© Login with a web browser<br>
Commandä¸­ä¼šç»™ä¸€ä¸²ä»£ç ï¼Œå¤åˆ¶ä»£ç -&gt;CMDå›è½¦-&gt;è‡ªåŠ¨è·³è½¬åˆ°Web-&gt;ç²˜è´´ä»£ç -&gt;ç¡®è®¤-&gt;ç¡®è®¤æˆæƒ</p>
<pre><code>è¿™é‡Œä¹Ÿå¯ä»¥é€‰æ‹©ä½¿ç”¨ Token ä»£æ›¿ web browserã€‚ä½†æ˜¯è¿™ç§æ–¹å¼éœ€è¦ç”Ÿæˆä¸€ä¸ªToken
ç”ŸæˆURLå¦‚ä¸‹ï¼š
:  -&gt;   https://github.com/settings/tokens

ç‚¹å‡» Generate bew token ï¼Œæ–°å»ºä¸€ä¸ªæ–° tokenï¼š
    æ³¨æ„ï¼š éœ€è¦æŠŠ repoçš„æ‰€æœ‰æƒé™å‹¾ä¸Š
               å¤–åŠ ä¸€ä¸ªadmin:orgä¸‹é¢çš„  read:org  é€‰é¡¹
    æ¸©é¦¨æç¤ºï¼š  read:org  å¿…é¡»å‹¾ä¸Šï¼Œä¸ç„¶åˆ›å»ºå¤±è´¥ã€‚
</code></pre>
<p>ç¬¬3ä¸ªé€‰é¡¹ï¼šé€‰æ‹©SSHï¼ˆHTTPSä¹Ÿå¯ä»¥ï¼‰</p>
<h3 id="æŸ¥çœ‹ç™»å½•çŠ¶æ€">æŸ¥çœ‹ç™»å½•çŠ¶æ€</h3>
<pre><code>gh auth status
</code></pre>
<h3 id="é€€å‡ºç™»å½•">é€€å‡ºç™»å½•</h3>
<pre><code>gh auth logout
</code></pre>
<h1 id="ä»“åº“">ä»“åº“</h1>
<h3 id="åˆ›å»ºä»“åº“">åˆ›å»ºä»“åº“</h3>
<pre><code>gh repo create my-gh
    -&gt; Public
    -&gt; xxx  in your current directoryï¼ˆY/Nï¼‰ y   (å›è½¦é»˜è®¤å°±æ˜¯yesï¼Œä¸‹åŒ)
    -&gt; Create a local project directory for xxx ï¼ˆY/Nï¼‰y
</code></pre>
<h3 id="æŸ¥çœ‹è¿œç¨‹æƒé™">æŸ¥çœ‹è¿œç¨‹æƒé™</h3>
<pre><code>git remote -v
    origin  https://github.com/Cythonlin/my-gh.git (fetch)
    origin  https://github.com/Cythonlin/my-gh.git (push)
</code></pre>
<h3 id="å…‹éš†">å…‹éš†</h3>
<pre><code>gh repo clone gin-gonic/gin
cd gin
git remote -v
    origin  https://github.com/gin-gonic/gin.git (fetch)
    origin  https://github.com/gin-gonic/gin.git (push)
    # æˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œè¿™åˆ†æ”¯å¹¶ä¸æ˜¯æˆ‘ä»¬çš„è‡ªå·±çš„
    # æ‰€ä»¥æˆ‘ä»¬å¯ä»¥ fork ä¸‹æ¥
</code></pre>
<h3 id="fork">fork</h3>
<p>fork æŒ‡çš„æ˜¯ï¼ŒæŠŠå…‹éš†åˆ°è‡ªå·±çš„ä»“åº“ï¼Œä½œä¸ºä¸Šæ¸¸ï¼ˆupstreamï¼‰é¡¹ç›®ï¼Œç„¶åè‡ªå·±å°±å¯è‡ªç”±åŒæ­¥å®ƒ</p>
<pre><code>cd gin  # ä¸Šé¢å·²ç»è¿›æ­¤è·¯å¾„ï¼Œè¿™æ­¥å¯çœ
gh repo fork
    -&gt; Would you like to add a remote for the fork? (Y/n) å›è½¦yes
</code></pre>
<p>ä¸Šé¢æ˜¯å…ˆclone,ç„¶åè¿›å…¥è·¯å¾„ï¼Œå†fork<br>
å¦‚æœäº‹å…ˆæœªcloneï¼Œ ä¹Ÿå¯ä»¥ç”¨gh repo fork + ç”¨æˆ·å/ä»“åº“åï¼Œ ç›´æ¥ fork+cloneä¸€æ­¥åˆ°ä½</p>
<pre><code># è¿™å°±ä¸éœ€è¦åƒä¸Šé¢å…ˆ cdè¿›å…¥cloneçš„ç›®å½•ä¸‹å†forkäº†ï¼Œè¿™ç§æ–¹å¼ç›´æ¥forkå³å¯
gh repo fork pytorch/pytorch
</code></pre>
<h1 id="gist">Gist</h1>
<p>gistæ˜¯githubåˆ†äº«æ•°æ®å†…å®¹çš„å¹³å° -&gt; <a href="https://gist.github.com/">https://gist.github.com/</a><br>
ä¸Šé¢çš„åœ°å€å¯ä»¥åˆ†äº«å…¬æœ‰/ç§æœ‰çš„æ–‡ä»¶ï¼Œåˆ›å»ºä¸Šä¼ åï¼ŒGithubä¼šè·³è½¬ç”Ÿæˆä¸€ä¸ªé“¾æ¥ï¼Œæ¥ç»™æˆ‘ä»¬ä½¿ç”¨</p>
<h3 id="github-cliå®ç°gist">github-cliå®ç°Gist</h3>
<p>é»˜è®¤æ˜¯ç§æœ‰çš„ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š</p>
<pre><code>gh gist create 1.txt
</code></pre>
<p>å…¬æœ‰å‘½ä»¤å¦‚ä¸‹ï¼š</p>
<pre><code>gh gist create 1.txt --public
</code></pre>
<p>äºŒè€…éƒ½ä¼šç”Ÿæˆä¸ªURLï¼Œå³å¯è®¿é—®ã€‚<br>
ä¹Ÿå¯ä»¥ä¸€ä¸ªgistä¸­å­˜2ä¸ªæ–‡ä»¶ï¼š</p>
<pre><code>gh gist create 1.txt 2.txt
</code></pre>
<h3 id="ä¿®æ”¹-gistå…±äº«çš„æ–‡ä»¶å‚æ•°ä¸ºç”Ÿæˆurlçš„å°¾éƒ¨è·¯å¾„å‚æ•°">ä¿®æ”¹ Gistå…±äº«çš„æ–‡ä»¶ï¼Œå‚æ•°ä¸ºç”Ÿæˆurlçš„å°¾éƒ¨è·¯å¾„å‚æ•°</h3>
<pre><code>gh gist edit 5ff497631f0cc1e0a4463079a6a9eeff
</code></pre>
<h3 id="åˆ—å‡º-ä¸Šä¼ è¿‡çš„gistæ–‡ä»¶-publicä»£è¡¨å…¬æœ‰æ–‡ä»¶-secretç§æœ‰-ä¸åŠ å‚æ•°ä»£è¡¨æ‰€æœ‰">åˆ—å‡º ä¸Šä¼ è¿‡çš„Gistæ–‡ä»¶, --publicä»£è¡¨å…¬æœ‰æ–‡ä»¶ï¼Œ--secretç§æœ‰, ä¸åŠ å‚æ•°ä»£è¡¨æ‰€æœ‰</h3>
<pre><code>gh gist list
gh gist list --secret  
gh gist list --public
</code></pre>
<h1 id="pr-pull-request">PR (Pull Requestï¼‰</h1>
<h2 id="præ¦‚å¿µ">PRæ¦‚å¿µ</h2>
<p>&quot;æˆ‘forkäº†ä½ ä»¬çš„ä»£ç ï¼Œç°åœ¨æˆ‘å‘é€ä¸€ä¸ªè¯·æ±‚ï¼Œè¯·ä½ ä»¬å›æ”¶æˆ‘çš„ä»£ç &quot;ğŸ˜‚</p>
<h2 id="præµç¨‹">PRæµç¨‹</h2>
<ol>
<li>forkåˆ«äººä»“åº“ï¼ˆå…ˆforkåœ¨cloneï¼Œ å‰é¢å·²ç»æåˆ° gh å¯ä»¥ç›´æ¥forkä¸€æ­¥åˆ°ä½äº†ï¼‰</li>
<li>åˆ‡æ¢åˆ†æ”¯ï¼ˆä¹Ÿå¯ä»¥åœ¨ master ä¸‹ï¼‰ï¼Œadd,commit,push ä¿®æ”¹ä»£ç ã€‚</li>
<li>åœ¨ä½ forkåçš„ä»“åº“ä¸»é¡µç‚¹å‡»å³ä¸Šè§’çš„ Compare &amp; pull request æäº¤åˆå¹¶ç”³è¯·</li>
<li>ç­‰å¾…åˆ«äººåˆå¹¶ä½ çš„è¯·æ±‚</li>
</ol>
<h2 id="å®éªŒæµç¨‹">å®éªŒæµç¨‹</h2>
<h3 id="ä¸€-ç”¨å½“å‰çš„å·å»-forkå¦ä¸€ä¸ªå·å¦å¤–é‚£ä¸ªä¹Ÿæ˜¯è‡ªå·±çš„å·æ–¹ä¾¿åšå®éªŒçš„ä»“åº“">ä¸€ã€ç”¨å½“å‰çš„å·ï¼Œå» forkå¦ä¸€ä¸ªå·ï¼ˆå¦å¤–é‚£ä¸ªä¹Ÿæ˜¯è‡ªå·±çš„å·æ–¹ä¾¿åšå®éªŒï¼‰çš„ä»“åº“ã€‚</h3>
<pre><code>gh repo fork hacker-lin/bio2bioes
cd bio2bioes
</code></pre>
<h3 id="äºŒ-åˆ›å»ºåˆ‡æ¢åˆ†æ”¯addcommitpushadd">äºŒã€ åˆ›å»º+åˆ‡æ¢åˆ†æ”¯+add+commit+push+add</h3>
<pre><code>git checkout -b dev
echo 111 &gt; 1.txt
git add . &amp;&amp; git commit -m &quot;my_test&quot; 
åˆ°è¿™é‡Œå³å¯ï¼Œå…ˆä¸è¦pushï¼ˆä¸‹é¢PRåˆ›å»ºçš„è¿‡ç¨‹ï¼Œä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬pushï¼Œè¿™é‡Œpushæˆ‘è¯•äº†ä¼šå‡ºé”™ï¼‰
</code></pre>
<h3 id="ä¸‰-ç”¨github-cliå‘½ä»¤-ä»£æ›¿-ç‚¹å‡»compare-pull-request-æŒ‰é’®æäº¤åˆå¹¶ç”³è¯·">ä¸‰ã€ç”¨GitHUB-Cliå‘½ä»¤ ä»£æ›¿ ç‚¹å‡»Compare &amp; pull request æŒ‰é’®æäº¤åˆå¹¶ç”³è¯·</h3>
<p>åˆ›å»ºPR</p>
<pre><code>gh pr create
     -&gt; Where should we push the 'dev' branch # é€‰ç¬¬ä¸€ä¸ª
     -&gt; Title  # éšä¾¿å†™ä¸ª  some update
     -&gt; Body # ç›´æ¥ å›è½¦ è·³è¿‡å°±è¡Œã€‚
     -&gt; What's next? # é€‰ Submit  æäº¤å³å¯ 
</code></pre>
<p>åˆ—å‡ºæäº¤çš„PR</p>
<pre><code>gh pr list 
# æ‰§è¡Œåï¼Œä½ ä¼šå‘ç° PRä¿¡æ¯ï¼Œ # åé¢çš„æ•°å­—è®°ä½ä¸‹é¢Closedå’Œdiffç”¨å¾—åˆ°
</code></pre>
<p>Merge PR</p>
<pre><code>gh pr merge
    -&gt; What merge method would you like to use # Create a merge commitå³å¯
</code></pre>
<p>Closed PR</p>
<pre><code>gh pr close 3
# è¿™ä¸ª3 å°±æ˜¯ä¸Šé¢ gh pr list çš„ç»“æœ
</code></pre>
<p>æ¯”è¾ƒ PR ä¿¡æ¯</p>
<pre><code>gh pr diff 3
# æœ€ä¸‹é¢æ˜¯æœ€æ–°çš„
</code></pre>
<p>æŸ¥çœ‹ PR è¯¦ç»†ä¿¡æ¯</p>
<pre><code>gh pr status
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[GO => äº¤æ¢å…ƒç´ çš„å››ç§æ–¹å¼]]></title>
        <id>https://cythonlin.github.io/post/go-greater-jiao-huan-yuan-su-de-si-chong-fang-shi/</id>
        <link href="https://cythonlin.github.io/post/go-greater-jiao-huan-yuan-su-de-si-chong-fang-shi/">
        </link>
        <updated>2020-09-29T04:22:34.000Z</updated>
        <content type="html"><![CDATA[<h1 id="python-and-go">Python and Go</h1>
<p>ä¸‹é¢ä»£ç ï¼Œé™¤äº†æ•°æ®å£°æ˜ä¸å®šä¹‰ï¼Œå…¶ä»–Py å’Œ Go çš„è¯­æ³•éƒ½æ˜¯ä¸€æ‘¸ä¸€æ ·çš„ï¼ˆä¸»è¦å¼ºè°ƒ å¼‚æˆ–æ–¹å¼ï¼‰<br>
a := 1<br>
b := 3</p>
<h3 id="æ–¹å¼0">æ–¹å¼0</h3>
<pre><code>c := 0
c = a
a = b
b = c
</code></pre>
<h3 id="æ–¹å¼1">æ–¹å¼1</h3>
<pre><code>a,b = b,a
</code></pre>
<h3 id="æ–¹å¼2">æ–¹å¼2</h3>
<pre><code>a = a + b
b = a - b
a = a - b
</code></pre>
<h3 id="æ–¹å¼3-æ³¨å¿…é¡»æ˜¯æ•´å½¢-pyä¹Ÿä¸€æ ·">æ–¹å¼3 ï¼ˆæ³¨ï¼šå¿…é¡»æ˜¯æ•´å½¢ï¼Œ Pyä¹Ÿä¸€æ ·ï¼‰</h3>
<pre><code>a = a ^ b
b = a ^ b
a = a ^ b

fmt.Println(a)
fmt.Println(b)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RS => æ¨èç³»ç»Ÿï¼ˆä¸‰ï¼‰ç¦»çº¿å¬å›ä¸æ’åº ]]></title>
        <id>https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-san-chi-xian-zhao-hui-yu-pai-xu/</id>
        <link href="https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-san-chi-xian-zhao-hui-yu-pai-xu/">
        </link>
        <updated>2020-09-29T04:21:44.000Z</updated>
        <content type="html"><![CDATA[<h1 id="å¬å›è®¾è®¡">å¬å›è®¾è®¡</h1>
<h3 id="å¬å›æ’åºæµç¨‹">å¬å›æ’åºæµç¨‹</h3>
<h4 id="åŒ¿åç”¨æˆ·">åŒ¿åç”¨æˆ·ï¼š</h4>
<pre><code>é€šå¸¸ä½¿ç”¨ç”¨æˆ·å†·å¯åŠ¨æ–¹æ¡ˆï¼ŒåŒºåˆ«åœ¨äºuser_idä¸ºåŒ¿åç”¨æˆ·æ‰‹æœºè¯†åˆ«å·(é»‘é©¬å¤´æ¡ä¸å…è®¸åŒ¿åç”¨æˆ·)
æ‰€æœ‰åªæ­£é’ˆå¯¹äºç™»å½•ç”¨æˆ·ï¼š
</code></pre>
<h4 id="ç”¨æˆ·å†·å¯åŠ¨å‰æœŸç‚¹å‡»è¡Œä¸ºè¾ƒå°‘æƒ…å†µ">ç”¨æˆ·å†·å¯åŠ¨ï¼ˆå‰æœŸç‚¹å‡»è¡Œä¸ºè¾ƒå°‘æƒ…å†µï¼‰</h4>
<p>éä¸ªæ€§åŒ–æ¨è<br>
çƒ­é—¨å¬å›ï¼šè‡ªå®šä¹‰çƒ­é—¨è§„åˆ™ï¼Œæ ¹æ®å½“å‰æ—¶é—´æ®µçƒ­ç‚¹å®šæœŸæ›´æ–°ç»´æŠ¤äººç‚¹æ–‡ç« åº“<br>
æ–°æ–‡ç« å¬å›ï¼šä¸ºäº†æé«˜æ–°æ–‡ç« çš„æ›å…‰ç‡ï¼Œå»ºç«‹æ–°æ–‡ç« åº“ï¼Œè¿›è¡Œæ¨è<br>
ä¸ªæ€§åŒ–æ¨èï¼š<br>
åŸºäºå†…å®¹çš„ååŒè¿‡æ»¤åœ¨çº¿å¬å›ï¼šåŸºäºç”¨æˆ·å®æ—¶å…´è¶£ç”»åƒç›¸ä¼¼çš„å¬å›ç»“æœç”¨äºé¦–é¡µçš„ä¸ªæ€§åŒ–æ¨è</p>
<h4 id="åæœŸç¦»çº¿éƒ¨åˆ†ç”¨æˆ·ç‚¹å‡»è¡Œä¸ºè¾ƒå¤šç”¨æˆ·ç”»åƒå®Œå–„">åæœŸç¦»çº¿éƒ¨åˆ†ï¼ˆç”¨æˆ·ç‚¹å‡»è¡Œä¸ºè¾ƒå¤šï¼Œç”¨æˆ·ç”»åƒå®Œå–„ï¼‰</h4>
<p>å»ºç«‹ç”¨æˆ·é•¿æœŸå…´è¶£ç”»åƒï¼ˆè¯¦ç»†ï¼‰ï¼šåŒ…æ‹¬ç”¨æˆ·å„ä¸ªç»´åº¦çš„å…´è¶£ç‰¹å¾<br>
è®­ç»ƒæ’åºæ¨¡å‹<br>
LRæ¨¡å‹ã€FTRLã€Wide&amp;Deep<br>
ç¦»çº¿éƒ¨åˆ†çš„å¬å›ï¼š<br>
åŸºäºæ¨¡å‹ååŒè¿‡æ»¤æ¨èç¦»çº¿å¬å›ï¼šALS<br>
åŸºäºå†…å®¹çš„ç¦»çº¿å¬å›ï¼šæˆ–è€…ç§°åŸºäºç”¨æˆ·ç”»åƒçš„å¬å›</p>
<h3 id="å¬å›è¡¨è®¾è®¡ä¸æ¨¡å‹å¬å›">å¬å›è¡¨è®¾è®¡ä¸æ¨¡å‹å¬å›</h3>
<h4 id="å¬å›è¡¨è®¾è®¡">å¬å›è¡¨è®¾è®¡</h4>
<p>æˆ‘ä»¬çš„å¬å›æ–¹å¼æœ‰å¾ˆå¤šç§ã€‚<br>
å¤šè·¯å¬å›ç»“æœå­˜å‚¨æ¨¡å‹å¬å› ä¸ å†…å®¹å¬å›çš„ç»“æœ éœ€è¦è¿›è¡Œç›¸åº”é¢‘é“æ¨èåˆå¹¶ã€‚<br>
æ–¹æ¡ˆï¼šåŸºäºæ¨¡å‹ä¸åŸºäºå†…å®¹çš„å¬å›ç»“æœå­˜å…¥åŒä¸€å¼ è¡¨ï¼Œé¿å…å¤šå¼ è¡¨è¿›è¡Œè¯»å–å¤„ç†<br>
ç”±äºHBASEæœ‰å¤šä¸ªç‰ˆæœ¬æ•°æ®åŠŸèƒ½å­˜åœ¨çš„æ”¯æŒ<br>
TTL=&gt;7776000, VERSIONS=&gt;999999<br>
å¦‚ä¸‹ï¼š<br>
create 'cb_recall', {NAME=&gt;'als', TTL=&gt;7776000, VERSIONS=&gt;999999}<br>
alter 'cb_recall', {NAME=&gt;'content', TTL=&gt;7776000, VERSIONS=&gt;999999}<br>
alter 'cb_recall', {NAME=&gt;'online', TTL=&gt;7776000, VERSIONS=&gt;999999}</p>
<pre><code># ä¾‹å­ï¼ˆå¤šç‰ˆæœ¬ï¼‰ï¼š
put 'cb_recall', 'recall:user:5', 'als:1',[45,3,5,10]
put 'cb_recall', 'recall:user:5', 'als:1',[289,11,65,52,109,8]
put 'cb_recall', 'recall:user:5', 'als:2',[1,2,3,4,5,6,7,8,9,10]
put 'cb_recall', 'recall:user:2', 'content:1',[45,3,5,10,289,11,65,52,109,8]
put 'cb_recall', 'recall:user:2', 'content:2',[1,2,3,4,5,6,7,8,9,10]


hbase(main):084:0&gt; desc 'cb_recall'
Table cb_recall is ENABLED                                                                             
cb_recall                                                                                              
COLUMN FAMILIES DESCRIPTION                                                                            
{NAME =&gt; 'als', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false'
, KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 
'7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE
_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_
OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                    
{NAME =&gt; 'content', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'fa
lse', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL
          =&gt; '7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', C
ACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS
_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                
{NAME =&gt; 'online', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'fal
se', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL 
=&gt; '7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CA
CHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_
ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                 
3 row(s)	
</code></pre>
<p>ï¼ˆå‡ ä¹ä¸ç”¨ï¼‰åœ¨HIVEç”¨æˆ·æ•°æ®æ•°æ®åº“ä¸‹å»ºç«‹HIVEå¤–éƒ¨è¡¨,è‹¥hbaseè¡¨æœ‰ä¿®æ”¹ï¼Œåˆ™è¿›è¡ŒHIVE è¡¨åˆ é™¤æ›´æ–°<br>
create external table cb_recall_hbase(<br>
user_id STRING comment &quot;userID&quot;,<br>
als map&lt;string, ARRAY<BIGINT>&gt; comment &quot;als recall&quot;,<br>
content map&lt;string, ARRAY<BIGINT>&gt; comment &quot;content recall&quot;,<br>
online map&lt;string, ARRAY<BIGINT>&gt; comment &quot;online recall&quot;)<br>
COMMENT &quot;user recall table&quot;<br>
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'<br>
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,als:,content:,online:&quot;)<br>
TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;cb_recall&quot;);</p>
<h4 id="å¢åŠ ä¸€ä¸ªå†å²å¬å›ç»“æœè¡¨">å¢åŠ ä¸€ä¸ªå†å²å¬å›ç»“æœè¡¨</h4>
<pre><code>create 'history_recall', {NAME=&gt;'channel', TTL=&gt;7776000, VERSIONS=&gt;999999}

put 'history_recall', 'recall:user:5', 'als:1',[1,2,3]
put 'history_recall', 'recall:user:5', 'als:1',[4,5,6,7]
put 'history_recall', 'recall:user:5', 'als:1',[8,9,10]
</code></pre>
<p>ä¸ºä»€ä¹ˆå¢åŠ å†å²å¬å›è¡¨ï¼Ÿ<br>
1ã€ç›´æ¥åœ¨å­˜å‚¨å¬å›ç»“æœéƒ¨åˆ†è¿›è¡Œè¿‡æ»¤ï¼Œæ¯”ä¹‹åæ’åºè¿‡æ»¤ï¼ŒèŠ‚çœæ’åºæ—¶é—´<br>
2ã€é˜²æ­¢Redisç¼“å­˜æ²¡æœ‰æ¶ˆè€—å®Œï¼Œé€ æˆé‡å¤æ¨èï¼Œä»æºå¤´è¿›è¡Œè¿‡æ»¤</p>
<h3 id="åŸºäºæ¨¡å‹å¬å›é›†åˆè®¡ç®—">åŸºäºæ¨¡å‹å¬å›é›†åˆè®¡ç®—</h3>
<h4 id="alsæ¨¡å‹æ¨èå®ç°">ALSæ¨¡å‹æ¨èå®ç°</h4>
<p>æ­¥éª¤ï¼š<br>
1ã€æ•°æ®ç±»å‹è½¬æ¢,clickedä»¥åŠç”¨æˆ·IDä¸æ–‡ç« IDå¤„ç†<br>
2ã€ALSæ¨¡å‹è®­ç»ƒä»¥åŠæ¨è<br>
3ã€æ¨èç»“æœè§£æå¤„ç†<br>
4ã€æ¨èç»“æœå­˜å‚¨<br>
æ•°æ®ç±»å‹è½¬æ¢,clicked( bool è½¬ int)<br>
ur.spark.sql(&quot;use profile&quot;)<br>
user_article_click = ur.spark.sql(&quot;select * from user_article_basic&quot;).<br>
select(['user_id', 'article_id', 'clicked'])<br>
# æ›´æ¢ç±»å‹<br>
def change_types(row):<br>
return row.user_id, row.article_id, int(row.clicked)</p>
<pre><code>user_article_click = user_article_click.rdd.map(change_types).toDF(['user_id', 'article_id', 'clicked'])
</code></pre>
<p>è¿™æ­¥å¤„ç†ç»“æœæ ¼å¼å¦‚ä¸‹ï¼š<br>
user_id	article_id	clicked<br>
0<br>
1<br>
ç”¨æˆ·IDä¸æ–‡ç« IDå¤„ç†ï¼Œç¼–ç¨‹IDç´¢å¼•ï¼ˆåŸç”¨æˆ·IDå’Œæ–‡ç« IDæ˜¯é•¿å­—ç¬¦ä¸²ï¼ŒALSæ¨¡å‹ä¸èƒ½å¤„ç†ï¼Œè¦é‡æ–°ç¼–æ’IDç´¢å¼•ï¼‰<br>
from pyspark.ml.feature import StringIndexer<br>
from pyspark.ml import Pipeline<br>
# ç”¨æˆ·å’Œæ–‡ç« IDè¶…è¿‡ALSæœ€å¤§æ•´æ•°å€¼ï¼Œéœ€è¦ä½¿ç”¨StringIndexerè¿›è¡Œè½¬æ¢<br>
user_id_indexer = StringIndexer(inputCol='user_id', outputCol='als_user_id')<br>
article_id_indexer = StringIndexer(inputCol='article_id', outputCol='als_article_id')<br>
pip = Pipeline(stages=[user_id_indexer, article_id_indexer])<br>
pip_fit = pip.fit(user_article_click)<br>
als_user_article_click = pip_fit.transform(user_article_click)<br>
ALS æ¨¡å‹è®­ç»ƒä¸æ¨èï¼ˆALSæ¨¡å‹éœ€è¦è¾“å‡ºç”¨æˆ·IDåˆ—ï¼Œæ–‡ç« IDåˆ—ä»¥åŠç‚¹å‡»åˆ—ï¼‰<br>
from pyspark.ml.recommendation import ALS<br>
# æ¨¡å‹è®­ç»ƒå’Œæ¨èé»˜è®¤æ¯ä¸ªç”¨æˆ·å›ºå®šæ–‡ç« ä¸ªæ•°<br>
als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1)<br>
model = als.fit(als_user_article_click)<br>
recall_res = model.recommendForAllUsers(100)<br>
ç»“æœï¼š<br>
als_user_id	recommendations<br>
1			[[article_id, åˆ†æ•°]]</p>
<h4 id="æ¨èç»“æœå¤„ç†">æ¨èç»“æœå¤„ç†</h4>
<p>é€šè¿‡StringIndexerå˜æ¢åçš„ä¸‹æ ‡çŸ¥é“åŸæ¥çš„å’Œç”¨æˆ·ID<br>
# recall_reså¾—åˆ°éœ€è¦ä½¿ç”¨StringIndexerå˜æ¢åçš„ä¸‹æ ‡<br>
# ä¿å­˜åŸæ¥çš„ä¸‹è¡¨æ˜ å°„å…³ç³»<br>
refection_user = als_user_article_click.groupBy(['user_id']).max('als_user_id').withColumnRenamed(<br>
'max(als_user_id)', 'als_user_id')<br>
refection_article = als_user_article_click.groupBy(['article_id']).max('als_article_id').withColumnRenamed(<br>
'max(als_article_id)', 'als_article_id')</p>
<pre><code># Joinæ¨èç»“æœä¸ refection_useræ˜ å°„å…³ç³»è¡¨
# +-----------+--------------------+-------------------+
# | als_user_id | recommendations | user_id |
# +-----------+--------------------+-------------------+
# | 8 | [[163, 0.91328144]... | 2 |
 # | 0 | [[145, 0.653115], ... | 1106476833370537984 |
 
recall_res = recall_res.join(refection_user, on=['als_user_id'], how='left').select(
['als_user_id', 'recommendations', 'user_id'])
</code></pre>
<p>å¯¹æ¨èæ–‡ç« IDåå¤„ç†ï¼šå¾—åˆ°æ¨èåˆ—è¡¨,è·å–æ¨èåˆ—è¡¨ä¸­çš„IDç´¢å¼•<br>
# Joinæ¨èç»“æœä¸ refection_articleæ˜ å°„å…³ç³»è¡¨<br>
# +-----------+-------+----------------+<br>
# | als_user_id | user_id | als_article_id |<br>
# +-----------+-------+----------------+<br>
# | 8 | 2 | [163, 0.91328144] |<br>
# | 8 | 2 | [132, 0.91328144] |<br>
import pyspark.sql.functions as F<br>
recall_res = recall_res.withColumn('als_article_id', F.explode('recommendations')).drop('recommendations')</p>
<pre><code># +-----------+-------+--------------+
# | als_user_id | user_id | als_article_id |
# +-----------+-------+--------------+
# | 8 | 2 | 163 |
# | 8 | 2 | 132 |
def _article_id(row):
	return row.als_user_id, row.user_id, row.als_article_id[0]
</code></pre>
<p>è¿›è¡Œç´¢å¼•å¯¹åº”æ–‡ç« IDè·å–<br>
als_recall = recall_res.rdd.map(_article_id).toDF(['als_user_id', 'user_id', 'als_article_id'])<br>
als_recall = als_recall.join(refection_article, on=['als_article_id'], how='left').select(<br>
['user_id', 'article_id'])<br>
# å¾—åˆ°æ¯ä¸ªç”¨æˆ·ID å¯¹åº”æ¨èæ–‡ç« <br>
# +-------------------+----------+<br>
# | user_id |				 article_id |<br>
# +-------------------+----------+<br>
# | 1106476833370537984 |   44075 |<br>
# | 1 | 					 44075 |<br>
è·å–æ¯ä¸ªæ–‡ç« å¯¹åº”çš„é¢‘é“ï¼Œæ¨èç»™ç”¨æˆ·æ—¶æŒ‰ç…§é¢‘é“å­˜å‚¨:<br>
ur.spark.sql(&quot;use toutiao&quot;)<br>
news_article_basic = ur.spark.sql(&quot;select article_id, channel_id from news_article_basic&quot;)</p>
<pre><code>als_recall = als_recall.join(news_article_basic, on=['article_id'], how='left')
als_recall = als_recall.groupBy(['user_id', 'channel_id']).agg(F.collect_list('article_id')).withColumnRenamed(
                    'collect_list(article_id)', 'article_list')

als_recall = als_recall.dropna()
</code></pre>
<h4 id="å¬å›ç»“æœå­˜å‚¨">å¬å›ç»“æœå­˜å‚¨</h4>
<p>HBASEè¡¨è®¾è®¡æ¦‚è§ˆï¼š<br>
put 'cb_recall', 'recall:user:5', 'als:1',[45,3,5,10,289,11,65,52,109,8]<br>
put 'cb_recall', 'recall:user:5', 'als:2',[1,2,3,4,5,6,7,8,9,10]<br>
å­˜å‚¨ä»£ç å¦‚ä¸‹ï¼š<br>
def save_offline_recall_hbase(partition):<br>
&quot;&quot;&quot;ç¦»çº¿æ¨¡å‹å¬å›ç»“æœå­˜å‚¨<br>
&quot;&quot;&quot;<br>
import happybase<br>
pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)<br>
for row in partition:<br>
with pool.connection() as conn:<br>
# è·å–å†å²çœ‹è¿‡çš„è¯¥é¢‘é“æ–‡ç« <br>
history_table = conn.table('history_recall')<br>
# å¤šä¸ªç‰ˆæœ¬<br>
data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),<br>
'channel:{}'.format(row.channel_id).encode())</p>
<pre><code>            history = []
            if len(data) &gt;= 2:
                for l in data[:-1]:
                    history.extend(eval(l))
            else:
                history = []

            # è¿‡æ»¤reco_articleä¸history
            reco_res = list(set(row.article_list) - set(history))

            if reco_res:

                table = conn.table('cb_recall')
                # é»˜è®¤æ”¾åœ¨æ¨èé¢‘é“
                table.put('recall:user:{}'.format(row.user_id).encode(),
                          {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                # æ”¾å…¥å†å²æ¨èè¿‡æ–‡ç« 
                history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                  {'channel:{}'.format(row.channel_id): str(reco_res).encode()})
            conn.close()

als_recall.foreachPartition(save_offline_recall_hbase)
</code></pre>
<h3 id="ç¦»çº¿ç”¨æˆ·åŸºäºå†…å®¹å¬å›é›†">ç¦»çº¿ç”¨æˆ·åŸºäºå†…å®¹å¬å›é›†</h3>
<p>ç›®æ ‡<br>
çŸ¥é“ç¦»çº¿å†…å®¹å¬å›çš„æ¦‚å¿µ<br>
çŸ¥é“å¦‚ä½•è¿›è¡Œå†…å®¹å¬å›è®¡ç®—å­˜å‚¨è§„åˆ™<br>
åº”ç”¨<br>
åº”ç”¨sparkå®Œæˆç¦»çº¿ç”¨æˆ·åŸºäºå†…å®¹çš„ååŒè¿‡æ»¤æ¨è</p>
<h4 id="åŸºäºå†…å®¹å¬å›å®ç°æ–‡ç« å‘é‡ä¹‹å‰å·²ç»å¼„å¥½äº†">åŸºäºå†…å®¹å¬å›å®ç°ï¼ˆæ–‡ç« å‘é‡ä¹‹å‰å·²ç»å¼„å¥½äº†ï¼‰</h4>
<p>è¿‡æ»¤ç”¨æˆ·ç‚¹å‡»çš„æ–‡ç« <br>
# åŸºäºå†…å®¹ç›¸ä¼¼å¬å›ï¼ˆç”»åƒå¬å›ï¼‰<br>
ur.spark.sql(&quot;use profile&quot;)<br>
user_article_basic = self.spark.sql(&quot;select * from user_article_basic&quot;)<br>
user_article_basic = user_article_basic.filter(&quot;clicked=True&quot;)</p>
<pre><code>def save_content_filter_history_to__recall(partition):
    &quot;&quot;&quot;è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„æ¯ä¸ªæ“ä½œæ–‡ç« çš„ç›¸ä¼¼æ–‡ç« ï¼Œè¿‡æ»¤ä¹‹åï¼Œå†™å…¥contentå¬å›è¡¨å½“ä¸­ï¼ˆæ”¯æŒä¸åŒæ—¶é—´æˆ³ç‰ˆæœ¬ï¼‰
    &quot;&quot;&quot;
    import happybase
    pool = happybase.ConnectionPool(size=10, host='hadoop-master')

    # è¿›è¡Œä¸ºç›¸ä¼¼æ–‡ç« è·å–
    with pool.connection() as conn:

        # key:   article_id,    column:  similar:article_id
        similar_table = conn.table('article_similar')
        # å¾ªç¯partition
        for row in partition:
            # è·å–ç›¸ä¼¼æ–‡ç« ç»“æœè¡¨
            similar_article = similar_table.row(str(row.article_id).encode(),
                                                columns=[b'similar'])
            # ç›¸ä¼¼æ–‡ç« ç›¸ä¼¼åº¦æ’åºè¿‡æ»¤ï¼Œå¬å›ä¸éœ€è¦å¤ªå¤§çš„æ•°æ®ï¼Œ ç™¾ä¸ªï¼Œåƒ
            _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True)
            if _srt:
                # æ¯æ¬¡è¡Œä¸ºæ¨è10ç¯‡æ–‡ç« 
                reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10]

                # è·å–å†å²çœ‹è¿‡çš„è¯¥é¢‘é“æ–‡ç« 
                history_table = conn.table('history_recall')
                # å¤šä¸ªç‰ˆæœ¬
                data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                           'channel:{}'.format(row.channel_id).encode())

                history = []
                if len(data) &gt;= 2:
                    for l in data[:-1]:
                        history.extend(eval(l))
                else:
                    history = []

                # è¿‡æ»¤reco_articleä¸history
                reco_res = list(set(reco_article) - set(history))

                # è¿›è¡Œæ¨èï¼Œæ”¾å…¥åŸºäºå†…å®¹çš„å¬å›è¡¨å½“ä¸­ä»¥åŠå†å²çœ‹è¿‡çš„æ–‡ç« è¡¨å½“ä¸­
                if reco_res:
                    # content_table = conn.table('cb_content_recall')
                    content_table = conn.table('cb_recall')
                    content_table.put(&quot;recall:user:{}&quot;.format(row.user_id).encode(),
                                      {'content:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                    # æ”¾å…¥å†å²æ¨èè¿‡æ–‡ç« 
                    history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                      {'channel:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

        conn.close()

user_article_basic.foreachPartition(save_content_filter_history_to__recall)
</code></pre>
<h3 id="ç¦»çº¿ç”¨æˆ·å¬å›å®šæ—¶æ›´æ–°">ç¦»çº¿ç”¨æˆ·å¬å›å®šæ—¶æ›´æ–°</h3>
<h4 id="å®šæ—¶æ›´æ–°ä»£ç ">å®šæ—¶æ›´æ–°ä»£ç </h4>
<pre><code>import os
import sys
# å¦‚æœå½“å‰ä»£ç æ–‡ä»¶è¿è¡Œæµ‹è¯•éœ€è¦åŠ å…¥ä¿®æ”¹è·¯å¾„ï¼Œå¦åˆ™åé¢çš„å¯¼åŒ…å‡ºç°é—®é¢˜
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(BASE_DIR))
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline

from pyspark.ml.recommendation import ALS
from offline import SparkSessionBase
from datetime import datetime
import time
import numpy as np


class UpdateRecall(SparkSessionBase):

    SPARK_APP_NAME = &quot;updateRecall&quot;
    ENABLE_HIVE_SUPPORT = True

    def __init__(self, number):

        self.spark = self._create_spark_session()
        self.N = number

    def update_als_recall(self):
        &quot;&quot;&quot;
        æ›´æ–°åŸºäºæ¨¡å‹ï¼ˆALSï¼‰çš„ååŒè¿‡æ»¤å¬å›é›†
        :return:
        &quot;&quot;&quot;
        # è¯»å–ç”¨æˆ·è¡Œä¸ºåŸºæœ¬è¡¨
        self.spark.sql(&quot;use profile&quot;)
        user_article_click = self.spark.sql(&quot;select * from user_article_basic&quot;).\
            select(['user_id', 'article_id', 'clicked'])

        # æ›´æ¢ç±»å‹
        def change_types(row):
            return row.user_id, row.article_id, int(row.clicked)

        user_article_click = user_article_click.rdd.map(change_types).toDF(['user_id', 'article_id', 'clicked'])
        # ç”¨æˆ·å’Œæ–‡ç« IDè¶…è¿‡ALSæœ€å¤§æ•´æ•°å€¼ï¼Œéœ€è¦ä½¿ç”¨StringIndexerè¿›è¡Œè½¬æ¢
        user_id_indexer = StringIndexer(inputCol='user_id', outputCol='als_user_id')
        article_id_indexer = StringIndexer(inputCol='article_id', outputCol='als_article_id')
        pip = Pipeline(stages=[user_id_indexer, article_id_indexer])
        pip_fit = pip.fit(user_article_click)
        als_user_article_click = pip_fit.transform(user_article_click)

        # æ¨¡å‹è®­ç»ƒå’Œæ¨èé»˜è®¤æ¯ä¸ªç”¨æˆ·å›ºå®šæ–‡ç« ä¸ªæ•°
        als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1)
        model = als.fit(als_user_article_click)
        recall_res = model.recommendForAllUsers(self.N)

        # recall_reså¾—åˆ°éœ€è¦ä½¿ç”¨StringIndexerå˜æ¢åçš„ä¸‹æ ‡
        # ä¿å­˜åŸæ¥çš„ä¸‹è¡¨æ˜ å°„å…³ç³»
        refection_user = als_user_article_click.groupBy(['user_id']).max('als_user_id').withColumnRenamed(
            'max(als_user_id)', 'als_user_id')
        refection_article = als_user_article_click.groupBy(['article_id']).max('als_article_id').withColumnRenamed(
            'max(als_article_id)', 'als_article_id')

        # Joinæ¨èç»“æœä¸ refection_useræ˜ å°„å…³ç³»è¡¨
        # +-----------+--------------------+-------------------+
        # | als_user_id | recommendations | user_id |
        # +-----------+--------------------+-------------------+
        # | 8 | [[163, 0.91328144]... | 2 |
        #        | 0 | [[145, 0.653115], ... | 1106476833370537984 |
        recall_res = recall_res.join(refection_user, on=['als_user_id'], how='left').select(
            ['als_user_id', 'recommendations', 'user_id'])

        # Joinæ¨èç»“æœä¸ refection_articleæ˜ å°„å…³ç³»è¡¨
        # +-----------+-------+----------------+
        # | als_user_id | user_id | als_article_id |
        # +-----------+-------+----------------+
        # | 8 | 2 | [163, 0.91328144] |
        # | 8 | 2 | [132, 0.91328144] |
        import pyspark.sql.functions as F
        recall_res = recall_res.withColumn('als_article_id', F.explode('recommendations')).drop('recommendations')

        # +-----------+-------+--------------+
        # | als_user_id | user_id | als_article_id |
        # +-----------+-------+--------------+
        # | 8 | 2 | 163 |
        # | 8 | 2 | 132 |
        def _article_id(row):
            return row.als_user_id, row.user_id, row.als_article_id[0]

        als_recall = recall_res.rdd.map(_article_id).toDF(['als_user_id', 'user_id', 'als_article_id'])
        als_recall = als_recall.join(refection_article, on=['als_article_id'], how='left').select(
            ['user_id', 'article_id'])
        # å¾—åˆ°æ¯ä¸ªç”¨æˆ·ID å¯¹åº”æ¨èæ–‡ç« 
        # +-------------------+----------+
        # | user_id | article_id |
        # +-------------------+----------+
        # | 1106476833370537984 | 44075 |
        # | 1 | 44075 |
        # åˆ†ç»„ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·ï¼Œæ¨èåˆ—è¡¨
        # als_recall = als_recall.groupby('user_id').agg(F.collect_list('article_id')).withColumnRenamed(
        #     'collect_list(article_id)', 'article_list')
        self.spark.sql(&quot;use toutiao&quot;)
        news_article_basic = self.spark.sql(&quot;select article_id, channel_id from news_article_basic&quot;)
        als_recall = als_recall.join(news_article_basic, on=['article_id'], how='left')
        als_recall = als_recall.groupBy(['user_id', 'channel_id']).agg(F.collect_list('article_id')).withColumnRenamed(
            'collect_list(article_id)', 'article_list')
        als_recall = als_recall.dropna()

        # å­˜å‚¨
        def save_offline_recall_hbase(partition):
            &quot;&quot;&quot;ç¦»çº¿æ¨¡å‹å¬å›ç»“æœå­˜å‚¨
            &quot;&quot;&quot;
            import happybase
            pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)
            for row in partition:
                with pool.connection() as conn:
                    # è·å–å†å²çœ‹è¿‡çš„è¯¥é¢‘é“æ–‡ç« 
                    history_table = conn.table('history_recall')
                    # å¤šä¸ªç‰ˆæœ¬
                    data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                               'channel:{}'.format(row.channel_id).encode())

                    history = []
                    if len(data) &gt;= 2:
                        for l in data[:-1]:
                            history.extend(eval(l))
                    else:
                        history = []

                    # è¿‡æ»¤reco_articleä¸history
                    reco_res = list(set(row.article_list) - set(history))

                    if reco_res:

                        table = conn.table('cb_recall')
                        # é»˜è®¤æ”¾åœ¨æ¨èé¢‘é“
                        table.put('recall:user:{}'.format(row.user_id).encode(),
                                  {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()})
                        conn.close()

                        # æ”¾å…¥å†å²æ¨èè¿‡æ–‡ç« 
                        history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                          {'channel:{}'.format(row.channel_id): str(reco_res).encode()})
                    conn.close()

        als_recall.foreachPartition(save_offline_recall_hbase)

    def update_content_recall(self):
        &quot;&quot;&quot;
        æ›´æ–°åŸºäºå†…å®¹ï¼ˆç”»åƒï¼‰çš„æ¨èå¬å›é›†, word2vecç›¸ä¼¼
        :return:
        &quot;&quot;&quot;
        # åŸºäºå†…å®¹ç›¸ä¼¼å¬å›ï¼ˆç”»åƒå¬å›ï¼‰
        ur.spark.sql(&quot;use profile&quot;)
        user_article_basic = self.spark.sql(&quot;select * from user_article_basic&quot;)
        user_article_basic = user_article_basic.filter(&quot;clicked=True&quot;)

        def save_content_filter_history_to__recall(partition):
            &quot;&quot;&quot;è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„æ¯ä¸ªæ“ä½œæ–‡ç« çš„ç›¸ä¼¼æ–‡ç« ï¼Œè¿‡æ»¤ä¹‹åï¼Œå†™å…¥contentå¬å›è¡¨å½“ä¸­ï¼ˆæ”¯æŒä¸åŒæ—¶é—´æˆ³ç‰ˆæœ¬ï¼‰
            &quot;&quot;&quot;
            import happybase
            pool = happybase.ConnectionPool(size=10, host='hadoop-master')

            # è¿›è¡Œä¸ºç›¸ä¼¼æ–‡ç« è·å–
            with pool.connection() as conn:

                # key:   article_id,    column:  similar:article_id
                similar_table = conn.table('article_similar')
                # å¾ªç¯partition
                for row in partition:
                    # è·å–ç›¸ä¼¼æ–‡ç« ç»“æœè¡¨
                    similar_article = similar_table.row(str(row.article_id).encode(),
                                                        columns=[b'similar'])
                    # ç›¸ä¼¼æ–‡ç« ç›¸ä¼¼åº¦æ’åºè¿‡æ»¤ï¼Œå¬å›ä¸éœ€è¦å¤ªå¤§çš„æ•°æ®ï¼Œ ç™¾ä¸ªï¼Œåƒ
                    _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True)
                    if _srt:
                        # æ¯æ¬¡è¡Œä¸ºæ¨è10ç¯‡æ–‡ç« 
                        reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10]

                        # è·å–å†å²çœ‹è¿‡çš„è¯¥é¢‘é“æ–‡ç« 
                        history_table = conn.table('history_recall')
                        # å¤šä¸ªç‰ˆæœ¬
                        data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                                   'channel:{}'.format(row.channel_id).encode())

                        history = []
                            if len(_history_data) &gt; 1:
                                for l in _history_data:
                                    history.extend(l)

                        # è¿‡æ»¤reco_articleä¸history
                        reco_res = list(set(reco_article) - set(history))

                        # è¿›è¡Œæ¨èï¼Œæ”¾å…¥åŸºäºå†…å®¹çš„å¬å›è¡¨å½“ä¸­ä»¥åŠå†å²çœ‹è¿‡çš„æ–‡ç« è¡¨å½“ä¸­
                        if reco_res:
                            # content_table = conn.table('cb_content_recall')
                            content_table = conn.table('cb_recall')
                            content_table.put(&quot;recall:user:{}&quot;.format(row.user_id).encode(),
                                              {'content:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                            # æ”¾å…¥å†å²æ¨èè¿‡æ–‡ç« 
                            history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                              {'channel:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                conn.close()

        user_article_basic.foreachPartition(save_content_filter_history_to__recall)


if __name__ == '__main__':
    ur = UpdateRecall(500)
    ur.update_als_recall()
    ur.update_content_recall()
</code></pre>
<p>å®šæ—¶æ›´æ–°ä»£ç ï¼Œåœ¨main.pyå’Œupdate.pyä¸­æ·»åŠ ä»¥ä¸‹ä»£ç ï¼š<br>
from offline.update_recall import UpdateRecall<br>
from schedule.update_profile import update_user_profile, update_article_profile, update_recall</p>
<pre><code>def update_recall():
    &quot;&quot;&quot;
    æ›´æ–°ç”¨æˆ·çš„å¬å›é›†
    :return:
    &quot;&quot;&quot;
    udp = UpdateRecall(200)
    udp.update_als_recall()
    udp.update_content_recall()
</code></pre>
<p>mainä¸­æ·»åŠ <br>
scheduler.add_job(update_recall, trigger='interval', hour=3)</p>
<h1 id="æ’åºè®¾è®¡">æ’åºè®¾è®¡</h1>
<h3 id="æ’åºæ¨¡å‹">æ’åºæ¨¡å‹</h3>
<p>å®½æ¨¡å‹ + ç‰¹å¾â¼¯ç¨‹<br>
LR/MLR + éIDç±»ç‰¹å¾(â¼ˆâ¼¯ç¦»æ•£/GBDT/FM)<br>
spark ä¸­å¯ä»¥ç›´æ¥ä½¿ç”¨<br>
å®½æ¨¡å‹ + æ·±æ¨¡å‹<br>
wide&amp;deep,DeepFM<br>
ä½¿ç”¨TensorFlowè¿›è¡Œè®­ç»ƒ<br>
æ·±æ¨¡å‹ï¼š<br>
DNN + ç‰¹å¾embedding<br>
ä½¿ç”¨TensorFlowè¿›è¡Œè®­ç»ƒ</p>
<h3 id="ç‰¹å¾å¤„ç†åŸåˆ™">ç‰¹å¾å¤„ç†åŸåˆ™</h3>
<p>ç¦»æ•£æ•°æ®<br>
one-hotç¼–ç <br>
è¿ç»­æ•°æ®<br>
å½’ä¸€åŒ–<br>
å›¾ç‰‡/æ–‡æœ¬<br>
æ–‡ç« æ ‡ç­¾/å…³é”®è¯æå–<br>
embedding</p>
<h3 id="ä¼˜åŒ–è®­ç»ƒæ–¹å¼">ä¼˜åŒ–è®­ç»ƒæ–¹å¼</h3>
<p>ä½¿ç”¨Batch SGDä¼˜åŒ–<br>
åŠ å…¥æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ</p>
<h3 id="spark-lr-è¿›è¡Œé¢„ä¼°">spark LR è¿›è¡Œé¢„ä¼°</h3>
<p>ç›®çš„ï¼šé€šè¿‡LRæ¨¡å‹è¿›è¡ŒCTRé¢„ä¼°<br>
æ­¥éª¤ï¼š<br>
1ã€éœ€è¦é€šè¿‡sparkè¯»å–HIVEå¤–éƒ¨è¡¨ï¼Œéœ€è¦æ–°çš„sparksessioné…ç½®<br>
å¢åŠ HBASEé…ç½®<br>
2ã€è¯»å–ç”¨æˆ·ç‚¹å‡»è¡Œä¸ºè¡¨ï¼Œä¸ç”¨æˆ·ç”»åƒå’Œæ–‡ç« ç”»åƒï¼Œæ„é€ è®­ç»ƒæ ·æœ¬<br>
3ã€LRæ¨¡å‹è¿›è¡Œè®­ç»ƒ<br>
4ã€LRæ¨¡å‹é¢„æµ‹ã€ç»“æœè¯„ä¼°</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => HBase]]></title>
        <id>https://cythonlin.github.io/post/py-greater-hbase/</id>
        <link href="https://cythonlin.github.io/post/py-greater-hbase/">
        </link>
        <updated>2020-09-29T04:21:09.000Z</updated>
        <content type="html"><![CDATA[<h1 id="æŠ¥é”™">æŠ¥é”™</h1>
<p>è‹¥list æˆ–å…¶ä»–å‘½ä»¤ æœ‰å¦‚ä¸‹é”™è¯¯ï¼š<br>
ERROR: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing<br>
åˆ™ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ï¼š<br>
cd $HBASE_HOME/bin<br>
./hbase zkcli<br>
ls /<br>
rmr /hbase<br>
ls /<br>
é€€å‡º zookeeper cliï¼Œ åˆ é™¤hdfsä¸­çš„ /hbase<br>
hdfs dfs -rm -r /hbase<br>
ç„¶åé‡å¯hbase:<br>
cd $HBASE_HOME/bin<br>
./stop-hbase.sh<br>
./start-hbase.sh<br>
è‹¥stop hbaseçš„æ—¶å€™å‡ºç° ..... åœæ­¢ä¸æ‰ï¼Œ åˆ™ï¼š<br>
cd $HBASE_HOME/bin<br>
./hbase-daemons.sh stop regionserver</p>
<pre><code># ./start-hbase.sh
# kill -9 pidæ¥ç»ˆæ­¢hbaseçš„è¿›ç¨‹
</code></pre>
<h1 id="hbaseå‘½ä»¤">HBaseå‘½ä»¤</h1>
<p>https://blog.csdn.net/vbirdbest/article/details/88236575</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RS => æ¨èç³»ç»Ÿï¼ˆäºŒï¼‰ç¦»çº¿ç”»åƒæ„å»º ]]></title>
        <id>https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-er-chi-xian-hua-xiang-gou-jian/</id>
        <link href="https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-er-chi-xian-hua-xiang-gou-jian/">
        </link>
        <updated>2020-09-29T04:20:35.000Z</updated>
        <content type="html"><![CDATA[<h1 id="æ–‡ç« ç¦»çº¿ç”»åƒæ„å»º">æ–‡ç« ç¦»çº¿ç”»åƒæ„å»º</h1>
<h3 id="sparké…ç½®åŸºç±»æŠ½å–">Sparké…ç½®åŸºç±»æŠ½å–</h3>
<pre><code>from pyspark import SparkConf
from pyspark.sql import SparkSession
import os

class SparkSessionBase(object):

	SPARK_APP_NAME = None
	SPARK_URL = &quot;yarn&quot;

	SPARK_EXECUTOR_MEMORY = &quot;2g&quot;
	SPARK_EXECUTOR_CORES = 2
	SPARK_EXECUTOR_INSTANCES = 2

	ENABLE_HIVE_SUPPORT = False

	def _create_spark_session(self):
		conf = SparkConf()  # åˆ›å»ºspark configå¯¹è±¡
		config = (
			(&quot;spark.app.name&quot;, self.SPARK_APP_NAME),  # è®¾ç½®å¯åŠ¨çš„sparkçš„appåç§°ï¼Œæ²¡æœ‰æä¾›ï¼Œå°†éšæœºäº§ç”Ÿä¸€ä¸ªåç§°
			(&quot;spark.executor.memory&quot;, self.SPARK_EXECUTOR_MEMORY),  # è®¾ç½®è¯¥appå¯åŠ¨æ—¶å ç”¨çš„å†…å­˜ç”¨é‡ï¼Œé»˜è®¤2g
	 		(&quot;spark.master&quot;, self.SPARK_URL),  # spark masterçš„åœ°å€
			(&quot;spark.executor.cores&quot;, self.SPARK_EXECUTOR_CORES),  # è®¾ç½®spark executorä½¿ç”¨çš„CPUæ ¸å¿ƒæ•°ï¼Œé»˜è®¤æ˜¯1æ ¸å¿ƒ
			(&quot;spark.executor.instances&quot;, self.SPARK_EXECUTOR_INSTANCES),
			(&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;),
		)

	conf.setAll(config)

	# åˆ©ç”¨configå¯¹è±¡ï¼Œåˆ›å»ºspark session
	if self.ENABLE_HIVE_SUPPORT:
		return SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()
	else:
		return SparkSession.builder.config(conf=conf).getOrCreate()
</code></pre>
<h3 id="ä¸»åº”ç”¨å¯¼å…¥åŸºç±»">ä¸»åº”ç”¨å¯¼å…¥åŸºç±»</h3>
<pre><code># pip install pyspark
# pip install findspark

import findspark
findspark.init()

import os
import sys
# å¦‚æœå½“å‰ä»£ç æ–‡ä»¶è¿è¡Œæµ‹è¯•éœ€è¦åŠ å…¥ä¿®æ”¹è·¯å¾„ï¼Œé¿å…å‡ºç°åå¯¼åŒ…é—®é¢˜
BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))
sys.path.insert(0, os.path.join(BASE_DIR))
print(BASE_DIR)
PYSPARK_PYTHON = &quot;/miniconda2/envs/reco_sys/bin/python&quot;
# å½“å­˜åœ¨å¤šä¸ªç‰ˆæœ¬æ—¶ï¼Œä¸æŒ‡å®šå¾ˆå¯èƒ½ä¼šå¯¼è‡´å‡ºé”™
os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHON
os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHON
from offline import SparkSessionBase

class OriginArticleData(SparkSessionBase):

	SPARK_APP_NAME = &quot;mergeArticle&quot;
	SPARK_URL = &quot;yarn&quot;

	ENABLE_HIVE_SUPPORT = True

	def __init__(self):
		self.spark = self._create_spark_session()

oa = OriginArticleData()   # oaå°±æ˜¯å¸¦æœ‰é…ç½®çš„ sparkSessionçš„å®ä¾‹åŒ–å¯¹è±¡
</code></pre>
<h3 id="æ–‡ç« -è¡¨-åˆå¹¶">æ–‡ç«  è¡¨ åˆå¹¶</h3>
<p>æ–‡ç« åŸºæœ¬ä¿¡æ¯è¡¨+æ–‡ç« å†…å®¹è¡¨+é¢‘é“è¡¨ï¼š<br>
titlce_content = oa.spark.sql(&quot;select a.article_id, a.channel_id, a.title, b.content from news_article_basic a inner join news_article_content b on a.article_id=b.article_id)<br>
å› ä¸ºå¾—åˆ°çš„æ˜¯ DFç±»å‹ï¼Œæƒ³è¦ç”¨SQLï¼Œå¯ä»¥æŠŠDFæ³¨å†Œä¸ºä¸´æ—¶è¡¨<br>
titlce_content.registerTempTable('temptable')<br>
å†æŠŠ é¢‘é“è¡¨ çš„ é¢‘é“å åˆå¹¶è¿›æ¥<br>
channel_title_content = oa.spark.sql(&quot;select t.*, n.channel_name from temptable t left join news_channel n on t.channel_id=n.channel_id&quot;)</p>
<h3 id="æ–‡ç« -å­—æ®µ-åˆå¹¶">æ–‡ç«  å­—æ®µ åˆå¹¶</h3>
<p>å°† æ–‡ç« æ ‡é¢˜+æ–‡ç« å†…å®¹+æ–‡ç« é¢‘é“ çš„åˆ—ï¼Œæ‹¼æ¥æˆä¸€ä¸ªå¤§å­—ç¬¦ä¸²<br>
import pyspark.sql.functions as F<br>
import gc</p>
<pre><code># å¢åŠ channelçš„åå­—ï¼Œåé¢ä¼šä½¿ç”¨
basic_content.registerTempTable(&quot;temparticle&quot;)
channel_basic_content = oa.spark.sql(
	&quot;select t.*, n.channel_name from temparticle t left join news_channel n on t.channel_id=n.channel_id&quot;
)

# åˆ©ç”¨concat_wsæ–¹æ³•ï¼Œå°†å¤šåˆ—æ•°æ®åˆå¹¶ä¸ºä¸€ä¸ªé•¿æ–‡æœ¬å†…å®¹ï¼ˆé¢‘é“ï¼Œæ ‡é¢˜ä»¥åŠå†…å®¹åˆå¹¶ï¼‰
oa.spark.sql(&quot;use article&quot;)
sentence_df = channel_basic_content.select(&quot;article_id&quot;, &quot;channel_id&quot;, &quot;channel_name&quot;, &quot;title&quot;, &quot;content&quot;, \
	F.concat_ws(
		&quot;,&quot;,											# æŒ‡å®šå¤§å­—ç¬¦ä¸²åˆ†éš”ç¬¦
		channel_basic_content.channel_name,         
		channel_basic_content.title,
		channel_basic_content.content					
		).alias(&quot;sentence&quot;)                    # æ–°åˆ— å¤§å­—ç¬¦ä¸² å–å
	)
del basic_content
del channel_basic_content
gc.collect()

# sentence_df.write.insertInto(&quot;article_data&quot;)       # å†™å…¥æå‰åˆ›å»ºå¥½çš„Hiveè¡¨ä¸­
</code></pre>
<h3 id="åˆ†è¯">åˆ†è¯</h3>
<pre><code>def segmentation(partition):          # å°±è¿™ä¸€è¡Œçš„ç¼©è¿›éœ€è¦è°ƒæ•´ä¸‹
import os
import re

import jieba
import jieba.analyse
import jieba.posseg as pseg
import codecs

abspath = &quot;/root/words&quot;

# ç»“å·´åŠ è½½ç”¨æˆ·è¯å…¸
userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;)
jieba.load_userdict(userDict_path)

# åœç”¨è¯æ–‡æœ¬
stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;)

def get_stopwords_list():
    &quot;&quot;&quot;è¿”å›stopwordsåˆ—è¡¨&quot;&quot;&quot;
    stopwords_list = [i.strip()
                      for i in codecs.open(stopwords_path).readlines()]
    return stopwords_list

# æ‰€æœ‰çš„åœç”¨è¯åˆ—è¡¨
stopwords_list = get_stopwords_list()

# åˆ†è¯
def cut_sentence(sentence):
    &quot;&quot;&quot;å¯¹åˆ‡å‰²ä¹‹åçš„è¯è¯­è¿›è¡Œè¿‡æ»¤ï¼Œå»é™¤åœç”¨è¯ï¼Œä¿ç•™åè¯ï¼Œè‹±æ–‡å’Œè‡ªå®šä¹‰è¯åº“ä¸­çš„è¯ï¼Œé•¿åº¦å¤§äº2çš„è¯&quot;&quot;&quot;
    # print(sentence,&quot;*&quot;*100)
    # eg:[pair('ä»Šå¤©', 't'), pair('æœ‰', 'd'), pair('é›¾', 'n'), pair('éœ¾', 'g')]
    seg_list = pseg.lcut(sentence)
    seg_list = [i for i in seg_list if i.flag not in stopwords_list]
    filtered_words_list = []
    for seg in seg_list:
        # print(seg)
        if len(seg.word) &lt;= 1:
            continue
        elif seg.flag == &quot;eng&quot;:
            if len(seg.word) &lt;= 2:
                continue
            else:
                filtered_words_list.append(seg.word)
        elif seg.flag.startswith(&quot;n&quot;):
            filtered_words_list.append(seg.word)
        elif seg.flag in [&quot;x&quot;, &quot;eng&quot;]:  # æ˜¯è‡ªå®šä¸€ä¸ªè¯è¯­æˆ–è€…æ˜¯è‹±æ–‡å•è¯
            filtered_words_list.append(seg.word)
    return filtered_words_list

for row in partition:
    sentence = re.sub(&quot;&lt;.*?&gt;&quot;, &quot;&quot;, row.sentence)    # æ›¿æ¢æ‰æ ‡ç­¾æ•°æ®
    words = cut_sentence(sentence)
    yield row.article_id, row.channel_id, words
</code></pre>
<h3 id="è®¡ç®—-tf-idf">è®¡ç®— TF-IDF</h3>
<p>TF:<br>
ktt.spark.sql(&quot;use article&quot;)<br>
article_dataframe = ktt.spark.sql(&quot;select * from article_data limit 20&quot;)<br>
words_df = article_dataframe.rdd.mapPartitions(segmentation).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;words&quot;])</p>
<pre><code>from pyspark.ml.feature import CountVectorizer
# æ€»è¯æ±‡çš„å¤§å°ï¼Œæ–‡æœ¬ä¸­å¿…é¡»å‡ºç°çš„æ¬¡æ•°
cv = CountVectorizer(inputCol=&quot;words&quot;, outputCol=&quot;countFeatures&quot;, vocabSize=200*10000, minDF=1.0)
# è®­ç»ƒè¯é¢‘ç»Ÿè®¡æ¨¡å‹
cv_model = cv.fit(words_df)
cv_model.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;)

# cv_model.vocabulary æŸ¥çœ‹ç»Ÿè®¡è¯è¡¨ï¼ˆç›¸å½“äºgroupbyç»“æœçš„ key,  ä½†ä¸åŒ…æ‹¬valueï¼‰
</code></pre>
<p>è®­ç»ƒTF-IDF:<br>
# è¯è¯­ä¸è¯é¢‘ç»Ÿè®¡<br>
from pyspark.ml.feature import CountVectorizerModel<br>
cv_model = CountVectorizerModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;)<br>
# å¾—å‡ºè¯é¢‘å‘é‡ç»“æœ<br>
cv_result = cv_model.transform(words_df)<br>
# è®­ç»ƒIDFæ¨¡å‹ (æŠŠ tfç»“æœä¼ è¿›å»ï¼Œå…¶å®è¯´æ˜¯ IDFæ¨¡å‹ï¼Œè®¡ç®—ç»“æœå¾—å‡ºçš„å°±æ˜¯ TF-IDF)<br>
from pyspark.ml.feature import IDF<br>
idf = IDF(inputCol=&quot;countFeatures&quot;, outputCol=&quot;idfFeatures&quot;)<br>
idfModel = idf.fit(cv_result)<br>
idfModel.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/IDF.model&quot;)</p>
<pre><code># idfModel.idf.toArray()[:20]    æŸ¥çœ‹é€†æ–‡æ¡£é¢‘ç‡çŸ©é˜µ
</code></pre>
<p>TF-IDFç»“æœæ•°æ®æ ¼å¼ï¼š<br>
åˆ—1ï¼Œ åˆ—...ï¼Œ åˆ— TF-IDF<br>
(1000,[804,1032],[6.349777077,7.0761797]) ã€‚ã€‚ã€‚<br>
ä½¿ç”¨TF-IDFæ¨¡å‹ï¼Œå–Top-Kä¸ªè¯ï¼š<br>
from pyspark.ml.feature import CountVectorizerModel<br>
cv_model = CountVectorizerModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model&quot;)<br>
from pyspark.ml.feature import IDFModel<br>
idf_model = IDFModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model&quot;)<br>
cv_result = cv_model.transform(words_df)<br>
tfidf_result = idf_model.transform(cv_result)</p>
<pre><code>def func(partition):            
	TOPK = 20
	for row in partition:
		# æ‰¾åˆ°ç´¢å¼•ä¸IDFå€¼å¹¶è¿›è¡Œæ’åº
		_ = list(zip(row.idfFeatures.indices, row.idfFeatures.values))   # [ (indexes,values)ï¼Œ  (indexes,values)]
		_ = sorted(_, key=lambda x: x[1], reverse=True)
		result = _[:TOPK]
		for word_index, tfidf in result:
			yield row.article_id, row.channel_id, int(word_index), round(float(tfidf), 4)
			# yieldè¿™å¥æ³¨å®šäº†è¿”å›ç»“æœæ ¼å¼ (å¤šå±‚forå¾ªç¯ yieldï¼Œ åŸæœ¬ä¸€è¡Œæ•°æ®æŒ‰æ¯ä¸ªå•è¯çˆ†ç‚¸å±•å¼€)
			# article_id,   channel_id,   word_index,   tfidf
			# 1				 100         40         15.5
			# 1				 100         14         10.3         
			# 1				 100         23         13.2
            
            
_keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;index&quot;, &quot;tfidf&quot;])
</code></pre>
<p>æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ï¼š æ„æˆ  è¯+TFIDFå€¼, è€Œä¸æ˜¯ç´¢å¼•+TFIDF<br>
cv_model.vocabulary ç»“æœæ˜¯æ‰€æœ‰å•è¯çš„åˆ—è¡¨ã€‚ ä¸Šé¢çš„ indexå°±æ˜¯å¯¹åº”è¿™ä¸ªåˆ—è¡¨çš„ç´¢å¼•<br>
æœ€ç»ˆæ„å»ºä¸€ä¸ªè¯å…¸+ç´¢å¼•è¡¨ï¼š<br>
index	word<br>
..		..	<br>
ç„¶åå°† ä¸»è¡¨ï¼ˆæ–‡ç« id,é¢‘é“id,ç´¢å¼•ï¼Œtfidfï¼‰ä¸ è¯å…¸è¡¨ï¼ˆindex+wordï¼‰ åˆå¹¶<br>
å¾—åˆ°  ï¼ˆæ–‡ç« id,é¢‘é“id, è¯ï¼Œ tfidfï¼‰</p>
<h3 id="è®¡ç®—-textrank">è®¡ç®— TextRank</h3>
<p>TextRankå’Œæ ¸å¿ƒå°±æ˜¯è®¾å®šä¸€ä¸ªå›ºå®šçª—å£æ¥æ»‘åŠ¨<br>
æŠŠæ¯ä¸ªçª—å£å†…çš„æ¯ä¸ªè¯ï¼Œ è®¾ä¸ºå­—å…¸çš„Key, valueå°±æ˜¯ä»–é™„è¿‘çš„nä¸ªè¯çš„åˆ—è¡¨<br>
ç„¶åæ¯ä¸ªè¯éƒ½è¿™æ ·åšï¼Œ é‡åˆ°ç›¸åŒçš„è¯å°±è¿½åŠ åˆ°å­—å…¸çš„value åˆ—è¡¨ä¸­<br>
# åˆ†è¯<br>
def textrank(partition):<br>
import os</p>
<pre><code>import jieba
import jieba.analyse
import jieba.posseg as pseg
import codecs

abspath = &quot;/root/words&quot;

# ç»“å·´åŠ è½½ç”¨æˆ·è¯å…¸
userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;)
jieba.load_userdict(userDict_path)

# åœç”¨è¯æ–‡æœ¬
stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;)

def get_stopwords_list():
    &quot;&quot;&quot;è¿”å›stopwordsåˆ—è¡¨&quot;&quot;&quot;
    stopwords_list = [i.strip()
                      for i in codecs.open(stopwords_path).readlines()]
    return stopwords_list

# æ‰€æœ‰çš„åœç”¨è¯åˆ—è¡¨
stopwords_list = get_stopwords_list()

class TextRank(jieba.analyse.TextRank):
    def __init__(self, window=20, word_min_len=2):
        super(TextRank, self).__init__()
        self.span = window  # çª—å£å¤§å°
        self.word_min_len = word_min_len  # å•è¯çš„æœ€å°é•¿åº¦
        # è¦ä¿ç•™çš„è¯æ€§ï¼Œæ ¹æ®jieba github ï¼Œå…·ä½“å‚è§https://github.com/baidu/lac
        self.pos_filt = frozenset(
            ('n', 'x', 'eng', 'f', 's', 't', 'nr', 'ns', 'nt', &quot;nw&quot;, &quot;nz&quot;, &quot;PER&quot;, &quot;LOC&quot;, &quot;ORG&quot;))

    def pairfilter(self, wp):
        &quot;&quot;&quot;è¿‡æ»¤æ¡ä»¶ï¼Œè¿”å›Trueæˆ–è€…False&quot;&quot;&quot;

        if wp.flag == &quot;eng&quot;:
            if len(wp.word) &lt;= 2:
                return False

        if wp.flag in self.pos_filt and len(wp.word.strip()) &gt;= self.word_min_len \
                and wp.word.lower() not in stopwords_list:
            return True
# TextRankè¿‡æ»¤çª—å£å¤§å°ä¸º5ï¼Œå•è¯æœ€å°ä¸º2
textrank_model = TextRank(window=5, word_min_len=2)
allowPOS = ('n', &quot;x&quot;, 'eng', 'nr', 'ns', 'nt', &quot;nw&quot;, &quot;nz&quot;, &quot;c&quot;)

for row in partition:
    tags = textrank_model.textrank(row.sentence, topK=20, withWeight=True, allowPOS=allowPOS, withFlag=False)
    for tag in tags:
        yield row.article_id, row.channel_id, tag[0], tag[1]

# è®¡ç®—textrank
textrank_keywords_df = article_dataframe.rdd.mapPartitions(textrank).toDF(
	[&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;textrank&quot;]
)

# textrank_keywords_df.write.insertInto(&quot;textrank_keywords_values&quot;)
</code></pre>
<p>textrankè¿è¡Œç»“æœå¦‚ä¸‹ï¼š<br>
hive&gt; select * from textrank_keywords_values limit 20;<br>
OK<br>
æ–‡ç« ID  channel   word    textrank<br>
98319   17      var     20.6079<br>
98323   17      var     7.4938<br>
98326   17      var     104.9128<br>
ç„¶åå’Œ tfidfä¸€æ · æ ¹æ® textrankå€¼ï¼Œ  å–TOP-Kä¸ªè¯</p>
<h3 id="è®¡ç®—-ä¸»é¢˜è¯-å’Œ-å…³é”®è¯">è®¡ç®— ä¸»é¢˜è¯ å’Œ å…³é”®è¯</h3>
<p>å…³é”®è¯ï¼šTEXTRANKè®¡ç®—å‡ºçš„ç»“æœTOPKä¸ªè¯ä»¥åŠæƒé‡<br>
ä¸»é¢˜è¯ï¼šTEXTRANKçš„TOPKè¯ ä¸ ITFDFè®¡ç®—çš„TOPKä¸ªè¯çš„äº¤é›†<br>
æ ¼å¼å¦‚ä¸‹ï¼š<br>
hive&gt; desc article_profile;<br>
OK<br>
article_id              int                     article_id<br>
channel_id              int                     channel_id<br>
keywords               map								 keywords<br>
topics						  array								topics</p>
<pre><code>hive&gt; select * from article_profile limit 1;    
# è¿™é‡ŒæŠŠç»“æœæŒ‰è¡Œæ’åˆ—å¼€æ–¹ä¾¿è§‚çœ‹
article_id			26
channel_id		   17            
å…³é”®è¯å­—å…¸		  {&quot;ç­–ç•¥&quot;:0.3973770571351729,&quot;jpg&quot;:0.9806348975390871,&quot;ç”¨æˆ·&quot;:1.2794959063944176,&quot;strong&quot;:1.6488457985625076,&quot;æ–‡ä»¶&quot;:0.28144603583387057,&quot;é€»è¾‘&quot;:0.45256526469610714,&quot;å½¢å¼&quot;:0.4123994242601279,&quot;å…¨è‡ª&quot;:0.9594604850547191,&quot;h2&quot;:0.6244481634710125,&quot;ç‰ˆæœ¬&quot;:0.44280276959510817,&quot;Adobe&quot;:0.8553618185108718,&quot;å®‰è£…&quot;:0.8305037437573172,&quot;æ£€æŸ¥æ›´æ–°&quot;:1.8088946300014435,&quot;äº§å“&quot;:0.774842382276899,&quot;ä¸‹è½½é¡µ&quot;:1.4256311032544344,&quot;è¿‡ç¨‹&quot;:0.19827163395829256,&quot;json&quot;:0.6423301791599972,&quot;æ–¹å¼&quot;:0.582762869780791,&quot;é€€å‡ºåº”ç”¨&quot;:1.2338671268242603,&quot;Setup&quot;:1.004399549339134} 

ä¸»é¢˜è¯åˆ—è¡¨				[&quot;Electron&quot;,&quot;å…¨è‡ªåŠ¨&quot;,&quot;äº§å“&quot;,&quot;ç‰ˆæœ¬å·&quot;,&quot;å®‰è£…åŒ…&quot;,&quot;æ£€æŸ¥æ›´æ–°&quot;,&quot;æ–¹æ¡ˆ&quot;,&quot;ç‰ˆæœ¬&quot;,&quot;é€€å‡ºåº”ç”¨&quot;,&quot;é€»è¾‘&quot;,&quot;å®‰è£…è¿‡ç¨‹&quot;,&quot;æ–¹å¼&quot;,&quot;å®šæ€§&quot;,&quot;æ–°ç‰ˆæœ¬&quot;,&quot;Setup&quot;,&quot;é™é»˜&quot;,&quot;ç”¨æˆ·&quot;]
</code></pre>
<h3 id="å¢é‡æ›´æ–°-ç¦»çº¿æ–‡ç« ç”»åƒ">å¢é‡æ›´æ–° ç¦»çº¿æ–‡ç« ç”»åƒ</h3>
<p>æ›´æ–°æµç¨‹ï¼š<br>
1ã€toutiao æ•°æ®åº“ä¸­ï¼Œnews_article_content ä¸news_article_basicâ€”&gt;æ›´æ–°åˆ°articleæ•°æ®åº“ä¸­article_dataè¡¨ï¼Œæ–¹ä¾¿æ“ä½œ<br>
2. ç¬¬ä¸€æ¬¡ï¼šæ‰€æœ‰æ›´æ–°ï¼Œåé¢å¢é‡æ¯å¤©çš„æ•°æ®æ›´æ–°26æ—¥ï¼š1ï¼š00<sub>2ï¼š00ï¼Œ2ï¼š00</sub>3ï¼š00ï¼Œå·¦é—­å³å¼€,ä¸€ä¸ªå°æ—¶æ›´æ–°ä¸€æ¬¡<br>
3ã€åˆšæ‰æ–°æ›´æ–°çš„æ–‡ç« ï¼Œé€šè¿‡å·²æœ‰çš„idfè®¡ç®—å‡ºtfidfå€¼ä»¥åŠhive çš„textrank_keywords_values<br>
4ã€æ›´æ–°hiveçš„article_profile</p>
<p>ç¦»çº¿æ›´æ–°æ–‡ç« ç”»åƒ ä»£ç ç»„è£…ï¼šPycharm<br>
æ³¨æ„åœ¨Pycharmä¸­è¿è¡Œè¦è®¾ç½®ç¯å¢ƒï¼š</p>
<pre><code>PYTHONUNBUFFERED=1
JAVA_HOME=/root/bigdata/jdk
SPARK_HOME=/root/bigdata/spark
HADOOP_HOME=/root/bigdata/hadoop
PYSPARK_PYTHON=/root/anaconda3/envs/reco_sys/bin/python
PYSPARK_DRIVER_PYTHON=/root/anaconda3/envs/reco_sys/bin/python
</code></pre>
<p>å…·ä½“ä»£ç å¦‚ä¸‹ï¼š<br>
import os<br>
import sys<br>
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(<strong>file</strong>)))<br>
sys.path.insert(0, os.path.join(BASE_DIR))<br>
from offline import SparkSessionBase<br>
from datetime import datetime<br>
from datetime import timedelta<br>
import pyspark.sql.functions as F<br>
import pyspark<br>
import gc</p>
<pre><code>class UpdateArticle(SparkSessionBase):
&quot;&quot;&quot;
æ›´æ–°æ–‡ç« ç”»åƒ
&quot;&quot;&quot;
SPARK_APP_NAME = &quot;updateArticle&quot;
ENABLE_HIVE_SUPPORT = True

SPARK_EXECUTOR_MEMORY = &quot;7g&quot;

def __init__(self):
    self.spark = self._create_spark_session()

    self.cv_path = &quot;hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model&quot;
    self.idf_path = &quot;hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model&quot;

def get_cv_model(self):
    # è¯è¯­ä¸è¯é¢‘ç»Ÿè®¡
    from pyspark.ml.feature import CountVectorizerModel
    cv_model = CountVectorizerModel.load(self.cv_path)
    return cv_model

def get_idf_model(self):
    from pyspark.ml.feature import IDFModel
    idf_model = IDFModel.load(self.idf_path)
    return idf_model

@staticmethod
def compute_keywords_tfidf_topk(words_df, cv_model, idf_model):
    &quot;&quot;&quot;ä¿å­˜tfidfå€¼é«˜çš„20ä¸ªå…³é”®è¯
    :param spark:
    :param words_df:
    :return:
    &quot;&quot;&quot;
    cv_result = cv_model.transform(words_df)
    tfidf_result = idf_model.transform(cv_result)
    # print(&quot;transform compelete&quot;)

    # å–TOP-Nçš„TFIDFå€¼é«˜çš„ç»“æœ
    def func(partition):
        TOPK = 20
        for row in partition:
            _ = list(zip(row.idfFeatures.indices, row.idfFeatures.values))
            _ = sorted(_, key=lambda x: x[1], reverse=True)
            result = _[:TOPK]
            #         words_index = [int(i[0]) for i in result]
            #         yield row.article_id, row.channel_id, words_index

            for word_index, tfidf in result:
                yield row.article_id, row.channel_id, int(word_index), round(float(tfidf), 4)

    _keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;index&quot;, &quot;tfidf&quot;])

    return _keywordsByTFIDF

def merge_article_data(self):
    &quot;&quot;&quot;
    åˆå¹¶ä¸šåŠ¡ä¸­å¢é‡æ›´æ–°çš„æ–‡ç« æ•°æ®
    :return:
    &quot;&quot;&quot;
    # è·å–æ–‡ç« ç›¸å…³æ•°æ®, æŒ‡å®šè¿‡å»ä¸€ä¸ªå°æ—¶æ•´ç‚¹åˆ°æ•´ç‚¹çš„æ›´æ–°æ•°æ®
    # å¦‚ï¼š26æ—¥ï¼š1ï¼š00~2ï¼š00ï¼Œ2ï¼š00~3ï¼š00ï¼Œå·¦é—­å³å¼€
    self.spark.sql(&quot;use toutiao&quot;)
    _yester = datetime.today().replace(minute=0, second=0, microsecond=0)
    start = datetime.strftime(_yester + timedelta(days=0, hours=-1, minutes=0), &quot;%Y-%m-%d %H:%M:%S&quot;)
    end = datetime.strftime(_yester, &quot;%Y-%m-%d %H:%M:%S&quot;)

    # åˆå¹¶åä¿ç•™ï¼šarticle_idã€channel_idã€channel_nameã€titleã€content
    # +----------+----------+--------------------+--------------------+
    # | article_id | channel_id | title | content |
    # +----------+----------+--------------------+--------------------+
    # | 141462 | 3 | test - 20190316 - 115123 | ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œå¿ƒæƒ…å¾ˆç¾ä¸½ï¼ï¼ï¼ |
    basic_content = self.spark.sql(
        &quot;select a.article_id, a.channel_id, a.title, b.content from news_article_basic a &quot;
        &quot;inner join news_article_content b on a.article_id=b.article_id where a.review_time &gt;= '{}' &quot;
        &quot;and a.review_time &lt; '{}' and a.status = 2&quot;.format(start, end))
    # å¢åŠ channelçš„åå­—ï¼Œåé¢ä¼šä½¿ç”¨
    basic_content.registerTempTable(&quot;temparticle&quot;)
    channel_basic_content = self.spark.sql(
        &quot;select t.*, n.channel_name from temparticle t left join news_channel n on t.channel_id=n.channel_id&quot;)

    # åˆ©ç”¨concat_wsæ–¹æ³•ï¼Œå°†å¤šåˆ—æ•°æ®åˆå¹¶ä¸ºä¸€ä¸ªé•¿æ–‡æœ¬å†…å®¹ï¼ˆé¢‘é“ï¼Œæ ‡é¢˜ä»¥åŠå†…å®¹åˆå¹¶ï¼‰
    self.spark.sql(&quot;use article&quot;)
    sentence_df = channel_basic_content.select(&quot;article_id&quot;, &quot;channel_id&quot;, &quot;channel_name&quot;, &quot;title&quot;, &quot;content&quot;, \
                                               F.concat_ws(
                                                   &quot;,&quot;,
                                                   channel_basic_content.channel_name,
                                                   channel_basic_content.title,
                                                   channel_basic_content.content
                                               ).alias(&quot;sentence&quot;)
                                               )
    del basic_content
    del channel_basic_content
    gc.collect()

    sentence_df.write.insertInto(&quot;article_data&quot;)
    return sentence_df

def generate_article_label(self, sentence_df):
    &quot;&quot;&quot;
    ç”Ÿæˆæ–‡ç« æ ‡ç­¾  tfidf, textrank
    :param sentence_df: å¢é‡çš„æ–‡ç« å†…å®¹
    :return:
    &quot;&quot;&quot;
    # è¿›è¡Œåˆ†è¯
    words_df = sentence_df.rdd.mapPartitions(segmentation).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;words&quot;])
    cv_model = self.get_cv_model()
    idf_model = self.get_idf_model()

    # 1ã€ä¿å­˜æ‰€æœ‰çš„è¯çš„idfçš„å€¼ï¼Œåˆ©ç”¨idfä¸­çš„è¯çš„æ ‡ç­¾ç´¢å¼•
    # å·¥å…·ä¸ä¸šåŠ¡éš”ç¦»
    _keywordsByTFIDF = UpdateArticle.compute_keywords_tfidf_topk(words_df, cv_model, idf_model)

    keywordsIndex = self.spark.sql(&quot;select keyword, index idx from idf_keywords_values&quot;)

    keywordsByTFIDF = _keywordsByTFIDF.join(keywordsIndex, keywordsIndex.idx == _keywordsByTFIDF.index).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;tfidf&quot;])

    keywordsByTFIDF.write.insertInto(&quot;tfidf_keywords_values&quot;)

    del cv_model
    del idf_model
    del words_df
    del _keywordsByTFIDF
    gc.collect()

    # è®¡ç®—textrank
    textrank_keywords_df = sentence_df.rdd.mapPartitions(textrank).toDF(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;textrank&quot;])
    textrank_keywords_df.write.insertInto(&quot;textrank_keywords_values&quot;)

    return textrank_keywords_df, keywordsIndex

def get_article_profile(self, textrank, keywordsIndex):
    &quot;&quot;&quot;
    æ–‡ç« ç”»åƒä¸»é¢˜è¯å»ºç«‹
    :param idf: æ‰€æœ‰è¯çš„idfå€¼
    :param textrank: æ¯ä¸ªæ–‡ç« çš„textrankå€¼
    :return: è¿”å›å»ºç«‹å·å¢é‡æ–‡ç« ç”»åƒ
    &quot;&quot;&quot;
    keywordsIndex = keywordsIndex.withColumnRenamed(&quot;keyword&quot;, &quot;keyword1&quot;)
    result = textrank.join(keywordsIndex, textrank.keyword == keywordsIndex.keyword1)

    # 1ã€å…³é”®è¯ï¼ˆè¯ï¼Œæƒé‡ï¼‰
    # è®¡ç®—å…³é”®è¯æƒé‡
    _articleKeywordsWeights = result.withColumn(&quot;weights&quot;, result.textrank * result.idf).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;weights&quot;])

    # åˆå¹¶å…³é”®è¯æƒé‡åˆ°å­—å…¸
    _articleKeywordsWeights.registerTempTable(&quot;temptable&quot;)
    articleKeywordsWeights = self.spark.sql(
        &quot;select article_id, min(channel_id) channel_id, collect_list(keyword) keyword_list, collect_list(weights) weights_list from temptable group by article_id&quot;)
    def _func(row):
        return row.article_id, row.channel_id, dict(zip(row.keyword_list, row.weights_list))
    articleKeywords = articleKeywordsWeights.rdd.map(_func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keywords&quot;])

    # 2ã€ä¸»é¢˜è¯
    # å°†tfidfå’Œtextrankå…±ç°çš„è¯ä½œä¸ºä¸»é¢˜è¯
    topic_sql = &quot;&quot;&quot;
            select t.article_id article_id2, collect_set(t.keyword) topics from tfidf_keywords_values t
            inner join 
            textrank_keywords_values r
            where t.keyword=r.keyword
            group by article_id2
            &quot;&quot;&quot;
    articleTopics = self.spark.sql(topic_sql)

    # 3ã€å°†ä¸»é¢˜è¯è¡¨å’Œå…³é”®è¯è¡¨è¿›è¡Œåˆå¹¶ï¼Œæ’å…¥è¡¨
    articleProfile = articleKeywords.join(articleTopics,
                                          articleKeywords.article_id == articleTopics.article_id2).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keywords&quot;, &quot;topics&quot;])
    articleProfile.write.insertInto(&quot;article_profile&quot;)

    del keywordsIndex
    del _articleKeywordsWeights
    del articleKeywords
    del articleTopics
    gc.collect()

    return articleProfile


if __name__ == '__main__':
ua = UpdateArticle()
sentence_df = ua.merge_article_data()
if sentence_df.rdd.collect():
    rank, idf = ua.generate_article_label(sentence_df)
    articleProfile = ua.get_article_profile(rank, idf)
</code></pre>
<p>ä½¿ç”¨å·¥å…·ï¼šSupervisor+Apscheduler<br>
# pip install APScheduler<br>
from apscheduler.schedulers.blocking import BlockingScheduler<br>
from apscheduler.executors.pool import ProcessPoolExecutor</p>
<pre><code>from scheduler.update import update_article_profile

# åˆ›å»ºschedulerï¼Œå¤šè¿›ç¨‹æ‰§è¡Œ
executors = {
	'default': ProcessPoolExecutor(3)
}
scheduler = BlockingScheduler(executors=executors)
# æ·»åŠ å®šæ—¶æ›´æ–°ä»»åŠ¡æ›´æ–°æ–‡ç« ç”»åƒ,æ¯éš”ä¸€å°æ—¶æ›´æ–°ï¼Œ triggerè¿˜æœ‰å…¶ä»–å®šæ—¶æ–¹å¼
scheduler.add_job(update_article_profile, trigger='interval', hours=1)
scheduler.start()
</code></pre>
<p>è‡ªå®šä¹‰Logger:<br>
import logging<br>
import logging.handlers<br>
import os</p>
<pre><code>logging_file_dir = '/root/logs/'
def create_logger():
	# ç¦»çº¿å¤„ç†æ›´æ–°æ‰“å°æ—¥å¿—
	 log_trace = logging.getLogger('offline')
     
	trace_file_handler = logging.FileHandler(
        os.path.join(logging_file_dir, 'offline.log')
    )
	 trace_file_handler.setFormatter(logging.Formatter('%(message)s'))
    
    log_trace.addHandler(trace_file_handler)
    log_trace.setLevel(logging.INFO)
</code></pre>
<p>supervisorç®¡ç†apscheduler:<br>
[program:offline]<br>
environment=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python<br>
command=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/scheduler/main.py<br>
directory=/root/toutiao_project/scheduler<br>
user=root<br>
autorestart=true<br>
redirect_stderr=true<br>
stdout_logfile=/root/logs/offlinesuper.log<br>
loglevel=info<br>
stopsignal=KILL<br>
stopasgroup=true<br>
killasgroup=true</p>
<h3 id="word2vecä¸æ–‡ç« ç›¸ä¼¼åº¦">Word2Vecä¸æ–‡ç« ç›¸ä¼¼åº¦</h3>
<pre><code>w2v.spark.sql(&quot;use article&quot;)
article = w2v.spark.sql(&quot;select * from article_data where channel_id=18 limit 2&quot;)
words_df = article.rdd.mapPartitions(segmentation).toDF(['article_id', 'channel_id', 'words'])
</code></pre>
<p>Spark Word2Vec APIä»‹ç»ï¼š<br>
æ¨¡å—ï¼šfrom pyspark.ml.feature import Word2Vec</p>
<pre><code>APIï¼šclass pyspark.ml.feature.Word2Vec(vectorSize=100, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, seed=None, inputCol=None, outputCol=None, windowSize=5, maxSentenceLength=1000)
å‚æ•°è¯´æ˜ï¼š
	vectorSize=100: è¯å‘é‡é•¿åº¦
	minCountï¼šè¿‡æ»¤æ¬¡æ•°å°äºé»˜è®¤5æ¬¡çš„è¯
	windowSize=5ï¼šè®­ç»ƒæ—¶å€™çš„çª—å£å¤§å°
	inputCol=Noneï¼šè¾“å…¥åˆ—å
	outputCol=Noneï¼šè¾“å‡ºåˆ—å
</code></pre>
<p>Spark Word2Vecè®­ç»ƒä¿å­˜æ¨¡å‹ï¼š<br>
new_word2Vec = Word2Vec(vectorSize=100, inputCol=&quot;words&quot;, outputCol=&quot;model&quot;, minCount=3)<br>
new_model = new_word2Vec.fit(words_df)<br>
new_model.save(&quot;hdfs://hadoop-master:9000/headlines/models/test.word2vec&quot;)<br>
ä¸Šä¼ å†å²æ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼š<br>
hadoop dfs -put ./word2vec_model /headlines/models/</p>
<h3 id="å¢é‡æ›´æ–°-æ–‡ç« å‘é‡è®¡ç®—">å¢é‡æ›´æ–°-æ–‡ç« å‘é‡è®¡ç®—</h3>
<p>æœ‰äº†è¯å‘é‡ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°ä¸€ç¯‡æ–‡ç« çš„å‘é‡äº†ï¼Œä¸ºäº†åé¢å¿«é€Ÿä½¿ç”¨æ–‡ç« çš„å‘é‡ï¼Œæˆ‘ä»¬ä¼šå°†æ¯ä¸ªé¢‘é“æ‰€æœ‰çš„æ–‡ç« å‘é‡ä¿å­˜èµ·æ¥ã€‚</p>
<p>ç›®çš„ï¼šä¿å­˜æ‰€æœ‰å†å²è®­ç»ƒçš„æ–‡ç« å‘é‡<br>
æ­¥éª¤ï¼š<br>
1ã€åŠ è½½æŸä¸ªé¢‘é“æ¨¡å‹ï¼Œå¾—åˆ°æ¯ä¸ªè¯çš„å‘é‡<br>
2ã€è·å–é¢‘é“çš„æ–‡ç« ç”»åƒï¼Œå¾—åˆ°æ–‡ç« ç”»åƒçš„å…³é”®è¯(æ¥ç€ä¹‹å‰å¢é‡æ›´æ–°çš„æ–‡ç« article_profile)<br>
3ã€è®¡ç®—å¾—åˆ°æ–‡ç« æ¯ä¸ªè¯çš„å‘é‡<br>
4ã€è®¡ç®—å¾—åˆ°æ–‡ç« çš„å¹³å‡è¯å‘é‡å³æ–‡ç« çš„å‘é‡<br>
åŠ è½½æŸä¸ªé¢‘é“æ¨¡å‹ï¼Œå¾—åˆ°æ¯ä¸ªè¯çš„å‘é‡<br>
from pyspark.ml.feature import Word2VecModel<br>
channel_id = 18<br>
channel = &quot;python&quot;<br>
wv_model = Word2VecModel.load(<br>
&quot;hdfs://hadoop-master:9000/headlines/models/word2vec_model/channel_%d_%s.word2vec&quot; % (channel_id, channel))<br>
vectors = wv_model.getVectors()<br>
è·å–æ–°å¢çš„æ–‡ç« ç”»åƒï¼Œå¾—åˆ°æ–‡ç« ç”»åƒçš„å…³é”®è¯ï¼š<br>
# é€‰å‡ºæ–°å¢çš„æ–‡ç« çš„ç”»åƒåšæµ‹è¯•ï¼Œä¸ŠèŠ‚è®¡ç®—çš„ç”»åƒä¸­æœ‰ä¸åŒé¢‘é“çš„ï¼Œæˆ‘ä»¬é€‰å–Pythoné¢‘é“çš„è¿›è¡Œè®¡ç®—æµ‹è¯•<br>
# æ–°å¢çš„æ–‡ç« ç”»åƒè·å–éƒ¨åˆ†<br>
profile = w2v.spark.sql(&quot;select * from article_profile where channel_id=18 limit 10&quot;)<br>
# profile = articleProfile.filter('channel_id = {}'.format(channel_id))</p>
<pre><code>profile.registerTempTable(&quot;incremental&quot;)
articleKeywordsWeights = w2v.spark.sql(
                &quot;select article_id, channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight&quot;)
_article_profile = articleKeywordsWeights.join(vectors, vectors.word==articleKeywordsWeights.keyword, &quot;inner&quot;)
</code></pre>
<p>è®¡ç®—å¾—åˆ°æ–‡ç« çš„å¹³å‡è¯å‘é‡å³æ–‡ç« çš„å‘é‡<br>
def avg(row):<br>
x = 0<br>
for v in row.vectors:<br>
x += v<br>
#  å°†å¹³å‡å‘é‡ä½œä¸ºarticleçš„å‘é‡<br>
return row.article_id, row.channel_id, x / len(row.vectors)</p>
<pre><code>articleKeywordVectors.registerTempTable(&quot;tempTable&quot;)
articleVector = w2v.spark.sql(
	&quot;select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from tempTable group by article_id&quot;).rdd \
    # åˆ†ç»„ä¹‹åï¼Œ æ±‚mapçš„å¹³å‡ä¹‹å‰ï¼Œ ç»“æœæ˜¯  artile_id, channel_id, vector_list # vector_list æ˜¯äºŒç»´æ•°ç»„ã€‚
    .map(avg).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;articleVector&quot;])
    # æ±‚mapçš„å¹³å‡ä¹‹åç»“æœæ˜¯ article_id, channel_id, article_vector # article_vector ä»£è¡¨æ–‡ç« å‘é‡
</code></pre>
<h3 id="æ–‡ç« ç›¸ä¼¼åº¦è®¡ç®—">æ–‡ç« ç›¸ä¼¼åº¦è®¡ç®—</h3>
<h4 id="å­˜åœ¨çš„é—®é¢˜ä»¥è¿›è¡ŒæŸé¢‘é“å…¨é‡æ‰€æœ‰çš„ä¸¤ä¸¤ç›¸ä¼¼åº¦è®¡ç®—-ä½†æ˜¯äº‹å®å½“æ–‡ç« é‡è¾¾åˆ°åƒä¸‡çº§åˆ«æˆ–è€…ä¸Šäº¿çº§åˆ«ç‰¹å¾ä¹Ÿä¼šä¸Šäº¿çº§åˆ«è®¡ç®—é‡å°±ä¼šå¾ˆå¤§-ä»¥ä¸‹æœ‰ä¸¤ç§ç±»å‹è§£å†³æ–¹æ¡ˆ">å­˜åœ¨çš„é—®é¢˜ï¼šä»¥è¿›è¡ŒæŸé¢‘é“å…¨é‡æ‰€æœ‰çš„ä¸¤ä¸¤ç›¸ä¼¼åº¦è®¡ç®—ã€‚ä½†æ˜¯äº‹å®å½“æ–‡ç« é‡è¾¾åˆ°åƒä¸‡çº§åˆ«æˆ–è€…ä¸Šäº¿çº§åˆ«ï¼Œç‰¹å¾ä¹Ÿä¼šä¸Šäº¿çº§åˆ«ï¼Œè®¡ç®—é‡å°±ä¼šå¾ˆå¤§ã€‚ä»¥ä¸‹æœ‰ä¸¤ç§ç±»å‹è§£å†³æ–¹æ¡ˆï¼š</h4>
<ol>
<li>æ¯ä¸ªé¢‘é“çš„æ–‡ç« å…ˆè¿›è¡Œèšç±»ï¼ˆç¼ºç‚¹ï¼Œï¼ˆåˆ†æˆå‡ ä¸ªç°‡ï¼‰ä¹Ÿæ˜¯ä¸ªè¶…å‚æ•°ï¼‰</li>
<li>å±€éƒ¨æ•æ„Ÿå“ˆå¸ŒLSH(Locality Sensitive Hashing)<br>
åŸºæœ¬æ€æƒ³1ï¼šLSHç®—æ³•åŸºäºä¸€ä¸ªå‡è®¾ï¼Œå¦‚æœä¸¤ä¸ªæ–‡æœ¬åœ¨åŸæœ‰çš„æ•°æ®ç©ºé—´æ˜¯ç›¸ä¼¼çš„ï¼Œé‚£ä¹ˆåˆ†åˆ«ç»è¿‡å“ˆå¸Œå‡½æ•°è½¬æ¢ä»¥åçš„å®ƒä»¬ä¹Ÿå…·æœ‰å¾ˆé«˜çš„ç›¸ä¼¼åº¦<br>
åŸºæœ¬æ€æƒ³2: ç»å¸¸ä½¿ç”¨çš„å“ˆå¸Œå‡½æ•°ï¼Œå†²çªæ€»æ˜¯éš¾ä»¥é¿å…ã€‚LSHå´ä¾èµ–äºå†²çªï¼Œåœ¨è§£å†³NNS(Nearest neighbor search )æ—¶ï¼Œæˆ‘ä»¬æœŸæœ›ï¼š<br>
ç¦»å¾—è¶Šè¿‘çš„å¯¹è±¡ï¼Œå‘ç”Ÿå†²çªçš„æ¦‚ç‡è¶Šé«˜<br>
ç¦»å¾—è¶Šè¿œçš„å¯¹è±¡ï¼Œå‘ç”Ÿå†²çªçš„æ¦‚ç‡è¶Šä½</li>
</ol>
<h4 id="å±€éƒ¨æ•æ„Ÿå“ˆå¸Œlshlocality-sensitive-hashing-lshè¿‡ç¨‹">å±€éƒ¨æ•æ„Ÿå“ˆå¸ŒLSH(Locality Sensitive Hashing) LSHè¿‡ç¨‹ï¼š</h4>
<p>mini hashing(ç•¥)	<br>
Random Projectionï¼ˆç‰¹å¾å‹ç¼©ï¼‰ï¼š<br>
Random Projectionæ˜¯ä¸€ç§éšæœºç®—æ³•.éšæœºæŠ•å½±çš„ç®—æ³•æœ‰å¾ˆå¤šï¼Œå¦‚PCAã€Gaussian random projection - é«˜æ–¯éšæœºæŠ•å½±ã€‚<br>
éšæœºæ¡¶æŠ•å½±æ˜¯ç”¨äºæ¬§å‡ é‡Œå¾·è·ç¦»çš„ LSH familyã€‚å…¶LSH familyå°†xç‰¹å¾å‘é‡æ˜ å°„åˆ°éšæœºå•ä½çŸ¢é‡vï¼Œå¹¶å°†æ˜ å°„ç»“æœåˆ†ä¸ºå“ˆå¸Œæ¡¶ä¸­ã€‚å“ˆå¸Œè¡¨ä¸­çš„æ¯ä¸ªä½ç½®è¡¨ç¤ºä¸€ä¸ªå“ˆå¸Œæ¡¶ã€‚	<br>
ä½¿å¾—ï¼š<br>
ç¦»å¾—è¶Šè¿‘çš„å¯¹è±¡ï¼Œå‘ç”Ÿå†²çªçš„æ¦‚ç‡è¶Šé«˜<br>
ç¦»å¾—è¶Šè¿œçš„å¯¹è±¡ï¼Œå‘ç”Ÿå†²çªçš„æ¦‚ç‡è¶Šä½</p>
<h4 id="ä»£ç å®ç°">ä»£ç å®ç°ï¼š</h4>
<p>è¯»å–æ•°æ®ï¼Œè¿›è¡Œç±»å‹å¤„ç†(æ•°ç»„è½¬æ¢ç±»å‹ä¸ºVector)ï¼š<br>
from pyspark.ml.linalg import Vectors<br>
# é€‰å–éƒ¨åˆ†æ•°æ®åšæµ‹è¯•<br>
article_vector = w2v.spark.sql(&quot;select article_id, articlevector from article_vector where channel_id=18 limit 10&quot;)<br>
train = articlevector.select(['article_id', 'articleVector'])</p>
<pre><code>def _array_to_vector(row):
	return row.article_id, Vectors.dense(row.articleVector)

train = train.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])
</code></pre>
<p>ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆBRPè¿›è¡ŒFITï¼‰ï¼š<br>
å‡½æ•°å‚æ•°è¯´æ˜ï¼š<br>
class pyspark.ml.feature.BucketedRandomProjectionLSH(inputCol=None, outputCol=None, seed=None, numHashTables=1, bucketLength=None)<br>
inputCol=Noneï¼šè¾“å…¥ç‰¹å¾åˆ—<br>
outputCol=Noneï¼šè¾“å‡ºç‰¹å¾åˆ—<br>
numHashTables=1ï¼šå“ˆå¸Œè¡¨æ•°é‡ï¼Œå‡ ä¸ªhash functionå¯¹æ•°æ®è¿›è¡Œhashæ“ä½œ<br>
bucketLength=Noneï¼šæ¡¶çš„æ•°é‡ï¼Œå€¼è¶Šå¤§ç›¸åŒæ•°æ®è¿›å…¥åˆ°åŒä¸€ä¸ªæ¡¶çš„æ¦‚ç‡è¶Šé«˜<br>
method:<br>
# è®¡ç®—df1æ¯ä¸ªæ–‡ç« ç›¸ä¼¼çš„df2æ•°æ®é›†çš„æ•°æ®<br>
approxSimilarityJoin(df1, df2, 2.0, distCol='EuclideanDistance')  # è½¬ä¸ºå‘é‡</p>
<pre><code># ä»£ç è°ƒç”¨ï¼š
from pyspark.ml.feature import BucketedRandomProjectionLSH

brp = BucketedRandomProjectionLSH(inputCol='articleVector', outputCol='hashes', numHashTables=4.0, bucketLength=10.0)
model = brp.fit(æ—§æ–‡ç« å‘é‡)
</code></pre>
<p>è®¡ç®—ç›¸ä¼¼çš„æ–‡ç« ä»¥åŠç›¸ä¼¼åº¦<br>
# è®¡ç®—æ–‡ç« å’Œæ–‡ç« ä¹‹é—´çš„ç›¸ä¼¼åº¦<br>
similar = model.approxSimilarityJoin(æ–°å¢æ–‡ç« å‘é‡, æ–°å¢æ–‡ç« å‘é‡, 2.0, distCol='Similarity')  # è¾“å‡ºåˆ—å<br>
similar.sort(['EuclideanDistance']).show()<br>
è®¡ç®—ç»“æœï¼š<br>
datasetA(æ–°å¢),    datasetBï¼ˆæ—§çš„ï¼‰,     Similarity<br>
[2,[æ–‡ç« å‘é‡]]		[5,[æ–‡ç« å‘é‡]]			0.0051<br>
[1,[æ–‡ç« å‘é‡]]		[3,[æ–‡ç« å‘é‡]]			0.0054<br>
[2,[æ–‡ç« å‘é‡]]		[8,[æ–‡ç« å‘é‡]]			0.0053<br>
[1,[æ–‡ç« å‘é‡]]		[4,[æ–‡ç« å‘é‡]]			0.0052<br>
[2,[æ–‡ç« å‘é‡]]		[7,[æ–‡ç« å‘é‡]]			0.0055<br>
...</p>
<h3 id="æ–‡ç« ç›¸ä¼¼åº¦å­˜å‚¨-hbase">æ–‡ç« ç›¸ä¼¼åº¦å­˜å‚¨ HBase</h3>
<h4 id="å­˜å‚¨ç›®æ ‡å­˜å‚¨-æ–‡ç« ç›¸ä¼¼æ–‡ç« -ç›¸ä¼¼åº¦">å­˜å‚¨ç›®æ ‡ï¼šå­˜å‚¨ æ–‡ç« ï¼Œç›¸ä¼¼æ–‡ç« ï¼Œ ç›¸ä¼¼åº¦</h4>
<p>è°ƒç”¨foreachPartitionï¼š<br>
foreachPartitionä¸åŒäºmapå’ŒmapPartitionã€‚<br>
æ— è¿”å›ç»“æœï¼Œä¸»è¦ç”¨äºç¦»çº¿åˆ†æä¹‹åçš„æ•°æ®ï¼ˆæ•°æ®åº“å­˜å‚¨ç­‰ï¼‰è½åœ°<br>
å¦‚æœæƒ³è¦è¿”å›æ–°çš„ä¸€ä¸ªæ•°æ®DFï¼Œå°±ä½¿ç”¨mapåè€…ã€‚<br>
æˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä¸ªHBaseå­˜å‚¨æ–‡ç« ç›¸ä¼¼åº¦çš„è¡¨ï¼š<br>
create 'article_similar', 'similar'</p>
<pre><code># å­˜å‚¨æ ¼å¼å¦‚ä¸‹ï¼š
		 è¡¨           row_key  column_family   value
	put 'article_similar', '1', 	'similar:1',    0.2
	put 'article_similar', '1', 	'similar:2',    0.34
</code></pre>
<p>HBase å¼€å¯å¤±è´¥å¯èƒ½çš„åŸå› çš„ï¼š</p>
<ol>
<li>
<p>æ—¶é—´æœªåŒæ­¥çš„è§£å†³åŠæ³•ï¼š<br>
ntpdate 0.cn.pool.ntp.org<br>
æˆ–<br>
ntpdate ntp1.aliyun.com</p>
</li>
<li>
<p>thriftæœåŠ¡æœªå¼€å¯çš„è§£å†³åŠæ³•ï¼š<br>
hbase-daemon.sh start thrift<br>
happybaseä»£ç å®ç°ï¼š<br>
def save_hbase(partition):<br>
import happybase<br>
pool = happybase.ConnectionPool(size=3, host='hadoop-master')</p>
<p>with pool.connection() as conn:<br>
# å»ºè®®è¡¨çš„è¿æ¥<br>
table = conn.table('article_similar')<br>
for row in partition:<br>
if row.datasetA.article_id == row.datasetB.article_id:<br>
pass<br>
else:<br>
table.put(str(row.datasetA.article_id).encode(),<br>
{&quot;similar:{}&quot;.format(row.datasetB.article_id).encode(): b'%0.4f' % (row.EuclideanDistance)})<br>
# æ‰‹åŠ¨å…³é—­æ‰€æœ‰çš„è¿æ¥<br>
conn.close()</p>
<p>similar.foreachPartition(save_hbase)</p>
</li>
</ol>
<h3 id="æ–‡ç« ç›¸ä¼¼åº¦å¢é‡æ›´æ–°ä»£ç æ•´ç†">æ–‡ç« ç›¸ä¼¼åº¦å¢é‡æ›´æ–°ä»£ç æ•´ç†</h3>
<pre><code>def compute_article_similar(self, articleProfile):
    &quot;&quot;&quot;
    è®¡ç®—å¢é‡æ–‡ç« ä¸å†å²æ–‡ç« çš„ç›¸ä¼¼åº¦ word2vec
    :return:
    &quot;&quot;&quot;
    # å¾—åˆ°è¦æ›´æ–°çš„æ–°æ–‡ç« é€šé“ç±»åˆ«(ä¸é‡‡ç”¨)
    # all_channel = set(articleProfile.rdd.map(lambda x: x.channel_id).collect())
    def avg(row):
        x = 0
        for v in row.vectors:
            x += v
        #  å°†å¹³å‡å‘é‡ä½œä¸ºarticleçš„å‘é‡
        return row.article_id, row.channel_id, x / len(row.vectors)

    for channel_id, channel_name in CHANNEL_INFO.items():

        profile = articleProfile.filter('channel_id = {}'.format(channel_id))
        wv_model = Word2VecModel.load(
            &quot;hdfs://hadoop-master:9000/headlines/models/channel_%d_%s.word2vec&quot; % (channel_id, channel_name))
        vectors = wv_model.getVectors()

        # è®¡ç®—å‘é‡
        profile.registerTempTable(&quot;incremental&quot;)
        articleKeywordsWeights = ua.spark.sql(
            &quot;select article_id, channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight where channel_id=%d&quot; % channel_id)

        articleKeywordsWeightsAndVectors = articleKeywordsWeights.join(vectors,
                                                        vectors.word == articleKeywordsWeights.keyword, &quot;inner&quot;)
        articleKeywordVectors = articleKeywordsWeightsAndVectors.rdd.map(
            lambda r: (r.article_id, r.channel_id, r.keyword, r.weight * r.vector)).toDF(
            [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;weightingVector&quot;])

        articleKeywordVectors.registerTempTable(&quot;tempTable&quot;)
        articleVector = self.spark.sql(
            &quot;select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from tempTable group by article_id&quot;).rdd.map(
            avg).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;articleVector&quot;])

        # å†™å…¥æ•°æ®åº“
        def toArray(row):
            return row.article_id, row.channel_id, [float(i) for i in row.articleVector.toArray()]
        articleVector = articleVector.rdd.map(toArray).toDF(['article_id', 'channel_id', 'articleVector'])
        articleVector.write.insertInto(&quot;article_vector&quot;)

        import gc
        del wv_model
        del vectors
        del articleKeywordsWeights
        del articleKeywordsWeightsAndVectors
        del articleKeywordVectors
        gc.collect()

        # å¾—åˆ°å†å²æ•°æ®, è½¬æ¢æˆå›ºå®šæ ¼å¼ä½¿ç”¨LSHè¿›è¡Œæ±‚ç›¸ä¼¼
        train = self.spark.sql(&quot;select * from article_vector where channel_id=%d&quot; % channel_id)

        def _array_to_vector(row):
            return row.article_id, Vectors.dense(row.articleVector)
        train = train.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])
        test = articleVector.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])

        brp = BucketedRandomProjectionLSH(inputCol='articleVector', outputCol='hashes', seed=12345,
                                          bucketLength=1.0)
        model = brp.fit(train)
        similar = model.approxSimilarityJoin(test, train, 2.0, distCol='EuclideanDistance')

        def save_hbase(partition):
            import happybase
            for row in partition:
                pool = happybase.ConnectionPool(size=3, host='hadoop-master')
                # article_similar article_id similar:article_id sim
                with pool.connection() as conn:
                    table = connection.table(&quot;article_similar&quot;)
                    for row in partition:
                        if row.datasetA.article_id == row.datasetB.article_id:
                            pass
                        else:
                            table.put(str(row.datasetA.article_id).encode(),
                                      {b&quot;similar:%d&quot; % row.datasetB.article_id: b&quot;%0.4f&quot; % row.EuclideanDistance})
                    conn.close()
        similar.foreachPartition(save_hbase)
</code></pre>
<p>æ·»åŠ å‡½æ•°åˆ°ä¸»å‡½æ•°ä¸­æ–‡ä»¶ä¸­ï¼Œä¿®æ”¹updateæ›´æ–°ä»£ç ï¼š<br>
ua = UpdateArticle()<br>
sentence_df = ua.merge_article_data()<br>
if sentence_df.rdd.collect():<br>
rank, idf = ua.generate_article_label(sentence_df)<br>
articleProfile = ua.get_article_profile(rank, idf)<br>
ua.compute_article_similar(articleProfile)</p>
<h1 id="ç”¨æˆ·ç”»åƒæ„å»ºä¸æ›´æ–°">ç”¨æˆ·ç”»åƒæ„å»ºä¸æ›´æ–°</h1>
<h3 id="ç»„æˆæˆåˆ†">ç»„æˆæˆåˆ†</h3>
<p>ç”¨æˆ·åŸºæœ¬ä¿¡æ¯+ç”¨æˆ·è¡Œä¸º(å†å²+æ–°å¢)<br>
ç”¨æˆ·è¡Œä¸ºåŒ…æ‹¬ï¼š<br>
hive&gt; select * from user_action limit 1;<br>
OK<br>
2019-03-05 10:19:40		0		{&quot;action&quot;:&quot;exposure&quot;,&quot;userId&quot;:&quot;2&quot;,&quot;articleId&quot;:&quot;[16000, 44371, 16421, 16181, 17454]&quot;,&quot;algorithmCombine&quot;:&quot;C2&quot;} 2019-03-05<br>
æˆ‘ä»¬éœ€è¦å¯¹ç”¨æˆ·è¡Œä¸ºï¼ˆå­—å…¸ï¼‰æ•°æ®æ ¼å¼å¹³é“ºå¤„ç†<br>
user_id	action_time	article_id	share	click	collected	exposure	read_time</p>
<h3 id="æ­¥éª¤">æ­¥éª¤ï¼š</h3>
<p>1ã€åˆ›å»ºHIVEåŸºæœ¬æ•°æ®è¡¨<br>
2ã€è¯»å–å›ºå®šæ—¶é—´å†…çš„ç”¨æˆ·è¡Œä¸ºæ—¥å¿—<br>
3ã€è¿›è¡Œç”¨æˆ·æ—¥å¿—æ•°æ®å¤„ç†<br>
4ã€å­˜å‚¨åˆ°user_article_basicè¡¨ä¸­</p>
<h4 id="åˆ›å»ºhiveåŸºæœ¬æ•°æ®è¡¨">åˆ›å»ºHIVEåŸºæœ¬æ•°æ®è¡¨</h4>
<pre><code>create table user_article_basic(
user_id BIGINT comment &quot;userID&quot;,
action_time STRING comment &quot;user actions time&quot;,
article_id BIGINT comment &quot;articleid&quot;,
channel_id INT comment &quot;channel_id&quot;,
shared BOOLEAN comment &quot;is shared&quot;,
clicked BOOLEAN comment &quot;is clicked&quot;,
collected BOOLEAN comment &quot;is collected&quot;,
exposure BOOLEAN comment &quot;is exposured&quot;,
read_time STRING comment &quot;reading time&quot;)
COMMENT &quot;user_article_basic&quot;
CLUSTERED by (user_id) into 2 buckets
STORED as textfile
LOCATION '/user/hive/warehouse/profile.db/user_article_basic';
</code></pre>
<h4 id="è¯»å–å¢é‡ç”¨æˆ·è¡Œä¸ºæ•°æ®-å›ºå®šæ—¶é—´å†…çš„ç”¨æˆ·è¡Œä¸ºæ—¥å¿—">è¯»å–å¢é‡ç”¨æˆ·è¡Œä¸ºæ•°æ®-å›ºå®šæ—¶é—´å†…çš„ç”¨æˆ·è¡Œä¸ºæ—¥å¿—</h4>
<p>å…³è”å†å²æ—¥æœŸæ–‡ä»¶<br>
# åœ¨è¿›è¡Œæ—¥å¿—ä¿¡æ¯çš„å¤„ç†ä¹‹å‰ï¼Œå…ˆå°†æˆ‘ä»¬ä¹‹å‰å»ºç«‹çš„user_actionè¡¨ä¹‹é—´è¿›è¡Œæ‰€æœ‰æ—¥æœŸå…³è”ï¼Œspark hiveä¸ä¼šè‡ªåŠ¨å…³è”<br>
import pandas as pd<br>
from datetime import datetime</p>
<pre><code>def datelist(beginDate, endDate):
	date_list=[datetime.strftime(x,'%Y-%m-%d') for x in list(pd.date_range(start=beginDate, end=endDate))]
	return date_list

dl = datelist(&quot;2019-03-05&quot;, time.strftime(&quot;%Y-%m-%d&quot;, time.localtime()))

fs = pyhdfs.HdfsClient(hosts='hadoop-master:50070')
for d in dl:
	try:
		_localions = '/user/hive/warehouse/profile.db/user_action/' + d
		if fs.exists(_localions):
			uup.spark.sql(&quot;alter table user_action add partition (dt='%s') location '%s'&quot; % (d, _localions))
	except Exception as e:
		# å·²ç»å…³è”è¿‡çš„å¼‚å¸¸å¿½ç•¥,partitionä¸hdfsæ–‡ä»¶ä¸ç›´æ¥å…³è”
		pass
sqlDF = uup.spark.sql(
	&quot;select actionTime, readTime, channelId, param.articleId, param.algorithmCombine, param.action, param.userId from user_action where dt={}&quot;.format(time_str)
)
</code></pre>
<h4 id="åŸå§‹æ•°æ®æ ¼å¼ä¸ç›®æ ‡æ•°æ®æ ¼å¼">åŸå§‹æ•°æ®æ ¼å¼ä¸ç›®æ ‡æ•°æ®æ ¼å¼</h4>
<p>åŸå§‹æ•°æ®æ ¼å¼ï¼šï¼ˆè¡Œä¸ºå‚æ•°éƒ½ç®—åœ¨ actionåˆ—å†…ï¼‰<br>
actionTime	readTime	channelID	articleId	ç®—æ³•åç§°	action		userId<br>
123						1						exposure	1<br>
321						1						click		1<br>
ç›®æ ‡æ•°æ®æ ¼å¼ï¼ˆè¡Œä¸ºå‚æ•°1æ‹†4ï¼Œactionåˆ—è¢«çˆ†ç‚¸ä¸º4åˆ—ï¼Œå‡ä¸ºboolç±»å‹ï¼‰<br>
user_id	action_time	article_id	shared	clicked	collected	expore	read_time<br>
1					1			false	false	false		true<br>
1					2			false	true	false		true</p>
<h4 id="è¿›è¡Œç”¨æˆ·æ—¥å¿—æ•°æ®æ ¼å¼å¤„ç†">è¿›è¡Œç”¨æˆ·æ—¥å¿—æ•°æ®æ ¼å¼å¤„ç†</h4>
<pre><code>if sqlDF.collect():
def _compute(row):
    # è¿›è¡Œåˆ¤æ–­è¡Œä¸ºç±»å‹
    _list = []
    if row.action == &quot;exposure&quot;:
        for article_id in eval(row.articleId):
            _list.append(
                [row.userId, row.actionTime, article_id, row.channelId, False, False, False, True, row.readTime])
        return _list
    else:
        class Temp(object):
            shared = False
            clicked = False
            collected = False
            read_time = &quot;&quot;

        _tp = Temp()
        if row.action == &quot;share&quot;:
            _tp.shared = True
        elif row.action == &quot;click&quot;:
            _tp.clicked = True
        elif row.action == &quot;collect&quot;:
            _tp.collected = True
        elif row.action == &quot;read&quot;:
            _tp.clicked = True
        else:
            pass
        _list.append(
            [row.userId, row.actionTime, int(row.articleId), row.channelId, _tp.shared, _tp.clicked, _tp.collected,
             True,
             row.readTime])
        return _list
# è¿›è¡Œå¤„ç†
# æŸ¥è¯¢å†…å®¹ï¼Œå°†åŸå§‹æ—¥å¿—è¡¨æ•°æ®è¿›è¡Œå¤„ç†
_res = sqlDF.rdd.flatMap(_compute)
data = _res.toDF([&quot;user_id&quot;, &quot;action_time&quot;,&quot;article_id&quot;, &quot;channel_id&quot;, &quot;shared&quot;, &quot;clicked&quot;, &quot;collected&quot;, &quot;exposure&quot;, &quot;read_time&quot;])
</code></pre>
<h4 id="å°†ä¸Šè¿°ç›®æ ‡æ ¼å¼çš„æ•°æ®æŒ‰ç…§-userid-å’Œ-articleid-åˆ†ç»„">å°†ä¸Šè¿°ç›®æ ‡æ ¼å¼çš„æ•°æ®æŒ‰ç…§ userid å’Œ articleid åˆ†ç»„</h4>
<p>å…ˆåˆå¹¶å†å²æ•°æ®ï¼Œå­˜å‚¨åˆ°user-article-basicè¡¨ä¸­<br>
# åˆå¹¶å†å²æ•°æ®ï¼Œæ’å…¥è¡¨ä¸­<br>
old = uup.spark.sql(&quot;select * from user_article_basic&quot;)<br>
# ç”±äºåˆå¹¶çš„ç»“æœä¸­ä¸æ˜¯å¯¹äºuser_idå’Œarticle_idå”¯ä¸€çš„ï¼Œä¸€ä¸ªç”¨æˆ·ä¼šå¯¹æ–‡ç« å¤šç§æ“ä½œ<br>
new_old = old.unionAll(data)<br>
HIVEç›®å‰æ”¯æŒhiveç»ˆç«¯æ“ä½œACIDï¼Œä¸æ”¯æŒpythonçš„pysparkåŸå­æ€§æ“ä½œï¼Œå¹¶ä¸”å¼€å¯é…ç½®ä¸­å¼€å¯åŸå­æ€§ç›¸å…³é…ç½®ä¹Ÿä¸è¡Œã€‚<br>
new_old.registerTempTable(&quot;temptable&quot;)<br>
# æŒ‰ç…§ç”¨æˆ·ï¼Œæ–‡ç« åˆ†ç»„å­˜æ”¾è¿›å»<br>
uup.spark.sql(<br>
&quot;insert overwrite table user_article_basic select user_id, max(action_time) as action_time, &quot;<br>
&quot;article_id, max(channel_id) as channel_id, max(shared) as shared, max(clicked) as clicked, &quot;<br>
&quot;max(collected) as collected, max(exposure) as exposure, max(read_time) as read_time from temptable &quot;<br>
&quot;group by user_id, article_id&quot;)</p>
<h3 id="ç”¨æˆ·ç”»åƒæ ‡ç­¾æƒé‡è®¡ç®—">ç”¨æˆ·ç”»åƒæ ‡ç­¾æƒé‡è®¡ç®—</h3>
<h4 id="å¦‚ä½•å­˜å‚¨">å¦‚ä½•å­˜å‚¨</h4>
<p>ç”¨æˆ·ç”»åƒï¼Œä½œä¸ºç‰¹å¾æä¾›ç»™ä¸€äº›ç®—æ³•æ’åºï¼Œæ–¹ä¾¿ä¸å¿«é€Ÿè¯»å–ä½¿ç”¨<br>
é€‰æ‹©å­˜å‚¨åœ¨Hbaseå½“ä¸­ã€‚<br>
ç„¶åç”¨ Hive å¤–è¡¨å…³è” hbase<br>
å¦‚æœç¦»çº¿åˆ†æä¹Ÿæƒ³è¦ä½¿ç”¨æˆ‘ä»¬å¯ä»¥å»ºç«‹HIVEåˆ°Hbaseçš„å¤–éƒ¨è¡¨ã€‚</p>
<h4 id="hbaseè¡¨è®¾è®¡">HBaseè¡¨è®¾è®¡</h4>
<pre><code>		table_name		column1 column2  column3 
create 'user_profile', 'basic','partial','env'

					row_key   column_family					value
put 'user_profile', 'user:2', 'partial:{channel_id}:{topic}': weights
put 'user_profile', 'user:2', 'basic:{info}': value
put 'user_profile', 'user:2', 'env:{info}': value
</code></pre>
<h4 id="hiveè¡¨è®¾è®¡">Hiveè¡¨è®¾è®¡</h4>
<pre><code>create external table user_profile_hbase(
user_id STRING comment &quot;userID&quot;,
information map&lt;string, DOUBLE&gt; comment &quot;user basic information&quot;,
article_partial map&lt;string, DOUBLE&gt; comment &quot;article partial&quot;,
env map&lt;string, INT&gt; comment &quot;user env&quot;)
COMMENT &quot;user profile table&quot;
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,basic:,partial:,env:&quot;)
TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;user_profile&quot;);
</code></pre>
<h4 id="spark-sqlå…³è”è¡¨è¯»å–é—®é¢˜">Spark SQLå…³è”è¡¨è¯»å–é—®é¢˜</h4>
<p>åˆ›å»ºå…³è”è¡¨ä¹‹åï¼Œç¦»çº¿è¯»å–è¡¨å†…å®¹éœ€è¦ä¸€äº›ä¾èµ–åŒ…ã€‚è§£å†³åŠæ³•ï¼š<br>
æ‹·è´/root/bigdata/hbase/lib/ä¸‹é¢hbase-<em>.jar åˆ° /root/bigdata/spark/jars/ç›®å½•ä¸‹<br>
æ‹·è´/root/bigdata/hive/lib/h</em>.jar åˆ° /root/bigdata/spark/jars/ç›®å½•ä¸‹<br>
ä¸Šè¿°æ“ä½œä¸‰å°è™šæ‹Ÿæœºéƒ½æ‰§è¡Œä¸€éã€‚</p>
<h4 id="ç”¨æˆ·ç”»åƒé¢‘é“å…³é”®è¯è·å–ä¸æƒé‡è®¡ç®—">ç”¨æˆ·ç”»åƒé¢‘é“å…³é”®è¯è·å–ä¸æƒé‡è®¡ç®—</h4>
<p>ç›®æ ‡ï¼šè·å–ç”¨æˆ·1~25é¢‘é“(ä¸åŒ…æ‹¬æ¨èé¢‘é“)çš„å…³é”®è¯ï¼Œå¹¶è®¡ç®—æƒé‡<br>
1ã€è¯»å–user-article-basicè¡¨ï¼Œåˆå¹¶è¡Œä¸ºè¡¨ä¸æ–‡ç« ç”»åƒä¸­çš„ä¸»é¢˜è¯<br>
2ã€è¿›è¡Œç”¨æˆ·æƒé‡è®¡ç®—å…¬å¼ã€åŒæ—¶è½åœ°å­˜å‚¨<br>
# è·å–åŸºæœ¬ç”¨æˆ·è¡Œä¸ºä¿¡æ¯ï¼Œç„¶åè¿›è¡Œæ–‡ç« ç”»åƒçš„ä¸»é¢˜è¯åˆå¹¶<br>
uup.spark.sql(&quot;use profile&quot;)<br>
# å–å‡ºæ—¥å¿—ä¸­çš„channel_id<br>
user_article_ = uup.spark.sql(&quot;select * from user_article_basic&quot;).drop('channel_id')<br>
uup.spark.sql('use article')<br>
article_label = uup.spark.sql(&quot;select article_id, channel_id, topics from article_profile&quot;)<br>
# åˆå¹¶ä½¿ç”¨æ–‡ç« ä¸­æ­£ç¡®çš„channel_id<br>
click_article_res = user_article_.join(article_label, how='left', on=['article_id'])<br>
å°†å…³é”®è¯å­—æ®µçš„åˆ—è¡¨çˆ†ç‚¸<br>
import pyspark.sql.functions as F<br>
click_article_res = click_article_res.withColumn('topic', F.explode('topics')).drop('topics')<br>
çˆ†ç‚¸åæ ¼å¼å¦‚ä¸‹ï¼š<br>
user_id article_id topic ...<br>
1     1			 python<br>
1     1			 golang<br>
1     1			 linux<br>
...    ...		  ...</p>
<h4 id="ç”¨æˆ·ç”»åƒä¹‹æ ‡ç­¾æƒé‡ç®—æ³•">ç”¨æˆ·ç”»åƒä¹‹æ ‡ç­¾æƒé‡ç®—æ³•</h4>
<p>ç”¨æˆ·æ ‡ç­¾æƒé‡ =( è¡Œä¸ºç±»å‹æƒé‡ä¹‹å’Œ) Ã— æ—¶é—´è¡°å‡<br>
è¡Œä¸ºç±»å‹æƒé‡ çš„ åˆ†å€¼ çš„ç¡®å®šéœ€è¦æ•´ä½“åå•†<br>
è¡Œä¸º					åˆ†å€¼<br>
é˜…è¯»æ—¶é—´&lt;1000ms		  1<br>
é˜…è¯»æ—¶é—´&gt;=1000ms	  2<br>
æ”¶è—					2<br>
åˆ†äº«					3<br>
ç‚¹å‡»					5<br>
æ—¶é—´è¡°å‡: 1/(log(t)+1) ,tä¸ºæ—¶é—´å‘ç”Ÿæ—¶é—´è·ç¦»å½“å‰æ—¶é—´çš„å¤§å°ã€‚<br>
# è®¡ç®—æ¯ä¸ªç”¨æˆ·å¯¹æ¯ç¯‡æ–‡ç« çš„æ ‡ç­¾çš„æƒé‡<br>
def compute_weights(rowpartition):<br>
&quot;&quot;&quot;å¤„ç†æ¯ä¸ªç”¨æˆ·å¯¹æ–‡ç« çš„ç‚¹å‡»æ•°æ®<br>
&quot;&quot;&quot;<br>
weightsOfaction = {<br>
&quot;read_min&quot;: 1,<br>
&quot;read_middle&quot;: 2,<br>
&quot;collect&quot;: 2,<br>
&quot;share&quot;: 3,<br>
&quot;click&quot;: 5<br>
}</p>
<pre><code>import happybase
from datetime import datetime
import numpy as np
#  ç”¨äºè¯»å–hbaseç¼“å­˜ç»“æœé…ç½®
pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090)

# è¯»å–æ–‡ç« çš„æ ‡ç­¾æ•°æ®
# è®¡ç®—æƒé‡å€¼
# æ—¶é—´é—´éš”
for row in rowpartition:
    t = datetime.now() - datetime.strptime(row.action_time, '%Y-%m-%d %H:%M:%S')
    # æ—¶é—´è¡°å‡ç³»æ•°
    time_exp = 1 / (np.log(t.days + 1) + 1)

    if row.read_time == '':
        r_t = 0
    else:
        r_t = int(row.read_time)
    # æµè§ˆæ—¶é—´åˆ†æ•°
    is_read = weightsOfaction['read_middle'] if r_t &gt; 1000 else weightsOfaction['read_min']

    # æ¯ä¸ªè¯çš„æƒé‡åˆ†æ•°
    weigths = time_exp * (
                row.shared * weightsOfaction['share'] + row.collected * weightsOfaction['collect'] + row.
                clicked * weightsOfaction['click'] + is_read)

#        with pool.connection() as conn:
#            table = conn.table('user_profile')
#            table.put('user:{}'.format(row.user_id).encode(),
#                      {'partial:{}:{}'.format(row.channel_id, row.topic).encode(): json.dumps(
#                          weigths).encode()})
#            conn.close()

click_article_res.foreachPartition(compute_weights)
</code></pre>
<p>è½åœ°Hbaseä¸­ä¹‹åï¼Œåœ¨HBASEä¸­æŸ¥è¯¢ï¼Œhappybaseæˆ–è€…hbaseç»ˆç«¯<br>
import happybase<br>
# ç”¨äºè¯»å–hbaseç¼“å­˜ç»“æœé…ç½®<br>
pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090)</p>
<pre><code>with pool.connection() as conn:
	table = conn.table('user_profile')
	# è·å–æ¯ä¸ªé”® å¯¹åº”çš„æ‰€æœ‰åˆ—çš„ç»“æœ
	data = table.row(b'user:2', columns=[b'partial'])
	conn.close()

# ç­‰ä»·äº  hbase(main):015:0&gt; get 'user_profile', 'user:2'
</code></pre>
<h3 id="åŸºç¡€ä¿¡æ¯ç”»åƒæ›´æ–°">åŸºç¡€ä¿¡æ¯ç”»åƒæ›´æ–°</h3>
<pre><code>def update_user_info(self):
    &quot;&quot;&quot;
    æ›´æ–°ç”¨æˆ·çš„åŸºç¡€ä¿¡æ¯ç”»åƒ
    :return:
    &quot;&quot;&quot;
    self.spark.sql(&quot;use toutiao&quot;)

    user_basic = self.spark.sql(&quot;select user_id, gender, birthday from user_profile&quot;)

    # æ›´æ–°ç”¨æˆ·åŸºç¡€ä¿¡æ¯
    def _udapte_user_basic(partition):
        &quot;&quot;&quot;æ›´æ–°ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
        &quot;&quot;&quot;
        import happybase
        #  ç”¨äºè¯»å–hbaseç¼“å­˜ç»“æœé…ç½®
        pool = happybase.ConnectionPool(size=10, host='172.17.0.134', port=9090)
        for row in partition:

            from datetime import date
            age = 0
            if row.birthday != 'null':
                born = datetime.strptime(row.birthday, '%Y-%m-%d')
                today = date.today()
                age = today.year - born.year - ((today.month, today.day) &lt; (born.month, born.day))

            with pool.connection() as conn:
                table = conn.table('user_profile')
                table.put('user:{}'.format(row.user_id).encode(),
                          {'basic:gender'.encode(): json.dumps(row.gender).encode()})
                table.put('user:{}'.format(row.user_id).encode(),
                          {'basic:birthday'.encode(): json.dumps(age).encode()})
                conn.close()

    user_basic.foreachPartition(_udapte_user_basic)
    logger.info(
        &quot;{} INFO completely update infomation of basic&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
        
# hbase(main):016:0&gt; get 'user_profile', 'user:2'
</code></pre>
<h3 id="ç”¨æˆ·ç”»åƒå¢é‡æ›´æ–°å®šæ—¶å¼€å¯">ç”¨æˆ·ç”»åƒå¢é‡æ›´æ–°å®šæ—¶å¼€å¯:</h3>
<p>ç”¨æˆ·ç”»åƒå¢é‡æ›´æ–°ä»£ç æ•´ç†<br>
æ·»åŠ å®šæ—¶ä»»åŠ¡ä»¥åŠè¿›ç¨‹ç®¡ç†<br>
from offline.update_user import UpdateUserProfile</p>
<pre><code>def update_user_profile():
	&quot;&quot;&quot;
	æ›´æ–°ç”¨æˆ·ç”»åƒ
	&quot;&quot;&quot;
	uup = UpdateUserProfile()
	if uup.update_user_action_basic():
		uup.update_user_label()
		uup.update_user_info()
scheduler.add_job(update_user_profile, trigger='interval', hours=2)</code></pre>
]]></content>
    </entry>
</feed>