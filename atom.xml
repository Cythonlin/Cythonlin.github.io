<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://cythonlin.github.io</id>
    <title>Cython_lin</title>
    <updated>2020-09-29T04:17:36.976Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://cythonlin.github.io"/>
    <link rel="self" href="https://cythonlin.github.io/atom.xml"/>
    <logo>https://cythonlin.github.io/images/avatar.png</logo>
    <icon>https://cythonlin.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, Cython_lin</rights>
    <entry>
        <title type="html"><![CDATA[RS => 推荐系统（一）环境配置+数据收集]]></title>
        <id>https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-yi-huan-jing-pei-zhi-shu-ju-shou-ji/</id>
        <link href="https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-yi-huan-jing-pei-zhi-shu-ju-shou-ji/">
        </link>
        <updated>2020-09-29T04:17:17.000Z</updated>
        <content type="html"><![CDATA[<h1 id="python环境">Python环境</h1>
<p>miniconda创建虚拟环境：<br>
conda create -n reco_sys python=3.6.7<br>
激活/退出 虚拟环境：<br>
conda activate spider-venv<br>
conda deactivate<br>
2个slave需要安装依赖：<br>
yum -y install gcc<br>
安装模块：<br>
pip install redis supervisor apscheduler chardet jieba jupyter numpy pandas scipy scikit-learn pyspark findspark happybase pyhdfs -i https://pypi.douban.com/simple</p>
<h1 id="大数据环境">大数据环境</h1>
<h3 id="lambda环境启动脚本配置">Lambda环境启动脚本配置</h3>
<p>禁用+关闭防火墙：<br>
systemctl disable firewalld.service<br>
systemctl stop firewalld.service<br>
同步系统时间（不这样做 hbase可能启动失败）（3台都运行命令）：<br>
yum install ntpdate -y<br>
ntpdate 0.cn.pool.ntp.org</p>
<p>创建综合启动脚本<br>
vi start.sh<br>
/root/bigdata/hadoop/sbin/start-all.sh<br>
start-hbase.sh<br>
/root/bigdata/spark/sbin/start-all.sh</p>
<pre><code>vi stop.sh
    /root/bigdata/spark/sbin/stop-all.sh 
    stop-hbase.sh
    /root/bigdata/hadoop/sbin/stop-all.sh
</code></pre>
<p>开启<br>
/root/scripts/start.sh<br>
停止<br>
/root/scripts/stop.sh</p>
<h3 id="ui地址查log也可">UI地址（查log也可）：</h3>
<p>Hadoop UI:<br>
http://192.168.19.137:8088<br>
YARN UI：<br>
http://192.168.19.137:50070<br>
Hbase UI:<br>
http://192.168.19.137:16010<br>
Spark UI：<br>
http://192.168.19.137:8080/</p>
<h3 id="数据库运行">数据库运行：</h3>
<p>MySQL启动：<br>
systemctl start docker<br>
docker start mysql<br>
Hive元数据服务开启:<br>
nohup hive --service metastore &amp;</p>
<h3 id="spark相关问题">spark相关问题</h3>
<p>spark on yarn 启动巨慢，解决办法：<br>
hadoop fs -mkdir -p /system/spark-lib<br>
hadoop fs -put /root/bigdata/spark-2.2.2-bin-hadoop2.7/jars/* /system/spark-lib<br>
hadoop fs -chmod -R 755 /system/spark-lib<br>
cd $SPARK_HOME/conf<br>
cp spark-defaults.conf.template spark-defaults.conf<br>
vi spark-defaults.conf<br>
spark.yarn.jars    hdfs://192.168.19.137:9000//system/spark-lib/*</p>
<pre><code>如果用的是 jupyter, 记得重启 jupyter服务
jupyter notebook --allow-root --ip 0.0.0.0
</code></pre>
<h3 id="hdfs-hive相关问题">HDFS-Hive相关问题</h3>
<p>内部表修改为外部表：<br>
alter table user_profile SET TBLPROPERTIES('EXTERNAL'='TRUE');</p>
<h1 id="数据构成">数据构成</h1>
<h3 id="数据库1-toutiao">数据库1： toutiao</h3>
<pre><code>news_article_basic   # 文章标题
news_article_content  # 文章内容  
news_channel       # 文章频道（类别）
user_basic			  # 用户业务数据 
user_profile       # 用户私人信息
</code></pre>
<h3 id="数据库2-profile">数据库2： profile</h3>
<pre><code>user_action			  # 用户行为日志
</code></pre>
<h3 id="数据库2-article">数据库2： article</h3>
<pre><code>article_data       # 合并文章标题+内容+频道后的存储结果
...
</code></pre>
<h1 id="数据迁移sqoop">数据迁移（Sqoop）</h1>
<h3 id="耗时">耗时</h3>
<pre><code>4000w (10+g): 30+ min
</code></pre>
<h3 id="检测sqoop是否能连通mysql-并列出mysql所有数据库">检测Sqoop是否能连通MySQL, 并列出MySQL所有数据库:</h3>
<pre><code>sqoop list-databases --connect jdbc:mysql://192.168.19.137:3306/ --username root -P
</code></pre>
<h3 id="全量导入方式不推荐">全量导入方式（不推荐）：</h3>
<pre><code>#!/bin/bash
array=(user_profile user_basic news_user_channel news_channel user_follow user_blacklist user_search news_collection news_article_basic news_article_content news_read news_article_statistic user_material)

for table_name in ${array[@]};
do
    sqoop import \
        --connect jdbc:mysql://192.168.19.137/toutiao \
        --username root \
        --password password \
        --table $table_name \
        --m 5 \
        --hive-home /root/bigdata/hive \
        --hive-import \
        --create-hive-table  \
        --hive-drop-import-delims \
        --warehouse-dir /user/hive/warehouse/toutiao.db \
        --hive-table toutiao.$table_name
done
</code></pre>
<h3 id="增量导入方式">增量导入方式</h3>
<p>方式1： 通过指定递增的字段来导入（不推荐，因为某些字段的值不是递增的）<br>
append：即通过指定一个递增的列，如：--incremental append --check-column num_iid --last-value 0<br>
方式2：incremental： 时间戳<br>
--incremental lastmodified <br>
--check-column column <br>
--merge-key key <br>
--last-value '2012-02-01 11:0:00'</p>
<pre><code>就是只导入check-column的列比'2012-02-01 11:0:00'更大（新）的数据,按照key合并
</code></pre>
<h3 id="增量导入位置">增量导入位置</h3>
<ol>
<li>直接sqoop导入到hive(–incremental lastmodified 模式不支持导入 Hive )</li>
<li>sqoop导入到hdfs，然后建立hive表关联<br>
--target-dir /user/hive/warehouse/toutiao.db/</li>
</ol>
<h3 id="sqoop导入到hdfshive表关联到hdfs填坑">sqoop导入到hdfs，hive表关联到hdfs填坑</h3>
<p>现象：<br>
查出一堆 null<br>
原因：<br>
sqoop 导出的 hdfs 分片数据，都是使用逗号 , 分割的。<br>
由于 hive 默认的分隔符是 /u0001（Ctrl+A）,为了平滑迁移，需要在创建表格时指定数据的分割符号。<br>
解决方式：<br>
导入数据到hive中，需要在创建HIVE表加入 row format delimited fields terminated by ','</p>
<h3 id="sqoop迁移到hdfs后hive创建表并指定关联位置实例">sqoop迁移到hdfs后，Hive创建表并指定关联位置实例</h3>
<pre><code>create table user_profile(
	user_id BIGINT comment &quot;userID&quot;,
	gender BOOLEAN comment &quot;gender&quot;)
COMMENT &quot;toutiao user profile&quot;
row format delimited fields terminated by ','
LOCATION '/user/hive/warehouse/toutiao.db/user_profile';
</code></pre>
<p>注：<br>
5个表中，只有 news_article_content<br>
（因为这个表奇怪字符太多，是全量导入的，hive不需要手动创建表，会自动创建的）<br>
而这个表从 hdfs 拿到的数据是已经做过过滤的，所以不需要 加分隔符了，也就是不需要下面这行代码：<br>
row format delimited fields terminated by ','</p>
<h1 id="flume日志收集到hive中">Flume日志收集到Hive中</h1>
<h3 id="新建数据库">新建数据库</h3>
<pre><code>create database if not exists profile comment &quot;use action&quot; location '/user/hive/warehouse/profile.db/';
</code></pre>
<h3 id="创建表的格式语法实例如下">创建表的格式语法实例如下：</h3>
<pre><code>create table user_action(
actionTime STRING comment &quot;user actions time&quot;,
readTime STRING comment &quot;user reading time&quot;,
channelId INT comment &quot;article channel id&quot;,
param map&lt;string, string&gt; comment &quot;action parameter&quot;)
COMMENT &quot;user primitive action&quot;
PARTITIONED BY(dt STRING)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
LOCATION '/user/hive/warehouse/profile.db/user_action';
</code></pre>
<p>文档中：Hive建表，有个问题 map 需要指定数据类型:<br>
param map&lt;string, string&gt; comment &quot;action parameter&quot;)</p>
<pre><code>map 要向上面一样指定 &lt;string, string&gt; 才可以， 不然会报如下错误：
'''mismatched input 'comment' expecting &lt; near 'map' in map type'''
</code></pre>
<p>疑难参数解读：<br>
PARTITIONED BY(dt STRING)：  hive按照 dt 字段分区<br>
为什么要分区：<br>
Hive适合处理大的文件内容量，少的文件数量。<br>
Flume收集日志可能来一点日志就加到一个新文件中。<br>
如此一来，文件零散的特别多。 Hive处理的会很慢。</p>
<pre><code>		所以，Hive指定个分区，来把小文件们分成几大块（就是几个分区），这样处理会更快
        
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'： 处理Json格式数据
</code></pre>
<p>数据导入步骤如下（虚代替Flume）（操作完成后是查不到数据的，需要关联，下面会解释）：<br>
hadoop fs -put /root/data/backup/profile.db/user_action/*  /user/hive/warehouse/profile.db/user_action/</p>
<pre><code># 删除: hadoop fs -rmr /user/hive/warehouse/profile.db/*
</code></pre>
<h3 id="flume-收集配置">Flume 收集配置</h3>
<p>进入flume/conf目录<br>
创建一个collect _ click.conf的文件，写入flume的配置：<br>
a1.sources = s1<br>
a1.sinks = k1<br>
a1.channels = c1</p>
<pre><code>a1.sources.s1.channels= c1
a1.sources.s1.type = exec
a1.sources.s1.command = tail -F /root/logs/userClick.log
a1.sources.s1.interceptors=i1 i2
a1.sources.s1.interceptors.i1.type=regex_filter
a1.sources.s1.interceptors.i1.regex=\\{.*\\}
a1.sources.s1.interceptors.i2.type=timestamp

# channel1
a1.channels.c1.type=memory
a1.channels.c1.capacity=30000
a1.channels.c1.transactionCapacity=1000

# k1
a1.sinks.k1.type=hdfs
a1.sinks.k1.channel=c1
a1.sinks.k1.hdfs.path=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d
a1.sinks.k1.hdfs.useLocalTimeStamp = true
a1.sinks.k1.hdfs.fileType=DataStream
a1.sinks.k1.hdfs.writeFormat=Text
a1.sinks.k1.hdfs.rollInterval=0
a1.sinks.k1.hdfs.rollSize=10240
a1.sinks.k1.hdfs.rollCount=0
a1.sinks.k1.hdfs.idleTimeout=60
</code></pre>
<p>参数说明：<br>
sources：为实时查看文件末尾，interceptors解析json文件<br>
channels：指定内存存储，并且制定batchData的大小，PutList和TakeList的大小见参数，Channel总容量大小见参数<br>
指定sink：形式直接到hdfs，以及路径，文件大小策略默认1024、event数量策略、文件闲置时间<br>
开始收集：<br>
/root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1</p>
<h3 id="hive-关联分区">Hive 关联分区：</h3>
<p>如果不关联分区，无论是 Flume收集到HDFS的分区数据，还是我们传进去HDFS模拟的分区数据 通过Hive是查不到的<br>
关联分区如下操作：<br>
alter table user_action add partition (dt='2018-12-11') location &quot;/user/hive/warehouse/profile.db/user_action/2018-12-11/&quot;</p>
<h1 id="进程管理-supervisor">进程管理 Supervisor</h1>
<h3 id="正常配置流程">正常配置流程</h3>
<p>安装:<br>
pip install supervisor<br>
创建配置文件（当前目录执行，或者主目录执行都可，总配置文件就会生成到当前目录下）：<br>
echo_supervisord_conf &gt; supervisord.conf<br>
创建自定义配置文件目录：<br>
mkdir /etc/supervisor<br>
vim 打开编辑supervisord.conf文件，修改最后1行：<br>
[include]<br>
files = relative/directory/<em>.ini<br>
为<br>
[include]<br>
files = /etc/supervisor/</em>.conf<br>
将最开始生成的 supervisord.conf 复制到 /etc/ 下， 然后主文件就不用动了：<br>
cp supervisord.conf /etc/<br>
最后在  /etc/supervisor 这个目录中，自定义我们自己需要的 启动程序的配置文件模板，这里为 vi reco.conf：<br>
见下面Flume案例</p>
<h3 id="flumesupervisor-配置案例">Flume+Supervisor 配置案例</h3>
<p>flume启动需要相关hadoop,java环境，可以在shell脚本汇总添加:<br>
先创建一个存放此shell脚本的目录：<br>
mkdir /root/toutiao_project/scripts<br>
打开文件:<br>
vi /root/toutiao_project/scripts/collect_click.sh<br>
并写入:<br>
#!/usr/bin/env bash</p>
<pre><code>export JAVA_HOME=/root/bigdata/jdk
export HADOOP_HOME=/root/bigdata/hadoop
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin

/root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1
</code></pre>
<p>并在	/etc/supervisor 的reco.conf添加:<br>
[program:flume]<br>
command=/bin/bash /root/toutiao_project/scripts/collect_click.sh<br>
user=root<br>
autorestart=true<br>
redirect_stderr=true<br>
stdout_logfile=/root/logs/collect.log<br>
loglevel=info<br>
stopsignal=KILL<br>
stopasgroup=true<br>
killasgroup=true</p>
<p>启动 supervisor服务:<br>
supervisord -c /etc/supervisord.conf<br>
查看 supervisor是否运行：<br>
ps aux | grep supervisord<br>
管理 supervisor进程管理界面，输入命令：<br>
supervisorctl<br>
管理界面可通过如下 命令+进程名 来管理进程：<br>
start flume<br>
stop flume<br>
restart flume</p>
<pre><code>status		# 查看所有进程状态
update		# 重启配置文件修改过的程序
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => PySpark-Spark SQL]]></title>
        <id>https://cythonlin.github.io/post/py-greater-pyspark-spark-sql/</id>
        <link href="https://cythonlin.github.io/post/py-greater-pyspark-spark-sql/">
        </link>
        <updated>2020-09-29T04:16:24.000Z</updated>
        <content type="html"><![CDATA[<h1 id="spark-sql">Spark SQL</h1>
<p>Spark SQL 分为三类：</p>
<ol>
<li>SQL</li>
<li>DataFrame (参考pandas，但略有不同)</li>
<li>Datasets (由于python是动态的，所以不支持python)</li>
</ol>
<h3 id="初始环境">初始环境：</h3>
<pre><code>import findspark
findspark.init()

from pyspark.sql import SparkSession        
spark = SparkSession.builder.appName('myspark').getOrCreate()    # 初始化session
# spark.sparkContext.parallelize([1,2,3,4]).collect()  # 里面包含之前说过的sparkContext
...
中间这部分留给下面写
...
spark.stop()         # 关闭 session
</code></pre>
<p>从json导入为df:<br>
df = spark.read.json(&quot;file:///home/lin/data/user.json&quot;,multiLine=True)<br>
打印DF字段信息：<br>
df.printSchema()</p>
<pre><code>root
 |-- age: long (nullable = true)
 |-- gender: string (nullable = true)
 |-- name: string (nullable = true)
</code></pre>
<h3 id="crud">CRUD</h3>
<p>增<br>
from pyspark.sql import functions as f</p>
<pre><code># schema就相当于 pandas 指定的 columns, 双层序列，  [2x4] 的样本
df1 = spark.createDataFrame([[1,2,3,4],[5,6,7,8]],schema=['1_c','2_c', '3_c', '4_c'])
+-----+-----+-----+-----+
|1\_col|2\_col|3\_col|4\_col|
+-----+-----+-----+-----+
|    1|    2|    3|    4|
|    5|    6|    7|    8|
+-----+-----+-----+-----+

# lit可以在指定空列的时候，指定 null值， 或者 int型（里面有很多类型，可以发现）
# df2 = df1.withColumn('null_col', f.lit(None)).withColumn('digit_col', f.lit(2))
df2 = df1.withColumn('5_col', df1['4_col']+1)   # 在原来列字段基础上。
df2.show()
</code></pre>
<p>删<br>
df2 = df1.drop('age')        # 删除 age列<br>
df2 = df1.dropna()           # 删除空行<br>
df2 = df1.drop_duplicates()  # 删除重复-行<br>
改<br>
和&quot;增&quot;，差不多，只不过字段，指定为原有字段字符串即可。</p>
<p>查<br>
下面的讲的（投影、过滤、排序、分组），几乎都是查。</p>
<h3 id="投影">投影</h3>
<p>投影所有：<br>
df.show(n=20)        # 默认就是 n=20 只返回 前20条记录</p>
<pre><code>+---+------+--------+
|age|gender|    name|
+---+------+--------+
| 18|   man|zhangsan|
+---+------+--------+
</code></pre>
<p>选中某列投影：<br>
df.select('name','age').show()        # 若直接写 '*', 和直接 df.show()是一个效果</p>
<pre><code>+--------+---+
|    name|age|
+--------+---+
|zhangsan| 18|
+--------+---+
</code></pre>
<p>或者用另两种方式投影（投影过程可计算）：<br>
df.select(df['name'],df['age']+20).show() # 同上，这是另一种写法，注意一下列名<br>
df.select(df.name, df.age+20).show()      # 同上，这是另二种写法，注意一下列名</p>
<pre><code>+--------+----------+
|    name|(age + 20)|
+--------+----------+
|zhangsan|        38|
+--------+----------+
</code></pre>
<p>取出前N条DF，并转化为 [ {} , {} ] 格式<br>
df_user.take(1)  # [Row(age=18, name='张三')]<br>
df_user.head(1)  # [Row(age=18, name='张三')]<br>
df_user.first()  # Row(age=18, name='张三')    # 注意,无列表</p>
<h3 id="排序">排序</h3>
<pre><code>df_user.head(1)
df_user.sort(df_user.name.desc()).show()

# 另外说明一点, df的每个熟悉都有,一些操作符函数, desc()就是一种操作符函数
</code></pre>
<h3 id="过滤">过滤：</h3>
<pre><code>df.filter( df['age'] &gt; 15).show()

+---+------+----+
|age|gender|name|
+---+------+----+
+---+------+----+
</code></pre>
<h3 id="分组">分组：</h3>
<pre><code>df.groupBy('name').count().show()

+--------+-----+
|    name|count|
+--------+-----+
|zhangsan|    1|
+--------+-----+
</code></pre>
<h3 id="join">Join</h3>
<pre><code>df_user.join(df_user, on=df_user.name==df_user.name, how='inner').show()

+----+---+----+---+
|name|age|name|age|
+----+---+----+---+
|李四| 20|李四| 20 |
|张三| 18|张三| 18 |
+----+---+----+---+
# 特别提醒， 此 Join， 只要都进来是 DF格式的任何数据库，都可 Join
# 比如： MySQL 和 Hive  ,  Json 也可。
</code></pre>
<h3 id="储存为临时视图表-并调用sql语句">储存为临时视图（表), 并调用sql语句：</h3>
<pre><code>df.createOrReplaceTempView('user')                  # 创建为 user临时视图
df_sql = spark.sql('select * from user').show()     # spark.sql返回的还是df, 所以要show()

+---+------+--------+
|age|gender|    name|
+---+------+--------+
| 18|   man|zhangsan|
+---+------+--------+
</code></pre>
<h3 id="rdd-与-df互转">RDD 与 DF互转</h3>
<p>RDD -&gt; DF<br>
### RDD -&gt; DF 需要把RDD做成两种格式(任选其一)<br>
### 第一种 Row 格式<br>
from pyspark import Row<br>
rdd_user = spark.sparkContext.parallelize( [('张三',18), ('李四',20)] )<br>
rdd_user_row = rdd_user.map(lambda x:Row(name=x[0], age=x[1]))<br>
print(rdd_user_row.collect()) # [Row(age=18, name='张三'), Row(age=20, name='李四')]<br>
df_user = spark.createDataFrame(rdd_user_row)</p>
<pre><code>### 第二种 [('张三', 18),('李四', 20)]
    rdd_user = spark.sparkContext.parallelize( [('张三',18), ('李四',20)] )
    df_user = rdd_user.toDF(['name', 'age'])        # 给定列名
df_user.show()
</code></pre>
<p>DF -&gt; RDD<br>
rdd_row = df_user.rdd.map(lambda x: x.asDict())  # 或者 x.name, x.age取值<br>
rdd_row.collect()  # [{'age': 18, 'name': '张三'}, {'age': 20, 'name': '李四'}]</p>
<h3 id="csv读写">CSV读写</h3>
<p>从HDFS中读取(我们先新建一个CSV并扔到HDFS中),<br>
vi mydata.csv:<br>
name,age<br>
zhangsan,18<br>
lisi, 20</p>
<pre><code>hadoop fs -mkdir /data                 # 在HDFS中新建一个目录 /data
hadoop fs -put mydata.csv /data        # 并把本地 mydata.csv扔进去 (-get可拿出来)
</code></pre>
<p>在代码中读取 HDFS数据:<br>
df = spark.read.csv(&quot;hdfs:///data/mydata.csv&quot;, header=True)<br>
df.show()<br>
# header=True 代表, csv文件的第一行作为csv的抬头(列名)<br>
# df.write.csv(&quot;hdfs:///data/mydata.csv&quot;, header=True)   # read改为write就变成了写</p>
<h3 id="hive读写">Hive读写</h3>
<p>Hive的配置与依赖之前讲过了（最值得注意的是需要先启动一个 metadata的服务）<br>
先验传送门：<a href="https://segmentfault.com/a/1190000020841646">https://segmentfault.com/a/1190000020841646</a><br>
import findspark<br>
findspark.init()</p>
<pre><code>from pyspark.sql import SparkSession   

spark = SparkSession.builder\
    .appName(&quot;Spark Hive Example&quot;)\
    .master(&quot;local[*]&quot;)\
    .config(&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;)\
    .enableHiveSupport()\
    .getOrCreate()

spark.sql(&quot;use mydatabase&quot;)        # 执行Hive 的 SQL, 切换数据库（前提你得有）
</code></pre>
<p>读：<br>
df = spark.table('person').show()  # 直接对表操作 (注意，sql语句也可)<br>
写：<br>
df = spark.table('person')<br>
df2 = df.withColumn('nickname', df.name)  # 稍微变动一下，添一个字段<br>
df2.write.saveAsTable(&quot;new_person&quot;)       # 写入新表</p>
<h3 id="mysql读写">MySQL读写</h3>
<p>读：<br>
# 注意0：有好几种方式，我只列举一个 成对的读写配置。<br>
# 注意1: url中 &quot;hive&quot;是数据库名. 你也可以起为别的名<br>
# 注意2：table的值--&quot;TBLS&quot;,  它是 MySQL中&quot;hive库&quot;中的一个表。<br>
# 注意3：特别注意！ TBLS不是我们想要的表。他只是一个大表，管理了我们hive的信息<br>
#        TBLS中的 一个列属性 &quot;TBL_NAME&quot; 才是真正我们需要的表！！</p>
<pre><code>df = spark.read.jdbc(
    url='jdbc:mysql://localhost:3306/hive?user=root&amp;password=123',
    table=&quot;TBLS&quot;,
    properties={&quot;driver&quot;:&quot;com.mysql.jdbc.Driver&quot;},
)

df.show()
df.select(&quot;TBL_NAME&quot;).show()
</code></pre>
<p>写：<br>
df.write.jdbc(<br>
url='jdbc:mysql://localhost:3306/hive?user=root&amp;password=123',<br>
table=&quot;new_table&quot;,<br>
mode='append',<br>
properties={&quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;}<br>
)</p>
<pre><code># 同是特别注意： 和读一样， 它写入的新表，也是一个整体的表结构。
#     此表的一个列&quot;TBL_NAME&quot;，它才对应了我们真正要操作的表</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => PySpark-Spark Core（RDD）]]></title>
        <id>https://cythonlin.github.io/post/py-greater-pyspark-spark-corerdd/</id>
        <link href="https://cythonlin.github.io/post/py-greater-pyspark-spark-corerdd/">
        </link>
        <updated>2020-09-29T04:15:57.000Z</updated>
        <content type="html"><![CDATA[<h1 id="前言">前言</h1>
<p>第一篇传送门：<a href="https://segmentfault.com/a/1190000020841646">https://segmentfault.com/a/1190000020841646</a></p>
<h1 id="rdd认知">RDD认知</h1>
<h3 id="rdd是什么">RDD是什么？</h3>
<p>RDD: 弹性分布式数据集（Resiliennt Distributed Datasets）</p>
<p>转为格式RDD的几种方式：<br>
1. parallelize:<br>
rdd = sc.parallelize([1,2,3,4,5])   # 里面传的就是普通python类型</p>
<pre><code>2. 读文件/读数据库/读ES等各种方式，此处以读文件为例：
    rdd = sc.textFile('file:///home/lin/data/hello.txt')
</code></pre>
<h3 id="rdd核心概念">RDD核心概念</h3>
<p>Application:<br>
application: 一个app 就是一个自定义的 py脚本（被 spark-submit提交的）或一个spark-shell<br>
app = 1个 driver + 多个executors(相当于多个进程)</p>
<pre><code>注意：数据在不同的 app之间 不能被共享， 若想要共享（需要考虑外部存储）
</code></pre>
<p>Driver:<br>
每一个.py脚本中都有一个 sparkcontext，它就是driver<br>
Worker Node:<br>
相当于standalone 的 slave节点<br>
Executor:<br>
Executor(进程)：每个Driver中都有多个 Executors</p>
<pre><code>并且可以运行多个 Tasks
</code></pre>
<p>Job:<br>
job:  对应下面即将要说的 action   : collect() 等</p>
<pre><code>一个 task 对应 一个 job  (一个 transformation 对应 一个 action)
一个 job 对应 多个 task  (多个 transformations链式调用之后，再调用一个action)
</code></pre>
<p>Task:<br>
task: 对应下面即将要说的 transformation   :map() 等<br>
每个task可用一个线程执行。多个task可并行</p>
<p>Stage:<br>
一个job被切分为多份<br>
Cluster Manager:<br>
管理 从 Standalone, YARN, Mesos 中获取的资源<br>
就是 --master 指定的参数<br>
其中 还包括 空间 内存等参数配置<br>
Cache:<br>
缓存： ### persist &amp; cache &amp; unpersist 三种API可供选择<br>
Lineage(依赖,血缘关系)：<br>
依赖：<br>
父              子               孙<br>
RDD1  -&gt; map-&gt;  RDD2 -&gt; filter-&gt; RDD3<br>
服务器1：        part1 -&gt;        part1-&gt;          part1<br>
服务器2：        part2 -&gt;        part2-&gt;          part2<br>
服务器3：        part3 -&gt;        part3-&gt;          part3</p>
<pre><code>    如上图: 假如 RDD3 的 part2 挂了， 那么就会退回到 RDD2的part2再计算一遍。
           而不是回到&quot;最初&quot;的起点。
           
窄依赖（Narrow, 依赖的很少，很窄）：
    重点:  '子part' 只依赖一个 '父part'。
    map, filter 等:  元素被摊分在每一个part中， 子part出错就找&quot;对应&quot;（一个） 父part即可。

宽依赖（Wide, 依赖的很多，很宽）：
    重点:  '子part' 依赖多个 '父part' 同时计算得到。
    shuffle操作: xxBy, join等： 子part出错 找&quot;对应&quot;（多个） 父part 重新共同计算。
</code></pre>
<p>stage:<br>
遇到 1个宽依赖， 就会做 shuffle操作。<br>
然后就会把&quot;之前&quot;的 “所有窄依赖”划分为 &quot;1个stage&quot;。<br>
最后，整体全部，也当作 &quot;1个stage&quot;。</p>
<h3 id="官档图">官档图</h3>
<p>传送门：<a href="http://spark.apache.org/docs/latest/cluster-overview.html">http://spark.apache.org/docs/latest/cluster-overview.html</a><br>
<img src="/img/bVbzEU3" alt="image.png" loading="lazy"></p>
<h1 id="rdd两大算子">RDD两大算子</h1>
<h2 id="transformation-lazy">Transformation （Lazy）</h2>
<p>主要机制：各种操作不会被立刻执行，但这些操作之间的关系会被记录下来，等待下面action调用。<br>
直观理解举例：<br>
1. 像 sqlalchemy 中的 filter(), groupby(), page()等操作<br>
2. 像 tensorflow1.x 中的 sess.run() 之前的各种操作<br>
3. 像 数据库的事务，在提交之前的各种操作<br>
接下来介绍，Transformation 的各种操作。</p>
<h3 id="map">map</h3>
<pre><code>同 python 的 map。
你只需记住RDD类型里面包裹的就是我们熟悉的python类型
所以： 
    python 的 map 怎么用， RDD对象的 map 就怎么用， 下面filter同理

只举一个语法格式例子：（下面同理）
    rdd.map(lambda x:x+1)
</code></pre>
<h3 id="filter">filter</h3>
<pre><code>同上，同python
</code></pre>
<h3 id="flatmap">flatMap</h3>
<pre><code>和 map 几乎差不多。
唯一有一点区别：
    map 每次基于单个元素，返回什么，那最终结果就是什么（最后拼成序列）。
    flatMap 每次基于单个元素，若返回的是序列（列表等），那么会自动被解包，并一字排开返回。
</code></pre>
<h3 id="groupby-和-groupbykey">groupBy 和 groupByKey</h3>
<p>说一下 没有key, 和 带有key的区别（后面同理，就不啰嗦了）：<br>
没有key:<br>
1. 一般必须需要一个 函数句柄 (lambda), 而这个句柄是针对（操作后新形成的key）使用的<br>
2. 针对一层序列   [, , ...]<br>
带有key<br>
1. 一般无参<br>
2. 针对双层序列   [(),(),...]<br>
直接上例子了（对比着看）：<br>
rdd1 = sc.parallelize(['a','b','c','a'])                      # 一层序列<br>
rdd2 = sc.parallelize( [('a',1),('b',2), ('c',3), ('a',4)] )  # 双层序列</p>
<pre><code>group1 = rdd1.groupBy(lambda x:x)   # 针对 一层序列， 注意这里，必须写 函数句柄
group2 = rdd2.groupByKey()          # 针对 双层序列

print( group1.collect() )
print( group2.collect() )

# 可以这样告诉你， 他们俩的最外层结果是一样的: [{key:value}, ...], 
结果如下 ~~~~
[
    ('a', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb7e4384c88&gt;),     
    ('b', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb7e43848d0&gt;),
    ('c', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb7e4384940&gt;)
]
# 如果加了count(), 那么它们的结果就是一样的了，返回统计的个数， 等到 action再说。
</code></pre>
<h3 id="reducebykey">reduceByKey</h3>
<p>照应双层或多层序列，或者 承接 groupByKey()<br>
rdd = sc.parallelize(['Tom', 'Jerry', 'Tom', 'Putch'])<br>
rdd.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).collect()</p>
<pre><code># 结果（可以忽略上面的 collect(), 它属于action，放在这里方便贴个结果）
&gt;&gt; [('Tom', 2), ('Jerry', 1), ('Putch', 1)]
</code></pre>
<h3 id="sortby-和-sortbykey">sortBy 和 sortByKey</h3>
<p>sortBy：根据元素排序（这里的例子是根据key排序， a[1]代表根据value排序）<br>
&gt;&gt;&gt; a = sc.parallelize([['z',1], ['b',4],['h',3]])<br>
&gt;&gt;&gt; a.sortBy(lambda a:a[0]).collect()<br>
[['b', 4], ['h', 3], ['z', 1]]<br>
&gt;&gt;&gt; a.sortBy(lambda a:a[1]).collect()<br>
[['z', 1], ['h', 3], ['b', 4]]<br>
sortByKey：根据key排序<br>
&gt;&gt;&gt; a.sortByKey().collect()<br>
[('b', 4), ('h', 3), ('z', 1)]<br>
可选参数：<br>
ascending=False  (默认为True升序)</p>
<h3 id="union">union</h3>
<pre><code>rdd1.union(rdd2)    # 相当于 python的 &quot;列表加法&quot; 或者 python的 &quot;extend&quot;
</code></pre>
<h3 id="distinct">distinct</h3>
<pre><code>rdd.distinct()      # 去重
</code></pre>
<h3 id="join">join</h3>
<p>前提： （我的理解就是，能转化成 python 字典的列表格式即可）<br>
eg:  [ [1,2], [3,4], [5,6] ]<br>
两层列表<br>
每层列表的每个元素中，  只有2个元素</p>
<pre><code>错误格式示例：
    [['a','b','c'], ['d','e','f']]

也不能说错误吧，不过若是这种3个-多个子元素的格式， join时默认会取前2个元素。其余丢弃。
</code></pre>
<p>内连接（innerJoin）：<br>
左外连接（leftOuterJoin）：<br>
右外连接（rightOuterJoin）：<br>
全外连接（fullOuterJoin）：</p>
<p>完整示例：<br>
rdd1 = sc.parallelize( [['a','b'], ['d','e']] )       # 左<br>
rdd2 = sc.parallelize( [['a','c'], ['e','f']] )       # 右<br>
# 开头说过:能转化成字典的列表格式即可，或者你可以写成这样（但是不能传原生字典进去）：<br>
rdd1 = sc.parallelize( list({'a': 'b', 'd': 'e'}.items()) )<br>
rdd2 = sc.parallelize( list({'a': 'c', 'e': 'f'}.items()) )</p>
<pre><code># 内连接（交集）
print( rdd1.join(rdd2).collect() )             # [('a', ('b', 'c'))]

# 左连接（左并集）
print( rdd1.leftOuterJoin(rdd2).collect() )    # [('d', ('e', None)), ('a', ('b', 'c'))]

# 右连接（右并集）
print( rdd1.rightOuterJoin(rdd2).collect() )   # [('a', ('b', 'c')), ('e', (None, 'f'))]

# 全连接（并集）
print( rdd1.fullOuterJoin(rdd2).collect() )    # [('d', ('e', None)), ('a', ('b', 'c')), ('e', (None, 'f'))]
</code></pre>
<h3 id="persist-cache-unpersist">persist &amp; cache &amp; unpersist</h3>
<p>cache(): 缓存<br>
persist(): 持久化<br>
unpersist(): 清空缓存  （他属于 action-立即触发， 为了方便对比，我就一起放到了这里）<br>
官档：http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence</p>
<h2 id="action-commit">Action (Commit)</h2>
<p>主要机制：拿到 transformation 记录的关系， 用 action的各种操作来真正触发、执行、返回结果。<br>
对应上面，继续直观举例：<br>
1. 像 sqlalchemy 中的 commit()<br>
2. 像 tensorflow1.x 中的 sess.run()<br>
3. 像 数据库的事务的 &quot;提交&quot;<br>
接下来介绍，Action 的各种操作。</p>
<h3 id="collect">collect</h3>
<p>执行transformation记录的关系 并 返回结果， 在Pyspark中就是RDD类型 转 Python数据类型。<br>
(中间你可以链式调用各种 transformation方法，结尾调用一个 collect(), 就可以出结果了)<br>
rdd1.xx().xx().collect()</p>
<h3 id="count">count</h3>
<p>统计元素项的个数，同上语法, 同上理念，触发返回结果<br>
rdd2 = sc.parallelize( [['a','c','d'], ['e','f','g']] )<br>
rdd2.count()  # 无参<br>
&gt;&gt; 2</p>
<h3 id="reduce">reduce</h3>
<pre><code>rdd2 = sc.parallelize( [['a','c','d'], ['e','f','g']] )
rdd2.reduce(lambda x,y:x+y)    # 参数为2个参数的函数句柄，做&quot;累&quot;的操作，（累加，累乘）等
&gt;&gt; ['a', 'c', 'd', 'e', 'f', 'g']
</code></pre>
<h3 id="take">take</h3>
<p>相当于mysql的limit操作，取前n个<br>
rdd2 = sc.parallelize( [['a','c','d'], ['e','f','g']] )<br>
rdd2.take(0)  # []<br>
rdd2.take(1)  # [['a', 'c', 'd']]<br>
rdd2.take(2)  # [['a', 'c', 'd'], ['e', 'f', 'g']]</p>
<pre><code>再次强调： take的参数是，个数的意思，而不是索引，不要混淆额
</code></pre>
<h3 id="top">top</h3>
<p>返回最大的n个元素（会自动给你排序的）<br>
rdd2 = sc.parallelize( [1,2,3,8,5,3,6,8])<br>
rdd2.top(3)<br>
&gt;&gt; [8, 8, 6]</p>
<h3 id="foreach">foreach</h3>
<p>遍历每个元素，对子元素做-对应函数句柄的操作，下面说这个action的两点注意事项：<br>
注意1： 无返回值（返回None）<br>
注意2： 通常用作 print(), 但是它不会在notebook中打印， 而是在你后台开启的spark中打印。<br>
rdd2 = sc.parallelize( [['a','c','d'], ['e','f','g']] )<br>
rdd2.foreach(lambda x:print(x))</p>
<pre><code>&gt;&gt; ['a', 'c', 'd']
   ['e', 'f', 'g']
</code></pre>
<h3 id="saveastextfile">saveAsTextFile</h3>
<pre><code>rdd = sc.textFile('file:///home/lin/data')
rdd.saveAsTextFile('file:///home/lin/mydata')  

# 这里有个注意事项： saveAsTextFile的参数路径不能在都进来的路径范围内。
# 或者说，读是从这个文件夹A（这是最后一级的目录）读进来的， 写就不能写入文件夹A了
# 另外， mydata是目录名， 进去你会看见 part-00000  这样的文件名，这才是真数据文件。
</code></pre>
<h1 id="spark优化相关">Spark优化相关</h1>
<h3 id="序列化">序列化：</h3>
<pre><code>好处1：网络传输必备
好处2：节省内存
两种方式序列化方式：
    1. Java内部序列化（默认，较慢，但兼容性好）
    2. Kryo （较快，但兼容性不太好） 
</code></pre>
<h3 id="内存管理">内存管理：</h3>
<p>可分为 execution（进程执行） 和 storage（存储）<br>
execution相关操作: shuffle, join, sort, aggregation<br>
storage相关操作  :   cache，<br>
特点：<br>
execution 和 storage 共享整体内存：<br>
execution起到 &quot;存霸&quot; 的角色:<br>
1. 若 execution区域内存 不够用了， 它会去抢夺 storage 区域的内存（不归还）<br>
2. 当然，可以为 storage 设置阈值 （必须给 storage留下多少）<br>
具体分配多少：<br>
总内存 = n<br>
execution内存 = (总内存 - 300M) * 50%<br>
storage内存   = (总内存 - 300M) * 50%</p>
<pre><code>说白了，就是留给JVM 300M， 然后 execution 和 storage 各分一半。
</code></pre>
<p>查看内存占用情况<br>
可通过WebUI查看<br>
（序列化后存储，通常会节省内存）</p>
<h3 id="broadcasting-variable">Broadcasting Variable</h3>
<p>情景：正常来说，每个 task（map, filter等） 都会占用1份数据，100个task就会拿100份数据。<br>
这种情况造成了数据的冗余， BroadCasting Variable（广播变量）就是解决这一问题的。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PY => Ubuntu-Hadoop-YARN-HDFS-Hive-Spark安装配置]]></title>
        <id>https://cythonlin.github.io/post/py-greater-ubuntu-hadoop-yarn-hdfs-hive-spark-an-zhuang-pei-zhi/</id>
        <link href="https://cythonlin.github.io/post/py-greater-ubuntu-hadoop-yarn-hdfs-hive-spark-an-zhuang-pei-zhi/">
        </link>
        <updated>2020-09-29T04:15:19.000Z</updated>
        <content type="html"><![CDATA[<h1 id="环境条件">环境条件</h1>
<p>Java 8<br>
Python 3.7<br>
Scala 2.12.10<br>
Spark 2.4.4<br>
hadoop 2.7.7<br>
hive 2.3.6<br>
mysql 5.7<br>
mysql-connector-java-5.1.48.jar</p>
<p>R 3.1+（可以不安装）</p>
<h1 id="安装java">安装Java</h1>
<p>先验传送门：https://segmentfault.com/a/1190000020746647#articleHeader0</p>
<h1 id="安装python">安装Python</h1>
<p>用Ubuntu自带Python3.7</p>
<h1 id="安装scala">安装Scala</h1>
<p>下载：https://downloads.lightbend.com/scala/2.12.10/scala-2.12.10.tgz<br>
解压：<br>
tar -zxvf 下载好的Scala<br>
配置：<br>
vi ~/.bashrc<br>
export SCALA_HOME=/home/lin/spark/scala-2.12.10<br>
export PATH=<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>S</mi><mi>C</mi><mi>A</mi><mi>L</mi><msub><mi>A</mi><mi>H</mi></msub><mi>O</mi><mi>M</mi><mi>E</mi></mrow><mi mathvariant="normal">/</mi><mi>b</mi><mi>i</mi><mi>n</mi><mo>:</mo></mrow><annotation encoding="application/x-tex">{SCALA_HOME}/bin:</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault">A</span><span class="mord mathdefault">L</span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span><span class="mord">/</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span></span></span>PATH<br>
保存退出<br>
激活配置：<br>
source ~/.bashrc</p>
<h1 id="安装hadoop">安装Hadoop</h1>
<p>提前说明： 若不使用HDFS 和 YARN，整个 Hadoop安装可忽略，可以直接安装spark。<br>
下载：<a href="https://archive.apache.org/dist/hadoop/common/">https://archive.apache.org/dist/hadoop/common/</a><br>
详细地址: https://archive.apache.org/dist/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz<br>
解压：<br>
tar -zxvf 下载好的 hadoop<br>
配置hadoop:<br>
vi ~/.bashrc<br>
export HADOOP_HOME=/home/lin/hadoop/hadoop-2.7.7<br>
export PATH=<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>H</mi><mi>A</mi><mi>D</mi><mi>O</mi><mi>O</mi><msub><mi>P</mi><mi>H</mi></msub><mi>O</mi><mi>M</mi><mi>E</mi></mrow><mi mathvariant="normal">/</mi><mi>b</mi><mi>i</mi><mi>n</mi><mo>:</mo></mrow><annotation encoding="application/x-tex">{HADOOP_HOME}/bin:</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span><span class="mord">/</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span></span></span>PATH<br>
激活配置：<br>
source ~/.bashrc</p>
<h3 id="hdfs配置">HDFS配置：</h3>
<p>进入解压后的 etc/hadoop   (注意这不是根目录的etc， 而是解压后的hadoop目录下的etc)<br>
echo $JAVA_HOME        # 复制打印出的路径</p>
<pre><code>vi hadoop-env.sh:  （找到 export JAVA_HOME 这行，并替换为如下）
    export JAVA_HOME=/home/lin/spark/jdk1.8.0_181  
    
vi core-site.xml:  （hdfs后面为 主机名:端口号）  （主机名就是终端显示的 @后面的~~~）
    &lt;property&gt;
      &lt;name&gt;fs.default.name&lt;/name&gt;
      &lt;value&gt;hdfs://lin:8020&lt;/value&gt;       
    &lt;/property&gt;
    
vi hdfs-site.xml： (同样在 &lt;configuration&gt; 之间加入) (/home/lin/hdfs是我已经有的目录)
    &lt;property&gt;
      &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
      &lt;value&gt;/home/lin/hdfs/dfs/name&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
      &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
      &lt;value&gt;/home/lin/hdfs/dfs/data&lt;/value&gt;
    &lt;/property&gt;
</code></pre>
<p>格式化HDFS：<br>
hadoop namenode -format<br>
# 然后去刚才上面配置的这个路径里面是否有新东西出现： /home/lin/hdfs<br>
开启HDFS (先进入 sbin目录下，   sbin 和 etc bin是同级的， 这里说的都是hadoop里的目录):<br>
./start-dfs.sh</p>
<pre><code># 一路yes, 若让你输入密码， 则输入对应服务器的密码。（我这里都是本机）
# 若提示权限错误，继续往下看（支线）
    sudo passwd root              # 激活ubuntu的root用户，并设置密码
    vi /etc/ssh/sshd_config：
        PermitRootLogin yes       # 任意位置添加这个（此选项之前可能是存在的注释后再添加）
    service ssh restart 
</code></pre>
<p>查看HDFS里面根目录 / 的内容：<br>
hadoop fs -ls /<br>
向HDFS里面根目录 / 中 传入文件：<br>
echo test &gt; test.txt     # 首先，随便建立一个文件</p>
<pre><code>hadoop fs -put test.txt /     # 向HDFS里面根目录 / 中 传入文件

hadoop fs -ls /          # 再次查看，就发现有 test.txt 文件了。
</code></pre>
<p>从HDFS里面根目录 / 中 读取文件test.txt：<br>
hadoop fs -text /test.txt<br>
从Hadoop WebUI 中查看刚才的文件是否存在：<br>
http://192.168.0.108:50070/           # 50070是默认端口</p>
<pre><code>点击右侧下拉框 &quot;Utilities&quot;  -&gt;  &quot;Browser the file system &quot;
清晰可见， 我们的test.txt 躺在那里~
</code></pre>
<h3 id="yarn配置">YARN配置</h3>
<p>还是etc/hadoop这个目录:<br>
cp mapred-site.xml.template mapred-site.xml</p>
<pre><code>vi mapred-site.xml：
    &lt;property&gt;
      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
      &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
    
vi yarn-site.xml:
    &lt;property&gt;
      &lt;name&gt;yarn.nodemanager.services&lt;/name&gt;
      &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;     
</code></pre>
<p>启动 YARN: （还是 sbin目录下）<br>
./start-yarn.sh</p>
<pre><code># 同样，若有密码，输入机器的密码即可
</code></pre>
<p>从Hadoop WebUI 中查看YARN：<br>
http://192.168.0.108:8088/</p>
<h1 id="安装mysql">安装MySQL</h1>
<p>下面要用MySQL, 所以单独提一下MySQL 的 安装与配置:<br>
其实MySQL是不需要单独说的, (但我装的时候出现了和以往的不同经历), 所以还是说一下吧:<br>
apt-get install mysql-server-5.7<br>
安装容易, 不同版本的MySQL配置有些鸡肋 (我用的是 Ubuntu19):<br>
vi /etc/mysql/mysql.conf.d/mysqld.cnf:<br>
bind-address     0.0.0.0               # 找到修改一下即可<br>
修改密码+远程连接权限 (默认无密码):<br>
mysql                # 啥参数也不用加, 直接就能进去<br>
use mysql<br>
update mysql.user set authentication_string=password(&quot;123&quot;) where user=&quot;root&quot;;<br>
update user set plugin=&quot;mysql_native_password&quot;;<br>
flush privileges;</p>
<pre><code>select host from user;
update user set host ='%' where user ='root';
flush privileges;
</code></pre>
<p>重启服务:<br>
systemctl restart mysql<br>
服务端连接测试:<br>
mysql -uroot -p<br>
# 密码 123<br>
远程连接测试 (Navicat):<br>
成功</p>
<h1 id="安装hive">安装Hive</h1>
<p>下载: https://archive.apache.org/dist/hive/hive-2.3.6/apache-hive-2.3.6-bin.tar.gz</p>
<p>解压：<br>
tar -zxvf apache-hive-2.3.6-bin.tar.gz<br>
配置Hive：<br>
vi ~/.bashrc<br>
export HIVE_HOME=/home/lin/hive/apache-hive-2.3.6-bin<br>
export PATH=<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>H</mi><mi>I</mi><mi>V</mi><msub><mi>E</mi><mi>H</mi></msub><mi>O</mi><mi>M</mi><mi>E</mi></mrow><mi mathvariant="normal">/</mi><mi>b</mi><mi>i</mi><mi>n</mi><mo>:</mo></mrow><annotation encoding="application/x-tex">{HIVE_HOME}/bin:</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span><span class="mord">/</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span></span></span>PATH<br>
激活配置：<br>
source ~/.bashrc<br>
Hive其他相关配置(同理进入hive解压目录的 conf目录中):<br>
cp hive-env.sh.template hive-env.sh<br>
vi hive-env.sh:<br>
HADOOP_HOME=/home/lin/hadoop/hadoop-2.7.7<br>
Hive-MySQL相关配置(同是在 conf目录下):<br>
vi hive-site.xml:  (特别注意后两个 <property> 里面的内容, 自己修改一下用户名和密码)<br>
<configuration><br>
<property><br>
<name>javax.jdo.option.ConnectionURL</name><br>
<value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false</value><br>
</property><br>
<property><br>
<name>javax.jdo.option.ConnectionDriverName</name><br>
<value>com.mysql.jdbc.Driver</value><br>
</property><br>
<property><br>
<name>javax.jdo.option.ConnectionUserName</name><br>
<value>root</value><br>
</property><br>
<property><br>
<name>javax.jdo.option.ConnectionPassword</name><br>
<value>123</value><br>
</property><br>
</configuration></p>
<p>下载 jdbc-mysql驱动,并放入Hive中,操作如下  (因为我们上面hive-site.xml 用的是mysql):</p>
<ol>
<li>
<p>首先下载: http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.48/mysql-connector-java-5.1.48.jar</p>
</li>
<li>
<p>将此jar文件放入 hive的lib目录中(和 conf同级):</p>
</li>
<li>
<p>将此jar文件再copy一份放入 spark的 jar目录下（为了后续jupyter直连MySQL(不通过Hive)使用）<br>
初始化(先确保之前的HDFS和MySQL已经启动):<br>
schematool -dbType mysql -initSchema</p>
<h1 id="注意事项1-这个用命令初始化的-步骤是-hive-2-才需要做的">注意事项1: 这个用命令初始化的 步骤是 Hive 2.+ 才需要做的</h1>
<h1 id="注意事项2-初始化一次即可-多次初始化会使得mysql有重复的键-报错">注意事项2: 初始化一次即可, 多次初始化,会使得MySQL有重复的键. 报错.</h1>
</li>
</ol>
<p>开启 metastore 服务：<br>
nohup hive --service metastore &amp;<br>
检测是否初始化(去MySQL表中查看):<br>
use hive<br>
show tables;<br>
# 若有数据,则说明初始化成功<br>
启动hive:<br>
hive<br>
建库, 建表测试 (注意,千万不要用 user这种关键字当作表名等):<br>
HIVE中输入:<br>
create database mydatabase;<br>
use mydatabase;<br>
create table person (name string);</p>
<pre><code>MySQL中查看Hive表的相关信息: 
    select * from TBLS;                 # 查看所有表结构信息
    select * from COLUMNS_V2;           # 查看所有列的信息
</code></pre>
<p>向Hive导入文件:<br>
vi hive_data.txt: (写入如下两行)<br>
tom catch jerry<br>
every one can learn AI</p>
<pre><code>load data local inpath '/home/lin/data/hive_data.txt' into table person;
</code></pre>
<p>查询:<br>
select * from person;<br>
PySpark客户端配置连接代码:<br>
import findspark<br>
findspark.init()</p>
<pre><code>from pyspark.sql import SparkSession    

spark = SparkSession.builder\
    .appName(&quot;Spark Hive Example&quot;)\
    .master(&quot;local[*]&quot;)\
    .config(&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;)\
    .enableHiveSupport()\
    .getOrCreate()
    
spark.sql(&quot;use mydatabase&quot;).show()
spark.sql('show tables').show()
</code></pre>
<h1 id="安装spark">安装Spark</h1>
<p>下载：<a href="https://www.apache.org/dyn/closer.lua/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz">spark-2.4.4-bin-hadoop2.7.tgz</a>：<br>
粗糙传送门：<a href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a><br>
详细传送门：<a href="http://ftp.riken.jp/net/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz"><strong>http://ftp.riken.jp/net/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz</strong></a></p>
<p>解压：<br>
tar -zxvf 下载好的spark-bin-hadoop<br>
配置spark：<br>
vi ~/.bashrc<br>
export SPARK_HOME=home/lin/spark/spark-2.4.4-bin-hadoop2.7<br>
export PATH=<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>S</mi><mi>P</mi><mi>A</mi><mi>R</mi><msub><mi>K</mi><mi>H</mi></msub><mi>O</mi><mi>M</mi><mi>E</mi></mrow><mi mathvariant="normal">/</mi><mi>b</mi><mi>i</mi><mi>n</mi><mo>:</mo></mrow><annotation encoding="application/x-tex">{SPARK_HOME}/bin:</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span><span class="mord">/</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span></span></span>PATH<br>
激活配置：<br>
source ~/.bashrc</p>
<h1 id="最后一步python环境可能出错">最后一步（Python环境可能出错）</h1>
<p>pyspark脚本默认调用的是 &quot;python&quot; 这个名， 而ubuntu默认只有&quot;python&quot; 和 &quot;python3&quot;。<br>
所以我们需要做如下软连接，来使得可以输入python， 直接寻找python3.7命令（不要用alias）<br>
ln -s /usr/bin/python3.7 /usr/bin/python</p>
<h1 id="测试">测试</h1>
<p>服务端直接输入命令：<br>
pyspark<br>
或远程浏览器输入:<br>
http://192.xx.xx.xx:4040/jobs</p>
<h1 id="远程使用jupyter连接">远程使用Jupyter连接</h1>
<p>安装 Jupyter Notebook:</p>
<pre><code>pip3 install jupyter   
# 若新环境，需要安pip:  apt-get install python3-pip
</code></pre>
<p>pip 安装 findspark 和 pyspark</p>
<pre><code>pip install pyspark
pip install findspark                          （Linux服务端）
</code></pre>
<p>启动 Jupyter Notebook 服务（--ip指定 0.0.0.0），(--allow-root若不加上可能会报错)</p>
<pre><code>jupyter notebook --allow-root --ip 0.0.0.0     （Linux服务端）
</code></pre>
<hr>
<p>下面说的是Jupyter Notebook 客户端（Windows10）<br>
下面两行findspark代码必须放在每个py脚本的第一行</p>
<pre><code>import findspark
findspark.init()

PYSPARK_PYTHON = &quot;/usr/bin/python&quot;
os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHON
os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHON
</code></pre>
<p>然后才可正常写其他代码<br>
from pyspark import SparkConf, SparkContext</p>
<pre><code>sc = SparkContext(
    master='local[*]',   # 下面会讲这个参数
    appName='myPyspark', # 随便起名
) 
# 这句话，就把 spark启动起来了，然后才可以通过浏览器访问了。 4040
# 如果你 python魔法玩的6，那么提到上下文，你应该会自动想到 with语句 （__enter__,__exit__）
# 不写参数，本地运行，这样也是可以的，  sc = SparkContext() 

raw_data = [1,2,3]
rdd_data = sc.parallelize(raw_data)  # python列表类型 转 spark的RDD
print(rdd_data)
raw_data = rdd_data.collect()        # spark的RDD 转回到 python列表类型
print(raw_data)

sc.stop()    # 关闭spark, 同理，浏览器也就访问不到了。
</code></pre>
<p>解释 SparkContext 的 master参数：</p>
<ol>
<li>&quot;local&quot; : 表示只用一个线程，本地运行。</li>
<li>&quot;local[*]&quot; : 表示用(cpu的个数)个线程，本地运行。</li>
<li>&quot;local[n]&quot; : 表示用n个线程，本地运行。</li>
<li>&quot;spark://ip:host&quot; : 连其他集群<br>
回顾环境问题 并 解释 &quot;本地&quot; 的概念：</li>
<li>在 Linux 中 安装了 Spark全套环境。</li>
<li>在 Linux 中 安装了 Jupyter， 并启动了 Jupyter Notebook 服务。</li>
<li>在 Win10 中 远程连接 Linux中的 &quot;Jupyter Notebook&quot;服务 写业务代码（相当于客户端连接）<br>
所以， 之前所说的 &quot;本地&quot;, 这个词归根结底是相对于 Linux来说的，我们写代码一直操作的是Linux。</li>
</ol>
<h1 id="通常使用spark-submit">通常使用spark-submit</h1>
<p>首先：我们自己编写一个包含各种 pyspark-API 的 xx.py 脚本<br>
如果：你用了我上面推荐的 Jupyter Notebook，你会发现文件是.ipynb格式，可以轻松转.py<br>
<img src="/img/bVbzB5s" alt="image.png" loading="lazy"><br>
最后提交py脚本：<br>
spark-submit --master local[*] --name myspark /xx/xx/myspark.py</p>
<pre><code># 你会发现 --master 和 --name  就是上面我们代码中配置的选项，对号入座写入即可。
# /xx/xx/myspark.py 就是 py脚本的绝对路径。 喂给spark，让他去执行。即可。
</code></pre>
<h1 id="standalone部署spark">Standalone部署Spark</h1>
<p>介绍：<br>
Standalone部署需要同时启动：<br>
master端<br>
slave 端<br>
按着下面配置，最后一条  ./start-all.sh 即可同时启动。<br>
查看 JAVA_HOME环境变量。<br>
echo $JAVA_HOME</p>
<pre><code># 记住结果，复制出来
</code></pre>
<p>进入conf目录，做一些配置（conf和spark中的bin目录同级）：<br>
cp spark-env.sh.template spark-env.sh<br>
vi spark-env.sh：（里面写）<br>
JAVA_HOME=上面的结果</p>
<pre><code>cp slaves.template slaves
vi slaves: （localhost改成本机机器名）
    lin
</code></pre>
<p>上面配置完毕后，进入sbin目录（和上面的conf在一个目录中）<br>
./start-all.sh          # 启动</p>
<pre><code># 若提示权限错误，继续往下看（支线）
sudo passwd root              # 激活ubuntu的root用户，并设置密码
vi /etc/ssh/sshd_config：
    PermitRootLogin yes       # 任意位置添加这个（此选项之前可能是存在的注释后再添加）
service ssh restart
</code></pre>
<p>启动没报错，会给你弹出一条绝对路径的日志文件 xxx<br>
cat xxx         # 即可看见启动状态 ，各种日志信息</p>
<pre><code>其中有几条信息:
    Successfully started service 'WorkerUI' on port 8082  （浏览器访问 8082端口）
    Successfully registered with master spark://lin:7077  （代码上下文访问）
其中，有些信息可能未打印出来： 建议浏览器中：( 8080-8082 )端口都可以尝试一下。        
</code></pre>
<p>输入命令，查看启动状态：<br>
jps             # 若同时有 worker 和 master 说明启动成功<br>
测试：<br>
pyspark --master spark://lin:7077</p>
<pre><code># WebUI 的 Worker端，就可看见有 一个Job被添加了进来
</code></pre>
<h1 id="yarn部署spark">YARN部署Spark</h1>
<p>配置：<br>
echo $HADOOP_HOME<br>
# 我的是 /home/lin/hadoop/hadoop-2.7.7</p>
<pre><code>进入spark解压包的路径的 conf 目录中:
vi spark-env.sh:   ( etc/hadoop前面就是刚才 echo出来的，  etc/hadoop大家都是一样的)
    HADOOP_CONF_DIR=/home/lin/hadoop/hadoop-2.7.7/etc/hadoop
</code></pre>
<p>启动spark：<br>
spark-submit --master yarn --name myspark  script/myspark.py<br>
# 注意 --master 的值改成了 yarn ， 其他不变。</p>
<pre><code>或者你可以：
     pyspark --master yarn     
看到启动成功，说明配置成功
</code></pre>
<h1 id="spark历史服务配置">Spark历史服务配置</h1>
<p>痛点：有时我们的spark上下文 stop后，WebUI就不可访问了。<br>
若有未完成，或者历史信息, 也就看不到了。<br>
这时我们配置 history 服务就可在 context stop后，仍可查看 未完成job。</p>
<p>预新建一个HDFS目录myhistory (根路径下),下面用得到：<br>
hadoop fs -mkdir /myhistory<br>
首先，进入 spark解压包的 conf目录下：<br>
cp spark-defaults.conf.template spark-defaults.conf</p>
<pre><code>vi spark-defaults.conf: (解开如下注释, lin本机名称, 放在HDFS的根路径下的myhistory)
    spark.eventLog.enabled           true
    spark.eventLog.dir               hdfs://lin:8020/myhistory
    
vi spark-env.sh:  (我们之前 把template 复制过一次，所以这次直接编辑即可)
    SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://lin:8020/myhistory&quot;
</code></pre>
<p>启动（进入 spark解压包的 sbin目录下）：<br>
./start-history-server.sh</p>
<pre><code># cat 输入的信息（日志文件）。 即可查看是否启动成功
# WebUI默认是 ：http://192.168.0.108:18080/
</code></pre>
<p>测试：<br>
浏览器中访问History WebUI： http://192.168.0.108:18080/<br>
发现啥也没有： 这是正常的，因为我们还没运行 spark context主程序脚本。<br>
---------------------------------------------------------------------<br>
运行spark-context主程序脚本：<br>
spark-submit script/myspark.py<br>
# 这个脚本是随便写的，没什么意义。 不过里面有个我们常用的一个注意事项！！！<br>
# 我的这个脚本的 context 用完，被我 stop了<br>
# 所以我们访问不到它的运行状态的 Spark Context 的 WebUI</p>
<pre><code>    # 但是我们刚才辛辛苦苦配置Spark history 服务，并启动了它。
    # 所以 context的信息，被写入了我们刚才配置的 Spark history 中
    # 所以 我们再次访问 Spark history WebUI 即可看到有内容被写入进来。
---------------------------------------------------------------------
再次访问History WebUI： http://192.168.0.108:18080/
你就会发现，里面有内容了(spark history服务已经为我们干活了)~~~~   
</code></pre>
<h1 id="免密码登录">免密码登录</h1>
<p>环境Ubuntu(CentOS应该也可，很少用)<br>
免密码登录设置：<br>
cd ~<br>
ssh-keygen -t rsa -P &quot;&quot;<br>
cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys<br>
chmod 600 .ssh/authorized_keys<br>
注意几种情况：<br>
你如果是root用户，那么你需要切换到 /root/ 执行上面的命令<br>
如果是普通用户， 那么你需要切换到 /home/xxx/ 执行上面的命令</p>
<pre><code>这个要特别注意一下，有时候用 sudo -s ,路径是没有自动切换的。
需要我们自己手动切换一下 &quot;家&quot; 路径
</code></pre>
<h1 id="自定义脚本启动服务">自定义脚本启动服务</h1>
<p>下面内容仅供个人方便， shell不熟，用py脚本了, 你随意。<br>
vi start.py: (此脚本用于启动上面配置的 HDFS,YARN,SparkHistory 和 Jupyter Notebook)<br>
import os<br>
import subprocess as sub</p>
<pre><code>###### 启动  HDFS + YARN ###############
hadoop_path = os.environ['HADOOP_HOME']
hadoop_sbin = os.path.join(hadoop_path, 'sbin')

os.chdir(hadoop_sbin)
sub.run('./start-dfs.sh')
sub.run('./start-yarn.sh')

###### 启动 SparkHistory ##############
spark_path = os.environ['SPARK_HOME']
spark_sbin = os.path.join(spark_path, 'sbin')
os.chdir(spark_sbin)
sub.run('./start-history-server.sh')

###### 启动  Jupyter Notebook ###############
# home_path = os.environ['HOME']
home_path = '/home/lin'

os.chdir(home_path)
sub.run('jupyter notebook --allow-root --ip 0.0.0.0'.split())
</code></pre>
<p>之后每次重启，就不用进入每个目录去启动了。直接一条命令：<br>
sudo python start.py<br>
nohup hive --service metastore &amp;<br>
查看本脚本启动相关的WebUI：<br>
HDFS:            http://192.168.0.108:50070/<br>
YARN:            http://192.168.0.108:8088/<br>
SparkHistory:    http://192.168.0.108:18080/<br>
另附其他 WebUI：<br>
spark:           http://192.168.0.108:4040/<br>
standalone启动指定的端口(如果你使用的 standalone方式，而不是local,可能用到如下端口):<br>
pyspark --master spark://lin:7077</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI => Seq2Seq+Attention+Transformer(简)]]></title>
        <id>https://cythonlin.github.io/post/ai-greater-seq2seqattentiontransformerjian/</id>
        <link href="https://cythonlin.github.io/post/ai-greater-seq2seqattentiontransformerjian/">
        </link>
        <updated>2020-09-29T04:13:05.000Z</updated>
        <content type="html"><![CDATA[<h1 id="数据预处理tf20-keras-preprocessing">数据预处理（TF20-Keras-Preprocessing）</h1>
<h3 id="我们自己的普通数据集常用">我们自己的普通数据集（常用）</h3>
<p>主要使用tensorflow.keras.preprocessing这个库中的（image, text，sequence）这三个模块。<br>
text： 可以用来 （统计词频，分字，word_2_id, id_2_word等操作。）<br>
sequence 可以（给句子做结构化操作（填充0，裁剪长度））<br>
from tensorflow.keras.preprocessing.text import Tokenizer    # 主干，句子编码<br>
from tensorflow.keras.preprocessing.sequence import pad_sequences # 辅助，填充，剪枝</p>
<pre><code>q1 = '欢 迎 你 你 你'
q2 = '我 很 好'
q_list = [q1,q2]    # 需要特别注意，因为此API对英文友好，所以，我们必须把句子用 空格 隔开输入

token = Tokenizer(
    num_words=2, # num_words代表设置过滤num_words-1个词频， 例如num_words=2，
                 # 那么过滤掉2-1=1个词频， 所以一会你会看到下面词频为1的都被过滤掉了
)  # 这里面参数很多，还有标点符号过滤器等
token.fit_on_texts(q_list)  # 把原始句子集合，放进去拟合一下（封装成一个类）

print(token.document_count) # 2    # 句子个数
print(token.word_index)  # {'你': 1, '欢': 2, '迎': 3, '我': 4, '很': 5, '好': 6}   # word_2_id
print(token.index_word)  # {1: '你', 2: '欢', 3: '迎', 4: '我', 5: '很', 6: '好'}   # id_2_word
print(token.word_counts) # OrderedDict([('欢', 1), ('迎', 1), ('你', 3), ('我', 1), ('很', 1), ('好', 1)])  # 统计词频

seq = token.texts_to_sequences(q_list) # 先把所有的输入，变成一一变成编码化
print(seq) # [[1, 1, 1], []]     # 会不会好奇？数据怎么没了？因为我们上面设置了过滤词频为1的都过滤了

pad_seq = pad_sequences(
        seq,                # 输入编码化后的 句子
        maxlen=2,           # 统一句子最大长度
        padding='pre',      # 不足的补0， 从前面补0， （也可以用 post，代表后面）
        truncating='pre'    # 多余的长度裁剪，从前面裁剪
    )
print(pad_seq)     # 打印一下我们填充后的句子形状。
# [
#   [1 1],      # 如你所愿,最大长度为2，[1,1,1] 已经裁剪成了 [1,1]
#   [0 0],      # 如你所愿，之前[] ，已经都填满了0
# ]
</code></pre>
<p>虽然我们用不到 image这个模块数据增强模块，但是我把了解的API也写出来。<br>
train_datagen = keras.preprocessing.image.ImageDataGenerator( # 数据增强生成器（定义）<br>
rescale=1. / 255,         # 数据归一化<br>
rotation_range = 40,      #  -40-40  随机角度 （数据增强）<br>
width_shift_range = 0.2,  # 宽度位移（0-20%随机选个比例去平移） （数据增强）<br>
height_shift_range = 0.2, # 高度位移（同上）   （数据增强）<br>
shear_range=0.2,          # 图片剪切（0.2）    （数据增强）<br>
zoom_range=0.2,           # 图片缩放（0.2）    （数据增强）<br>
horizontal_flip=True,     # 图片随机水平反转    （数据增强）<br>
fill_mode='nearest',      # 图片填充像素（放大后失帧）用附近像素值来填充 （数据增强）<br>
)</p>
<pre><code># train_generator = train_datagen.flow_from_dataframe()  # 如果你用Pandas，你可以选这个
train_generator = train_datagen.flow_from_directory(     # 从文件中读取（Kaggle）
    train_dir,                       # 图片目录
    target_size = (height, width),   # 图片读取进来后缩放大小
    batch_size = batch_size,         # 就是批次
    seed=6,                          # 随机化种子
    shuffle=True,                    # 样本随机打散训练，增强模型泛化能力
    class_mode='categorical',        # label格式，是否需要one_hot， 是
)
...
...
train_num = train_generator.samples  # 打印样本形状

history = model.fit_generator(       # 注意我们上面是用的数据生成器，所以这要用 fit_generator
    train_generator,
    steps_per_epoch=train_num//batch_size, # 每个epoch多少 step(因为数据增强API是生成器方式，所以需要自己手动计算一下)
    epochs=epochs,
    validation_data=valid_generator,  # 如果你有验证集，你也可以用这个。否则可以不用
    validation_steps=valid_num//batch_size # 同上
)
</code></pre>
<h1 id="seq2seq">Seq2Seq</h1>
<h3 id="思想">思想</h3>
<pre><code>语言不同，那么我们可以搭建桥梁。 
即使我们表面上不相同。 但是我们映射到这个桥梁上的结果是几乎类似的。
</code></pre>
<h3 id="样本句子长度统一">样本句子长度统一</h3>
<p>为什么每个句子的长度需要统一？<br>
因为，每个句子for循环操作会很耗算力， 而转化为矩阵/向量化操作，会节约太多算力。<br>
因为矩阵运算严格要求样本的形状，所以每个句子的长度需要一致<br>
如何做到句子长度统一？<br>
填0， 对应TF操作就是padding， 不过TF20 的keras预处理包中已经有 成品的数据统一化操作。<br>
并且还具有 word_2_id，词向量编码操作。</p>
<h3 id="组成">组成</h3>
<ol>
<li>编码器 （输入每条样本句子的每个单词， 编码器的最后一个RNN单元，浓缩了整个句子的信息）</li>
<li>中间向量 （作为中间特征桥梁， 用来保存，输入进来的整个句子）</li>
<li>解码器 （中间向量作为解码器第一个RNN单元的输入，而每个单元的输出y,作为下一个单元的输入）<br>
其中解码器部分的输出y会用 softmax 对 词库（词典）求多分类概率。<br>
然后求损失（MSE或者CrossEntropy）<br>
注意了： softmax求出的概率该如何选择，这是个问题:<br>
假如: 每个单元的输出y的概率都取最大值, 那么可能一步错，步步错。 太极端了（贪心搜索）<br>
接下来，聊一聊一周 集束搜索的算法 BeamSearch</li>
</ol>
<h3 id="beamsearch">BeamSearch</h3>
<p>由于贪心搜索（只取概率的一个最大值，的结果不尽人意。所以 BeamSearch来啦）<br>
BeamSearch的主要思想:<br>
只取一个太冒险了，所以:     BeamSearch 取每个经过softmax输出概率集合的 Top-N个<br>
Top-N: 的 N 代表你保留几个概率   （举一反三理解: 贪心算法就是 Top-1）<br>
假如我们取Top-3个<br>
那么你一个RNN节点的预测y将会保留3个概率值， 并将这3个概率值作为 下一个节点的输入。<br>
具体流程看:下图 (可能有点丑)<br>
然后，我们会选择出:        3 个 &quot;红线&quot; 最优路径。<br>
最终: 我们通过单独的语言模型，来从这 3 个 &quot;红线&quot; 较优路径中，选出一个 最优路径。<br>
<img src="/img/bVbyAZs" alt="clipboard.png" loading="lazy"></p>
<h1 id="attention注意力机制">Attention(注意力机制)</h1>
<h3 id="前情回顾">前情回顾</h3>
<p>Seq2Seq 的 Encoder部分虽然用的是 高效的 LSTM，并且也很好的解决了，记忆的问题。<br>
但是他不能很好的解决每个单词的权重分配问题。<br>
虽然: Encoder的所有单元都会通过LSTM的记忆传递， 输入进“中间桥梁向量”。<br>
但是: 还是有&quot;偏心&quot;成分, 最后一个LSTM单元信息一定是最浓的。 （新鲜的，热乎的）<br>
所以: 你第1个LSTM单元的信息，或者说前面的LSTM单元的信息，这些记忆到最后可能会被稀释。<br>
为了解决上面的问题, Attention就出来帮忙了~~~</p>
<h3 id="attentioin原理">Attentioin原理</h3>
<p>我觉得墨迹半天不如自己画一张图~~~ （只会mspaint画图）<br>
<img src="/img/bVbyBuW" alt="clipboard.png" loading="lazy"><br>
上图中计算权重那里&quot;通过一个函数，可以是求相似度&quot;， 我简写了。 其实有两种常用的方式：<br>
Bahdanau注意力:<br>
weight = FC层( tanh ( FC层(Encoder的每个输出y) + FC层(Decoder的一个H) ) )<br>
luong注意力:<br>
weight = Encoder的每个输出y @ W随机权重矩阵 @ Decoder的一个H    # @是TF20的矩阵乘法操作符<br>
无论使用上面哪种: 都要套一层 Softmax<br>
weight = softmax(weight, axis=1)<br>
注意力向量C = sum( weight * Encoder的每个输出y , axis=1)   # 加权求和，最终得到一个向量<br>
Decoder的下一个输入 = concat( 注意力向量C, 上一个预测y4 )</p>
<h1 id="transformer">Transformer</h1>
<p>第一印象挑明： 他是一种无RNN的一种特殊的 Seq2Seq 模型。</p>
<p>RNN-LSTM-GRU虽然这些NN的主要特色就是&quot;时间序列&quot;。（缺点：慢，记忆弥散）<br>
但是我们上面说了，要想取得好的效果。那么需要加Attention。<br>
于是有人想到了，既然Attention效果这么好，为什么不直接用Attention呢？<br>
Attention效果虽好，关联性强，但是它不能保证时间序列模式。<br>
于是后来出现了 Transformer。（既能保证记忆注意力，又能保证时间序列）。具体如下！</p>
<h3 id="transformer整体结构组成">Transformer整体结构组成</h3>
<figure data-type="image" tabindex="1"><img src="/img/bVbA3uH" alt="image.png" loading="lazy"></figure>
<h3 id="self-attention">Self-Attention</h3>
<p>self-attention原理就是各种链式矩阵乘法（并行计算，可用GPU加速）<br>
self-attention计算过程如下：（假设输入句子切分单词为：矩阵X = [&quot;早&quot;,&quot;上&quot;,&quot;好&quot;]）<br>
矩阵X @ 权重矩阵Q（Q1，Q2，Q3）=&gt; Q矩阵（Q1，Q2，Q3）<br>
矩阵X @ 权重矩阵K（Q1，Q2，Q3）=&gt; K矩阵（Q1，Q2，Q3）<br>
矩阵X @ 权重矩阵V（Q1，Q2，Q3）=&gt; V矩阵（Q1，Q2，Q3）</p>
<pre><code>α = softmax( (Q矩阵 @ K矩阵) / q^0.5 )
self_attention = α @ V矩阵
# 单词1 = Q1*K1*V1 + Q1*K2*V2 + Q1*k3*V3
# 用自己的Q，查别人的KV，加权求和，最终得出的就是自己（自身单词的注意力）
</code></pre>
<h3 id="multi-head-self-attention">Multi-Head Self-Attention</h3>
<p>Multi-Head Attention 对 Self-Attention 对了如下扩展：<br>
self-attention:             一组 Q矩阵，K矩阵，V矩阵<br>
Multi-Head Self-Attention:  多组 Q矩阵，K矩阵，V矩阵<br>
扩张为多头注意力的过程：<br>
Q @ W ====&gt; [Q1, Q2, Q3]<br>
K @ W ====&gt; [K1, K2, K3]<br>
V @ W ====&gt; [V1, V2, V3]</p>
<pre><code>可理解为，多个卷积核的意思能提取不同特征的意思。
</code></pre>
<h3 id="position-encoder">Position Encoder</h3>
<p>上述的self-attention有个问题， 我们没有用到RNN等序列NN，那么矩阵相乘的过程中。<br>
单词的计算顺序可能是不同的。<br>
那么如何保证让他们位置有条不紊？<br>
可以使用位置编码，融入到Embedding，形成带有时间序列性质的模型。<br>
可自行查找计算位置编码的博文。</p>
<h3 id="传送门">传送门</h3>
<p>至于Transformer，现在官方已经有TF20和Pytorch的库了。<br>
传送门如下。<br>
<a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a><br>
Transformer延申的各种模型，像Bert等也有可调用的API<br>
<a href="https://huggingface.co/transformers/">https://huggingface.co/transformers/</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI => Pytorch与Tersorflow2.0简单对比]]></title>
        <id>https://cythonlin.github.io/post/ai-greater-pytorch-yu-tersorflow20-jian-dan-dui-bi/</id>
        <link href="https://cythonlin.github.io/post/ai-greater-pytorch-yu-tersorflow20-jian-dan-dui-bi/">
        </link>
        <updated>2020-09-29T04:12:32.000Z</updated>
        <content type="html"><![CDATA[<h1 id="前言">前言</h1>
<p>目前一些模型API尚未迁移到TF20中。 eg: CRF，Seq2Seq等<br>
如果退回TF10，有些伤。<br>
倒不如转至Torch。<br>
Pytorch的大部分思想和TF20大致相似。</p>
<p>至于安装，GPU我前面说过TF20。这里不赘述。<br>
官档安装：<a href="https://pytorch.org/get-started/locally/#start-locally">https://pytorch.org/get-started/locally/#start-locally</a></p>
<h3 id="注意">注意</h3>
<p>本文几乎通篇以代码案例 和 注释标注 的方式解释API。(模型的训练效果不做考虑。只看语法)<br>
你如果懂Tensorflow2.0（Stable），那么你看本文一定不费劲。<br>
Torch和TF20 很像！！！<br>
因此一些地方，我会列出 TF20 与 Torch的细节对比。</p>
<h1 id="开门案例1-mnist">开门案例1-MNIST</h1>
<h3 id="模块导入">模块导入</h3>
<pre><code>import torch
from torch import nn, optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
</code></pre>
<h3 id="数据预处理">数据预处理</h3>
<pre><code>data_preprocess = transforms.Compose([  # 顶预定数据处理函数，类似map()里的函数句柄
    transforms.Resize(28,28),           # 变形
    transforms.ToTensor(),              # numpy 转 Tensor
])

trian_dataset = datasets.MNIST(         # TF20在keras.datasets中,未归一化（0-255）
    '.',                                # 下载至当前目录， （图片0-1，已经被归一化了）
    train=True,                         # train=True， 代表直接给你切出 训练集
    download=True,                      # True，若未下载，则先下载
    transform=data_preprocess,          # 指定数据预处理函数。第一行我们指定的
)

test_dataset = datasets.MNIST(
    '.', 
    train=False, # False代表测试集       # 就说下这里， False代表 给你切出测试集
    download=True,
    transform=data_preprocess,
)

train = DataLoader(     # 对应TF20中的 tf.data.Dataset对数据二次预处理（分批，乱序）
    trian_dataset,      # 把上面第一次预处理的数据集 加载进来
    batch_size=16,      # mini-batch
    shuffle=True,       # 乱序，增强模型泛化能力
)

test = DataLoader(
    test_dataset,
    batch_size=16,
    shuffle=True,
)
</code></pre>
<h3 id="mnist模型定义-训练代码">MNIST模型（定义-训练代码）</h3>
<pre><code># 模型定义部分
class MyModel(nn.Module):             # TF20是 tk.models.Model
    def __init__(self):               # TF20 也是 __init__()
        super().__init__()

        self.model = nn.Sequential(   # tk.models.Sequential , 并且 TF里面 需要加一个 []
            nn.Linear(28*28, 256),    # tk.layers.Dense(256)
            nn.ReLU(),                # tk.layers.Relu()

            nn.Linear(256, 128),      # tk.layers.Dense(128)
            nn.ReLU(),               

            nn.Linear(128, 10),       # tk.layers.Dense(10)            
        ) 
    def forward(self, x):  # TF20是 __call__()
        x = x.view( x.size(0), 28*28 )      # x.view ==&gt; tf.reshape   x.size ==&gt; x.shape[0]
        y_predict = self.model(x)

        return y_predict
# -------------------------------华丽分割线---------------------------------     
# 模型训练部分
def main():
    vis = visdom.Visdom()
    model = MyModel()
    loss_ = nn.CrossEntropyLoss()     # 会将 y_predict自动加一层 softmax
    optimizer = optim.Adam(model.parameters())     # TF20: model.trainable_variables
    
    # visdom可视化
    # 这步是初始化坐标点，下面loss会用这个直接更新
    vis.line(
        [0],                    # x坐标
        [0],                    # y坐标
        win='loss',             # 窗口名称
        opts={'title': 'loss'}, # 窗口标题
    )
    
    for epoch in range(10):   # epochs
        for step, (x, y_true) in enumerate(train):
            y_predict = model(x)
            loss = loss_(y_predict, y_true)
            optimizer.zero_grad()           #  优化器清零 
            loss.backward()                 #  梯度计算
            optimizer.step()                #  梯度下降更新 tp.gradient(loss, variables)。 
            
            # 在上面的定义的基础上更新追加画点-连成线
            vis.line(
                [loss.item()],
                [step],
                win='loss',
                update='append', # 追加画点，而不是更新覆盖
            )
        print(loss.item())                  #  .item()  =&gt; 相当于 tensorflow 的 numpy()
        
        if epoch % 2 == 0:
            total_correct_samples = 0       # 用于记录（预测正确的样本的 总数量）
            total_samples = 0               # 用于记录（样本的 总数量）

            for x_test, y_test in test:
                y_pred = model(x_test)
                y_final_pred = y_pred.argmax(dim=1)     # TF20的坐标轴参数是 axis
                
                 # 每一批是 batch_size=16，我们要把它们都加在一起
                total_correct_samples += torch.eq(y_final_pred, y_test).float().sum().item()
                
                # 这里提一下 eq() 和 equal() 的返回值的区别， 自己看，我们通常用 eq
                # print( torch.equal( torch.Tensor([[1,2,3]]), torch.Tensor([[4,5,6]] ) ) ) 
                #结果:  False
                # print( torch.eq( torch.Tensor([[1,2,3]]), torch.Tensor([[4,5,6]] ) ) ) 
                #结果:  tensor([[0, 0, 0]], dtype=torch.uint8)

                per_sample = x_test.size(0)   # 再说一次， size(0) 相当于TF xx.shape[0]
                # 获取每批次样本数量, 虽然我们知道是 16
                # 但是最后一个batch_size 可能不是16，所以要准确获取。
                total_samples += per_sample

            acc = total_correct_samples / total_samples
            print(f'epoch: {epoch}, loss: {loss}, acc: {acc}')   
            
            # 测试部分
            vis.line(
                [acc],
                [step],
                win='acc',
                update='append', # 追加画点，而不是更新覆盖
            )

            x, label = iter(test).next()
            target_predict = model(x).argmax(dim=1)
            
            # 画出测试集图片
            viz.images(x, nrow=16, win=&quot;test_x&quot;, opts={'title': &quot;test_x&quot;}) 
            vis.text(    # 显示预测标签文本
                str(target_predict.detach().numpy() ),
                win = 'target_predict',
                opts = {&quot;title&quot;: target_predict}
            )
            vis.text(    # 显示真值文本
                str(label.detach().numpy() ),
                win = 'target_true',
                opts = {&quot;target_true&quot;: target_predict}
            )
main()
</code></pre>
<h3 id="模型可视化visdom">模型可视化（visdom)</h3>
<h5 id="安装-和-运行-和-使用">安装 和 运行 和 使用</h5>
<pre><code>安装
    pip install visdom
运行
    python -m visdom.server  （第一次可能会有点慢）
    
# 语法和Tensorboard很像

使用
    import visdom
    见上代码 vis.xxxxx
</code></pre>
<h1 id="案例2-cifar10cnn">案例2-CIFAR10+CNN</h1>
<h3 id="说明">说明</h3>
<p>模块导入和数据预处理部分和案例1的 MNIST一模一样。<br>
只要稍稍修改 datasets.MNIST ==&gt;  datasets.CIFAR10 即可， 简单的不忍直视~~</p>
<h3 id="代码如下">代码如下：</h3>
<p>模型定义部分：<br>
class MyModel(nn.Module):        # 温馨提示， 这是 Mmodule, 不是model<br>
def <strong>init</strong>(self):<br>
&quot;&quot;&quot;<br>
先注明一下：<br>
TF中输入图片形状为       (样本数, 高，宽，图片通道)<br>
PyTorch中输入图片形状为  (样本数, 图片通道，高，宽)<br>
&quot;&quot;&quot;</p>
<pre><code>        super().__init__()
        self.conv = nn.Sequential( # 再强调一遍，没有 []
            nn.Conv2d(
                in_channels=3,    # 对应TF  图片通道数（或者上一层通道）
                out_channels=8,   # 对应TF  filters, 卷积核数量
                kernel_size=3,    # 卷积核大小
                stride=1,         # 步长, TF 是 strides，  特别注意
                padding=0,        # no padding, 默认
            ),
            nn.ReLU(),
            nn.MaxPool2d(
                kernel_size=3,    # 滑动窗口大小
                stride=None,      # 默认为None， 意为和 kernel_size相同大小
            ),

            nn.Conv2d(
                in_channels=8,    # 对应TF  图片通道数（或者上一层通道）
                out_channels=16,   # 对应TF  filters, 卷积核数量
                kernel_size=3,    # 卷积核大小
                stride=1,         # 步长, TF 是 strides，  特别注意
                padding=0,        # no padding, 默认
            ),
            nn.ReLU(),
            nn.MaxPool2d(
                kernel_size=2,    # 滑动窗口大小
                stride=None,      # 默认为None， 意为和 kernel_size相同大小
            ),
        )
        self.dense = nn.Sequential(
            nn.Linear(16*4*4, 128),    # 对应TF Dense
            nn.Linear(128, 64),
            nn.Linear(64, 10),
        )
    def forward(self, x):
        conv_output = self.conv(x)
        # (16, 16, 4.4)
        conv_output_reshape = conv_output.view(-1, 16*4*4)
        dense_output = self.dense(conv_output_reshape)

        return dense_output
</code></pre>
<p>模型训练（模型调用+模型训练的定义）<br>
def main():<br>
vis = visdom.Visdom()</p>
<pre><code>    epochs = 100

    device = torch.device('cuda')  # 预定义 GPU 槽位（一会往里面塞 模型和数据。）

    model = MyModel().to(device)   # 模型转为 GPU 计算
    
    # CrossEntropyLoss 会自动把下面的 dense_output ，也就是y_predict 加一层 softmax
    loss_ = nn.CrossEntropyLoss().to(device)  
    optimizer = optim.Adam( model.parameters() )

    for epoch in range(epochs):
        for step, (x_train, y_train) in enumerate(train):
            x_train, y_train = x_train.to(device), y_train.to(device)
            dense_output = model(x_train)

            loss = loss_(dense_output, y_train)

            optimizer.zero_grad()   # 上一个例子提到过，梯度清零
            loss.backward()         # 反向传播， 并将梯度累加到 optimizer中
            optimizer.step()        # 相当于做了 w = w - lr * 梯度

        print(loss.item())          # item() 意思就是 tensor转numpy,TF中的 API是 xx.numpy()

        sample_correct_numbers = 0
        sample_total_numbers = 0

        with torch.no_grad():   # 测试部分不需要计算梯度，因此可以包裹在上下文中。
            for x_test, y_test in test:
                x_test, y_test = x_test.to(device), y_test.to(device)
                
                # softmax 的 y_predict  与  y_test的 one-hot做交叉熵
                y_predict = model(x_test).argmax(dim=1) 
                sample_correct_numbers += torch.eq(y_predict, y_test).float().sum().item()
                sample_total_numbers += x_test.size(0)  # 每批样本的总数加在一起
            acc = sample_correct_numbers / sample_total_numbers
            print(acc)

main()
</code></pre>
<h1 id="案例3cifar10resnet-18">案例3：CIFAR10+ResNet-18</h1>
<p>###结构图体系：<br>
<img src="/img/bVbyS1X" alt="image.png" loading="lazy"><br>
上述结构说明：<br>
1conv + (2+2+2+2)*2 + 1 fc = 18层<br>
1conv + (3+4+6+3)*2 + 1 fc = 34层<br>
1conv + (3+4+6+3)*3 + 1 fc = 50层<br>
1conv + (3+4+23+3)*3 + 1 fc = 101层<br>
1conv + (3+8+36+3)*3 + 1 fc = 152层</p>
<h3 id="代码实现">代码实现</h3>
<p>模块导入<br>
import cv2<br>
import torch<br>
from torch import nn, optim<br>
from torchvision import datasets, transforms<br>
from torch.utils.data import DataLoader<br>
import visdom<br>
import torch.nn.functional as F<br>
数据导入预处理<br>
data_preprocess = transforms.Compose([<br>
transforms.Resize(32,32),<br>
transforms.ToTensor(),<br>
transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))<br>
])</p>
<pre><code>train_dataset = datasets.CIFAR10(
    '.',
    train=True,
    download=True,
    transform=data_preprocess,
)


test_dataset = datasets.CIFAR10(
    '.',
    train=False, # False代表测试集
    download=True,
    transform=data_preprocess,
)


train = DataLoader(
    train_dataset,
    batch_size=16,
    shuffle=True,
)

test = DataLoader(
    test_dataset,
    batch_size=16,
    shuffle=True,
)
</code></pre>
<p>基础块定义（BasicBlock）：<br>
class BasicBlock(nn.Module):<br>
&quot;&quot;&quot;单个残差块 2个卷积+2个BN&quot;&quot;&quot;<br>
def <strong>init</strong>(self, input_channel, output_channel, stride=1):<br>
super().<strong>init</strong>()<br>
self.major = nn.Sequential(<br>
# 第一个Conv的步长为指定步长，允许降采样，允许输出输出通道不一致<br>
nn.Conv2d(input_channel,output_channel,kernel_size=3,stride=stride, padding=1),<br>
nn.BatchNorm2d(output_channel),<br>
nn.ReLU(inplace=True),<br>
# 第二个Conv的步长为定长1， 输入输出通道不变（缓冲输出）<br>
nn.Conv2d(output_channel, output_channel, kernel_size=3, stride=1, padding=1),<br>
nn.BatchNorm2d(output_channel),<br>
# 第二个Conv就不用ReLU了， 因为一会需要和 x加在一起，最后最一层大的Relu<br>
)<br>
# 若输入通道==输出通道，且步长为1，意味着图片未被降采样，则残差网络课直接为普通网络</p>
<pre><code>        self.shortcut = nn.Sequential()
        # 若输入输出通道不匹配，这时需要将图片做同样的变换，才能加在一起。
        if input_channel != output_channel or stride != 1:
            self.shortcut = nn.Sequential(
                nn.Conv2d(
                    input_channel,
                    output_channel,
                    kernel_size=(1,1),
                    stride = stride
                ),
                nn.BatchNorm2d(output_channel)
            )            

    def forward(self, x):
        major_out = self.major(x)        # 主干网络的输出
        shotcut_out = self.shortcut(x)   # 残差网络的输出
        # 上面这两个网络是平行的关系，  因为 它们的输出不是链式的， 而是  都是同样的 x。

        # 拼接主干网络+残差网络，F 相当于TF20的 tf.nn 里面单独有各种 loss函数
        return F.relu(major_out + shotcut_out)  # 最后在拼接后的网络外面加一层relu 
</code></pre>
<p>ResNet+ResBlock定义：<br>
class ResNet(nn.Module):<br>
def <strong>init</strong>(self, layers):  # layers用来接受，用户想要指定 ResNet的形状<br>
super().<strong>init</strong>()</p>
<pre><code>        self.conv1 = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
        )        

        self.res_net = nn.Sequential(
            *self.ResBlock(32,64, layers[0],stride=2),    # 16
            *self.ResBlock(64,128, layers[1],stride=2),   # 8
            *self.ResBlock(128,256, layers[2],stride=2),  # 4
            *self.ResBlock(256,512, layers[3],stride=2),  # 2
        )
        # 因为我们一会需要展平，里面填&quot;通道*宽度*高度&quot;, &quot;输出通道&quot;
        self.dense = nn.Linear(512 * 2 * 2, 10)  
        
    def forward(self, x):
        out = self.conv1(x)
        out = self.res_net(out)

        out = out.view(x.size(0), -1)# 卷积展平操作 ， torch中没有flatten所以我们就得手工
        out = self.dense(out)
        return out

    def ResBlock(self, input_channel, output_channel, block_nums=2, stride=2):
        # 自定义规定，第一个block缩小的(对应通道翻倍)，其余block大小不变
        # 通道翻倍，步长*2，特征减半
        all_block = [BasicBlock(input_channel, output_channel,stride=stride)]   

        for x in range(1,block_nums):
            all_block.append(BasicBlock(output_channel, output_channel,stride=1))
        return all_block

# resnet = ResNet(layers=[2,2,2,2])
# out = resnet(torch.randn(4,3,32,32))
# print(out.shape)
</code></pre>
<p>模型训练：<br>
def main():<br>
vis = visdom.Visdom()</p>
<pre><code>    epochs = 5

    device = torch.device('cuda')

    model = ResNet(layers=[2,2,2,2]).to(device) 
    
    # 会自动把下面的 dense_output ，也就是y_predict 加一层 softmax，y_true做one-hot
    loss_ = nn.CrossEntropyLoss().to(device)  
    optimizer = optim.Adam( model.parameters(), lr=0.0001)

    for epoch in range(epochs):
        total_loss = 0.0
        for step, (x_train, y_train) in enumerate(train):
            x_train, y_train = x_train.to(device), y_train.to(device)
            dense_output = model(x_train)

            loss = loss_(dense_output, y_train)

            optimizer.zero_grad()   # 上一个例子提到过，梯度清零
            loss.backward()         # 反向传播， 并将梯度累加到 optimizer中
            optimizer.step()        # 相当于做了 w = w - lr * 梯度
            total_loss += loss.item() # item()就是 tensor转numpy, TF中的 API是 xx.numpy()
            if step % 50 == 49:
                print('epoch:',epoch, 'loss:', total_loss / step) 

        sample_correct_numbers = 0
        sample_total_numbers = 0

        with torch.no_grad():   # 测试部分不需要计算梯度，因此可以包裹在上下文中。
            for x_test, y_test in test:
                x_test, y_test = x_test.to(device), y_test.to(device)
                # softmax 的 y_predict  与  y_test的 one-hot做交叉熵
                y_predict = model(x_test).argmax(dim=1) 
                sample_correct_numbers += torch.eq(y_predict, y_test).float().sum().item()
                sample_total_numbers += x_test.size(0)  # 每批样本的总数加在一起
            acc = sample_correct_numbers / sample_total_numbers
            print(acc)
    torch.save(model, 'model.pkl')  # 保存整个模型
main()
</code></pre>
<p>测试数据预处理(我随便在网上下载下来的 1 张图片)：<br>
# 这是Cifar-10数据的标准标签<br>
label = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']</p>
<pre><code>plane = cv2.imread('plane.jpg')              # 我用的opencv
plane = cv2.cvtColor(plane, cv2.COLOR_BGR2RGB) # opencv读的数据格式是BGR，所以转为RGB

plane = (plane - 127.5) / 127.5    # 二话不说，保持模型输入数据的概率分布，先做归一化
plane = cv2.resize(plane, (32,32)) # 图片缩小到32x32,和模型的输入保持一致
plane = torch.Tensor(plane)        # 转换成 tensor
plane = plane.view(1,32,32,3)      # 增加一个维度
plane = plane.repeat(16,1,1,1)     # 我就用一张图片，为了满足模型的形状16，我复制了16次     
plane = plane.permute([0,3,1,2])   # 虽然torch也有 像TF那样的transpose，但是只能操作2D

device = torch.device('cuda')      # 先定义一个cuda设备对象
plane = plane.to(device)           # 我们训练集用的cuda， 所以预测数据也要转为cuda
</code></pre>
<p>正式输入模型预测：<br>
model = torch.load('model.pkl')    # 读取出 我们训练到最后整个模型<br>
# 说明一下，如果你的预测是另一个脚本中，class ResNet 的代码定义部分也要复制过来</p>
<pre><code>out = model(plane)                 # 预测结果，形状为[16,10] 16个样本，10个预测概率，
label_indexes = out.argmax(dim=1)  # 取10个概率最大值的索引。 （1轴），形状为 [16,1]
print(label_indexes)
for i in label_indexes:            # i为每个样本预测的最大概率值 的 索引位置。
    print(label[i])                # 拿着预测标签的索引  去 真实标签中找到真实标签</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI => Tensorflow2.0语法 - keras_API的使用(三)]]></title>
        <id>https://cythonlin.github.io/post/ai-greater-tensorflow20-yu-fa-keras_api-de-shi-yong-san/</id>
        <link href="https://cythonlin.github.io/post/ai-greater-tensorflow20-yu-fa-keras_api-de-shi-yong-san/">
        </link>
        <updated>2020-09-29T04:12:09.000Z</updated>
        <content type="html"><![CDATA[<h1 id="前言">前言</h1>
<p>keras接口大都实现了 _<em>call</em>_ 方法。<br>
母类 _<em>call</em>_ 调用了 call()。<br>
因此下面说的几乎所有模型/网络层 都可以在定义后，直接像函数一样调用。<br>
eg:<br>
模型对象(参数)<br>
网络层对象(参数)<br>
我们还可以实现继承模板</p>
<h1 id="导入">导入</h1>
<pre><code>from tensorflow import keras
</code></pre>
<h1 id="metrics-统计平均">metrics (统计平均)</h1>
<p>里面有各种度量值的接口<br>
如：二分类、多分类交叉熵损失容器，MSE、MAE的损失值容器， Accuracy精确率容器等。<br>
下面以Accuracy伪码为例：<br>
acc_meter = keras.metrics.Accuracy() # 建立一个容器<br>
for _ in epoches:<br>
for _ in batches:<br>
y = ...<br>
y_predict = ...<br>
acc_meter.update_state(y, y_predict) # 每次扔进去数据，容器都会自动计算accuracy，并储存</p>
<pre><code>        if times % 100 == 0: # 一百次一输出, 设置一个阈值/阀门
            print(acc_meter.result().numpy())   # 取出容器内所有储存的数据的，均值准确率
    acc_meter。reset_states()    # 容器缓存清空， 下一epoch从头计数。
</code></pre>
<h1 id="激活函数损失函数优化器">激活函数+损失函数+优化器</h1>
<p>导入方式：<br>
keras.activations.relu()    # 激活函数：以relu为例，还有很多<br>
keras.losses.categorical_crossentropy() # 损失函数：以交叉熵为例，还有很多<br>
keras.optimizers.SGD()      # 优化器：以随机梯度下降优化器为例<br>
keras.callbacks.EarlyStopping()  # 回调函数： 以‘按指定条件提前暂停训练’回调为例</p>
<h1 id="sequential继承自model属于模型">Sequential(继承自Model)属于模型</h1>
<h3 id="模型定义方式">模型定义方式</h3>
<p>定义方式1：<br>
model = keras.models.Sequential( [首层网络,第二层网络。。。] )<br>
定义方式1：<br>
model = keras.models.Sequential()<br>
model.add(首层网络)<br>
model.add(第二层网络)</p>
<h3 id="模型相关回调配置">模型相关回调配置</h3>
<pre><code>logdir = 'callbacks'
if not os.path.exists(logdir):
    os.mkdir(logdir)
save_model_file = os.path.join(logdir, 'mymodel.h5')

callbacks = [
    keras.callbacks.TensorBoard(logdir),    # 写入tensorboard
    keras.callbacks.ModelCheckpoint(output_model_file, save_best_only=True),  # 模型保存
    keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3)  # 按条件终止模型训练
    # 验证集，每次都会提升，如果提升不动了，提升小于这个min_delta阈值，则会耐心等待5次。
    # 5次过后，要是还提升这么点。就提前结束。
]
# 代码写在这里，如何传递调用， 下面 “模型相关量度配置” 会提到
</code></pre>
<h3 id="模型相关量度配置损失优化器准确率等">模型相关量度配置：(（损失，优化器，准确率等)</h3>
<p>说明，下面的各种量度属性，可通过字符串方式，也可通过上面讲的导入实例化对象方式。<br>
model.compile(<br>
loss=&quot;sparse_categorical_crossentropy&quot;,    # 损失函数，这是字符串方式<br>
optimizer= keras.optimizers.SGD()          # 这是实例化对象的方式，这种方式可以传参<br>
metrics=['accuracy']  # 这项会在fit()时打印出来<br>
)  # compile() 操作，没有真正的训练。<br>
model.fit(<br>
x,y,<br>
epochs=10,                              # 反复训练 10 轮<br>
validation_data = (x_valid,y_valid),    # 把划分好的验证集放进来（fit时打印loss和val）<br>
validation_freq = 5,                    # 训练5次，验证一次。  可不传，默认为1。<br>
callbacks=callbacks,                    # 指定回调函数， 请衔接上面‘模型相关回调配置’</p>
<pre><code>)   # fit()才是真正的训练 
</code></pre>
<h3 id="模型-验证测试">模型 验证&amp;测试</h3>
<p>一般我们会把 数据先分成三部分（如果用相同的数据，起不到测试和验证效果，参考考试作弊思想）：</p>
<ol>
<li>训练集: （大批量，主体）</li>
<li>测试集: （模型所有训练结束后， 才用到）</li>
<li>验证集: （训练的过程种就用到）<br>
说明1：（如何分离？）
<ol>
<li>它们的分离是需要（x,y）组合在一起的，如果手动实现，需要随机打散、zip等操作。</li>
<li>但我们可以通过 scikit-learn库，的 train_test_split() 方法来实现 （2次分隔）</li>
<li>可以使用 tf.split()来手动实现<br>
具体分离案例：参考上一篇文章： https://segmentfault.com/a/1190000020447666</li>
</ol>
</li>
</ol>
<p>说明2：（为什么我们有了测试集，还需要验证集？）</p>
<ol>
<li>测试集是用来在最终，模型训练成型后（参数固定），进行测试，并且返回的是预测的结果值！！！！</li>
<li>验证集是伴随着模型训练过程中而验证）<br>
代码如下：<br>
loss, accuracy = model.evaluate( (x_test, y_test) ) # 度量， 注意，返回的是精度指标等<br>
target = model.predict( (x_test, y_test) )          # 测试， 注意，返回的是 预测的结果！</li>
</ol>
<h3 id="可用参数">可用参数</h3>
<pre><code>model.trainable_variables    # 返回模型中所有可训练的变量
# 使用场景： 就像我们之前说过的 gradient 中用到的 zip(求导结果, model.trainable_variables)
</code></pre>
<h1 id="自定义model">自定义Model</h1>
<p>Model相当于母版， 你继承了它，并实现对应方法，同样也能简便实现模型的定义。</p>
<h1 id="自定义layer">自定义Layer</h1>
<p>同Model， Layer也相当于母版， 你继承了它，并实现对应方法，同样也能简便实现网络层的定义。</p>
<h1 id="模型保存与加载">模型保存与加载</h1>
<p>###方法1：之前callback说的<br>
###方法2：只保存weight(模型不完全一致)<br>
保存：<br>
model = keras.Sequential([...])<br>
...<br>
model.fit()<br>
model.save_weights('weights.ckpt')<br>
加载：<br>
假如在另一个文件中。（当然要把保存的权重要复制到本地目录）<br>
model = keras.Sequential([...])    # 此模型构建必须和保存时候定义结构的一模一样的！<br>
model.load_weights('weights.ckpt')<br>
model.evaluate(...)<br>
model.predict(...)</p>
<p>###方法3：保存整个模型（模型完全一致）<br>
保存：<br>
model = keras.Sequential([...])<br>
...<br>
model.fit()<br>
model.save('model.h5')    # 注意 这里变了，是 save<br>
加载:（直接加载即可，不需要重新复原建模过程）<br>
假如在另一个文件中。（当然要把保存的模型要复制到本地目录）<br>
model = keras.models.load_model('model.h5')  # load_model是在 keras.models下<br>
model.evaluate(...)<br>
model.predict(...)<br>
###方法4：导出可供其他语言使用（工业化）<br>
保存： （使用tf.saved_model模块）<br>
model = keras.Sequential([...])<br>
...<br>
model.fit()<br>
tf.saved_model.save(model, '目录')<br>
加载：（使用tf.saved_model模块）<br>
model = tf.saved_model.load('目录')</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI => 知识图谱之Neo4j-Cypher]]></title>
        <id>https://cythonlin.github.io/post/ai-greater-zhi-shi-tu-pu-zhi-neo4j-cypher/</id>
        <link href="https://cythonlin.github.io/post/ai-greater-zhi-shi-tu-pu-zhi-neo4j-cypher/">
        </link>
        <updated>2020-09-29T04:11:21.000Z</updated>
        <content type="html"><![CDATA[<h1 id="crud">CRUD</h1>
<h3 id="创建">创建</h3>
<p>普通无属性创建（默认给你创建一个ID）<br>
create (p:person)-[:eat]-&gt;(f:food)<br>
带有属性的创建（ {} ）<br>
create (p:person{name:'zhangsan'})-[:eat]-&gt;(f:food{name:'apple'})</p>
<p>给两个孤独的实体创建关系：<br>
match<br>
(a:animal),(c:color)<br>
create    （如果这里改为 merge 则是 “有则查询，无则创建”）<br>
(a)-[h:have]-&gt;(c) return h</p>
<pre><code>对应查询：
	match 
		(a:animal),(c:color) 
	return  a,c
</code></pre>
<h3 id="删除-delete">删除 (delete)</h3>
<pre><code>match
	(a:animal)-[h:have]-&gt;(c:color) 
delete a,h,c
</code></pre>
<h3 id="更新修改set">更新修改（set）</h3>
<pre><code>match
	(f:food) 
set f.age=20
</code></pre>
<h3 id="查询">查询</h3>
<p>主体查询结构<br>
match<br>
(p:)-[:关系名]-[别名2:实体名]<br>
return 别名1,别名2</p>
<p>普通条件查询1（whree）<br>
match<br>
(n:User)<br>
where<br>
n.name='Tom'<br>
return n<br>
普通条件查询2：（ {} ）<br>
match<br>
(p:person{name:'zhangsan'})-[:eat{level:1}]-&gt;(f:food{name:'apple'})<br>
return p,f<br>
正则条件查询(~)<br>
match<br>
(n:User)<br>
where<br>
n.name=~'T.*'<br>
return n<br>
包含条件查询（contains）<br>
match<br>
(n:User)<br>
where<br>
n.name contains 'T'<br>
return n</p>
<p>多度查询<br>
match (t:teacher)-[]-(s:student)-[]-(ss:score) return t,s,ss</p>
<pre><code># 注意1： [] 里面不写，代表所有关系
# 注意2： -  没有箭头，代表任意方向
# 注意3： 别名不可以重复指定， 所以我设置了 ss

多度关系： （通常是基于人脉来讲的）
1度关系：我 -&gt; 你
2度关系：我 -&gt; 你 -&gt; 他

理解技巧： 算几度关系时，把自己（节点）捂住不看， 然后剩下几个人员节点，就是几度关系

特别注意：
    多度查关系时，比如你查 3度关系的结果。
    neo4j的图可能会把， 2度关系也画出来，why? 因为他通过2度关系也可直接得出结果。
    （可理解为 条条大路通罗马。）
    ！！！但是最终有效的返回路径只是你最初想要的 3度。   （2度就不算了）
</code></pre>
<p>查询最短路径：<br>
match (t:teacher), (s:student),<br>
p=shortestpath( (t)-[*..]-(s) )<br>
return p</p>
<pre><code># 注意： p= 之前有个逗号 ，
</code></pre>
<p>查询所有最短路径：<br>
match (t:teacher), (s:student),<br>
p=allshortestpaths( (t)-[*..]-(s) )<br>
return p</p>
<pre><code># 注意1： 前面多个 all ，后面多个s
# 注意2： 所有最短路径的都会列出来。 人人平等~
</code></pre>
<h1 id="索引">索引</h1>
<h3 id="创建索引-create">创建索引 (create)</h3>
<pre><code>create index on  :food(name)      

# food为实体名，name为属性名， 同时注意这个 :    
</code></pre>
<h3 id="删除索引drop">删除索引（drop）</h3>
<pre><code>drop index on  :food(name)
</code></pre>
<h1 id="约束">约束</h1>
<h3 id="创建约束">创建约束</h3>
<pre><code>create constraint on (gf:girlfriend) assert (gf.name) is unique
</code></pre>
<h3 id="删除约束">删除约束</h3>
<pre><code>create constraint on (o:others) assert (o.name) is unique
</code></pre>
<h1 id="聚合">聚合</h1>
<h3 id="统计个数count">统计个数（count）</h3>
<pre><code>match ... return count(别名)        
</code></pre>
<h3 id="限制取多少条-limit">限制取多少条 （limit）</h3>
<pre><code>match ... return 别名 limit 5    # 只取5条
</code></pre>
<h1 id="知识图谱流程">知识图谱流程</h1>
<ol>
<li>数据抓取</li>
<li>知识模型设计</li>
<li>NER （远程监督）</li>
<li>关系抽取（Bootstrap）</li>
<li>知识推理</li>
<li>图谱存储（Neo4j Cypher）</li>
<li>检索/问答/推荐</li>
</ol>
<h3 id="实体抽取">实体抽取</h3>
<p>BILSTM+CRF</p>
<h3 id="关系抽取">关系抽取</h3>
<p>Bootstrap方法：<br>
1. 构建种子实体： &quot;猫&quot;， &quot;老鼠&quot;。</p>
<pre><code>2. 寻找包含 &quot;猫&quot;  &quot;老鼠&quot; 的 句子：
    找到句子：&quot;猫和老鼠是好朋友&quot;  
    可抽取关系: 和...是好朋友

3. 拿着抽取的关系再次寻找新句子：
    找到新句子：&quot;张三和里李四是好朋友&quot;
    提取出新实体: &quot;张三&quot;, &quot;李四&quot;
    
4. 寻找包含 &quot;张三&quot;  &quot;李四&quot; 的 句子：
    找到句子：&quot;张三经常和李四一起玩&quot;  
    可抽取关系:  经常和... 一起玩

5. 拿着抽取的关系再次寻找新句子：
    找到新句子：&quot;王五和赵六是好朋友&quot;
    提取出新实体: &quot;王五&quot;, &quot;赵六&quot;
...
...
循环反复</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI => NER之BIO转BIOES]]></title>
        <id>https://cythonlin.github.io/post/ai-greater-ner-zhi-bio-zhuan-bioes/</id>
        <link href="https://cythonlin.github.io/post/ai-greater-ner-zhi-bio-zhuan-bioes/">
        </link>
        <updated>2020-09-29T04:08:26.000Z</updated>
        <content type="html"><![CDATA[<h3 id="bio">BIO</h3>
<p>B: 命名实体的起始 或 单个字命名实体<br>
I: 命名实体的中间位置 或 结束位置<br>
O：非命名实体</p>
<h3 id="bioes">BIOES</h3>
<p>B: 命名实体的起始标注（Only哦）<br>
I: 命名实体的中间标注（Only哦）<br>
E: 命名实体的结尾标注（Only哦）</p>
<p>O: 非命名实体<br>
S: 单个字命名实体</p>
<h3 id="bio转bioes规则">BIO转BIOES规则</h3>
<figure data-type="image" tabindex="1"><img src="https://cythonlin.github.io/post-images/1601352615887.png" alt="" loading="lazy"></figure>
<h3 id="个人代码实现">个人代码实现</h3>
<p>传送门：<a href="https://github.com/hacker-lin/bio2bioes/">https://github.com/hacker-lin/bio2bioes/</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI => 安装neo4j (Linux)]]></title>
        <id>https://cythonlin.github.io/post/ai-greater-an-zhuang-neo4j-linux/</id>
        <link href="https://cythonlin.github.io/post/ai-greater-an-zhuang-neo4j-linux/">
        </link>
        <updated>2020-09-29T04:07:49.000Z</updated>
        <content type="html"><![CDATA[<h1 id="依赖java先装java">依赖java,先装java</h1>
<p>官网：<a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a><br>
找到： jdk-8u231-linux-x64.tar.gz  下载（可能会比较慢，还需要注册）</p>
<p>个人网盘分享：https://pan.baidu.com/s/1EMFM_Y_HT3bHFwncKj2orw&amp;shfl=shareset<br>
提取码: w24a</p>
<p>通过XFTP传送到服务器。<br>
任意目录（自己记住即可）：<br>
tar -zxvf jdk-8u231-linux-x64.tar.gz<br>
# 解压出来一个目录，记住找个名字，和当前路径，下面用<br>
# 我这里是 jdk1.8.0_181<br>
相关配置:<br>
vi ~/.bashrc</p>
<pre><code>export JAVA_HOME=/root/kg/neo4j/java/jdk1.8.0_181/
export JRE_HOME=${JAVA_HOME}/jre 
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib 
export PATH=${JAVA_HOME}/bin:$PATH 

保存退出
</code></pre>
<p>激活配置：<br>
source ~/.bashrc<br>
至此，java安装完成！</p>
<h1 id="安装neo4j">安装Neo4j:</h1>
<p>下载连接：<a href="https://neo4j.com/download-center/">https://neo4j.com/download-center/</a><br>
选个Linux版本的（需要上网）（记得下社区版的）<br>
同样传到Linux解压即可。</p>
<p>个人网盘分享：https://pan.baidu.com/s/1FnbZW0n2-w8yCZEXG3J6zw&amp;shfl=shareset<br>
提取码: 451c</p>
<p>因为我是云端运行。所以需要改下配置，支持远程访问。<br>
进入解压后的目录，我这里是 neo4j-community-3.5.11<br>
cd neo4j-community-3.5.11<br>
cd conf<br>
vi neo4j.conf</p>
<pre><code>添加如下配置（或者你能找到这行，直接解除注释也行）：
    dbms.connectors.default_listen_address=0.0.0.0
</code></pre>
<p>至此，neo4j安装完成。<br>
启动neo4j:<br>
cd neo4j-community-3.5.11<br>
cd bin<br>
./neo4j console<br>
看启动信息的url:port， 复制出来直接访问即可。<br>
xxxxxxxx:7474    # 若是阿里云，需要放通防火墙7474 和 7687<br>
成功进去后，会提示你输入默认用户名 和 密码：<br>
默认用户名为 ：neo4j<br>
默认密码也为 ：neo4j</p>
<p>密码忘记了？？<br>
进入 xx/data/dbms<br>
删除 auth 文件，重启服务</p>
]]></content>
    </entry>
</feed>