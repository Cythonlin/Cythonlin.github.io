{"posts":[{"title":"IDE => My Sublime（PY）","content":"FC #00FFFF 0,255,255 package control package control报错 😒No module named 'package_control' 进入preferences &gt;settings： 删除列表中的 &quot;0_package_control_loader&quot; 即可。 最终保留内容为： [ &quot;0_package_control_loader&quot;, &quot;Vintage&quot; ], 主题 首先安装一个包管理插件 因为Sublime Text3 一些配置被压缩了，通过这个插件可以直接查看内部设置 PackageResourceViewer 安装后， ctrl+shift+p，输入 prv (记住这个后续修改侧边栏之类的会用到) 安装 ctrl+shift+P material theme # 安装后再次 ctrl+shift+p # 激活： 输入ma （material-theme-darker） # 配置： 输入mc （配置color scheme: 选 material-theme） mc 的 tabs 的如下选项勾上（后3个）： material_theme_small_tab material_theme_tabs_autowidth material_theme_tabs_separator mc 的 sidebar 的如下选项 勾上（第4个和第5个）： material_theme_compact_sidebar material_theme_disable_fileicons 修改代码区字体类型和字体大小 preference -&gt; settings &quot;font_face&quot;: &quot;Courier New&quot;, &quot;font_size&quot;: 14, 修改侧边栏，后者顶部tab等配置 由于st3的配置文件被压缩，所以我们上面安装PackageResourceViewer，就是为了打开压缩的配置 输入 prvo （就是 PackageResourceViewer open resource） 再输入 mt（就是 material theme ） 再输入 mtst （就是 material-theme.sublime-theme ） 此时你会看到打开了一个配置文件，接下来再配置文件修改即可👇 修改顶部tab 搜索字样：// Tab Labels， 找到 font.size 😶（一个有2个 // tab Labels， 找到有 font.size的才是）： &quot;fg&quot;: [255, 255, 255], &quot;font.size&quot;: 19, &quot;font.face&quot;: &quot;Courier New&quot;, 搜索字样：// Tab selected label color， 修改fg如下： &quot;fg&quot;: [0,255,255, 255], 搜索字样：// Status bar labels （感觉status bar占地方，就view =&gt; hide status bar） &quot;color&quot;: [0, 255, 255, 100], 搜索字样：tool_tip_label_control 搜索字样：quick_panel_label （从上到下，第一个即可） &quot;font.size&quot;: 23, &quot;font.face&quot;: &quot;Courier New&quot;, 搜索字样：quick_panel_path_label &quot;font.size&quot;: 18, &quot;font.face&quot;: &quot;Courier New&quot;, 搜索字样：quick_panel_score_label &quot;font.size&quot;: 18, &quot;font.face&quot;: &quot;Courier New&quot;, 修改侧边栏 找到 sidebar_label ， 并添加： &quot;font.bold&quot;: true, &quot;color&quot;: [200, 125, 0, 200], &quot;font.size&quot;: 18, 添加编译环境 普通编译（基本不用配，ctrl+b激活） Tools =&gt; build system =&gt; new build system 输入如下内容 =&gt; 保存文件到目录 { &quot;cmd&quot;: [&quot;python&quot;, &quot;-u&quot;, &quot;$file&quot;], &quot;file_regex&quot;: &quot;^[ ]*File \\&quot;(...*?)\\&quot;, line ([0-9]*)&quot;, } 上面cmd内第一个参数写python.exe路径也可以 😊然后就可一选择并使用了： Tools =&gt; build system =&gt; python (ctrl+b) 删除编译的打印错误路径日志 上面已经安装过了，这里就不再安装了，直接面板输入 prvo =&gt; 输入default =&gt; 输入 exec.py 搜索字样：if &quot;PATH&quot; in merged_env: 😎注释如下信息： # if shell_cmd: # self.debug_text += &quot;[shell_cmd: &quot; + shell_cmd + &quot;]\\n&quot; # else: # self.debug_text += &quot;[cmd: &quot; + str(cmd) + &quot;]\\n&quot; # self.debug_text += &quot;[dir: &quot; + str(os.getcwd()) + &quot;]\\n&quot; # if &quot;PATH&quot; in merged_env: # self.debug_text += &quot;[path: &quot; + str(merged_env[&quot;PATH&quot;]) + &quot;]&quot; # else: # self.debug_text += &quot;[path: &quot; + str(os.environ[&quot;PATH&quot;]) + &quot;]&quot; Keys preference =&gt; key bindings 显示隐藏 工具菜单 {&quot;keys&quot;: [&quot;ctrl+m&quot;], &quot;command&quot;: &quot;toggle_menu&quot;}, 取消编译 { &quot;keys&quot;: [&quot;ctrl+shift+c&quot;], &quot;command&quot;: &quot;cancel_build&quot; } 显示/隐藏侧边栏 { &quot;keys&quot;: [&quot;ctrl+k&quot;], &quot;command&quot;: &quot;toggle_side_bar&quot; }, 代码块折叠/展开(默认的即可) 默认为 ctrl + shift + [ ] 各种搜索快捷键 😄代码搜索（最好用）： ctrl + p 有几种用法： 1. 直接输入内容，表示查找文件名 2. @函数名，表示查找函数名。eg @write_file 3. :数字n ，表示跳转到第n行。 eg :65 4. #变量名， 表示查找变量名。eg #name 文本搜索 ctrl + f 多文件搜索 😶注意：也许pycharm不需要关闭繁体也能用，但是sublime必须关繁体切换才能用此key ctrl + shift + f 替换(默认的即可) ctrl + H 选择括号内的内容(默认即可) ctrl + shift + m 切换标签页 ctrl + page up / page down 或者 alt + 1 alt + 2 ............. 后者 ctrl + tab 创建新标签页/新文件 ctrl + n 创建新sublime窗口 ctrl + shift + n 窗口并排 alt + shift + 1 // 1排 alt + shift + 2 // 2排 alt + shift + 3 // 3排 alt + shift + 4 // 4排 Plugins 语言插件（不建议使用） localizeMenu 路径提示 autofilename # 文件路径提示 侧边栏选项增强 sidebarenhancements 快捷键配置： { &quot;keys&quot;: [&quot;delete&quot;], &quot;command&quot;: &quot;side_bar_delete&quot; }, { &quot;keys&quot;: [&quot;f2&quot;], &quot;command&quot;: &quot;side_bar_rename&quot; }, { &quot;keys&quot;: [&quot;ctrl+shift+x&quot;], &quot;command&quot;: &quot;side_bar_move&quot; }, // { &quot;keys&quot;: [&quot;ctrl+shift+c&quot;], &quot;command&quot;: &quot;side_bar_copy&quot; }, // { &quot;keys&quot;: [&quot;ctrl+shift+v&quot;], &quot;command&quot;: &quot;side_bar_paste&quot; }, alignment Anaconda package settings -&gt; anaconda -&gt; settings user 😊加入下面的配置，可以起到语法格式化的作用（保存ctrl+s后自动格式化） { &quot;python_interpreter&quot;: &quot;D:/python391/python.exe&quot;, &quot;auto_formatting&quot;: true, &quot;autoformat_ignore&quot;: [ &quot;E309&quot;, &quot;E501&quot; ], &quot;pep8_ignore&quot;: [ &quot;E309&quot;, &quot;E501&quot; ], &quot;anaconda_linting&quot;:false, &quot;anaconda_linter_underlines&quot;: false, &quot;anaconda_linter_mark_style&quot;: &quot;None&quot;, &quot;display_signatures&quot;: false, // 这是函数内部帮助文档提示，我不需要，设false // &quot;disable_anaconda_completion&quot;: true //不能打开它, 打开它anaconda就没有意义了 // &quot;swallow_startup_errors&quot;: true // 若anaconda报错，则打开它，否则不用 } 代码跳转快捷键为默认 ctrl + alt + g sublimeREPL 直接搜 sublimeREPL 安装 tools =&gt; sublimeREPL Terminal插件: 😶打开当前文件所在Terminal preference =&gt; package settings =&gt; Terminal =&gt; Setting Users 我的是Windows Terminal { &quot;keys&quot;: [&quot;ctrl+shift+t&quot;], &quot;terminal&quot;: &quot;C:/Users/lin/AppData/Local/Microsoft/WindowsApps/wt.exe&quot;, &quot;parameters&quot;: [&quot;-d&quot;, &quot;.&quot;] } =&gt; terminal官档，也有其他OS版配置 VS code 倒叙说明 VSC频繁宕掉，已卸载😀，还是Pycharm+Sulime+Jupyter+Miniconda了 中文语言 ctrl + shift + p 输入 language display， 点击，左侧选择安装中文语言 配置默认终端 修改控制台字体(不然PS7 和 WSL 的ZSH图标显示不出来) 右下角 ⚙ 配置 =&gt; 搜索 terminal font =&gt; 将 terminal font family 写入=&gt; Hack NF 修改默认Terminal 😀 ctrl +~ 调出控制台 下拉菜单选择， 配置默认终端， 选择PS7即可。 然后关闭 终端 重新 按 ctrl +~ ， 再次打开终端即为 PS7 快捷键 调出控制台 ctrl + ~ 打开命令面板（和 sublime差不多） ctrl + shift + p 插件 python: vs右侧有，会推荐给你语言插件，直接点python即可 pylint（语法高亮提示）: 新建py文件运行，他会自动提示你安装pylint Python snippets（不建议用） 代码块补全 sublime keymap and settings importer sublime式 remote WSL 可以打开WSL环境的Vscode remote SSH ssh remote containers docker ","link":"https://cythonlin.github.io/post/ide-greater-my-sublimepy/"},{"title":" IDE => My Pycharm","content":"版本 2021.1 community EAP 主题 主题： 默认黑色模式主题即可（三方插件不稳定，容易宕） 加背景图片： （插件 backgroundimage） 安装插件之后 直接设置中搜 image (或者搜 appearance) 进去 下方就多了一个 Background image的选项👍 加载背景之后，注意了 👍👍 ctrl+shift+p, 搜 image ，然后，把所有选项关闭（或者直接禁用此插件即可） 图标插件 Atom Material Icons 恢复默认配置 File -&gt; Manage IDE Settings -&gt; Restore Default Settings 修改终端 因表情符号实在不能支持，所以把把PS5换个主题（PS7继续保留） locate negligible.omp.json 然后在 ps5配置文件，把这个 &quot;negligible.omp.json&quot; 文件位置替换即可 这是个 不错的主题，比之前的好，还支持 虚拟环境😀 换终端PS5: locate powershell.exe # 然后在 pycharm中搜 terminal ， 把此路径粘进去, 后面加个 /nologo 字体 界面：Droid Serif 代码：Courier New 终端：Hack Nerd Font 修改运行内存 locate pycharm64.exe.vmoptions sublime &quot;C:\\Program Files\\JetBrains\\PyCharm Community Edition 211.6085.15\\bin\\pycharm64.exe.vmoptions&quot; 改为： -Xmx3072m 陈列 左下角：正方形图标：类似Sublime, 集 Pannel于一体 悬停可以看到各种 功能 view 例如： alt + 1 显示/隐藏 项目功能侧边栏 右下角：解释器信息：类似Sublime, 在此可以切换/添加解释器（真方便） 快捷键 Sublime快捷键 选择 Sublime Text （ST若有快捷键，优先选择ST, 若ST无此快捷键，则仍可使用pycharm） 选择ST后，你可以使用 ctrl + shift + p ctrl + p ctrl + H ctrl + n ctrl + o 并且 仍可以使用 pycharm的: shift + f6 ....................... Pycharm快捷键 上面说了，虽然使用了Sublime的快捷键，但是一些sublime没有的，pycharm的依然能用 选中文本（但把tab刨除） 使用情况1：选中非空白 1. 按住滑轮，拖动选中全部 （什么范围都行） 2. 如果想一个空白都不要，那就继续 按 end, 再接 shift + home从尾选到头 使用情况2：选中空白tab 1. 按住滑轮，拖动选中全部（什么范围都行） 2. 按 home , 在接 shift + home 使用情况3：选中空白tab+文本（tab格式保留） 1. 按住滑轮，拖动选中全部（这里对范围就有一些要求了，你想要什么tab格式的，就从哪里拖） 2. 垂直拖+水平拖，即可 多功能小面板 ctrl + ` 代码格式化 ctrl + shift + L 插入片段(通常类插入方法) alt + insert 跳转到n行 ctrl + G 列出所当前工程所有文件，（需要一直按住，然后用鼠标去点击选择） ctrl + tab 改键（ctrl+shift+p，输入keymap-&gt;configure keymap） 字体大小（滑轮） 输入 font size (add mouse shortcut) increase font size decrease font size 输入 run context configuration 添加一个 ctrl + B 跳入源码 keymap输入：declaration or usages add mouse : ctrl + 左键 源码前进 搜navigate： （修改其下 forward） ctrl + alt + -&gt; 源码返回 搜navigate： （修改其下 back） ctrl + alt + &lt;- 缩进 搜 indent selection shift + tab Debug 说一下原按钮的快捷键及作用（从左到右，无图，自己对照）： 第一个：show execution point（alt + F10） 作用：只要你开了Dubug无论光标处于代码何处，都会跳到当前将要执行的断点处。 第二个：step over（F8） 作用：单步执行时，不会进入官方库 😀但是如果，点了 第三个 （step into）进去了， 那么 step over，会在里面继续单步执行的 第三个：step into（F7）（看源码用的） 作用：单步执行，会进入自定义函数（但不会进入官方标准函数） 会进入任何库的函数（通过模块，无论是标准模块还是第三方模块，都会进入） 第四个：step into my code（alt + shift + F7） 作用：加入你点了第三个 step into，不慎进入了多层源码。 😀这时有两种方法，可以让你退出来。 1. 进去了多少层，你就需要按多少次 step out （也就是第六个按钮） 2. 或者你可以一步到位， 直接按 此按钮（step into my code） 直接跳出来😀 第五个：force step into（alt + shift + F7） 暂未用 第六个：step out（F8） 跳出子函数（如果执行了一部分，那就跳出余下的部分）（并执行完毕） 如果函数是嵌套的，那么就一层一层跳出来，跳出源码上面说到了可用step into my code😀) 第七个：run to cursor（alt+F9） 此cursor并不是光标，而是断点。 意思就是， 执行到断点处停止 （如果是循环，那就执行一轮，到断点处停止，无论你中间有多少循环😀😀😀） （当时如果是外层的大循环，那就没办法了。。） ------------------------------- 😀不过可在最外面和外循环同级的下面的代码打个断点，并将内循环断点删掉，然后 step out即可 功能 修改代码区样式 ctrl + ` 或 ctrl+shift+p 或 ctrl +shift + s 输入 color scheme =&gt; 选择 Monokai 😉 类型注释+提示 def f(name: str): name.只要你写了:str这个注释， 这里就会弹出字符串对应的函数高亮提示 函数注释片段自动生成 搜docstring format (tools下的)： 将其值，选为 Epytext 然后函数内的头部输入 &quot;&quot;&quot;回车， 或者'''回车都可， 成功生成 插件 快捷键展示 Presentation Assistant 自定义代码段 搜 live template , 右上角加号 ：add template group 名为 py 继续右上角加号： add live template super(classclassclass, self).methodmethodmethod(endendend) @property def NAMENAMENAME(self): return ENDENDEND @NAMENAMENAME.setter def NAMENAMENAME(self, value): pass @NAMENAMENAME.deleter def NAMENAMENAME(self): pass for INDEXINDEXINDEX, VARVARVAR in enumerate(ITERABLEITERABLEITERABLE): ENDENDEND [VAREXPRVAR_EXPRVARE​XPR for VARVARVAR in ITERABLEITERABLEITERABLE if VAREXPRIFVAR_EXPR_IFVARE​XPRI​F] {KEYEXPRKEY_EXPRKEYE​XPR: VALEXPRVAL_EXPRVALE​XPR for VARVARVAR in ITERABLEITERABLEITERABLE if VAREXPRIFVAR_EXPR_IFVARE​XPRI​F} ","link":"https://cythonlin.github.io/post/rs-greater-9/"},{"title":"RS => 8","content":"RS =&gt; 8 ","link":"https://cythonlin.github.io/post/rs-greater-8/"},{"title":"RS => 7","content":"RS =&gt; 7 ","link":"https://cythonlin.github.io/post/rs-greater-7/"},{"title":"RS => 6","content":"RS =&gt; 6 ","link":"https://cythonlin.github.io/post/rs-greater-6/"},{"title":"RS => 5","content":"RS =&gt; 5 ","link":"https://cythonlin.github.io/post/rs-greater-5/"},{"title":"RS => 4","content":"ING.......... ","link":"https://cythonlin.github.io/post/rs-greater-4/"},{"title":"RS => 3","content":"ING.......... ","link":"https://cythonlin.github.io/post/rs-greater-3/"},{"title":"PY => Python基础冷点备忘（2017-2021）","content":"目的 把远古写的笔记全部删除，浓缩到此篇文章！ 知其然且知其所以然！ 冷门且高级且易忘！！！😏😏 放一个最简单的大招😏 知道单元素元组为什么要 ( ,) 这样设计，而非 ()？ a = (1+2) # 3 b = (1+2,) # (3,) c = (1+2) * (3+4) # 21 👈 就是为了避开这个 d = (1+2,) + (3+4,) # (3, 7) 可变类型与不可变类型： 可变类型：[], {} # 可增删改 查 不可变类型: int float str () # 无法增删改， 只可查 区别：+ 与 += ： +: 无论是可变类型 还是 不可变类型， 都是新指向的空间 +=: 对于可变类型：原地操作，原指向，原空间 😀建议😀 对于不可变类型：异地操作，新指向，新空间 =+: 正数赋值 字典与集合 字典如下函数与集合的关系： keys() values() items() 返回值皆支持集合操作 py39字典新增 | （并集运算符） |= （并集赋值符） &gt;&gt;&gt; a = {1:1, 2:2} &gt;&gt;&gt; b = {1:3, 3:4} &gt;&gt;&gt; a | b {1: 3, 2: 2, 3: 4} &gt;&gt;&gt; b | a {1: 1, 3: 4, 2: 2} # 十个字概括： 键异扩新项，键同赋新值 # 注意1： | 后为新项/新值 # 注意2： 此运算符，是计算出的新结果，需要赋给新变量使用 提一嘴集合： 赋新值运算符：（并交叉） a | b a &amp; b a - b 函数不建议使用： 略 高阶函数 总是记不住高阶函数的参数位置吗？ 那我觉得这个概念有必要提一嘴：👇 函数是一等公民 了解一下！！！ map很简单，这里只说map双参的实现方案： 方案1：偏函数，基础，略 from functools import partial 略 😀方案2：starmap 多参，这才是我想说的，我相信很少人用过 from itertools import starmap list( starmap( lambda x, y,z: print(x + 100, y + 200,z+300), ((1,2,3),(4,5,6)) # 可见虽然dict.items()，不是严格双层，但也帮我们强转 ) ) # 101 202 303 # 104 205 306 #😀😀 不仅可以传 Sim-dict 这种 Nx2格式的， 还能传 NxN格式的（N行N列） zip 很简单，唯一注意的就是不规则参数 只取前N为能1V1匹配到的，如果不匹配并不会报错，而是直接过滤单个的 In [132]: list(zip((1,2),(3,4,5))) Out[132]: [(1, 3), (2, 4)] 字典生成式 和 集合生成式 易混区分 集合生成式，（x,y必须要用序列包起来） set1 = {(x,y) for x,y in dict(a=1,b=2).items()} print(set1) 字典生成式，必须要写 : , 谨记 必须冒号，才是字典😀😀并且一定不要用括号 dict1 = {x:y for x,y in dict(a=1,b=2).items()} print(dict1) 三器 可迭代对象、生成器、迭代器三者的关系： 1. 迭代器，生成器一定是可迭代对象 2. 生成器是迭代器的一种（子集） 3. 可迭代对象：必须实现 __iter__方法 4. 迭代器：必须实现 __iter__方法 和__next__方法 5. 😀生成器：包含yield的函数 或 (生成器推导语法) 才是Generator生成器，否则都是迭代器 6. 😀生成器看着是函数，但是它底层依然是实现了 __iter__() 和 __next__() 的类 7. 可迭代对象都可被for循环 所遍历， 另外实现了 __getitem__的类 其对象也可for遍历 8. 工具包： from collections import Iterable,Iterator （PY39已被废除） 9. 新工具包：from collections.abc import Iterator,Generator,Iterable 验证生成器是否 是（迭代器，生成器，可迭代对象）代码： from collections.abc import Iterator,Generator,Iterable def fun(): for x in range(10): a = yield x print(a) f = fun() print(isinstance(f, Iterator)) # True print(isinstance(f, Generator)) # True print(isinstance(f, Iterable)) # True 验证不具有yield 或 ()推导生成式类是否为生成器： class A(): def __iter__(self): pass def __next__(self): pass obj = A() print(isinstance(obj, Iterator)) # True print(isinstance(obj, Generator)) # False # 👆上面这个False可以看出， 虽然实现了 __iter__和 __next__ # 👆但是并没有 yield 或 ()推导生成式，所以它只能叫迭代器，而不是生成器 print(isinstance(obj, Iterable)) # True Generator 推理单步迭代的3种方式： 先挑明： 他们都有 next(),和__next__() 但是😲 迭代器没有 send() 方法 生成器有 send() 方法：因为send()是专门为 yield提供的 代码解释： # next(f) 和 f.__next__() 这两种很简单，略 # 😀😀😀下面send()是重点，生成器专门为yield提供的 ---------------------------- def fun(): for x in range(10): a = yield x print(a) f = fun() # 无输出 f.send(None) # fun内无任何输出, 注意，连None都没有，只是开关的作用 c = f.send(&quot;100&quot;) # 执行后，输出值为 100 ， 也就是 打印a的值 print(c) # 这个返回值就是断点处x的值， 结果为 0， 因为这是第一次send 😀😀😀上述send(n)语法解释: 如果第一个使用send(n), 那么send参数必须是None， 死语法。（next不需要传参） Note1: 你也可使用 next(f) 和 f.__next__() 代替 第一个f.send(None) Note2: 第一个 next(f) 和 f.__next__() 和 f.send(None)， 都是起到开关的作用 yield流程： 1. f.send(None) # 长官输入指令给 fun函数，并执行到 😀yield 关键词及右侧停止。 2. a = yield x # 相当于一个士兵等待指令，等号😀左边还未执行，程序😀右边就被封锁了 3. f.send(100) # 长官输入100，士兵收到后就把程序😀右边解封，并执行等号左边 4. 于是 a 被赋值为 100 简单小结一下 iter(x) 可以把序列类型类型数据变为 迭代器，注意是迭代器 其实python里面不止有 Iterator 和 Generator 也有其他类型，如： zip()返回值 map()返回值 等 他们都是迭代器的一种。 isinstance True 还有一些，叫做Iterable可迭代对象 如常见序列， (),[],{}, {}.items(), range() 等 他们既不是迭代器，也不是生成器，它们只是可迭代对象 装饰器（不多废话，代码例子与注释鲜明联想） 标准装饰器： def f(func): @wraps(func) def f1(*args, **kwargs): print('上面加了-奶油') result = func(*args, **kwargs) # 我是面包 print('下面加了-沙拉') return result return f1 @f # 等价于 func = f(func) def func(): print(&quot;我是面包&quot;) func() # 输出结果 👇 # 上面加了-奶油 # 我是面包 # 下面加了-沙拉 带参数的装饰器：（把@单个符号看作一个部分， 把@后面所有当作整体然后返回结果 交给@） 其实就是最外面又套了个函数，然后这个函数 返回结果是返回之前的整体： from functools import wraps def 包装纸(纸质): # 带参数的装饰器 print(f&quot;在外面再包一层{纸质}&quot;) def f(func): @wraps(func) def f1(*args, **kwargs): print('上面加了-奶油') result = func(*args, **kwargs) # 我是面包 print('下面加了-沙拉') return result return f1 return f @包装纸(&quot;锡纸&quot;) # 等价于 func = f(func) def func(): print(&quot;我是面包&quot;) func() # 输出结果 👇 # 在外面再包一层锡纸 # 上面加了-奶油 # 我是面包 # 下面加了-沙拉 用装饰器装饰类： 和上面几乎一模一样， 就是装饰的是类 @f # 等价于 A = f(A) 😎看这里，就是这里变了，传递的是类，仅此而已 class A(): pass 类装饰器（上面说的都是用普通函数当作装饰器，其实类也可以作为装饰器）😎 # 其实这更简单... class A: def __init__(self, func): self.func = func def __call__(self, *args, **kwargs): self.func(*args, **kwargs) @A # 等价于 test = A(test) def test(): pass test() 跟着我的思路走😎： 1. @A 等价于 test = A(test) 2. 先执行 = 右面的，即 A(test)，这是类的实例化啊： 即第一步调用 __init__()， 3. __init__()会自动返回实例（底层是 __new__） 因为自动返回，所以无需return😎 4. 再执行等号左面的，即 test = 实例对象 5. 然后开始执行test()，即 实例对象(), 看到这里，自然而然就想到自动调 __call__() 继承（广度，C3, MRO） python3中super() 等价于 super(类, self)。 所以用哪个都无所谓，一模一样。 class A: def say(self, name): print(f'Im {name}') class B(A): def say(self, name): # 下面两种用法一模一样 super().say(name) # PY3新式 # super(B, self).say(name) # PY2遗留 👇super(D, self) 的定义说明： def super(cls, instance): mro_list = instance.__class_.mro() # 此类对应的下一个MRO类的 index，加1代表除了自己 next_mro_index = mro_list.index(cls) + 1 # 找到此索引对应的的类名并返回 return mro_list( next_mro_index ) 写了一大堆废话解释，全删了，直接记住下面的例子。 三个要点： Mro是广度， 遇super则跳（当yield看）。钻牛角尖毫无意义！！！！😑 1和2结合起来，先广度跳（B1跳B2， 然后跳Base， 再原路返回，完美） 秘诀： （😑 跳之前，从左到右，从下到上画 倒Z）（横画完，则向左上方画捺，再画横，循环...） （😑 跳之后，原路返回） 示例： class Base: def __init__(self, ): print(&quot;Base&quot;) class B1(Base): def __init__(self, ): print(&quot;B1&quot;) super(B1, self).__init__() print(&quot;sorry 1&quot;) class B2(Base): def __init__(self, ): print(&quot;B2&quot;) super(B2, self).__init__() print(&quot;sorry 2&quot;) class B12(B1,B2): def __init__(self, ): super(B12, self).__init__() b12 = B12() 打印结果👇： B1 B2 Base sorry 2 sorry 1 上下文管理器 实现方式1（协议__enter__+协议__exit__） class A: def __init__(self, filename, mode): self.f1 = open(filename, mode) print(&quot;人呢&quot;) def __enter__(self): print(&quot;打开了新文件&quot;) return self.f1 def __exit__(self, *args, **kwargs): print(&quot;关闭了新文件&quot;) self.f1.close() with A(&quot;1.txt&quot;, &quot;rb+&quot;) as f: print(&quot;读取了数据&quot;) print(f.read().decode(&quot;utf-8&quot;)) # 人呢 # 打开了新文件 # 读取了数据 # h哈哈哈哈哈哈哈哈哈 （这是文件read读出的内容） # 关闭了新文件 enter中return 的值， 就是 as 后面的值。 with后的 类必须实例化必须实例化 ，不然会报错 👍执行顺序是 init-&gt;enter-&gt;包围体-&gt;exit 实现方式2（@contextmanager）底层也是第一种 from contextlib import contextmanager as context @context def f(): print(&quot;yield之前的 f()调用&quot;) yield &quot;执行内容&quot; print(f&quot;yield之后&quot;, ) with f() as a: print(a) 属性描述符 方式1 class User: def __init__(self, name): self.__name = name def get_name(self): return self.__name def set_name(self, name): self.__name = name name = property(fget=get_name, fset=set_name, fdel=None) u = User(&quot;Tom&quot;) print(u.name) # Tom u.name = 'Bob' print(u.name) # Bob 方式2 class User: def __init__(self, name): self.__name = name @property def name(self): return self.__name @name.setter def name(self, name): self.__name=name u2 = User(&quot;Jack&quot;) print(u2.name) # Jack u2.name = &quot;Mike&quot; print(u2.name) # Mike 类魔法函数 __new__： 实例的生产器 class A: ''' 作用： 这句话是官方源码对 __new__作用的注释：&quot;&quot;&quot; Create and return a new object.&quot;&quot;&quot; 原理： 在实例化后，自动调用__init__, 其实在这之前还调用了__new__ ，可做拦截。 __new__的返回值， 就是实例化对象后的对象，返回什么对象就是什么。 并且返回值就是 __init__(self) 这里面的 self 使用： 若重写 __new__, 需要调用父类的 __new__() 。把super()结果返回就是正常使用的self return super(A, cls).__new__(cls) 扩展： 可用拦截这个特性，在实例传递给 __init__之前，对 instance做限制（单例） ''' # 单例 例子如下 instance = None # 类属性， 因为 __new__中本身就是cls类， 所以必须用cls调用 def __new__(cls, *args, **kwargs): if not cls.instance: cls.instance = super().__new__(cls) # 若无继承，则用object的new return cls.instance def __init__(self, value): print(value) # 实例化先把初始值传到了 __new__ (1,) # 然后才把初始值传到了 __init__ 1 # 实例化先把初始值传到了 __new__ (2,) # 然后才把初始值传到了 __init__ 2 # True 👍最后强调一下，调用基类的new时，注意传的参数， 只传 cls，不能多传。 👍我们自己定义的new, args和kwargs接收了实例化传的初始值， 并且还会把这个值无损传递给 init。 👍至于如何无损传递,不要钻牛角尖，也不要去new里面修改传递的参数，没意义，直接去 init操作值即可 __setattr__() 和__getattr__() 和__delattr__() &quot;&quot;&quot; __setattr__()：=号 属性赋值 会自动调用此方法 __getattr__()：.号 属性取值 会自动调用此方法 # 注：找不到属性才会调用此方法 __delattr__()：del 属性删除 会自动调用此方法 &quot;&quot;&quot; __getattribute__() &quot;&quot;&quot; 和 __getattr__() 一样，只不过 __getattribute__最先调用，并拦截了 __getattr__() &quot;&quot;&quot; __getitem__() &quot;&quot;&quot; 对对象进行 切片、索引、遍历 等 会自动调用此方法 &quot;&quot;&quot; __init_subclass__（） &quot;&quot;&quot; 被继承的类 会自动调用__init_subclass__ &quot;&quot;&quot; __contains__() xx in xx dir(a) 和 dir(A) 和 a.__dict__ 和 A.__dict__ 的区别 dir(): a：世间万物，全部都有（注意，你没听错，是全部😏） A：只比 a 少了一个 实例属性。 __dict__: a: 只列出（自己的实例属性 + 继承的实例属性） A: 只列出（自己的类属性+类方法+静态方法+实例方法） （注意😏没有继承，没有静态属性） 一些其他属性/方法 查看mro A.mro() 查看MRO列表的下一个基类。 A.__base__ 查看上一代的基类（直接继承关系的都算，间接的不算） A.__bases__ 查看对象的实例对应的原型类 a1.__class__ # 查看a1的类为 A 当然类也可以, eg： D.__class__ 结果是👍 &lt;class 'type'&gt; 👍type站在了食物链的顶端（和type() 查看类型是一个type） 👍毕竟python万物皆对象, 如果你觉得这句话不够严谨，你可以 print(type.__class__) print(type(type)) 结果都是， &lt;class 'type'&gt; 。 可以看作 type实现了😏😏&quot;小自举&quot; 浅拷贝+深拷贝（皮+肉举例） 谨记（copy不可变类型无意义， copy可变类型才有意义）： 无论深copy/浅copy 都无法 copy/deepcopy😏不可变类型的变量 即使你copy/deepcopy后，你id() 比较后，发现地址一致 用法（通常copy可变类型序列）： from copy import copy c1 = copy(&lt;序列&gt;) from copy import deepcopy d1 = deepcopy(&lt;序列&gt;) 深/浅copy区别： 浅拷贝：不同的皮肤 用 同一块肉 =&gt; 😏[ ]一层序列叫做皮 深拷贝：不同的皮肤 用 不同的肉 =&gt; 😏[ [] ] N层序列才叫肉 所以也就意味着： 一层序列（深/浅）无区别。 N 层序列（深/浅）才有区别： 浅：只能copy外层，内层公用地址。（别改内层数据，你改了，我也跟着变了。。） 深：全身copy个遍，全部新开地址。（你改你内外层的数据和我无关） Lambda lambda内执行多个函数: lambda: ([f1(), f2(), f3()]) lambda匿名 自执行函数（和JS很像）（lambda定义体必须用小括号括起来） (lambda x:print(x))(1) 解构赋值 a, *_ = range(10) *_, a = range(10) a,*_, b = range(10) 反射 模块反射两种方式 （import_module 和 __import__() ) 😏 必用import_mosule。 __import__()这个有BUG， 多路径不能导入 import_module from importlib import import_module parse = import_module('urllib.parse') parse.urlencode({'a':1}) # 正常运行 # 如果换成 __import__() 就会找不到模块，所以必用 import_module😏 变量反射 hasattr+getattr hasattr(random,'randint') randint = getattr(random,'randint') print(randint(0,1)) delattr: delattr(random, 'randint') 文件复制/假移动/真剪切 shutil模块很好用。 但是他的移动有说法： shutil.move() : 先复制后删除 （但稳定） os.rename(): 剪切（神速）也可改名（） 复制正常用即可： shutil.copy(源路径，目标路径) # 复制 目录遍历 os.walk &quot;&quot;&quot; os.walk() 是一个深度遍历模式的文件遍历函数 返回值是一个迭代器，遍历这个迭代器后，每一次的返回值都是如下顺序三种构成 1. current_path: 当前路径 2. dir_list: 其下目录列表 3. file_list: 其下文件列表 &quot;&quot;&quot; import os file_generator = os.walk('D:/虚拟E盘-代码空间/TF2') for current_dir, dir_list, file_list in file_generator: print(current_dir, dir_list, file_list) 非阻塞执行命令 主要代替os.system import subprocess res = subprocess.run('dir', shell=True, stdout=subprocess.PIPE) # 结果输入到res管道中去 print(res.stdout.decode('gbk')) # res管道中有输出日志，如果在win下，需要 decode global &amp; nonlocal global 作用于普通函数与类中 nonlocal 作用于闭包中 ","link":"https://cythonlin.github.io/post/rs-greater-2/"},{"title":"GO => Golang复习（未完待续）","content":"ENV GOROOT: 默认go安装根目录即可 D:\\code\\go GOPATH：工程目录（里面有bin, pkg, src） D:\\code\\go\\work # 我们只需用goland 去 open src目录即可 GOBIN：GOPATH里面的bin目录 作用： go install 生成的exe 会自动存到这里 D:\\code\\go\\work\\bin 此外我们还需将此目录放入到环境变量，随时随地exe执行 GO111MODULE off ：无模块支持，go 会从 GOPATH 和 vendor 文件夹寻找包。 on ：模块支持，go 会忽略 GOPATH 和 vendor 文件夹，只根据 go.mod 下载依赖。 auto ：在 $GOPATH/src 外面且根目录有 go.mod 文件时，开启模块支持。 平台+Go命令 查看GO的环境变量 go env | grep GOOS go env | grep GOARCH GOOS变量值是什么平台，编译后的文件就只能在什么平台运行 windows: go build .\\hello.go .\\hello.exe Goland terminal修改GOOS cmd: set GOOS=linux ps: $env:GOOS=&quot;linux&quot; # go env | grep GOOS # 此时已变为linux go build .\\hello.go .\\hello # 😴 执行失败，平台不兼容 linux: ./hello 执行成功 ./hello.exe 执行成功 （纯Linux是会执行失败的，但是这是WSL） # WSL特殊，因为WSL可以兼容执行windows exe的格式 go install 把目录下 go build 编译后的 exe文件，自动添加到 GOBIN目录下 # 需要把上面的环境变量配置好：GO111MODULE auto go install 赋值 := (python3.8也出了，不过，其用途是 条件赋值一体化) 特殊：空赋值（定义后可不使用，也不能使用）： _ a := 1 // a必须在下文使用 _,b := 1,2 // 必须在下文使用 // _,_ := 1,2 // 这种写法是错误的 _,_ = 1,2 // 当有所有变量都是空变量的时候， 用 = 即可，而不是用:= 真假 只有 true/false 不能使用 0 &quot;&quot; 之类的数据来代替真假（py可以， if 1:pass） i++ go: 只有 i++ 和 i+= , 且只能单独放一行 (i-- 和 i-= 也是可以的) py: 只有 i+= 交换变量的四种方式 下面代码，除了数据声明与定义，其他Py 和 Go 的语法都是一摸一样的（主要强调 异或方式） a := 1 b := 3 方式0 c := 0 c = a a = b b = c 方式1 a,b = b,a 方式2 a = a + b b = a - b a = a - b 方式3 （注：必须是整形， Py也一样） a = a ^ b b = a ^ b a = a ^ b fmt.Println(a) fmt.Println(b) string 原生字符串输入： python &quot;&quot;&quot; &quot;&quot;&quot; golang ` ` 反引号 python和golang都是内部不可变，外部可变（可以整体替换赋值） 都有 len() 数组 定长创建 方式1 arr1 := [3]int{1,2,3} 方式2 var arr2 [3]int arr2 = [3]int{1,2,3} fmt.Println(arr1) fmt.Println(arr2) 非定长创建 arr1 := []int{1,2,3} append: arr1 = append(arr1, 4) len: len(arr1) 😏cap: arr1 := []int{1,2,3} fmt.Println(len(arr1),cap(arr1)) // 3,3 arr1 = append(arr1, 4,5) // 5,6 fmt.Println(len(arr1),cap(arr1)) len()的变化是没什么好说的 cap() 首先 arr1的只有3个元素，容量就分了3个（满了） 既然满了，那就append一个元素，开一个空间？这太费劲了。 于是 go 会在你每次元素容量溢出的时候，为你😊2倍扩容。 😑注意0，不足1024，2倍扩容。 超过1024，1.25倍扩容。 😑注意1，这个二倍是在现有的容量进行2倍扩容。 eg: 3 * 2 = 6 6 * 2 = 12 ... 😊特别注意 如果是用 for 循环的方式append （） 那么你给多少长度，他就会给你扩充至多少。 😊虽然最后一次遍历输出看起来 len 和 cap 趋近相等 😊但是每一次循环的过程，还是采用 二倍扩容原则！！！ eg: arr1 := []int{1,2,3} fmt.Println(len(arr1),cap(arr1)) // 3,3 for i := 0; i &lt; 92; i++ { arr1 = append(arr1, i) } fmt.Println(len(arr1),cap(arr1)) // 95, 96 arr1 = append(arr1, 1,2,3,4,5) // 我们的容量只有96，所以越界了，2倍 fmt.Println(len(arr1),cap(arr1)) // 96, 192 遍历 方式1： for i, v := range arr2 { // 和python一样 v只是copy的值，并不能用v改变数组 fmt.Println(i,v) // index, value } // fmt.Println(i,v) // 报错 , 因为上面的 i, v 都是临时变量， python则体外可用 😎Note: Go的 i,和v都是临时面两 方式2： for i := 0; i &lt; len(arr2); i++ { fmt.Println(i,arr2[i]) } 切片 认知 和python一样，都是左闭右开 arr1 := []int{1,2,3,4,5,6,7} slice1 := arr1[2:5] fmt.Println(slice1) 使用方式 [:n] [m:] 数组转切片： // 这个和python截然相反，go意味着数组转切片，完全引用（指针），py表示浅拷贝 [:] 原理 切片就是数组的部分引用（指针）（2者都是互相，牵一发而动全身，） 😣这点和python完全相反（python 切出来就是新地址了） 创建指定空切片,长度len+容量cap 好处：一次性分配，成本开销低。 str2 := make([]string, 0, 20) // 如果不指定cap, 则默认与len一致 fmt.Println(len(str2), cap(str2)) // len=0, cap=20 浅拷贝 copy(新，旧) ， 二者必须都是切片类型 （数组转切片上面说了 [:]） arr1 := [5]int{1,2,3,4,5} slice1 := make([]int, len(arr1)) copy(slice1, arr1[:]) // arr1必须转切片， arr1[:] fmt.Println(slice1) if 无小括号 for 无小括号 指针 GC自动回收，不需要手动回收(py可 del 动态回收) new name := new(string) // new出来的就地址 *name = &quot;zhangsan&quot; ","link":"https://cythonlin.github.io/post/rs-greater/"},{"title":"RS => 推荐系统（七）DL-Wide&Deep","content":"TF20 =&gt; [TF20前面写过，直接见 =&gt; Tensorflow2.0语法 - 张量&amp;基本函数(一) =&gt; Tensorflow2.0语法 - dataset数据封装+训测验切割（二） =&gt; Tensorflow2.0语法 - keras_API的使用(三) =&gt; PY =&gt; Tensorflow2.0模型保存与部署(四) =&gt; Tensorflow2.0 GPU管理与分布式(五) ","link":"https://cythonlin.github.io/post/rs-greater-fu-xi/"},{"title":"RS => 推荐系统（六）推荐缓存服务+LR","content":"待推荐结果的redis缓存 目的： 对待推荐结果进行二级缓存，多级缓存减少数据库读取压力 步骤： 获取redis结果，进行判断 如果redis有 读取需要推荐的文章数量放回，并删除这些文章，并且放入推荐历史推荐结果中 如果redis当中不存在，则从wait_recommend中读取 如果wait_recommend中也没有，直接返回 如果wait_recommend有，从wait_recommend取出所有结果，定一个数量(如100篇)存入redis,剩下放回wait_recommend,不够100，全部放入redis，然后清空wait_recommend 从redis中拿出要推荐的文章结果，然后放入历史推荐结果中 增加一个缓存数据库 # 缓存在8号当中 cache_client = redis.StrictRedis(host=DefaultConfig.REDIS_HOST, port=DefaultConfig.REDIS_PORT, db=8, decode_responses=True) redis 8 号数据库读取 # 直接去redis拿取对应的键，如果为空 # 首先从获取redis结果，进行判断(缓存拿) key = 'reco:{}:{}:art'.format(temp.user_id, temp.channel_id) res = cache_client.zrevrange(key, 0, temp.article_num - 1) # 如果redis有，读取需要推荐的文章数量放回，并删除这些文章，并且放入推荐历史推荐结果中 if res: cache_client.zrem(key, *res) # 如果redis没有数据，进行wait_recommend读取，放入redis中 # 删除redis这个键 cache_client.delete(key) try: # 字符串变成列表 hbase_cache = eval(hbu.get_table_row('wait_recommend', 'reco:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode())) except Exception as e: logger.warning(&quot;{} WARN read user_id:{} wait_recommend exception:{} not exist&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, e)) hbase_cache = [] if not hbase_cache: # 如果wait_recommend中也没有，直接返回空，去进行一次召回读取、排序然后推荐 return hbase_cache # wait_recommend有，从wait_recommend取出所有结果，定一个数量(如100篇)的文章存入redis if len(hbase_cache) &gt; 100: logger.info( &quot;{} INFO reduce user_id:{} channel:{} wait_recommend data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) # 拿出固定数量（100）给redis cache_redis = hbase_cache[:100] # 放入redis缓存 cache_client.zadd(key, dict(zip(cache_redis, range(len(cache_redis))))) # 剩下的放回wait hbase结果 hbu.get_table_put('wait_recommend', 'reco:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), str(hbase_cache[100:]).encode()) else: logger.info( &quot;{} INFO delete user_id:{} channel:{} wait_recommend data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) # - wait_recommend不够一定数量，全部取出放入redis当中，直接推荐出去 # 清空wait_recommend hbu.get_table_put('wait_recommend', 'reco:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), str([]).encode()) # 放入redis缓存 cache_client.zadd(key, dict(zip(hbase_cache, range(len(hbase_cache))))) res = cache_client.zrevrange(key, 0, temp.article_num - 1) if res: cache_client.zrem(key, *res) 推荐出去的结果放入历史结果 # 进行类型转换 res = list(map(int, res)) logger.info(&quot;{} INFO get cache data and store user_id:{} channel:{} cache history data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) # 放入历史记录 hbu.get_table_put('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), str(res).encode(), timestamp=temp.time_stamp) return res 完整逻辑代码： from server import cache_client import logging from datetime import datetime logger = logging.getLogger('recommend') def get_cache_from_redis_hbase(temp, hbu): &quot;&quot;&quot; 进行用户频道推荐缓存结果的读取 :param temp: 用户请求的参数 :param hbu: hbase工具 :return: &quot;&quot;&quot; # - 首先从获取redis结果，进行判断(缓存拿) key = 'reco:{}:{}:art'.format(temp.user_id, temp.channel_id) res = cache_client.zrevrange(key, 0, temp.article_num - 1) # - 如果redis有，读取需要推荐的文章数量放回，并删除这些文章，并且放入推荐历史推荐结果中 if res: cache_client.zrem(key, *res) else: # - 如果redis当中不存在，则从wait_recommend中读取 # 删除redis这个键 cache_client.delete(key) try: # 字符串编程列表 hbase_cache = eval(hbu.get_table_row('wait_recommend', 'reco:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode())) except Exception as e: logger.warning(&quot;{} WARN read user_id:{} wait_recommend exception:{} not exist&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, e)) hbase_cache = [] if not hbase_cache: # - 如果wait_recommend中也没有，直接返回空，去进行一次召回读取、排序然后推荐 return hbase_cache # - wait_recommend有，从wait_recommend取出所有结果，定一个数量(如100篇)的文章存入redis if len(hbase_cache) &gt; 100: logger.info( &quot;{} INFO reduce user_id:{} channel:{} wait_recommend data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) # 拿出固定数量（100）给redis cache_redis = hbase_cache[:100] # 放入redis缓存 cache_client.zadd(key, dict(zip(cache_redis, range(len(cache_redis))))) # 剩下的放回wait hbase结果 hbu.get_table_put('wait_recommend', 'reco:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), str(hbase_cache[100:]).encode()) else: logger.info( &quot;{} INFO delete user_id:{} channel:{} wait_recommend data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) # - wait_recommend不够一定数量，全部取出放入redis当中，直接推荐出去 # 清空wait_recommend hbu.get_table_put('wait_recommend', 'reco:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), str([]).encode()) # 放入redis缓存 cache_client.zadd(key, dict(zip(hbase_cache, range(len(hbase_cache))))) res = cache_client.zrevrange(key, 0, temp.article_num - 1) if res: cache_client.zrem(key, *res) # 进行执行PL，然后写入历史推荐结果 # 进行类型转换 res = list(map(int, res)) logger.info(&quot;{} INFO get cache data and store user_id:{} channel:{} cache history data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) # 放入历史记录 hbu.get_table_put('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), str(res).encode(), timestamp=temp.time_stamp) return res 在推荐中心加入缓存逻辑 from server import redis_cache # 1、获取缓存 res = redis_cache.get_reco_from_cache(temp, self.hbu) # 如果没有，然后走一遍算法推荐 召回+排序，同时写入到hbase待推荐结果列表 if not res: logger.info(&quot;{} INFO get user_id:{} channel:{} recall/sort data&quot;. format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) res = self.user_reco_list(temp) 排序模型在线预测 前文 之前在CTR那里已经训练好了LR，并且后来特征中心已经把 用户和Item特征抽取出放到了hbase 接下来的步骤 召回之后的文章结果进行排序 步骤： 读取用户特征中心特征 读取文章特征中心特征、合并用户文章特征构造预测样本 预测并进行排序是筛选 配置Spark环境代码： import os import sys # 如果当前代码文件运行测试需要加入修改路径，避免出现后导包问题 BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd())) sys.path.insert(0, os.path.join(BASE_DIR)) PYSPARK_PYTHON = &quot;/miniconda2/envs/reco_sys/bin/python&quot; os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHON os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHON from pyspark import SparkConf from pyspark.sql import SparkSession from server.utils import HBaseUtils from server import pool from pyspark.ml.linalg import DenseVector from pyspark.ml.classification import LogisticRegressionModel import pandas as pd conf = SparkConf() config = ( (&quot;spark.app.name&quot;, &quot;sort&quot;), (&quot;spark.executor.memory&quot;, &quot;2g&quot;), # 设置该app启动时占用的内存用量，默认1g (&quot;spark.master&quot;, 'yarn'), (&quot;spark.executor.cores&quot;, &quot;2&quot;), # 设置spark executor使用的CPU核心数 ) conf.setAll(config) spark = SparkSession.builder.config(conf=conf).getOrCreate() 读取用户特征中心(10个)（用户特征权重： weights ） hbu = HBaseUtils(pool) # 排序 # 1、读取用户特征中心特征 try: user_feature = eval(hbu.get_table_row('ctr_feature_user', '{}'.format(1115629498121846784).encode(), 'channel:{}'.format(18).encode())) except Exception as e: user_feature = [] 读取文章特征中心111个： if user_feature: # 2、读取文章特征中心特征 # 当初存的时候是这样的存储格式（ 一共111个）： # channel_id: 1个特征 article_feature[0] # article_weights: 10个特征 article_feature[1:11] # articlevector: 100个特征 article_feature[11:] result = [] for article_id in [17749, 17748, 44371, 44368]: try: article_feature = eval(hbu.get_table_row('ctr_feature_article', '{}'.format(article_id).encode(), 'article:{}'.format(article_id).encode())) except Exception as e: article_feature = [0.0] * 111 f = [] # 第一个channel_id f.extend([article_feature[0]]) # 第二个article_vector f.extend(article_feature[11:]) # 第三个用户权重特征 f.extend(user_feature) # 第四个文章权重特征 f.extend(article_feature[1:11]) vector = DenseVector(f) result.append([1115629498121846784, article_id, vector]) 文章特征表110个，合并后👇 (合并特征向量(channel_id1个+文章向量100个+用户特征权重10个+文章关键词权重) = 121个特征) 最终结果result格式 [ [ article_id, user_id DenseVector([channel_id, weights, article_weights]) ], [ article_id, user_id DenseVector([channel_id, weights, article_weights]) ] ] 处理样本格式，模型加载预测 # 4、预测并进行排序是筛选 df = pd.DataFrame(result, columns=[&quot;user_id&quot;, &quot;article_id&quot;, &quot;features&quot;]) test = spark.createDataFrame(df) # 加载逻辑回归模型 model = LogisticRegressionModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/LR.obj&quot;) predict = model.transform(test) 预测结果进行筛选 def vector_to_double(row): return float(row.article_id), float(row.probability[1]) res = predict.select(['article_id', 'probability']).rdd.map(vector_to_double).toDF(['article_id', 'probability']).sort('probability', ascending=False) 获取排序之后前N个文章 article_list = [i.article_id for i in res.collect()] if len(article_list) &gt; 100: article_list = article_list[:100] reco_set = list(map(int, article_list)) 添加实时排序的模型预测 添加spark配置，grpc启动灰将spark相关信息初始化 from pyspark import SparkConf from pyspark.sql import SparkSession # spark配置 conf = SparkConf() conf.setAll(DefaultConfig.SPARK_GRPC_CONFIG) SORT_SPARK = SparkSession.builder.config(conf=conf).getOrCreate() # SPARK grpc配置 SPARK_GRPC_CONFIG = ( (&quot;spark.app.name&quot;, &quot;grpcSort&quot;), # 设置启动的spark的app名称，没有提供，将随机产生一个名称 (&quot;spark.master&quot;, &quot;yarn&quot;), (&quot;spark.executor.instances&quot;, 4) ) 添加模型服务预测模块，sort_service, 增加以下预测逻辑 from server import SORT_SPARK from pyspark.ml.linalg import DenseVector from pyspark.ml.classification import LogisticRegressionModel import pandas as pd import numpy as np from datetime import datetime import logging logger = logging.getLogger(&quot;recommend&quot;) 预测函数 def lr_sort_service(reco_set, temp, hbu): &quot;&quot;&quot; 排序返回推荐文章 :param reco_set:召回合并过滤后的结果 :param temp: 参数 :param hbu: Hbase工具 :return: &quot;&quot;&quot; # 排序 # 1、读取用户特征中心特征 try: user_feature = eval(hbu.get_table_row('ctr_feature_user', '{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode())) logger.info(&quot;{} INFO get user user_id:{} channel:{} profile data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) except Exception as e: user_feature = [] if user_feature: # 2、读取文章特征中心特征 result = [] for article_id in reco_set: try: article_feature = eval(hbu.get_table_row('ctr_feature_article', '{}'.format(article_id).encode(), 'article:{}'.format(article_id).encode())) except Exception as e: article_feature = [0.0] * 111 f = [] # 第一个channel_id f.extend([article_feature[0]]) # 第二个article_vector f.extend(article_feature[11:]) # 第三个用户权重特征 f.extend(user_feature) # 第四个文章权重特征 f.extend(article_feature[1:11]) vector = DenseVector(f) result.append([temp.user_id, article_id, vector]) # 4、预测并进行排序是筛选 df = pd.DataFrame(result, columns=[&quot;user_id&quot;, &quot;article_id&quot;, &quot;features&quot;]) test = SORT_SPARK.createDataFrame(df) # 加载逻辑回归模型 model = LogisticRegressionModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/LR.obj&quot;) predict = model.transform(test) def vector_to_double(row): return float(row.article_id), float(row.probability[1]) res = predict.select(['article_id', 'probability']).rdd.map(vector_to_double).toDF( ['article_id', 'probability']).sort('probability', ascending=False) article_list = [i.article_id for i in res.collect()] logger.info(&quot;{} INFO sorting user_id:{} recommend article&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id)) # 排序后，只将排名在前100个文章ID返回给用户推荐 if len(article_list) &gt; 100: article_list = article_list[:100] reco_set = list(map(int, article_list)) return reco_set 推荐中心加入排序 # 配置default RAParam = param( COMBINE={ 'Algo-1': (1, [100, 101, 102, 103, 104], [200]), # 首页推荐，所有召回结果读取+LR排序 'Algo-2': (2, [100, 101, 102, 103, 104], [200]) # 首页推荐，所有召回结果读取 排序 }, # reco_center from server.sort_service import lr_sort_service sort_dict = { 'LR': lr_sort_service, } # 排序代码逻辑 _sort_num = RAParam.COMBINE[temp.algo][2][0] reco_set = sort_dict[RAParam.SORT[_sort_num]](reco_set, temp, self.hbu) 为了测试,原来的数据重新插入一份，历史记录也删除掉，缓存也删掉， hbase(main):016:0* put 'cb_recall', 'recall:user:1115629498121846784', 'als:18', [19200, 17665, 16151, 16411, 19233, 13090,15140, 16421, 19494, 14381, 17966, 17454, 14125, 16174, 14899, 44339, 16437, 18743, 44090, 18238, 13890, 14915, 15429, 15944, 44371, 18005, 15196, 13410, 13672, 44137, 18795, 19052, 44652, 44654, 44657, 14961, 17522, 43894, 44412, 16000, 14208, 44419, 17802, 14223, 18836, 140956, 18335, 13728, 14498, 44451, 44456, 18609, 18353, 44468, 18103, 135869, 16062, 14015, 13757, 13249, 44483, 17605, 14021, 15309, 18127, 43983, 44754, 43986, 19413, 14805, 18904, 44761, 17114, 13272, 14810, 18907, 13022, 14299, 17120, 17632, 43997, 17889, 17385, 18156, 15085, 13295, 44020, 14839, 44024, 14585, 18172, 44541] Took 0.2007 seconds hbase(main):017:0&gt; get 'cb_recall', 'recall:user:1115629498121846784' COLUMN CELL als:13 timestamp=1558041571134, value=[141431] als:18 timestamp=1559205376286, value=[19200, 17665, 16151, 16411, 19233, 13090, 15140, 16421, 19494, 14381, 17966, 17454, 14125, 16174, 14899, 44339, 16 437, 18743, 44090, 18238, 13890, 14915, 15429, 15944, 44371, 18005, 15196 , 13410, 13672, 44137, 18795, 19052, 44652, 44654, 44657, 14961, 17522, 4 3894, 44412, 16000, 14208, 44419, 17802, 14223, 18836, 140956, 18335, 137 28, 14498, 44451, 44456, 18609, 18353, 44468, 18103, 135869, 16062, 14015 , 13757, 13249, 44483, 17605, 14021, 15309, 18127, 43983, 44754, 43986, 1 9413, 14805, 18904, 44761, 17114, 13272, 14810, 18907, 13022, 14299, 1712 0, 17632, 43997, 17889, 17385, 18156, 15085, 13295, 44020, 14839, 44024, 14585, 18172, 44541] als:5 timestamp=1558041564668, value=[141440] als:7 timestamp=1558041564688, value=[141437] 1 row(s) Took 0.1108 seconds supervisor添加grpc实时推荐程序 [program:online] environment=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python ,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python command=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/reco_sys/abtest/routing.py directory=/root/toutiao_project/reco_sys/abtest user=root autorestart=true redirect_stderr=true stdout_logfile=/root/logs/recommendsuper.log loglevel=info stopsignal=KILL stopasgroup=true killasgroup=true supervisor执行命令 update,开启实时排序测试。 ","link":"https://cythonlin.github.io/post/go-greater-fu-xi/"},{"title":"RS => 推荐系统（五）实时推荐+ABTest+推荐中心","content":"业务流程 后端发送推荐请求，实时推荐系统拿到请求参数 grpc对接 对用户做ABTest分流 ABTest实验中心，用于进行分流任务，方便测试调整不同的模型上线 推荐中心服务 根据用户在ABTest分配的不同的算法进行召回服务和排序服务读取返回结果 返回推荐结果和埋点参数封装 grpc接口对接介绍 推荐接口对接 请求参数： feed流推荐： 用户ID 频道ID 推荐文章数量 请求推荐时间戳 相似文章获取：文章ID，推荐文章数量 文章ID 推荐文章数量（猜你喜欢，非主推荐） 返回参数： feed流推荐：曝光参数，每篇文章的所有行为参数，上一条时间戳 # 埋点参数参考： # { # &quot;param&quot;: '{&quot;action&quot;: &quot;exposure&quot;, &quot;userId&quot;: 1, &quot;articleId&quot;: [1,2,3,4], &quot;algorithmCombine&quot;: &quot;c1&quot;}', # &quot;recommends&quot;: [ # {&quot;article_id&quot;: 1, &quot;param&quot;: {&quot;click&quot;: &quot;{&quot;action&quot;: &quot;click&quot;, &quot;userId&quot;: &quot;1&quot;, &quot;articleId&quot;: 1, &quot;algorithmCombine&quot;: 'c1'}&quot;, &quot;collect&quot;: &quot;&quot;, &quot;share&quot;: &quot;&quot;,&quot;read&quot;:&quot;&quot;}}, # {&quot;article_id&quot;: 2, &quot;param&quot;: {&quot;click&quot;: &quot;&quot;, &quot;collect&quot;: &quot;&quot;, &quot;share&quot;: &quot;&quot;, &quot;read&quot;:&quot;&quot;}}, # {&quot;article_id&quot;: 3, &quot;param&quot;: {&quot;click&quot;: &quot;&quot;, &quot;collect&quot;: &quot;&quot;, &quot;share&quot;: &quot;&quot;, &quot;read&quot;:&quot;&quot;}}, # {&quot;article_id&quot;: 4, &quot;param&quot;: {&quot;click&quot;: &quot;&quot;, &quot;collect&quot;: &quot;&quot;, &quot;share&quot;: &quot;&quot;, &quot;read&quot;:&quot;&quot;}} # ] # &quot;timestamp&quot;: 1546391572 # } 对接通信方式-GRPC GRPC优点： gRPC是由Google公司开源的高性能RPC框架。 gRPC支持多语言 gRPC原生使用C、Java、Go进行了三种实现，而C语言实现的版本进行封装后又支持C++、C#、Node、ObjC、 Python、Ruby、PHP等开发语言 gRPC支持多平台 支持的平台包括：Linux、Android、iOS、MacOS、Windows gRPC的消息协议使用Google自家开源的Protocol Buffers协议机制（proto3） 序列化 gRPC的传输使用HTTP/2标准，支持双向流和连接多路复用 使用方法 使用Protocol Buffers（proto3）的IDL接口定义语言定义接口服务，编写在文本文件（以.proto为后缀名）中。 使用protobuf编译器生成服务器和客户端使用的stub代码 代码结构 文件名后缀: .proto 版本声明：Protocol Buffers文档的第一行非注释行，为版本申明，不填写的话默认为版本2。 syntax = &quot;proto3&quot;; 或者 syntax = &quot;proto2&quot;; 消息类型：Protocol Buffers使用message定义消息数据。在Protocol Buffers中使用的数据都是通过message消息数据封装基本类型数据或其他消息数据，对应Python中的类。 message SearchRequest { string query = 1; int32 page_number = 2; int32 result_per_page = 3; } # string: 数据类型 # query: 数据别名 # 1: 字段编号 字段编号：在使用消息类型后不应更改。 请注意，1到15范围内的字段编号需要一个字节进行编码，包括字段编号和字段类型。16到2047范围内的字段编号占用两个字节 # singular：格式良好的消息可以包含该字段中的零个或一个（但不超过一个）。 # repeated：此字段可以在格式良好的消息中重复任意次数（包括零）。将保留重复值的顺序。对应Python的列表 message Result { string url = 1; string title = 2; repeated string snippets = 3; } 安装protobuf编译器和grpc库 pip install grpcio-tools 编译生成代码 python -m grpc_tools.protoc -I. --python_out=.. --grpc_python_out=.. itcast.proto -I表示搜索proto文件中被导入文件的目录 --python_out表示保存生成Python文件的目录，生成的文件中包含接口定义中的数据类型 --grpc_python_out表示保存生成Python文件的目录，生成的文件中包含接口定义中的服务类型 推荐接口protoco协议定义实例 创建abtest目录，将相关接口代码放入user_reco.proto协议文件 用户刷新feed流接口 user_recommend(User) returns (Track) 文章相似(猜你喜欢)接口 article_recommend(Article) returns(Similar) syntax = &quot;proto3&quot;; message User { string user_id = 1; int32 channel_id = 2; int32 article_num = 3; int64 time_stamp = 4; } // int32 ---&gt; int64 article_id message Article { int64 article_id = 1; int32 article_num = 2; } message param2 { string click = 1; string collect = 2; string share = 3; string read = 4; } message param1 { int64 article_id = 1; param2 params = 2; } message Track { string exposure = 1; repeated param1 recommends = 2; int64 time_stamp = 3; } message Similar { repeated int64 article_id = 1; } service UserRecommend { // feed recommend rpc user_recommend(User) returns (Track) {} rpc article_recommend(Article) returns(Similar) {} } 通过命令生成2个py文件（客户端，服务端） python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. user_reco.proto 👍一个文件名叫做：user_reco_pb2.py （主要功能是定义消息体） 👍另一个文件名叫做：user_reco_pb2_grpc.py（功能是定义服务，返回上面的消息体） Grpc服务编写（服务器端） 创建routing.py文件，写入服务端代码： import os import sys BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) sys.path.insert(0, os.path.join(BASE_DIR)) from concurrent import futures from abtest import user_reco_pb2 from abtest import user_reco_pb2_grpc from setting.default import DefaultConfig import grpc import time import json 需要添加grpc服务配置： # rpc RPC_SERVER = '192.168.19.137:9999' 完整代码(相当于django和flask的类视图)： # 基于用户推荐的rpc服务推荐 # 定义指定的rpc服务输入输出参数格式proto class UserRecommendServicer(user_reco_pb2_grpc.UserRecommendServicer): &quot;&quot;&quot; 对用户进行技术文章推荐 &quot;&quot;&quot; # 这个方法就是 GRPC生成的，复写即可 def user_recommend(self, request, context): &quot;&quot;&quot; 用户feed流推荐 :param request: :param context: :return: &quot;&quot;&quot; # 选择C4组合 user_id = request.user_id channel_id = request.channel_id article_num = request.article_num time_stamp = request.time_stamp # 解析参数，并进行推荐中心推荐(暂时使用假数据替代) class Temp(object): user_id = -10 algo = 'test' time_stamp = -10 tp = Temp() tp.user_id = user_id tp.time_stamp = time_stamp _track = add_track([], tp) # 解析返回参数到rpc结果参数 # 参数如下 # [ {&quot;article_id&quot;: 1, &quot;param&quot;: {&quot;click&quot;: &quot;&quot;, &quot;collect&quot;: &quot;&quot;, &quot;share&quot;: &quot;&quot;, 'detentionTime':''}}, # {&quot;article_id&quot;: 2, &quot;param&quot;: {&quot;click&quot;: &quot;&quot;, &quot;collect&quot;: &quot;&quot;, &quot;share&quot;: &quot;&quot;, 'detentionTime':''}}, # {&quot;article_id&quot;: 3, &quot;param&quot;: {&quot;click&quot;: &quot;&quot;, &quot;collect&quot;: &quot;&quot;, &quot;share&quot;: &quot;&quot;, 'detentionTime':''}}, # {&quot;article_id&quot;: 4, &quot;param&quot;: {&quot;click&quot;: &quot;&quot;, &quot;collect&quot;: &quot;&quot;, &quot;share&quot;: &quot;&quot;, 'detentionTime':''}} # ] # 第二个rpc参数 _param1 = [] for _ in _track['recommends']: # param的封装 _params = user_reco_pb2.param2(click=_['param']['click'], collect=_['param']['collect'], share=_['param']['share'], read=_['param']['read']) _p2 = user_reco_pb2.param1(article_id=_['article_id'], params=_params) _param1.append(_p2) # param return user_reco_pb2.Track(exposure=_track['param'], recommends=_param1, time_stamp=_track['timestamp']) # def article_recommend(self, request, context): # &quot;&quot;&quot; # 文章相似推荐 # :param request: # :param context: # :return: # &quot;&quot;&quot; # # 获取web参数 # article_id = request.article_id # article_num = request.article_num # # # 进行文章相似推荐,调用推荐中心的文章相似 # _article_list = article_reco_list(article_id, article_num, 105) # # # rpc参数封装 # return user_reco_pb2.Similar(article_id=_article_list) def serve(): # 多线程服务器 server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) # 注册本地服务（server里面只修改此类即可UserRecommendServicer） # 此类是我们自定义的，并且继承自， 用 grpc命令行生成的文件里的类 user_reco_pb2_grpc.add_UserRecommendServicer_to_server(UserRecommendServicer(), server) # 监听端口 server.add_insecure_port(DefaultConfig.RPC_SERVER) # 开始接收请求进行服务 server.start() # 使用 ctrl+c 可以退出服务 _ONE_DAY_IN_SECONDS = 60 * 60 * 24 try: while True: time.sleep(_ONE_DAY_IN_SECONDS) except KeyboardInterrupt: server.stop(0) if __name__ == '__main__': # 测试grpc服务 serve() 埋点参数的接口封装： class Temp(object): user_id = '1115629498121846784' algo = 'test' time_stamp = int(time.time() * 1000) _track = add_track([], Temp()) web后台请求传入的时间戳是time.time(),Out[3]: int(1558128143.8735564) * 1000的大小 def add_track(res, temp): &quot;&quot;&quot; 封装埋点参数 :param res: 推荐文章id列表 :param cb: 合并参数 :param rpc_param: rpc参数 :return: 埋点参数 文章列表参数 单文章参数 &quot;&quot;&quot; # 添加埋点参数 track = {} # 准备曝光参数 # 全部字符串形式提供，在hive端不会解析问题 _exposure = {&quot;action&quot;: &quot;exposure&quot;, &quot;userId&quot;: temp.user_id, &quot;articleId&quot;: json.dumps(res), &quot;algorithmCombine&quot;: temp.algo} track['param'] = json.dumps(_exposure) track['recommends'] = [] # 准备其它点击参数 for _id in res: # 构造字典 _dic = {} _dic['article_id'] = _id _dic['param'] = {} # 准备click参数 _p = {&quot;action&quot;: &quot;click&quot;, &quot;userId&quot;: temp.user_id, &quot;articleId&quot;: str(_id), &quot;algorithmCombine&quot;: temp.algo} _dic['param']['click'] = json.dumps(_p) # 准备collect参数 _p[&quot;action&quot;] = 'collect' _dic['param']['collect'] = json.dumps(_p) # 准备share参数 _p[&quot;action&quot;] = 'share' _dic['param']['share'] = json.dumps(_p) # 准备detentionTime参数 _p[&quot;action&quot;] = 'read' _dic['param']['read'] = json.dumps(_p) track['recommends'].append(_dic) track['timestamp'] = temp.time_stamp return track 客户端测试代码 测试客户端 import os import sys BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) sys.path.insert(0, os.path.join(BASE_DIR)) from abtest import user_reco_pb2_grpc from abtest import user_reco_pb2 import grpc from setting.default import DefaultConfig import time def test(): article_dict = {} # 构造传入数据 req_article = user_reco_pb2.User() req_article.user_id = '1115629498121846784' req_article.channel_id = 18 req_article.article_num = 10 req_article.time_stamp = int(time.time() * 1000) # req_article.time_stamp = 1555573069870 with grpc.insecure_channel(DefaultConfig.RPC_SERVER) as rpc_cli: print('''''') try: stub = user_reco_pb2_grpc.UserRecommendStub(rpc_cli) resp = stub.user_recommend(req_article) except Exception as e: print(e) article_dict['param'] = [] else: # 解析返回结果参数 article_dict['exposure_param'] = resp.exposure reco_arts = resp.recommends reco_art_param = [] reco_list = [] for art in reco_arts: reco_art_param.append({ 'artcle_id': art.article_id, 'params': { 'click': art.params.click, 'collect': art.params.collect, 'share': art.params.share, 'read': art.params.read } }) reco_list.append(art.article_id) article_dict['param'] = reco_art_param # 文章列表以及参数（曝光参数 以及 每篇文章的点击等参数） print(reco_list, article_dict) if __name__ == '__main__': test() ABtest ABtest有几个重要的功能: 一个是ABTest实时分流服务，根据用户设备信息、用户信息进行ab分流。 实时效果分析统计，将分流后程序点击、浏览等通过hive、hadoop程序统计后，在统计平台上进行展示。 用图表数据分析用户活跃度、点击率 流量切分参数配置 A/B测试的流量切分是在Rank Server端完成的。我们根据用户ID将流量切分为多个桶（Bucket），每个桶对应一种排序策略，桶内流量将使用相应的策略进行排序。使用ID进行流量切分，是为了保证用户体验的一致性。 from collections import namedtuple # abtest参数信息 # ABTest参数 param = namedtuple('RecommendAlgorithm', ['COMBINE', 'RECALL', 'SORT', 'CHANNEL', 'BYPASS'] ) RAParam = param( COMBINE={ 'Algo-1': (1, [100, 101, 102, 103, 104], []), # 首页推荐，所有召回结果读取+LR排序 'Algo-2': (2, [100, 101, 102, 103, 104], []) # 首页推荐，所有召回结果读取 排序 }, RECALL={ 100: ('cb_recall', 'als'), # 离线模型ALS召回，recall:user:1115629498121 column=als:18 101: ('cb_recall', 'content'), # 离线word2vec的画像内容召回 'recall:user:5', 'content:1' 102: ('cb_recall', 'online'), # 在线word2vec的画像召回 'recall:user:1', 'online:1' 103: 'new_article', # 新文章召回 redis当中 ch:18:new 104: 'popular_article', # 基于用户协同召回结果 ch:18:hot 105: ('article_similar', 'similar') # 文章相似推荐结果 '1' 'similar:2' }, SORT={ 200: 'LR', }, CHANNEL=25, BYPASS=[ { &quot;Bucket&quot;: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd'], &quot;Strategy&quot;: &quot;Algo-1&quot; }, { &quot;BeginBucket&quot;: ['e', 'f'], &quot;Strategy&quot;: &quot;Algo-2&quot; } ] ) 实验中心流量切分 哈希分桶，md5 推荐刷新逻辑(通过时间戳区分主要逻辑) ABTest分流逻辑实现代码如下 import hashlib from setting.default import DefaultConfig, RAParam def feed_recommend(user_id, channel_id, article_num, time_stamp): &quot;&quot;&quot; 1、根据web提供的参数，进行分流 2、找到对应的算法组合之后，去推荐中心调用不同的召回和排序服务 3、进行埋点参数封装 :param user_id:用户id :param article_num:推荐文章个数 :return: track:埋点参数结果: 参考上面埋点参数组合 &quot;&quot;&quot; # 产品前期推荐由于较少的点击行为，所以去做 用户冷启动 + 文章冷启动 # 用户冷启动：'推荐'频道：热门频道的召回+用户实时行为画像召回（在线的不保存画像） 'C2'组合 # # 其它频道：热门召回 + 新文章召回 'C1'组合 # 定义返回参数的类 class TempParam(object): user_id = -10 channel_id = -10 article_num = -10 time_stamp = -10 algo = &quot;&quot; temp = TempParam() temp.user_id = user_id temp.channel_id = channel_id temp.article_num = article_num # 请求的时间戳大小 temp.time_stamp = time_stamp # 先读取缓存数据redis+待推荐hbase结果 # 如果有返回并加上埋点参数 # 并且写入hbase 当前推荐时间戳用户（登录和匿名）的历史推荐文章列表 # 传入用户id为空的直接召回结果 if temp.user_id == &quot;&quot;: temp.algo = &quot;&quot; return add_track([], temp) # 进行分桶实现分流，制定不同的实验策略 bucket = hashlib.md5(user_id.encode()).hexdigest()[:1] if bucket in RAParam.BYPASS[0]['Bucket']: temp.algo = RAParam.BYPASS[0]['Strategy'] else: temp.algo = RAParam.BYPASS[1]['Strategy'] # 推荐服务中心推荐结果(这里做测试) track = add_track([], temp) return track 推荐中心逻辑 推荐中心推荐存储设计 HBASE 数据库表设计 wait_recommend: 经过各种多路召回，排序之后的待推荐结果保存 只要刷新一次，没有缓存，才主动收集各种召回集合一起给wait_recommend写入，所以不用设置多个版本 history_recommend: 每次真正推荐出去给用户的历史推荐结果列表 按照频道存储用户的历史推荐结果 需要保留多个版本，才需要建立版本信息 创建一个存储表（用来存储排序后推荐出去后， 剩余的Item）： create 'wait_recommend', 'channel' put 'wait_recommend', 'reco:1', 'channel:18', [17283, 140357, 14668, 15182, 17999, 13648, 12884, 17302, 13846, 18135] put 'wait_recommend', 'reco:1', 'channel:0', [17283, 140357, 14668, 15182, 17999, 13648, 12884, 17302, 13846, 18135] 创建一个历史hbase结果（用于存储推荐过的Item）： create 'history_recommend', {NAME=&gt;'channel', TTL=&gt;7776000, VERSIONS=&gt;999999} 86400 # 每次指定一个时间戳,可以达到不同版本的效果 put 'history_recommend', 'reco:his:1', 'channel:18', [17283, 140357, 14668, 15182, 17999, 13648, 12884, 17302, 13846, 18135] # 修改的时候必须指定family名称 hbase(main):084:0&gt; alter 'history_recommend',NAME =&gt; 'channel', TTL =&gt; '7776000' Updating all regions with the new schema... 1/1 regions updated. Done. Took 2.0578 seconds alter 'history_recommend',NAME =&gt; 'channel', VERSIONS=&gt;999999, TTL=&gt;7776000 放入历史数据，存在时间戳，到时候取出历史数据就是每个用户的历史时间戳可以 get &quot;history_recommend&quot;, 'reco:his:1', {COLUMN=&gt;'channel:18',VERSIONS=&gt;1000, TIMESTAMP=&gt;1546242869000} 👍这里与上次召回cb_recall以及history_recall有不同用处： history_recall:存放召回过的数据，用户过滤推荐初始的产生结果 history_recommend:存放的是某个用户在某频道的真正推荐过的历史记录 同时过滤掉新文章和热门文章的推荐结果 feed流 推荐中心逻辑 目的：根据ABTest分流之后的用户，进行制定算法的召回和排序读取 步骤： 1、Hbase数据库工具封装介绍 2、feed时间戳进行推荐逻辑判断 2、读取召回结果(无实时排序) 创建特征中心类： import os import sys BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) sys.path.insert(0, os.path.join(BASE_DIR)) import hashlib from setting.default import RAParam from server.utils import HBaseUtils from server import pool from server import recall_service from datetime import datetime import logging import json logger = logging.getLogger('recommend') def add_track(res, temp): &quot;&quot;&quot; 封装埋点参数 :param res: 推荐文章id列表 :param cb: 合并参数 :param rpc_param: rpc参数 :return: 埋点参数 文章列表参数 单文章参数 &quot;&quot;&quot; # 添加埋点参数 track = {} # 准备曝光参数 # 全部字符串形式提供，在hive端不会解析问题 _exposure = {&quot;action&quot;: &quot;exposure&quot;, &quot;userId&quot;: temp.user_id, &quot;articleId&quot;: json.dumps(res), &quot;algorithmCombine&quot;: temp.algo} track['param'] = json.dumps(_exposure) track['recommends'] = [] # 准备其它点击参数 for _id in res: # 构造字典 _dic = {} _dic['article_id'] = _id _dic['param'] = {} # 准备click参数 _p = {&quot;action&quot;: &quot;click&quot;, &quot;userId&quot;: temp.user_id, &quot;articleId&quot;: str(_id), &quot;algorithmCombine&quot;: temp.algo} _dic['param']['click'] = json.dumps(_p) # 准备collect参数 _p[&quot;action&quot;] = 'collect' _dic['param']['collect'] = json.dumps(_p) # 准备share参数 _p[&quot;action&quot;] = 'share' _dic['param']['share'] = json.dumps(_p) # 准备detentionTime参数 _p[&quot;action&quot;] = 'read' _dic['param']['read'] = json.dumps(_p) track['recommends'].append(_dic) track['timestamp'] = temp.time_stamp return track class RecoCenter(object): &quot;&quot;&quot;推荐中心 &quot;&quot;&quot; def __init__(self): self.hbu = HBaseUtils(pool) # self.recall_service = recall_service.ReadRecall() 并且添加了获取结果打印日志设置 # 实施推荐日志 # 离线处理更新打印日志 trace_file_handler = logging.FileHandler( os.path.join(logging_file_dir, 'recommend.log') ) trace_file_handler.setFormatter(logging.Formatter('%(message)s')) log_trace = logging.getLogger('recommend') log_trace.addHandler(trace_file_handler) log_trace.setLevel(logging.INFO) Hbase读取存储等工具类封装 get_table_row(self, table_name, key_format, column_format=None, include_timestamp=False): 获取具体表中的键、列族中的行数据 get_table_cells(self, table_name, key_format, column_format=None, timestamp=None, include_timestamp=False): 获取Hbase中多个版本数据 get_table_put(self, table_name, key_format, column_format, data, timestamp=None): 存储数据到Hbase当中 get_table_delete(self, table_name, key_format, column_format): 删除Hbase中的数据 封装代码如下： class HBaseUtils(object): &quot;&quot;&quot;HBase数据库读取工具类 &quot;&quot;&quot; def __init__(self, connection): self.pool = connection def get_table_row(self, table_name, key_format, column_format=None, include_timestamp=False): &quot;&quot;&quot; 获取HBase数据库中的行记录数据 :param table_name: 表名 :param key_format: key格式字符串, 如表的'user:reco:1', 类型为bytes :param column_format: column, 列族字符串,如表的 column 'als:18',类型为bytes :param include_timestamp: 是否包含时间戳 :return: 返回数据库结果data &quot;&quot;&quot; if not isinstance(key_format, bytes): raise KeyError(&quot;key_format or column type error&quot;) if not isinstance(table_name, str): raise KeyError(&quot;table_name should str type&quot;) with self.pool.connection() as conn: table = conn.table(table_name) if column_format: data = table.row(row=key_format, columns=[column_format], include_timestamp=include_timestamp) else: data = table.row(row=key_format) conn.close() if column_format: return data[column_format] else: # [(b'[141440]', 1555519429582)] # {'[141440]'} return data def get_table_cells(self, table_name, key_format, column_format=None, timestamp=None, include_timestamp=False): &quot;&quot;&quot; 获取HBase数据库中多个版本数据 :param table_name: 表名 :param key_format: key格式字符串, 如表的'user:reco:1', 类型为bytes :param column_format: column, 列族字符串,如表的 column 'als:18',类型为bytes :param timestamp: 指定小于该时间戳的数据 :param include_timestamp: 是否包含时间戳 :return: 返回数据库结果data &quot;&quot;&quot; if not isinstance(key_format, bytes) or not isinstance(column_format, bytes): raise KeyError(&quot;key_format or column type error&quot;) if not isinstance(table_name, str): raise KeyError(&quot;table_name should str type&quot;) with self.pool.connection() as conn: table = conn.table(table_name) data = table.cells(row=key_format, column=column_format, timestamp=timestamp, include_timestamp=include_timestamp) conn.close() # [(,), ()] return data def get_table_put(self, table_name, key_format, column_format, data, timestamp=None): &quot;&quot;&quot; :param table_name: 表名 :param key_format: key格式字符串, 如表的'user:reco:1', 类型为bytes :param column_format: column, 列族字符串,如表的 column 'als:18',类型为bytes :param data: 插入的数据 :param timestamp: 指定拆入数据的时间戳 :return: None &quot;&quot;&quot; if not isinstance(key_format, bytes) or not isinstance(column_format, bytes) or not isinstance(data, bytes): raise KeyError(&quot;key_format or column or data type error&quot;) if not isinstance(table_name, str): raise KeyError(&quot;table_name should str type&quot;) with self.pool.connection() as conn: table = conn.table(table_name) table.put(key_format, {column_format: data}, timestamp=timestamp) conn.close() return None def get_table_delete(self, table_name, key_format, column_format): &quot;&quot;&quot; 删除列族中的内容 :param table_name: 表名称 :param key_format: key :param column_format: 列格式 :return: &quot;&quot;&quot; if not isinstance(key_format, bytes) or not isinstance(column_format, bytes): raise KeyError(&quot;key_format or column type error&quot;) if not isinstance(table_name, str): raise KeyError(&quot;table_name should str type&quot;) with self.pool.connection() as conn: table = conn.table(table_name) table.delete(row=key_format, columns=[column_format]) conn.close() return None 增加feed_recommend_logic函数，进行时间戳逻辑判断 根据时间戳 时间戳T小于HBASE历史推荐记录 则获取历史记录，返回该时间戳T上次的时间戳T-1 时间戳T大于HBASE历史推荐记录，则获取新推荐，则获取HBASE数据库中最近的一次时间戳 如果有缓存，从缓存中拿，并且写入推荐历史表中 如果没有缓存，就进行一次指定算法组合的召回结果读取，排序，然后写入待推荐wait_recommend中，其中推荐出去的放入历史推荐表中 获取这个用户该频道的历史结果 # 判断用请求的时间戳大小决定获取历史记录还是刷新推荐文章 try: last_stamp = self.hbu.get_table_row('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), include_timestamp=True)[ 1] logger.info(&quot;{} INFO get user_id:{} channel:{} history last_stamp&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) except Exception as e: logger.warning(&quot;{} WARN read history recommend exception:{}&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e)) last_stamp = 0 如果历史时间戳最近的一次小于用户请求时候的时间戳（Hbase的时间戳是time.time() * 1000这个值的大小，与Web后台传入的一样类型，如果Web后台传入的不是此大小，注意修改）。 则返回推荐结果以及此次请求的上一次时间戳（用于用户获取历史记录）。 if last_stamp &lt; temp.time_stamp: # 1、获取缓存 # res = redis_cache.get_reco_from_cache(temp, self.hbu) # # # 如果没有，然后走一遍算法推荐 召回+排序，同时写入到hbase待推荐结果列表 # if not res: # logger.info(&quot;{} INFO get user_id:{} channel:{} recall/sort data&quot;. # format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) # # res = self.user_reco_list(temp) # 2、直接拿推荐结果 # res = self.user_reco_list(temp) #temp.time_stamp = int(last_stamp) track = add_track([], temp) 如果历史时间戳大于用户请求的这次时间戳，那么就是在获取历史记录，用户请求的历史时间戳是具体某个历史记录的时间戳T，Hbase当中不能够直接用T去获取，而需要去（T+N）&gt;T的时间戳获取，才能拿到包含T时间的结果，并且使用get_table_cells去获取。 分以下情况考虑： 1、如果没有历史数据，返回时间戳0以及结果空列表 2、如果历史数据只有一条，返回这一条历史数据以及时间戳正好为请求时间戳，修改时间戳为0，表示后面请求以后就没有历史数据了(APP的行为就是翻历史记录停止了) 3、如果历史数据多条，返回最近的第一条历史数据，然后返回之后第二条历史数据的时间戳 接着上面的if，写代码： else: logger.info(&quot;{} INFO read user_id:{} channel:{} history recommend data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) try: row = self.hbu.get_table_cells('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), timestamp=temp.time_stamp + 1, include_timestamp=True) except Exception as e: logger.warning(&quot;{} WARN read history recommend exception:{}&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e)) row = [] res = [] # 1、如果没有历史数据，返回时间戳0以及结果空列表 # 2、如果历史数据只有一条，返回这一条历史数据以及时间戳正好为请求时间戳，修改时间戳为0 # 3、如果历史数据多条，返回最近一条历史数据，然后返回 if not row: temp.time_stamp = 0 res = [] elif len(row) == 1 and row[0][1] == temp.time_stamp: res = eval(row[0][0]) temp.time_stamp = 0 elif len(row) &gt;= 2: res = eval(row[0][0]) temp.time_stamp = int(row[1][1]) res = list(map(int, res)) logger.info( &quot;{} INFO history:{}, {}&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), res, temp.time_stamp)) track = add_track(res, temp) # 曝光参数设置为空 track['param'] = '' return track 完整代码： def feed_recommend_logic(self, temp): &quot;&quot;&quot;推荐流业务逻辑 :param temp:ABTest传入的业务请求参数 &quot;&quot;&quot; # 判断用请求的时间戳大小决定获取历史记录还是刷新推荐文章 try: last_stamp = self.hbu.get_table_row('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), include_timestamp=True)[1] logger.info(&quot;{} INFO get user_id:{} channel:{} history last_stamp&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) except Exception as e: logger.warning(&quot;{} WARN read history recommend exception:{}&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e)) last_stamp = 0 # 如果小于，走一遍正常的推荐流程，缓存或者召回排序 logger.info(&quot;{} INFO history last_stamp:{},temp.time_stamp:{}&quot;. format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), last_stamp, temp.time_stamp)) if last_stamp &lt; temp.time_stamp: # 获取 res = redis_cache.get_reco_from_cache(temp, self.hbu) # 如果没有，然后走一遍算法推荐 召回+排序，同时写入到hbase待推荐结果列表 if not res: logger.info(&quot;{} INFO get user_id:{} channel:{} recall/sort data&quot;. format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) res = self.user_reco_list(temp) temp.time_stamp = int(last_stamp) track = add_track(res, temp) else: logger.info(&quot;{} INFO read user_id:{} channel:{} history recommend data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) try: row = self.hbu.get_table_cells('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), timestamp=temp.time_stamp + 1, include_timestamp=True) except Exception as e: logger.warning(&quot;{} WARN read history recommend exception:{}&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e)) row = [] res = [] # 1、如果没有历史数据，返回时间戳0以及结果空列表 # 2、如果历史数据只有一条，返回这一条历史数据以及时间戳正好为请求时间戳，修改时间戳为0 # 3、如果历史数据多条，返回最近一条历史数据，然后返回 if not row: temp.time_stamp = 0 res = [] elif len(row) == 1 and row[0][1] == temp.time_stamp: res = eval(row[0][0]) temp.time_stamp = 0 elif len(row) &gt;= 2: res = eval(row[0][0]) temp.time_stamp = int(row[1][1]) res = list(map(int, res)) logger.info( &quot;{} INFO history:{}, {}&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), res, temp.time_stamp)) track = add_track(res, temp) # 曝光参数设置为空 track['param'] = '' return track 修改ABTest中的推荐调用 from server.reco_center import RecoCenter # 推荐 track = RecoCenter().feed_recommend_logic(temp) 推荐中心时间戳获取逻辑测试 获取多版本历史记录： hbase(main):045:0&gt; get 'history_recommend', 'reco:his:1115629498121846784', {COLUMN=&gt;'channel:18', VERSIONS=&gt;999999} COLUMN CELL channel:18 timestamp=1559148615353, value=[15140, 16421, 19494, 14381, 17966] channel:18 timestamp=1558236647437, value=[18904, 14300, 44412, 18238, 18103, 43986, 44 339, 17454, 14899, 18335] channel:18 timestamp=1558236629309, value=[43997, 14299, 17632, 17120] channel:18 timestamp=1558236535794, value=[44657, 15085, 18156, 44654, 19052, 44652, 18 795, 17385, 44137, 17889] 召回集读取与推荐中心对接 添加一个server的目录 召回读取服务 添加一个召回集的结果读取服务recall_service.py 多路召回结果读取 目的：读取离线和在线存储的召回结果 hbase的存储：cb_recall, als, content, online 步骤： 1、初始化redis,hbase相关工具 2、在线画像召回，离线画像召回，离线协同召回数据的读取 3、redis新文章和热门文章结果读取 4、相似文章读取接口 初始化redis,hbase相关工具 import os import sys BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) sys.path.insert(0, os.path.join(BASE_DIR)) from server import redis_client from server import pool import logging from datetime import datetime from server.utils import HBaseUtils logger = logging.getLogger('recommend') class ReadRecall(object): &quot;&quot;&quot;读取召回集的结果 &quot;&quot;&quot; def __init__(self): self.client = redis_client self.hbu = HBaseUtils(pool) 在init文件中添加相关初始化数据库变量 import redis import happybase from setting.default import DefaultConfig from pyspark import SparkConf from pyspark.sql import SparkSession pool = happybase.ConnectionPool(size=10, host=&quot;hadoop-master&quot;, port=9090) # 加上decode_responses=True，写入的键值对中的value为str类型，不加这个参数写入的则为字节类型。 redis_client = redis.StrictRedis(host=DefaultConfig.REDIS_HOST, port=DefaultConfig.REDIS_PORT, db=10, decode_responses=True) 在线画像召回，离线画像召回，离线协同召回数据的读取 读取用户的指定列族的召回数据，并且读取之后要删除原来的推荐召回结果'cb_recall' def read_hbase_recall_data(self, table_name, key_format, column_format): &quot;&quot;&quot; 读取cb_recall当中的推荐数据 读取的时候可以选择列族进行读取als, online, content :return: &quot;&quot;&quot; recall_list = [] try: data = self.hbu.get_table_cells(table_name, key_format, column_format) # data是多个版本的推荐结果[[],[],[],] for _ in data: recall_list = list(set(recall_list).union(set(eval(_)))) # self.hbu.get_table_delete(table_name, key_format, column_format) except Exception as e: logger.warning(&quot;{} WARN read {} recall exception:{}&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), table_name, e)) return recall_list 测试： if __name__ == '__main__': rr = ReadRecall() # 召回结果的读取封装 print(rr.read_hbase_recall_data('cb_recall', b'recall:user:1114864874141253632', b'als:18')) redis新文章和热门文章结果读取 def read_redis_new_article(self, channel_id): &quot;&quot;&quot; 读取新闻章召回结果 :param channel_id: 提供频道 :return: &quot;&quot;&quot; logger.warning(&quot;{} WARN read channel {} redis new article&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), channel_id)) _key = &quot;ch:{}:new&quot;.format(channel_id) try: res = self.client.zrevrange(_key, 0, -1) except Exception as e: logger.warning(&quot;{} WARN read new article exception:{}&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e)) res = [] return list(map(int, res)) 热门文章读取:热门文章记录了很多，可以选取前K个: def read_redis_hot_article(self, channel_id): &quot;&quot;&quot; 读取新闻章召回结果 :param channel_id: 提供频道 :return: &quot;&quot;&quot; logger.warning(&quot;{} WARN read channel {} redis hot article&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), channel_id)) _key = &quot;ch:{}:hot&quot;.format(channel_id) try: res = self.client.zrevrange(_key, 0, -1) except Exception as e: logger.warning(&quot;{} WARN read new article exception:{}&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e)) res = [] # 由于每个频道的热门文章有很多，因为保留文章点击次数 res = list(map(int, res)) if len(res) &gt; self.hot_num: res = res[:self.hot_num] return res 测试： print(rr.read_redis_new_article(18)) print(rr.read_redis_hot_article(18)) 相似文章读取接口 会有接口获取固定的文章数量(用在APP中的猜你喜欢接口) def read_hbase_article_similar(self, table_name, key_format, article_num): &quot;&quot;&quot;获取文章相似结果 :param article_id: 文章id :param article_num: 文章数量 :return: &quot;&quot;&quot; # 第一种表结构方式测试： # create 'article_similar', 'similar' # put 'article_similar', '1', 'similar:1', 0.2 # put 'article_similar', '1', 'similar:2', 0.34 try: _dic = self.hbu.get_table_row(table_name, key_format) res = [] _srt = sorted(_dic.items(), key=lambda obj: obj[1], reverse=True) if len(_srt) &gt; article_num: _srt = _srt[:article_num] for _ in _srt: res.append(int(_[0].decode().split(':')[1])) except Exception as e: logger.error( &quot;{} ERROR read similar article exception: {}&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e)) res = [] return res 测试： print(rr.read_hbase_article_similar('article_similar', b'116644', 10)) 完整代码 import os import sys BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) sys.path.insert(0, os.path.join(BASE_DIR)) from server import redis_client from server import pool import logging from datetime import datetime from server.utils import HBaseUtils logger = logging.getLogger('recommend') class ReadRecall(object): &quot;&quot;&quot;读取召回集的结果 &quot;&quot;&quot; def __init__(self): self.client = redis_client self.hbu = HBaseUtils(pool) def read_hbase_recall_data(self, table_name, key_format, column_format): &quot;&quot;&quot;获取指定用户的对应频道的召回结果,在线画像召回，离线画像召回，离线协同召回 :return: &quot;&quot;&quot; # 获取family对应的值 # 数据库中的键都是bytes类型，所以需要进行编码相加 # 读取召回结果多个版本合并 recall_list = [] try: data = self.hbu.get_table_cells(table_name, key_format, column_format) for _ in data: recall_list = list(set(recall_list).union(set(eval(_)))) # 读取所有这个用户的在线推荐的版本，清空该频道的数据 # self.hbu.get_table_delete(table_name, key_format, column_format) except Exception as e: logger.warning( &quot;{} WARN read recall data exception:{}&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e)) return recall_list def read_redis_new_data(self, channel_id): &quot;&quot;&quot;获取redis新文章结果 :param channel_id: :return: &quot;&quot;&quot; # format结果 logger.info(&quot;{} INFO read channel:{} new recommend data&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), channel_id)) _key = &quot;ch:{}:new&quot;.format(channel_id) try: res = self.client.zrevrange(_key, 0, -1) except redis.exceptions.ResponseError as e: logger.warning(&quot;{} WARN read new article exception:{}&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e)) res = [] return list(map(int, res)) def read_redis_hot_data(self, channel_id): &quot;&quot;&quot;获取redis热门文章结果 :param channel_id: :return: &quot;&quot;&quot; # format结果 logger.info(&quot;{} INFO read channel:{} hot recommend data&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), channel_id)) _key = &quot;ch:{}:hot&quot;.format(channel_id) try: _res = self.client.zrevrange(_key, 0, -1) except redis.exceptions.ResponseError as e: logger.warning(&quot;{} WARN read hot article exception:{}&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e)) _res = [] # 每次返回前50热门文章 res = list(map(int, _res)) if len(res) &gt; 50: res = res[:50] return res def read_hbase_article_similar(self, table_name, key_format, article_num): &quot;&quot;&quot;获取文章相似结果 :param article_id: 文章id :param article_num: 文章数量 :return: &quot;&quot;&quot; # 第一种表结构方式测试： # create 'article_similar', 'similar' # put 'article_similar', '1', 'similar:1', 0.2 # put 'article_similar', '1', 'similar:2', 0.34 try: _dic = self.hbu.get_table_row(table_name, key_format) res = [] _srt = sorted(_dic.items(), key=lambda obj: obj[1], reverse=True) if len(_srt) &gt; article_num: _srt = _srt[:article_num] for _ in _srt: res.append(int(_[0].decode().split(':')[1])) except Exception as e: logger.error(&quot;{} ERROR read similar article exception: {}&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e)) res = [] return res if __name__ == '__main__': rr = ReadRecall() print(rr.read_hbase_article_similar('article_similar', b'13342', 10)) print(rr.read_hbase_recall_data('cb_recall', b'recall:user:1115629498121846784', b'als:18')) # rr = ReadRecall() # print(rr.read_redis_new_data(18)) 推荐中心---获取多路召回结果，过滤历史推荐记录逻辑 目的： 在推荐中加入召回文章结果以及过滤逻辑 把之前的推荐中心的业务中的 （召回）用刚做好的召回集填补上（并且） 步骤： 1、循环算法组合参数，遍历不同召回结果进行过滤 2、过滤当前该请求频道推荐历史结果，需要过滤0频道推荐结果，防止出现推荐频道与25个频道有重复推荐 3、过滤之后，推荐出去指定个数的文章列表，写入历史记录，剩下多的写入待推荐结果 1. 循环算法组合参数，遍历不同召回结果进行过滤 定义一个user_reco_list函数中实现读取用户的召回结果， # 所有合并的结果 reco_set = [] # - 1、循环算法组合参数，遍历不同召回结果合并，18 for _num in RAParam.COMBINE[temp.algo][1]: if _num == 103: # 读取新文章的结果，temp.channel_id _res = self.recall_service.read_redis_new_article(temp.channel_id) reco_set = list(set(reco_set).union(set(_res))) elif _num == 104: # 读取热门文章的数据 _res = self.recall_service.read_redis_hot_article(temp.channel_id) reco_set = list(set(reco_set).union(set(_res))) else: # 读取具体编号的对应表cb_recall,对应召回算法的召回结果als, content, online _res = self.recall_service.\\ read_hbase_recall_data(RAParam.RECALL[_num][0], 'recall:user:{}'.format(temp.user_id).encode(), '{}:{}'.format(RAParam.RECALL[_num][1], temp.channel_id).encode()) reco_set = list(set(reco_set).union(set(_res))) 2. 过滤当前该请求频道推荐历史结果 如果不是0频道需要过滤0频道推荐结果，防止出现， 比如Python频道和0频道相同的推荐结果 # - 2、过滤，该请求频道(18)的历史推荐记录过滤），推荐频道0频道 # - 0：APP 推荐(循环所有的频道召回结果)，0 频道也有历史记录 # temp.channel_id频道这个用户历史记录进行过滤 history_list = [] try: data = self.hbu.get_table_cells('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode()) for _ in data: history_list = list(set(history_list).union(set(eval(_)))) logger.info(&quot;{} INFO read user_id:{} channel_id:{} history data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) except Exception as e: # 打印日志 logger.warning( &quot;{} WARN filter history article exception:{}&quot;.format(datetime.now(). strftime('%Y-%m-%d %H:%M:%S'), e)) # 获取0频道的结果 try: data = self.hbu.get_table_cells('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(), 'channel:{}'.format(0).encode()) for _ in data: history_list = list(set(history_list).union(set(eval(_)))) logger.info(&quot;{} INFO filter user_id:{} channel:{} history data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, 0)) except Exception as e: logger.warning( &quot;{} WARN filter history article exception:{}&quot;.format(datetime.now(). strftime('%Y-%m-%d %H:%M:%S'), e)) reco_set = list(set(reco_set).difference(set(history_list))) 3. 过滤后，推荐出去指定个数的文章列表 写入历史记录，剩下多的写入待推荐结果 # - 4、返回结果： if not reco_set: return reco_set else: # - 如果有数据，小于需要推荐文章的数量N之后，放入历史推荐记录中history_recommend，返回结果给用 if len(reco_set) &lt;= temp.article_num: res = reco_set else: # - 如果有，350篇，取出N个，进行返回推荐，放入历史记录history_recommend # - (350- N)个文章，放入wait_recommend res = reco_set[:temp.article_num] logger.info( &quot;{} INFO put user_id:{} channel:{} wait data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) # 放入剩下多余的数据到wait_recommend当中 self.hbu.get_table_put('wait_recommend', 'reco:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), str(reco_set[temp.article_num:]).encode(), timestamp=temp.time_stamp) # 放入历史记录 self.hbu.get_table_put('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), str(res).encode(), timestamp=temp.time_stamp) # 放入历史记录日志 logger.info( &quot;{} INFO store recall/sorted user_id:{} channel:{} history_recommend data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) return res 修改调用读取召回数据的部分 # 2、不开启缓存 res = self.user_reco_list(temp) temp.time_stamp = int(last_stamp) track = add_track(res, temp) 运行grpc服务之后，测试结果 hbase(main):007:0&gt; get &quot;history_recommend&quot;, 'reco:his:1115629498121846784', {COLUMN=&gt;'channel:18',VERSIONS=&gt;1000} COLUMN CELL channel:18 timestamp=1558189615378, value=[13890, 14915, 13891, 15429, 15944, 44371, 18 005, 15196, 13410, 13672] channel:18 timestamp=1558189317342, value=[17966, 17454, 14125, 16174, 14899, 44339, 16 437, 18743, 44090, 18238] channel:18 timestamp=1558143073173, value=[19200, 17665, 16151, 16411, 19233, 13090, 15 140, 16421, 19494, 14381] 待推荐表中有 hbase(main):008:0&gt; scan 'wait_recommend' ROW COLUMN+CELL reco:1115629498121846784 column=channel:18, timestamp=1558189615378, value=[44137, 18795, 19052, 4465 2, 44654, 44657, 14961, 17522, 43894, 44412, 16000, 14208, 44419, 17802, 142 23, 18836, 140956, 18335, 13728, 14498, 44451, 44456, 18609, 18353, 44468, 1 8103, 135869, 16062, 14015, 13757, 13249, 44483, 17605, 14021, 15309, 18127, 43983, 44754, 43986, 19413, 14805, 18904, 44761, 17114, 13272, 14810, 18907 , 13022, 14300, 17120, 17632, 14299, 43997, 17889, 17385, 18156, 15085, 1329 5, 44020, 14839, 44024, 14585, 18172, 44541] 完整代码 def user_reco_list(self, temp): &quot;&quot;&quot; 获取用户的召回结果进行推荐 :param temp: :return: &quot;&quot;&quot; reco_set = [] # 1、循环算法组合参数，遍历不同召回结果进行过滤 for _num in RAParam.COMBINE[temp.algo][1]: # 进行每个召回结果的读取100,101,102,103,104 if _num == 103: # 新文章召回读取 _res = self.recall_service.read_redis_new_article(temp.channel_id) reco_set = list(set(reco_set).union(set(_res))) elif _num == 104: # 热门文章召回读取 _res = self.recall_service.read_redis_hot_article(temp.channel_id) reco_set = list(set(reco_set).union(set(_res))) else: _res = self.recall_service.\\ read_hbase_recall_data(RAParam.RECALL[_num][0], 'recall:user:{}'.format(temp.user_id).encode(), '{}:{}'.format(RAParam.RECALL[_num][1], temp.channel_id).encode()) # 进行合并某个协同过滤召回的结果 reco_set = list(set(reco_set).union(set(_res))) # reco_set都是新推荐的结果，进行过滤 history_list = [] try: data = self.hbu.get_table_cells('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode()) for _ in data: history_list = list(set(history_list).union(set(eval(_)))) logger.info(&quot;{} INFO filter user_id:{} channel:{} history data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) except Exception as e: logger.warning( &quot;{} WARN filter history article exception:{}&quot;.format(datetime.now(). strftime('%Y-%m-%d %H:%M:%S'), e)) # 如果0号频道有历史记录，也需要过滤 try: data = self.hbu.get_table_cells('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(), 'channel:{}'.format(0).encode()) for _ in data: history_list = list(set(history_list).union(set(eval(_)))) logger.info(&quot;{} INFO filter user_id:{} channel:{} history data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, 0)) except Exception as e: logger.warning( &quot;{} WARN filter history article exception:{}&quot;.format(datetime.now(). strftime('%Y-%m-%d %H:%M:%S'), e)) # 过滤操作 reco_set 与history_list进行过滤 reco_set = list(set(reco_set).difference(set(history_list))) # 排序代码逻辑 # _sort_num = RAParam.COMBINE[temp.algo][2][0] # reco_set = sort_dict[RAParam.SORT[_sort_num]](reco_set, temp, self.hbu) # 如果没有内容，直接返回 if not reco_set: return reco_set else: # 类型进行转换 reco_set = list(map(int, reco_set)) # 跟后端需要推荐的文章数量进行比对 article_num # article_num &gt; reco_set if len(reco_set) &lt;= temp.article_num: res = reco_set else: # 之取出推荐出去的内容 res = reco_set[:temp.article_num] # 剩下的推荐结果放入wait_recommend等待下次帅新的时候直接推荐 self.hbu.get_table_put('wait_recommend', 'reco:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), str(reco_set[temp.article_num:]).encode(), timestamp=temp.time_stamp) logger.info( &quot;{} INFO put user_id:{} channel:{} wait data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) # 放入历史记录表当中 self.hbu.get_table_put('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode(), str(res).encode(), timestamp=temp.time_stamp) # 放入历史记录日志 logger.info( &quot;{} INFO store recall/sorted user_id:{} channel:{} history_recommend data&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id)) return res ","link":"https://cythonlin.github.io/post/ide-greater-cheatsh/"},{"title":"PR => Pshell7+cheat.sh","content":"PS7 主要新增功能 &amp; 后台 &amp;&amp; || 管道 找到PS5配置文件 # 列出PS5配置文件 locate PowerShell_profile.ps1 # 打开配置文件 sublime C:\\Users\\lin\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1 修改PS5配置为如下内容 主要就是 iex 和 irm 两条别名命令添加回来，下面要用到 Get-Alias | ForEach-Object { if ($_.Name -ne &quot;cd&quot; -and $_.Name -ne &quot;start&quot; -and $_.Name -ne &quot;iex&quot; -and $_.Name -ne &quot;irm&quot;) { Remove-Item -Path (&quot;Alias:\\&quot; + $_.Name) -Force -ErrorAction &quot;SilentlyContinue&quot; } } Invoke-Expression (oh-my-posh --init --shell pwsh --config &quot;C:\\Users\\lin\\lin.omp.json&quot;) Write-Color -Text &quot; __ __ ___ &quot; -Color Cyan Write-Color -Text &quot; /\\ \\__/\\ \\ /\\_ \\ __ &quot; -Color Cyan Write-Color -Text &quot; ___ __ __\\ \\ ,_\\ \\ \\___ ___ ___ \\//\\ \\ /\\_\\ ___ &quot; -Color red Write-Color -Text &quot; /'___\\/\\ \\/\\ \\\\ \\ \\/\\ \\ _ \\\\ / __\\\\ /' _ \\\\ \\ \\ \\ \\/\\ \\ /' _ \\\\ &quot; -Color yellow Write-Color -Text &quot; /\\ \\__/\\ \\ \\_\\ \\\\ \\ \\_\\ \\ \\ \\ \\/\\ \\\\ \\/\\ \\/\\ \\ \\_\\ \\_\\ \\ \\/\\ \\/\\ \\ &quot; -Color green Write-Color -Text &quot; \\ \\____\\\\/\\____ \\\\ \\__\\\\ \\_\\ \\_\\ \\____/\\ \\_\\ \\_\\ /\\____\\\\ \\_\\ \\_\\ \\_\\&quot; -Color blue Write-Color -Text &quot; \\/____/ \\\\___// \\\\/__/ \\/_/\\/_/\\/___/ \\/_/\\/_/ \\/____/ \\/_/\\/_/\\/_/&quot; -Color Magenta Write-Color -Text &quot; /\\___/ &quot; -Color Cyan Write-Color -Text &quot; \\/__/ &quot; -Color Cyan Write-Color -Text &quot; &quot; function new_clear{ Clear-Host Write-Color -Text &quot; __ __ ___ &quot; -Color Cyan Write-Color -Text &quot; /\\ \\__/\\ \\ /\\_ \\ __ &quot; -Color Cyan Write-Color -Text &quot; ___ __ __\\ \\ ,_\\ \\ \\___ ___ ___ \\//\\ \\ /\\_\\ ___ &quot; -Color red Write-Color -Text &quot; /'___\\/\\ \\/\\ \\\\ \\ \\/\\ \\ _ \\\\ / __\\\\ /' _ \\\\ \\ \\ \\ \\/\\ \\ /' _ \\\\ &quot; -Color yellow Write-Color -Text &quot; /\\ \\__/\\ \\ \\_\\ \\\\ \\ \\_\\ \\ \\ \\ \\/\\ \\\\ \\/\\ \\/\\ \\ \\_\\ \\_\\ \\ \\/\\ \\/\\ \\ &quot; -Color green Write-Color -Text &quot; \\ \\____\\\\/\\____ \\\\ \\__\\\\ \\_\\ \\_\\ \\____/\\ \\_\\ \\_\\ /\\____\\\\ \\_\\ \\_\\ \\_\\&quot; -Color blue Write-Color -Text &quot; \\/____/ \\\\___// \\\\/__/ \\/_/\\/_/\\/___/ \\/_/\\/_/ \\/____/ \\/_/\\/_/\\/_/&quot; -Color Magenta Write-Color -Text &quot; /\\___/ &quot; -Color Cyan Write-Color -Text &quot; \\/__/ &quot; -Color Cyan Write-Color -Text &quot; &quot; } Set-Alias clear new_clear Set-Alias cls clear 下载PS7安装包 iex &quot;&amp; { $(irm https://aka.ms/install-powershell.ps1) } -UseMSI&quot; 新建PS7配置文件 注：管理员身份运行 cd &quot;C:/Program Files/PowerShell/7&quot; notepad profile.ps1 把上面PS5一模一样的内容复制进来 略 保存，退出 使用PS7命令行重新安装颜色字体 Install-Module -Name PSWriteColor -Scope CurrentUser 重新修改Terminal 配置文件 # 列出配置文件 locate settings.json | grep -i windowsterminal # 打开配置文件 sublime C:\\Users\\lin\\AppData\\Local\\Packages\\Microsoft.WindowsTerminal_8wekyb3d8bbwe\\LocalState\\settings.json 将settings.json重新粘入如下内容 // To view the default settings, hold &quot;alt&quot; while clicking on the &quot;Settings&quot; button. // For documentation on these settings, see: https://aka.ms/terminal-documentation { // 官方设置指南? &quot;$schema&quot;: &quot;https://aka.ms/terminal-profiles-schema&quot;, // 一些globals设置 &quot;theme&quot;: &quot;dark&quot;, // 窗口主题 // &quot;initialRows&quot;: 25, // &quot;initialCols&quot;: 100, &quot;launchMode&quot;: &quot;maximized&quot;, // 全屏 // 设置默认终端 // &quot;defaultProfile&quot;: &quot;{61c54bbd-c2c6-5271-96e7-009a87ff44bf}&quot;, // &quot;defaultProfile&quot;: &quot;{0caa0dad-35be-5f56-a8ff-afceeeaa610}1&quot;, &quot;defaultProfile&quot;: &quot;{574e775e-4f2a-5b96-ac1e-a2962a402336}&quot;, &quot;profiles&quot;: { &quot;defaults&quot;: { // Put settings here that you want to apply to all profiles &quot;colorScheme&quot;: &quot;Seafoam Pastel&quot;, &quot;useAcrylic&quot;: true, &quot;acrylicOpacity&quot;: 0.55, &quot;cursorShape&quot;: &quot;vintage&quot;, &quot;cursorHeight&quot;: 60, &quot;cursorColor&quot;: &quot;#B00C11&quot;, &quot;fontFace&quot;: &quot;Hack Nerd Font&quot;, &quot;fontSize&quot;: 18 }, &quot;list&quot;: [{ // Make changes here to the powershell.exe profile &quot;guid&quot;: &quot;{61c54bbd-c2c6-5271-96e7-009a87ff44bf}&quot;, &quot;name&quot;: &quot;PS5&quot;, &quot;commandline&quot;: &quot;PowerShell.exe /nologo&quot;, // /nologo 表示删除 banner &quot;startingDirectory&quot;: &quot;d:&quot;, &quot;hidden&quot;: false }, // { // // Make changes here to the powershell.exe profile // &quot;guid&quot;: &quot;{7c0b0203-de8d-490d-9183-a9e245a83be1}&quot;, // &quot;name&quot;: &quot;PWSH&quot;, // &quot;commandline&quot;: &quot;C://Program Files//PowerShell//7//pwsh.exe /nologo&quot;, // /nologo 表示删除 banner // &quot;startingDirectory&quot;: &quot;d:&quot;, // &quot;hidden&quot;: false // }, { // Make changes here to the cmd.exe profile &quot;guid&quot;: &quot;{0caa0dad-35be-5f56-a8ff-afceeeaa6101}&quot;, &quot;name&quot;: &quot;cmd&quot;, &quot;commandline&quot;: &quot;cmd.exe&quot;, &quot;startingDirectory&quot;: &quot;d:&quot;, &quot;hidden&quot;: false }, { &quot;guid&quot;: &quot;{b453ae62-4e3d-5e58-b989-0a998ec441b8}&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Azure Cloud Shell&quot;, &quot;source&quot;: &quot;Windows.Terminal.Azure&quot; }, // { // &quot;guid&quot;: &quot;{07b52e3e-de2c-5db4-bd2d-ba144ed6c273}&quot;, // &quot;hidden&quot;: false, // &quot;name&quot;: &quot;Ubuntu-20.04&quot;, // &quot;startingDirectory&quot;: &quot;d:&quot;, // &quot;source&quot;: &quot;Windows.Terminal.Wsl&quot; // }, // { // &quot;guid&quot;: &quot;{e05f1426-9e28-4be3-8f2b-3e2b48eae4a8}&quot;, // &quot;hidden&quot;: false, // &quot;name&quot;: &quot;ssh&quot;, // &quot;commandline&quot;: &quot;ssh root@127.0.0.1 -p 22&quot; // }, { &quot;guid&quot;: &quot;{e05f1426-9e28-4be3-8f2b-3e2b48eae4a8}&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Conda&quot;, &quot;commandline&quot;: &quot;%windir%//System32//cmd.exe \\&quot;\\/K\\&quot; D:/miniconda/Scripts/activate.bat D:/miniconda&quot;, &quot;startingDirectory&quot;: &quot;d:&quot; }, { &quot;guid&quot;: &quot;{574e775e-4f2a-5b96-ac1e-a2962a402336}&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;PS7&quot;, &quot;source&quot;: &quot;Windows.Terminal.PowershellCore&quot;, &quot;startingDirectory&quot;: &quot;d:&quot;, &quot;commandline&quot;: &quot;C://Program Files//PowerShell//7//pwsh.exe /nologo&quot;, // /nologo 表示删除 banner } ] }, // Add custom color schemes to this array &quot;schemes&quot;: [ { &quot;name&quot;: &quot;Seafoam Pastel&quot;, &quot;black&quot;: &quot;#000000&quot;, &quot;red&quot;: &quot;#ff7092&quot;, &quot;green&quot;: &quot;#00fbac&quot;, &quot;yellow&quot;: &quot;#fffa6a&quot;, &quot;blue&quot;: &quot;#00bfff&quot;, &quot;purple&quot;: &quot;#df95ff&quot;, &quot;cyan&quot;: &quot;#86cbfe&quot;, &quot;white&quot;: &quot;#ffffff&quot;, &quot;brightBlack&quot;: &quot;#000000&quot;, &quot;brightRed&quot;: &quot;#ff8aa4&quot;, &quot;brightGreen&quot;: &quot;#21f6bc&quot;, &quot;brightYellow&quot;: &quot;#fff787&quot;, &quot;brightBlue&quot;: &quot;#1bccfd&quot;, &quot;brightPurple&quot;: &quot;#e6aefe&quot;, &quot;brightCyan&quot;: &quot;#99d6fc&quot;, &quot;brightWhite&quot;: &quot;#ffffff&quot;, &quot;background&quot;: &quot;#332a57&quot;, &quot;foreground&quot;: &quot;#e5e5e5&quot; } ], // Add any keybinding overrides to this array. // To unbind a default keybinding, set the command to &quot;unbound&quot; &quot;keybindings&quot;: [], &quot;actions&quot;: [ // Copy and paste are bound to Ctrl+Shift+C and Ctrl+Shift+V in your defaults.json. // These two lines additionally bind them to Ctrl+C and Ctrl+V. // To learn more about selection, visit https://aka.ms/terminal-selection { &quot;command&quot;: {&quot;action&quot;: &quot;copy&quot;, &quot;singleLine&quot;: false }, &quot;keys&quot;: &quot;ctrl+c&quot; }, { &quot;command&quot;: &quot;paste&quot;, &quot;keys&quot;: &quot;ctrl+v&quot; }, // Press Ctrl+Shift+F to open the search box { &quot;command&quot;: &quot;find&quot;, &quot;keys&quot;: &quot;ctrl+f&quot; }, // settings { &quot;command&quot;: &quot;openSettings&quot;, &quot;keys&quot;: &quot;ctrl+'&quot; }, // Press Alt+Shift+D to open a new pane. // - &quot;split&quot;: &quot;auto&quot; makes this pane open in the direction that provides the most surface area. // - &quot;splitMode&quot;: &quot;duplicate&quot; makes the new pane use the focused pane's profile. // To learn more about panes, visit https://aka.ms/terminal-panes { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;auto&quot;, &quot;splitMode&quot;: &quot;duplicate&quot; }, &quot;keys&quot;: &quot;alt+shift+d&quot; }, { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;vertical&quot; }, &quot;keys&quot;: &quot;ctrl+plus&quot; }, { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;horizontal&quot; }, &quot;keys&quot;: &quot;ctrl+-&quot; }, { &quot;command&quot;: &quot;closePane&quot;, &quot;keys&quot;: &quot;ctrl+w&quot; } ] } 本次 settings.json主要对 Terminal做了如下修改 窗口开启为全屏 -&gt; launchMode 加入PS7 默认终端设为PS7 -&gt; defaultProfile Powershell5 改名为 PS5 加了一些后台便捷功能，整体profile.ps1文件如下： 注：这里加了 gsudo的配置，因此需要，先安装 gsudo Get-Alias | ForEach-Object { if ($_.Name -ne &quot;cd&quot; ` -and $_.Name -ne &quot;start&quot; ` -and $_.Name -ne &quot;iex&quot; ` -and $_.Name -ne &quot;irm&quot; ` -and $_.Name -ne &quot;select&quot; ` ) ` { Remove-Item -Path (&quot;Alias:\\&quot; + $_.Name) -Force -ErrorAction &quot;SilentlyContinue&quot; } } Invoke-Expression (oh-my-posh --init --shell pwsh --config &quot;C:\\Users\\lin\\lin.omp.json&quot;) Write-Color -Text &quot; __ __ ___ &quot; -Color Cyan Write-Color -Text &quot; /\\ \\__/\\ \\ /\\_ \\ __ &quot; -Color Cyan Write-Color -Text &quot; ___ __ __\\ \\ ,_\\ \\ \\___ ___ ___ \\//\\ \\ /\\_\\ ___ &quot; -Color red Write-Color -Text &quot; /'___\\/\\ \\/\\ \\\\ \\ \\/\\ \\ _ \\\\ / __\\\\ /' _ \\\\ \\ \\ \\ \\/\\ \\ /' _ \\\\ &quot; -Color yellow Write-Color -Text &quot; /\\ \\__/\\ \\ \\_\\ \\\\ \\ \\_\\ \\ \\ \\ \\/\\ \\\\ \\/\\ \\/\\ \\ \\_\\ \\_\\ \\ \\/\\ \\/\\ \\ &quot; -Color green Write-Color -Text &quot; \\ \\____\\\\/\\____ \\\\ \\__\\\\ \\_\\ \\_\\ \\____/\\ \\_\\ \\_\\ /\\____\\\\ \\_\\ \\_\\ \\_\\&quot; -Color blue Write-Color -Text &quot; \\/____/ \\\\___// \\\\/__/ \\/_/\\/_/\\/___/ \\/_/\\/_/ \\/____/ \\/_/\\/_/\\/_/&quot; -Color Magenta Write-Color -Text &quot; /\\___/ &quot; -Color Cyan Write-Color -Text &quot; \\/__/ &quot; -Color Cyan Write-Color -Text &quot; &quot; function new_clear{ Clear-Host Write-Color -Text &quot; __ __ ___ &quot; -Color Cyan Write-Color -Text &quot; /\\ \\__/\\ \\ /\\_ \\ __ &quot; -Color Cyan Write-Color -Text &quot; ___ __ __\\ \\ ,_\\ \\ \\___ ___ ___ \\//\\ \\ /\\_\\ ___ &quot; -Color red Write-Color -Text &quot; /'___\\/\\ \\/\\ \\\\ \\ \\/\\ \\ _ \\\\ / __\\\\ /' _ \\\\ \\ \\ \\ \\/\\ \\ /' _ \\\\ &quot; -Color yellow Write-Color -Text &quot; /\\ \\__/\\ \\ \\_\\ \\\\ \\ \\_\\ \\ \\ \\ \\/\\ \\\\ \\/\\ \\/\\ \\ \\_\\ \\_\\ \\ \\/\\ \\/\\ \\ &quot; -Color green Write-Color -Text &quot; \\ \\____\\\\/\\____ \\\\ \\__\\\\ \\_\\ \\_\\ \\____/\\ \\_\\ \\_\\ /\\____\\\\ \\_\\ \\_\\ \\_\\&quot; -Color blue Write-Color -Text &quot; \\/____/ \\\\___// \\\\/__/ \\/_/\\/_/\\/___/ \\/_/\\/_/ \\/____/ \\/_/\\/_/\\/_/&quot; -Color Magenta Write-Color -Text &quot; /\\___/ &quot; -Color Cyan Write-Color -Text &quot; \\/__/ &quot; -Color Cyan Write-Color -Text &quot; &quot; } function rjobs{ get-job | remove-job -force } function stops{ get-job | stop-job } function stop{ stop-job @args } function fg{ wait-job @args } Set-Alias jobs job function rjob{ remove-job @args } Set-Alias clear new_clear Set-Alias cls clear Set-Alias sudo gsudo 回忆（准备cheat.sh的介绍） 由于Scoop对cheat.sh支持不友好，我就又把WSL2加了进来 =&gt; WSL2+ZSH 同时也更新了最新的一次 WT的配置文件，如下： // To view the default settings, hold &quot;alt&quot; while clicking on the &quot;Settings&quot; button. // For documentation on these settings, see: https://aka.ms/terminal-documentation { // 官方设置指南? &quot;$schema&quot;: &quot;https://aka.ms/terminal-profiles-schema&quot;, // 一些globals设置 &quot;theme&quot;: &quot;dark&quot;, // 窗口主题 // &quot;initialRows&quot;: 25, // &quot;initialCols&quot;: 100, &quot;launchMode&quot;: &quot;maximized&quot;, // 全屏 // 设置默认终端 // &quot;defaultProfile&quot;: &quot;{61c54bbd-c2c6-5271-96e7-009a87ff44bf}&quot;, // &quot;defaultProfile&quot;: &quot;{0caa0dad-35be-5f56-a8ff-afceeeaa610}1&quot;, &quot;defaultProfile&quot;: &quot;{574e775e-4f2a-5b96-ac1e-a2962a402336}&quot;, &quot;profiles&quot;: { &quot;defaults&quot;: { // Put settings here that you want to apply to all profiles &quot;colorScheme&quot;: &quot;Seafoam Pastel&quot;, &quot;useAcrylic&quot;: true, &quot;acrylicOpacity&quot;: 0.55, &quot;cursorShape&quot;: &quot;vintage&quot;, &quot;cursorHeight&quot;: 60, &quot;cursorColor&quot;: &quot;#B00C11&quot;, &quot;fontFace&quot;: &quot;Hack Nerd Font&quot;, &quot;fontSize&quot;: 18 }, &quot;list&quot;: [ { &quot;guid&quot;: &quot;{07b52e3e-de2c-5db4-bd2d-ba144ed6c273}&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;UBT&quot;, &quot;source&quot;: &quot;Windows.Terminal.Wsl&quot;, &quot;startingDirectory&quot;: &quot;d:&quot; }, { &quot;guid&quot;: &quot;{e05f1426-9e28-4be3-8f2b-3e2b48eae4a8}&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;CON&quot;, &quot;commandline&quot;: &quot;%windir%//System32//cmd.exe \\&quot;\\/K\\&quot; D:/miniconda/Scripts/activate.bat D:/miniconda&quot;, &quot;startingDirectory&quot;: &quot;d:&quot; }, { &quot;guid&quot;: &quot;{574e775e-4f2a-5b96-ac1e-a2962a402336}&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;PS7&quot;, &quot;source&quot;: &quot;Windows.Terminal.PowershellCore&quot;, &quot;startingDirectory&quot;: &quot;d:&quot;, &quot;commandline&quot;: &quot;C://Program Files//PowerShell//7//pwsh.exe /nologo&quot;, // /nologo 表示删除 banner }, { // Make changes here to the cmd.exe profile &quot;guid&quot;: &quot;{0caa0dad-35be-5f56-a8ff-afceeeaa6101}&quot;, &quot;name&quot;: &quot;cmd&quot;, &quot;commandline&quot;: &quot;cmd.exe&quot;, &quot;startingDirectory&quot;: &quot;d:&quot;, &quot;hidden&quot;: false }, { // Make changes here to the powershell.exe profile &quot;guid&quot;: &quot;{61c54bbd-c2c6-5271-96e7-009a87ff44bf}&quot;, &quot;name&quot;: &quot;PS5&quot;, &quot;commandline&quot;: &quot;PowerShell.exe /nologo&quot;, // /nologo 表示删除 banner &quot;startingDirectory&quot;: &quot;d:&quot;, &quot;hidden&quot;: false }, // { // // Make changes here to the powershell.exe profile // &quot;guid&quot;: &quot;{7c0b0203-de8d-490d-9183-a9e245a83be1}&quot;, // &quot;name&quot;: &quot;PWSH&quot;, // &quot;commandline&quot;: &quot;C://Program Files//PowerShell//7//pwsh.exe /nologo&quot;, // /nologo 表示删除 banner // &quot;startingDirectory&quot;: &quot;d:&quot;, // &quot;hidden&quot;: false // }, { &quot;guid&quot;: &quot;{b453ae62-4e3d-5e58-b989-0a998ec441b8}&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;AWS&quot;, &quot;source&quot;: &quot;Windows.Terminal.Azure&quot; } // { // &quot;guid&quot;: &quot;{e05f1426-9e28-4be3-8f2b-3e2b48eae4a8}&quot;, // &quot;hidden&quot;: false, // &quot;name&quot;: &quot;ssh&quot;, // &quot;commandline&quot;: &quot;ssh root@127.0.0.1 -p 22&quot; // } ] }, // Add custom color schemes to this array &quot;schemes&quot;: [ { &quot;name&quot;: &quot;Seafoam Pastel&quot;, &quot;black&quot;: &quot;#000000&quot;, &quot;red&quot;: &quot;#ff7092&quot;, &quot;green&quot;: &quot;#00fbac&quot;, &quot;yellow&quot;: &quot;#fffa6a&quot;, &quot;blue&quot;: &quot;#00bfff&quot;, &quot;purple&quot;: &quot;#df95ff&quot;, &quot;cyan&quot;: &quot;#86cbfe&quot;, &quot;white&quot;: &quot;#ffffff&quot;, &quot;brightBlack&quot;: &quot;#000000&quot;, &quot;brightRed&quot;: &quot;#ff8aa4&quot;, &quot;brightGreen&quot;: &quot;#21f6bc&quot;, &quot;brightYellow&quot;: &quot;#fff787&quot;, &quot;brightBlue&quot;: &quot;#1bccfd&quot;, &quot;brightPurple&quot;: &quot;#e6aefe&quot;, &quot;brightCyan&quot;: &quot;#99d6fc&quot;, &quot;brightWhite&quot;: &quot;#ffffff&quot;, &quot;background&quot;: &quot;#332a57&quot;, &quot;foreground&quot;: &quot;#e5e5e5&quot; } ], // Add any keybinding overrides to this array. // To unbind a default keybinding, set the command to &quot;unbound&quot; &quot;keybindings&quot;: [], &quot;actions&quot;: [ // Copy and paste are bound to Ctrl+Shift+C and Ctrl+Shift+V in your defaults.json. // These two lines additionally bind them to Ctrl+C and Ctrl+V. // To learn more about selection, visit https://aka.ms/terminal-selection { &quot;command&quot;: {&quot;action&quot;: &quot;copy&quot;, &quot;singleLine&quot;: false }, &quot;keys&quot;: &quot;ctrl+c&quot; }, { &quot;command&quot;: &quot;paste&quot;, &quot;keys&quot;: &quot;ctrl+v&quot; }, // Press Ctrl+Shift+F to open the search box { &quot;command&quot;: &quot;find&quot;, &quot;keys&quot;: &quot;ctrl+f&quot; }, // settings { &quot;command&quot;: &quot;openSettings&quot;, &quot;keys&quot;: &quot;ctrl+'&quot; }, // Press Alt+Shift+D to open a new pane. // - &quot;split&quot;: &quot;auto&quot; makes this pane open in the direction that provides the most surface area. // - &quot;splitMode&quot;: &quot;duplicate&quot; makes the new pane use the focused pane's profile. // To learn more about panes, visit https://aka.ms/terminal-panes { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;auto&quot;, &quot;splitMode&quot;: &quot;duplicate&quot; }, &quot;keys&quot;: &quot;alt+shift+d&quot; }, { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;vertical&quot; }, &quot;keys&quot;: &quot;ctrl+plus&quot; }, { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;horizontal&quot; }, &quot;keys&quot;: &quot;ctrl+-&quot; }, { &quot;command&quot;: &quot;closePane&quot;, &quot;keys&quot;: &quot;ctrl+w&quot; } ] } Cheat.sh -&gt; cheat.sh 以下说的都是Cheat.sh的几种用法👇 For Curl(可以用下面WSL代替) 类似如下语法： curl cht.sh/python/string+time For Client（最好用） 说明一下，只有 Curl原生的方式是需要路径符号规则的， 如 /,+ 都是必须的，比较难用。 下面的所有方式就很方便了，往下看吧👇 WSL（命令行只能用WSL来运行cheat.sh） （cheat支持ZSH，恰好前面配置了ZSH，下面说如何支持ZSH） 先安装依赖： sudo apt install rlwrap # 下面这条语句是为 stealth Q 模式准备的（因为没图形界面，所以下面2条语句已废） sudo apt install xsel export DISPLAY=$(cat /etc/resolv.conf | grep nameserver | awk '{print $2}'):0 下载 cht.sh，应修改为全局命令 curl https://cht.sh/:cht.sh | sudo tee /usr/local/bin/cht.sh sudo chmod +x /usr/local/bin/cht.sh 👍正常命令行模式 cht.sh python string time 👍--shell 模式 cht.sh --shell cd python # cd 就代表选择哪类语言，或者技术领域 string time # 看见了吧，这次直接输入语句就搞定，不需要 写什么 cht.sh python了😊 Note: 如果查到了， q键 是退出文本， 输入 exit 是退出 --shell模式 👍支持ZSH(不然按tab找不到 cht.sh，加了下面的配置就有了) # 第二条命令curl ... 一次可能网有问题，注意有没有con't的提示，如果出错，那就再执行一次 mkdir ~/.zsh.d curl https://cheat.sh/:zsh &gt; ~/.zsh.d/_cht echo 'fpath=(~/.zsh.d/ $fpath)' &gt;&gt; ~/.zshrc # 然后重新登录/重新打开shell 👍Stealth mode （属于--shell 下面的一个参数， 进入--shell，用help命令可以查看所有参数） 研究了半天，很遗憾，Stealth mode模式，需要 server 的 GUI 需要安装 VcXsrv， 那这个 Stealth mode 模式就算了 Powershell(不可行) scoop install cht 但是这种方式不太行，因为是scoop下载的是 cht.exe 而非 cht.sh 即使下到了cht.sh。 BusyBox是 dash，而非 bash，不支持执行.sh脚本 即使用了真的bash（git bash, gow 这些工具，也缺依赖）并且不是在powershell运行。 所以我还是放弃了工具化的方式， 还是用WSL了 For Pycharm（也可以） Plugins cheat.sh code snippets 方式1：使用面板 调出面板快捷键 alt + p alt + p 在面板中输入语句测试： string time 和 time string 两个单词顺序是不同的，但是却有不同的语句块 方式2：直接在写代码 代码中直接写： string time 选中此代码 string time 按组合键复制生成的代码块 alt + c 然后全部松手再复制生成的代码块(直接一个单s键即可) s 修改快捷键(搜索， cheat.sh) ctrl + shift + c For Sublime =&gt; 插件地址（文档较少，讲的是Mac的，Windows我已参透，且听我细细道来=&gt;） 安装步骤 &quot;Preferences -&gt; Browse Packages&quot; git clone https://github.com/gauravk-in/cheat.sh-sublime-plugin.git CheatSheet ctrl+shift+p: 输入 Package Control: Satisfy Dependencies ctrl + ` 控制台查看是否有报错信息，若正常，即可使用 配置此插件的快捷键 作者文档没有写如何配置快捷键， stack，github搜了一圈，毫无信息。 于是打开了源码文件，看了下，有个 Default.sublime-commands 文件。 里面配置的是命令信息， 把主要字段加入到 全局Key-Binding即可： { &quot;keys&quot;: [&quot;alt+p&quot;], &quot;command&quot;: &quot;cheat_sheet&quot;, &quot;args&quot;:{&quot;with_comments&quot;: true}}, 特别注意： args参数必加， 搞了半天，没打开控制台看，后来提示缺 with_comments参数 去源码一看，需传递 with_comments， 并且合着sublime语法格式用 args包着。 使用方式： 1. 打开一个文件.py，写入语词（py文件默认py ，也可在py文件 加个java，写java语句） 2. 选中语句， 按 alt + p 😎😎😎😎😎 其他配置： 也可配置其他如：（底下调出输入框，来输入语句，但是都没上面配的选中替换好用） 1. 要是真的需要，则找上面的 &quot;Default.sublime-commands&quot; 文件😑。 2. 此文件里面有对应配置，看着对照配到全局 &quot;Key-Binding&quot; 👌 即可。 ","link":"https://cythonlin.github.io/post/pr-greater-powershell-7/"},{"title":"PR => Oh-my-Posh3+Banner","content":"选用兼容字体 安装字体 这里用Nerd字体，可以做到兼容 =&gt; Nerd官网，随便挑一个下载 我这里选的 Hack， =&gt;下载包地址 下载zip解压， 把所有字体选中， 右键 （为所有用户安装）。 给Windows Terminal设置默认字体 ctrl + ' 调出配置文件 或者 用sublime更方便操作 // 查出文件所在位置 locate settings.json | grep -i terminal sublime 文件 搜索关键词 fontFace所在行, 修改为： &quot;fontFace&quot;: &quot;Hack Nerd Font&quot;, Oh my Posh3 安装 方式1： scoop install https://github.com/JanDeDobbeleer/oh-my-posh3/releases/latest/download/oh-my-posh.json 方式2： scoop install oh-my-posh3 更新 scoop update oh-my-posh 生成配置文件 Invoke-Expression (oh-my-posh --init --shell pwsh --config &quot;$(scoop prefix oh-my-posh)/themes/jandedobbeleer.omp.json&quot;) 查看自带配置文件位置 locate jandedobbeleer.omp.json 把原配置文件copy一份给home目录，名为lin.omp.json cp C:\\Users\\lin\\scoop\\apps\\oh-my-posh\\3.85.2\\themes\\jandedobbeleer.omp.json C:\\Users\\lin\\lin.omp.json 使用自己的配置文件，并把它写入到ps配置文件中 Invoke-Expression (oh-my-posh --init --shell pwsh --config &quot;C:\\Users\\lin\\lin.omp.json&quot;) 打开lin.omp.json，copy入如下内容 { &quot;$schema&quot;: &quot;https://raw.githubusercontent.com/JanDeDobbeleer/oh-my-posh3/main/themes/schema.json&quot;, &quot;blocks&quot;: [ { &quot;type&quot;: &quot;prompt&quot;, &quot;alignment&quot;: &quot;left&quot;, &quot;segments&quot;: [ { &quot;type&quot;: &quot;session&quot;, &quot;style&quot;: &quot;powerline&quot;, &quot;foreground&quot;: &quot;#000000&quot;, &quot;background&quot;: &quot;#00FFFF&quot;, &quot;trailing_diamond&quot;: &quot;\\uE0B0&quot; }, { &quot;type&quot;: &quot;path&quot;, &quot;style&quot;: &quot;powerline&quot;, &quot;leading_diamond&quot;: &quot;\\uE0B0&quot;, &quot;trailing_diamond&quot;: &quot;\\uE0B0&quot;, &quot;powerline_symbol&quot;: &quot;\\uE0B0&quot;, &quot;foreground&quot;: &quot;#000000&quot;, &quot;background&quot;: &quot;#00FF00&quot;, &quot;properties&quot;: { &quot;prefix&quot;: &quot; \\uE5FF &quot;, &quot;style&quot;: &quot;folder&quot; } }, { &quot;type&quot;: &quot;git&quot;, &quot;style&quot;: &quot;powerline&quot;, &quot;powerline_symbol&quot;: &quot;\\uE0B0&quot;, &quot;foreground&quot;: &quot;#193549&quot;, &quot;background&quot;: &quot;#fffb38&quot;, &quot;properties&quot;: { &quot;display_stash_count&quot;: true, &quot;display_upstream_icon&quot;: true } }, { &quot;type&quot;: &quot;root&quot;, &quot;style&quot;: &quot;powerline&quot;, &quot;powerline_symbol&quot;: &quot;\\uE0B0&quot;, &quot;foreground&quot;: &quot;#193549&quot;, &quot;background&quot;: &quot;#ffff66&quot; }, { &quot;type&quot;: &quot;exit&quot;, &quot;style&quot;: &quot;powerline&quot;, &quot;foreground&quot;: &quot;#ffffff&quot;, &quot;background&quot;: &quot;#0000FF&quot;, &quot;leading_diamond&quot;: &quot;\\uE0B0&quot;, &quot;trailing_diamond&quot;: &quot;\\uE0B0&quot;, &quot;powerline_symbol&quot;: &quot;\\uE0B0&quot;, &quot;properties&quot;: { &quot;display_exit_code&quot;: false, &quot;always_enabled&quot;: true, &quot;error_color&quot;: &quot;#FF0000&quot;, &quot;color_background&quot;: true, &quot;success_icon&quot;:&quot;😄&quot;, &quot;error_icon&quot;: &quot;😭&quot; } } ] } ], &quot;final_space&quot;: true } Terminal Banner 安装色彩模块 Install-Module -Name PSWriteColor -Scope CurrentUser 颜色单词，为下面 -Color 准备 Black DarkBlue, DarkCyan, DarkRed, DarkMagenta, Gray, DarkGray, Blue, Green, Cyan, Red, Magenta, YellowWhite 去此网站把文本转ASCII Art =&gt; 这里我选用的是 ASCII Shadow 示例0： 遗憾的是，只有这个示例0能用，下面在PS中不能用，不过以后可能用得到 __ __ ___ /\\ \\__/\\ \\ /\\_ \\ __ __ __ __\\ \\ ,_\\ \\ \\___ ___ ___ \\//\\ \\ /\\_\\ ___ /'___\\/\\ \\/\\ \\\\ \\ \\/\\ \\ _ \\\\ / __\\\\ /' _ \\\\ \\ \\ \\ \\/\\ \\ /' _ \\\\ /\\ \\__/\\ \\ \\_\\ \\\\ \\ \\_\\ \\ \\ \\ \\/\\ \\ \\ \\/\\ \\/\\ \\ \\_\\ \\_\\ \\ \\/\\ \\/\\ \\ \\ \\____\\\\/\\____ \\\\ \\__\\\\ \\_\\ \\_\\ \\____/\\ \\_\\ \\_\\ /\\____\\\\ \\_\\ \\_\\ \\_\\ \\/____/ \\\\___// \\\\/__/ \\/_/\\/_/\\/___/ \\/_/\\/_/ \\/____/ \\/_/\\/_/\\/_/ /\\___/ \\/__/ 示例1： ██████╗ ██╗ ██╗████████╗██╗ ██╗ ██████╗ ███╗ ██╗ ██╔══██╗╚██╗ ██╔╝╚══██╔══╝██║ ██║██╔═══██╗████╗ ██║ ██████╔╝ ╚████╔╝ ██║ ███████║██║ ██║██╔██╗ ██║ ██╔═══╝ ╚██╔╝ ██║ ██╔══██║██║ ██║██║╚██╗██║ ██║ ██║ ██║ ██║ ██║╚██████╔╝██║ ╚████║ ╚═╝ ╚═╝ ╚═╝ ╚═╝ ╚═╝ ╚═════╝ ╚═╝ ╚═══╝ 示例2： ██╗ ██╗███╗ ██╗ ██║ ██║████╗ ██║ ██║ ██║██╔██╗ ██║ ██║ ██║██║╚██╗██║ ███████╗██║██║ ╚████║ 示例3： ██████╗██╗ ██╗████████╗██╗ ██╗ ██████╗ ███╗ ██╗ ██╗ ██╗███╗ ██╗ ██╔════╝╚██╗ ██╔╝╚══██╔══╝██║ ██║██╔═══██╗████╗ ██║ ██║ ██║████╗ ██║ ██║ ╚████╔╝ ██║ ███████║██║ ██║██╔██╗ ██║ ██║ ██║██╔██╗ ██║ ██║ ╚██╔╝ ██║ ██╔══██║██║ ██║██║╚██╗██║ ██║ ██║██║╚██╗██║ ╚██████╗ ██║ ██║ ██║ ██║╚██████╔╝██║ ╚████║ ███████╗██║██║ ╚████║ 写入PS配置文件 sublime C:\\Users\\lin\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1 Get-Alias | ForEach-Object { if ($_.Name -ne &quot;cd&quot;) { Remove-Item -Path (&quot;Alias:\\&quot; + $_.Name) -Force -ErrorAction &quot;SilentlyContinue&quot; } } Invoke-Expression (oh-my-posh --init --shell pwsh --config &quot;C:\\Users\\lin\\lin.omp.json&quot;) Write-Color -Text &quot; __ __ ___ &quot; -Color Cyan Write-Color -Text &quot; /\\ \\__/\\ \\ /\\_ \\ __ &quot; -Color Cyan Write-Color -Text &quot; ___ __ __\\ \\ ,_\\ \\ \\___ ___ ___ \\//\\ \\ /\\_\\ ___ &quot; -Color red Write-Color -Text &quot; /'___\\/\\ \\/\\ \\\\ \\ \\/\\ \\ _ \\\\ / __\\\\ /' _ \\\\ \\ \\ \\ \\/\\ \\ /' _ \\\\ &quot; -Color yellow Write-Color -Text &quot; /\\ \\__/\\ \\ \\_\\ \\\\ \\ \\_\\ \\ \\ \\ \\/\\ \\\\ \\/\\ \\/\\ \\ \\_\\ \\_\\ \\ \\/\\ \\/\\ \\ &quot; -Color green Write-Color -Text &quot; \\ \\____\\\\/\\____ \\\\ \\__\\\\ \\_\\ \\_\\ \\____/\\ \\_\\ \\_\\ /\\____\\\\ \\_\\ \\_\\ \\_\\&quot; -Color blue Write-Color -Text &quot; \\/____/ \\\\___// \\\\/__/ \\/_/\\/_/\\/___/ \\/_/\\/_/ \\/____/ \\/_/\\/_/\\/_/&quot; -Color Magenta Write-Color -Text &quot; /\\___/ &quot; -Color Cyan Write-Color -Text &quot; \\/__/ &quot; -Color Cyan Write-Color -Text &quot; &quot; function new_clear{ Clear-Host Write-Color -Text &quot; __ __ ___ &quot; -Color Cyan Write-Color -Text &quot; /\\ \\__/\\ \\ /\\_ \\ __ &quot; -Color Cyan Write-Color -Text &quot; ___ __ __\\ \\ ,_\\ \\ \\___ ___ ___ \\//\\ \\ /\\_\\ ___ &quot; -Color red Write-Color -Text &quot; /'___\\/\\ \\/\\ \\\\ \\ \\/\\ \\ _ \\\\ / __\\\\ /' _ \\\\ \\ \\ \\ \\/\\ \\ /' _ \\\\ &quot; -Color yellow Write-Color -Text &quot; /\\ \\__/\\ \\ \\_\\ \\\\ \\ \\_\\ \\ \\ \\ \\/\\ \\\\ \\/\\ \\/\\ \\ \\_\\ \\_\\ \\ \\/\\ \\/\\ \\ &quot; -Color green Write-Color -Text &quot; \\ \\____\\\\/\\____ \\\\ \\__\\\\ \\_\\ \\_\\ \\____/\\ \\_\\ \\_\\ /\\____\\\\ \\_\\ \\_\\ \\_\\&quot; -Color blue Write-Color -Text &quot; \\/____/ \\\\___// \\\\/__/ \\/_/\\/_/\\/___/ \\/_/\\/_/ \\/____/ \\/_/\\/_/\\/_/&quot; -Color Magenta Write-Color -Text &quot; /\\___/ &quot; -Color Cyan Write-Color -Text &quot; \\/__/ &quot; -Color Cyan Write-Color -Text &quot; &quot; } Set-Alias clear new_clear Set-Alias cls clear Banner 另一种解决思路 scoop install figlet figlet &quot;Cython lin&quot; ","link":"https://cythonlin.github.io/post/pr-greater-oh-my-posh3/"},{"title":"PR => Terminal 应用","content":"README 以后若发现 Windows应用程序或功能 可用Terminal命令代替且便捷实现，则记载于此。 Everything 命令行 - ES 下载地址： ES点此下载 意图 虽然 Everything图形界面工具已经很出色了。但我依然忘不掉 Linux 的 locate 命令 故有意将 Everything的命令行工具 es 替代 locate 使用方式 下载zip 解压 将 es.exe 改名为 locate.exe (Linux的locate命令使用习惯，若无此习惯，则跳过此步) 将此解压路径加入到环境变量 locate xxx ES 常用语法参数 我看了看 ES文档，抽取了一些个人认为好用的参数。 -r：正则 -s：以全路径排序（每级路径都会排序， 举个例子就和多级分组排序原理差不多） -size：文件大小 -dm：修改日期 -highlight：关键词高亮 解释的很清楚了，这里拿正则举一个例子， 因为es匹配原则是包含原则。 只要含有，就会列举出来，很宽柔。这里我们写一个严格匹配的正则： locate -r ^test.py$ 保存配置到文件（ es.exe 同目录下 es.ini）中，一劳永逸： -save-settings locate -size -dm -sizecolor 0x0d -dmcolor 0x0b -s -highlight -save-settings 清除配置：-clear-settings locate -clear-settings 个人认为 ES 命令行的一些优势 everything的图形界面一些高级功能都在工具栏，我是几乎不用。感觉麻烦。不如命令来的简洁。 正如前几篇文章所讲，把PS打造成 Sim-Linux， 所以我可以使用 grep 等命令。 这样我就可以使用 ES搜索命令 + grep 等做 多关键词 精细查找（everything图形高级功能应该也有） ","link":"https://cythonlin.github.io/post/pr-greater-terminal-ying-yong/"},{"title":"PR => Alias & Linux瑞士军刀","content":"介绍 前文 前面文章介绍了Windows Terminal 以及 WSL2 二者各有用途，但是用了一段时间发现，都不太方便 &quot;Sim-Bash&quot; 因为 Windows Terminal 的底层调用其实还是 PowerShell的命令。。 只不过 只是PowerShell（下面用简称PS来介绍）的命令全部都做了一层Alias操作。 此Alias有具体的语法，与Linux不一样的，但是意思差不多，就是给命令起别名。 就是因为PS这个别名，使得我们用Scoop安装的Linux同名命令冲突，并且用不了。 同名命令介绍 比如 ls 这个命令， PS用的 其他命令（类似dir）起个ls 这个别名 使得使用 ls 就可以像 Linux的Bash那样简单。 但是！！！！ PS的命令参数 和 Bash的命令参数是不一样的。。。 PS的 ls 命令 列举隐藏文件参数是 : -h Bash的 ls 命令 列举隐藏文件参数是 : -a 这就是 PS的命令不方便之处， ls只是个例子，还有很多其他命令有同样的问题。 Windows Terminal 就是用 PS “挂羊头卖狗肉” 罢了。。。 解决办法 我唯一想到的就是，删除所有PS命令的别名，一切命令由Scoop重新下载（很简单，后面说军刀） 然后按照这个思路寻找，找到可用的解决办法： 创建Windows PowerShell profile配置文件: New-Item -Type file -Force $profile 创建的此文件默认位置为：（我的用户名是 lin ） C:\\Users\\lin\\Documents\\WindowsPowerShell 写入如下代码，文件是(Microsoft.PowerShell_profile.ps1)： Get-Alias | ForEach-Object { if ($_.Name -ne &quot;cd&quot;) { Remove-Item -Path (&quot;Alias:\\&quot; + $_.Name) -Force -ErrorAction &quot;SilentlyContinue&quot; } } 此行命令意思就是清除所有PS 命令的别名。 （这个语句我学习加了一个条件 和 截获异常的语句） 如果不排除 cd 这个命令， 我们后续安装的模块是 没有cd这个安装包的。 因为 cd 是 LinuxBash 内建命令。 Windows无法安装， 即使我们安装了Bash, 也没有 cd。 所以，cd我们只能用ps自带的cd, 这是没办法的事情， 其余的命令别名就可以全删了。 至此，重新代开 Terminal。 遇到的问题 那就是每次打开新的Terminal 都会加载 上面Windows PowerShell profile这个配置文件里面的命令。可能是有缓存之类的信息，所以会发生文件不存在（就是类似空文件异常） 于是，查PowerShell的异常处理语句如上代码后追加 -ErrorAction &quot;SilentlyContinue 即可。 Scoop + Linux瑞士军刀-busybox -&gt;Scoop的安装教程见这里寻找 既然上文已删除了所有PS别名，那我们就需要下载我们要用的各种 Linux命令了， 比如 ls ps kill 各种常用 Linux命令。 不需要一个一个去手动 Scoop install 工具包， 只需一句： scoop install busybox 如此一来，我们就把 Windows Terminal 的各种蹩脚别名的PS命令 替换成了 Linux Bash命令！ 结束语 我并非吐槽PS命令，因为它可能有它专门的用途。 而是我更熟悉于 Linux 的 Bash 命令而已。 ","link":"https://cythonlin.github.io/post/pr-greater-del-alias-and-linux-rui-shi-jun-dao/"},{"title":"PR => MusicBee & VST","content":"MusicBee MusicBee 皮肤 Dark -&gt; Bee78 MusicBee 音质配置 首先要设置一下扬声器属性 右键播放下面的 “扬声器” 属性 -&gt; 高级 (把下面2个选项都 打上 √) -&gt; 允许应用程序独占控制该设备 -&gt; 给与独占模式应用程序优先 然后配置MusicBee 首选项 -&gt; 播放器 -&gt; 输出 -&gt; 选择 WSAAPI (Shared) ！！！！！！！这个至关重要，必须是 Shared -&gt; 使用32位输出+立体声混音至5.1+重新采样到 192K（这几个作用不太明显） 然后调控音量（比较重要）： MusicBee 的 内部音量 调到最低（但是不要调没）。 Windows扬声器音量调高。 MusicBee安装VST插槽 注：MusicBee不能直接安装VST插件， 需要官网安装一个VST插槽。用此插槽才可以安装其他VST插件 VST插槽下载地址 安装插槽 Preference -&gt; Plugins -&gt; Add Plugins -&gt; 插槽下载的位置 使用VST插槽安装VST插件 正常DLL文件安装方式 VST插件下载地址 注：MusicBee对VST插件支持很不友好，看了一大堆选了几个适合我这种初级选手能用的： 3D_Panner_2.0 wL_niceNwide Ceres (1) cs12-156 下载后直接解压，将解压文件夹直接放到 music的plugins目录下即可（需重启MusicBee） 痛苦安装方式（只用通过这种方式才能安装到Pitch Shift类插件） 这里提到一个超级老牌player -&gt; Winamp... ，现在应该没人用了。 或者都用Foobar2000了。 Winamp 我看论坛都是 2000年左右的评论。。。可知它的插件也特别旧了。 但是它的插件特别多。 可惜大多数放在 MusicBee上不好用或者崩溃。但没办法，还得用。 为什么这里要提到Winamp 因为MusicBee有个选项（从Winamp导入插件） 而 Winamp的插件大多数都是 EXE格式的， 需要识别 Winamp的安装路径，并且安装到Winamp路径下。 （其实EXE安装完也就是 DLL文件， 只不过它只能是这种方式。） 所以 Winamp 只是个过渡的工具人。。。 Winamp插件地址 Winamp插件 拾到可用的插件 take it easy 可以改变 Pitch 但是试过一段时间很遗憾，会让 MusicBee 宕掉。 最后一根救命稻草 pitchShifter.V1.01 pitchShifter地址 这个就用不到 Winmap了， 直接移动到 MusicBee 的 Plugins中即可 总结 找的很辛苦，普通的VST插件很好找， PITCH shift 这种的真的难找。 即使找到了，MusicBee有些也各种不支持，不兼容，宕掉。 最终插件合集： 3D_Panner_2.0 wL_niceNwide Ceres (1) cs12-156 pitchShifter （灵魂） 为何我如此执着音乐的变调，因为对于我来说，每一个Key都是一首新音乐！ ","link":"https://cythonlin.github.io/post/pr-greater-musicbee-and-winamp-and-vst/"},{"title":"PR => Terminal+WSL+ZSH+Docker","content":"快捷键 搜索功能 ctrl + shift + p 搜索文本 ctrl + shift + f 編輯配置文件 ctrl + ' (回車旁邊的符號 ') 橫向拆分窗口（下面action中，自定義配置） ctrl + +號 垂直拆分窗口（下面actrion中，自定義配置） ctrl + -號 關閉拆分的窗口（下面action中，自定義配置） ctrl + w 配置文件（ctrl + ' 后，全部内容整體替換即可） // To view the default settings, hold &quot;alt&quot; while clicking on the &quot;Settings&quot; button. // For documentation on these settings, see: https://aka.ms/terminal-documentation { // 官方设置指南? &quot;$schema&quot;: &quot;https://aka.ms/terminal-profiles-schema&quot;, // 一些globals设置 &quot;theme&quot;: &quot;dark&quot;, // 窗口主题 &quot;initialRows&quot;: 25, &quot;initialCols&quot;: 100, // 设置默认终端 &quot;defaultProfile&quot;: &quot;{61c54bbd-c2c6-5271-96e7-009a87ff44bf}&quot;, // &quot;defaultProfile&quot;: &quot;{0caa0dad-35be-5f56-a8ff-afceeeaa610}1&quot;, &quot;profiles&quot;: { &quot;defaults&quot;: { // Put settings here that you want to apply to all profiles &quot;colorScheme&quot;: &quot;Seafoam Pastel&quot;, &quot;useAcrylic&quot;: true, &quot;acrylicOpacity&quot;: 0.55, &quot;cursorShape&quot;: &quot;vintage&quot;, &quot;cursorHeight&quot;: 60, &quot;cursorColor&quot;: &quot;#B00C11&quot;, &quot;fontFace&quot;: &quot;YaHei Consolas Hybrid&quot;, &quot;fontSize&quot;: 18 }, &quot;list&quot;: [{ // Make changes here to the powershell.exe profile &quot;guid&quot;: &quot;{61c54bbd-c2c6-5271-96e7-009a87ff44bf}&quot;, &quot;name&quot;: &quot;PS&quot;, &quot;commandline&quot;: &quot;powershell.exe /nologo&quot;, // /nologo 表示删除 banner &quot;startingDirectory&quot;: &quot;d:&quot;, &quot;hidden&quot;: false }, { // Make changes here to the cmd.exe profile &quot;guid&quot;: &quot;{0caa0dad-35be-5f56-a8ff-afceeeaa6101}&quot;, &quot;name&quot;: &quot;cmd&quot;, &quot;commandline&quot;: &quot;cmd.exe&quot;, &quot;startingDirectory&quot;: &quot;d:&quot;, &quot;hidden&quot;: false }, { &quot;guid&quot;: &quot;{b453ae62-4e3d-5e58-b989-0a998ec441b8}&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Azure Cloud Shell&quot;, &quot;source&quot;: &quot;Windows.Terminal.Azure&quot; }, { &quot;guid&quot;: &quot;{07b52e3e-de2c-5db4-bd2d-ba144ed6c273}&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Ubuntu-20.04&quot;, &quot;startingDirectory&quot;: &quot;d:&quot;, &quot;source&quot;: &quot;Windows.Terminal.Wsl&quot; }, // { // &quot;guid&quot;: &quot;{e05f1426-9e28-4be3-8f2b-3e2b48eae4a8}&quot;, // &quot;hidden&quot;: false, // &quot;name&quot;: &quot;ssh&quot;, // &quot;commandline&quot;: &quot;ssh root@127.0.0.1 -p 22&quot; // }, { &quot;guid&quot;: &quot;{e05f1426-9e28-4be3-8f2b-3e2b48eae4a8}&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Conda&quot;, &quot;commandline&quot;: &quot;%windir%//System32//cmd.exe \\&quot;\\/K\\&quot; D:/miniconda/Scripts/activate.bat D:/miniconda&quot;, &quot;startingDirectory&quot;: &quot;d:&quot; }, ] }, // Add custom color schemes to this array &quot;schemes&quot;: [ { &quot;name&quot;: &quot;Seafoam Pastel&quot;, &quot;black&quot;: &quot;#000000&quot;, &quot;red&quot;: &quot;#ff7092&quot;, &quot;green&quot;: &quot;#00fbac&quot;, &quot;yellow&quot;: &quot;#fffa6a&quot;, &quot;blue&quot;: &quot;#00bfff&quot;, &quot;purple&quot;: &quot;#df95ff&quot;, &quot;cyan&quot;: &quot;#86cbfe&quot;, &quot;white&quot;: &quot;#ffffff&quot;, &quot;brightBlack&quot;: &quot;#000000&quot;, &quot;brightRed&quot;: &quot;#ff8aa4&quot;, &quot;brightGreen&quot;: &quot;#21f6bc&quot;, &quot;brightYellow&quot;: &quot;#fff787&quot;, &quot;brightBlue&quot;: &quot;#1bccfd&quot;, &quot;brightPurple&quot;: &quot;#e6aefe&quot;, &quot;brightCyan&quot;: &quot;#99d6fc&quot;, &quot;brightWhite&quot;: &quot;#ffffff&quot;, &quot;background&quot;: &quot;#332a57&quot;, &quot;foreground&quot;: &quot;#e5e5e5&quot; } ], // Add any keybinding overrides to this array. // To unbind a default keybinding, set the command to &quot;unbound&quot; &quot;keybindings&quot;: [], &quot;actions&quot;: [ // Copy and paste are bound to Ctrl+Shift+C and Ctrl+Shift+V in your defaults.json. // These two lines additionally bind them to Ctrl+C and Ctrl+V. // To learn more about selection, visit https://aka.ms/terminal-selection { &quot;command&quot;: {&quot;action&quot;: &quot;copy&quot;, &quot;singleLine&quot;: false }, &quot;keys&quot;: &quot;ctrl+c&quot; }, { &quot;command&quot;: &quot;paste&quot;, &quot;keys&quot;: &quot;ctrl+v&quot; }, // Press Ctrl+Shift+F to open the search box { &quot;command&quot;: &quot;find&quot;, &quot;keys&quot;: &quot;ctrl+f&quot; }, // settings { &quot;command&quot;: &quot;openSettings&quot;, &quot;keys&quot;: &quot;ctrl+'&quot; }, // Press Alt+Shift+D to open a new pane. // - &quot;split&quot;: &quot;auto&quot; makes this pane open in the direction that provides the most surface area. // - &quot;splitMode&quot;: &quot;duplicate&quot; makes the new pane use the focused pane's profile. // To learn more about panes, visit https://aka.ms/terminal-panes { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;auto&quot;, &quot;splitMode&quot;: &quot;duplicate&quot; }, &quot;keys&quot;: &quot;alt+shift+d&quot; }, { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;vertical&quot; }, &quot;keys&quot;: &quot;ctrl+plus&quot; }, { &quot;command&quot;: { &quot;action&quot;: &quot;splitPane&quot;, &quot;split&quot;: &quot;horizontal&quot; }, &quot;keys&quot;: &quot;ctrl+-&quot; }, { &quot;command&quot;: &quot;closePane&quot;, &quot;keys&quot;: &quot;ctrl+w&quot; } ] } 安装Scoop(并非Sqoop...) 作用 主要用在 单个纯 Windows Terminal 去安装 Linux工具使用。 但我发现 Win Terminal 的一些基础命令 还是没Linux好用（比如 ls无选项看不到隐藏文件） 所以我最后选择了 Win Terminal + WSL2 并用 所以Scoop这项就可以不用了 流程 因为用Powershell命令，会访问到raw.githubusercontent.com，所以先修改DNS C:\\Windows\\System32\\drivers\\etc 199.232.68.133 raw.githubusercontent.com ipconfig /flushdns 用Powershell命令正式安装Scoop: Set-ExecutionPolicy RemoteSigned -scope CurrentUser Invoke-Expression (New-Object System.Net.WebClient).DownloadString('https://get.scoop.sh') 更新可能会出现的BUG 安装应该是没问题的，不过以后使用的过程中可能会遇到更新的BUG。 cd C:/Users/lin/scoop/apps/ 删除这个目录下的 scoop目录 rm -rf scoop 然后重新安装 scoop(注意，删除的这个scoop是scoop内部的文件夹) Invoke-Expression (New-Object System.Net.WebClient).DownloadString('https://get.scoop.sh') 给scoop添加软件库(bucket) 列出可用的软件库(这种软件库会把一些带有GUI的软件放在里面)： scoop bucket known 添加软件库(这个是官方的) scoop bucket add extras 安装 deluge(下载torrent用的) scoop install deluge 添加自定义软件库 -&gt;官方教程 加速 Scoop的下载 scoop install aria2 Scoop 使用 Git 更新它自身，所以安一个Git 是需要的。 scoop install git 卸载语法 scoop uninstall xxxx 三方桌面程序 Winstep Xtreme 17.0+ (含 workshelf + NextStart) 卸载NextStart everything搜删 nextstart 注册表 HKEY_CURRENT_USER-&gt; SOFTWARE-&gt; WinSTEP2000-&gt; 把这里面的 Nextstart 和 Shared 整体删除 并且把 workshelf里面的 METARCode 项的值 删掉 三方 File Explorer RX 文件管理器 WSL2(附着在选项卡) 启用WSL(用PowerShell替代手动启用),（可稍后重启，继续下面） dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart 启用虚拟机功能（这部结束后，需要重启） dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart 下载 X64 WSL2 更新包，并双击安装 WSL2更新包地址 将WSL2设置为默认版本 wsl --set-default-version 2 MIcrosoft Store 安装 Ubuntu20 LTS 可以在 Windows Terminal 选项卡打开，也可直接打开Ubuntu Win Terminal 输入 wsl 即可进入 Ubuntu, 换源 cp -a /etc/apt/sources.list /etc/apt/sources.list.bak sudo sed -i &quot;s@http://.*archive.ubuntu.com@http://mirrors.huaweicloud.com@g&quot; /etc/apt/sources.list sudo sed -i &quot;s@http://.*security.ubuntu.com@http://mirrors.huaweicloud.com@g&quot; /etc/apt/sources.list sudo apt-get update 查看WSL版本信息与运行状态 wsl -l -v ZSH+Oh-my-zsh ZSH 安装ZSH sudo apt install zsh 查看版本： zsh --version 将ZSH设为默认shell: chsh -s $(which zsh) 退出当前用户lin，重新登录，并查看当前所用shell： echo $SHELL Oh-my-zsh 为普通用户安装oh-my-zsh： sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot; &quot;&quot; --unattended vi .zshrc (注意：下面这句话必须要加入到头部，我加到了第一行， it works😉) ZSH_DISABLE_COMPFIX=true source .zshrc 为root用户安装oh-my-zsh: # 说明， 我们把下面所有设置都写在lin用户的 .zshrc中 # 然后我们下面会创建一个软连接由 root用户的zsh配置指向 lin用户的zsh配置文件 # 这样做的目的就是 虽然 oh-my-zsh需要装2次，但是加软连接就可以把配置文件共享了 sudo -s sudo passwd root sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot; &quot;&quot; --unattended rm -rf /root/.oh-my-zsh rm -rf /root/.zshrc ln -s /home/lin/.oh-my-zsh /root/.oh-my-zsh ln -s /home/lin/.zshrc /root/.zshrc 设置主题：打开oh-my-zsh配置文件： vi ~/.zshrc 修改如下： ZSH_THEME=&quot;agnoster&quot; 激活： source ~/.zshrc 给win已有的Sublime设置环境变量 vi ~/.zshrc export PATH=&quot;$PATH:/mnt/d/ide/Sublime/sublime.exe&quot; alias sublime=sublime.exe 激活ZSH配置文件： source ~/.zshrc 本来想直接用 PS 的 locate，后来发现搜不到WSL里面的内容 没办法，只好用 mlocate了😉 sudo apt-get install mlocate updatedb # 手动更新搜索数据库（退出Shell，下次登录之前也会自动更新） ZSH插件 👍zsh-syntax-highlighting语法高亮(执行错误自动变红) # 安装 git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting # 配置：.zshrc中加入如下配置（设置完 然后 source 一下） # 向后面追加 plugins=() 空格或者换行分隔 plugins = ( git zsh-syntax-highlighting ) 👍autojump目录快速跳转（按 j 键盘跳转，不用cd，支持多级跳转，模糊匹配） # 安装 sudo apt-get install autojump # 配置，同上 autojump其实是维护一个目录数据库来达到跳转的目的，你也可以自己修改数据库 # 在数据库中添加一个目录 autojump -a [dir] # 提升当前目录的权重 autojump -i [value] # 降低当前目录的权重 autojump -d [value] # 显示数据库中的统计数据 autojump -s # 清除数据库中的目录 autojump --purge 举个例子，我用的WSL2，切换CDE盘让我感到难受，所以我可以升权 下面的3条命令，root用户和lin用户都要执行一次 cd /mnt/c &amp;&amp; autojump -a /mnt/c &amp;&amp; autojump -i 100 cd /mnt/d &amp;&amp; autojump -a /mnt/d &amp;&amp; autojump -i 100 cd /mnt/e &amp;&amp; autojump -a /mnt/e &amp;&amp; autojump -i 100 然后就可以通过，如下来切换盘符了： j c j d j e 👍zsh-autosuggestion语法补全（按 -&gt;方向键补全） # 安装 git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions # 配置同上，向 plugins=() 中追加 zsh-autosuggestions 总结上述插件最终安装 和 .zshrc配置： 安装 sudo apt-get install autojump git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting 设置 plugins=( git zsh-autosuggestions zsh-syntax-highlighting autojump ) 卸载 oh-my-zsh uninstall_oh_my_zsh 如何在Powershell中打开 Ubuntu 命令为： wsl NOTE：wsl和从 WT的右上角选择Ubuntu是一个效果 特别注意：不要用bash命令了，因为我们WSL已经把Bash换成了ZSH WSL 的 ls color 配色 vi ~/.zshrc #Change ls colours LS_COLORS=&quot;ow=01;36;40&quot; &amp;&amp; export LS_COLORS #make cd use the ls colours zstyle ':completion:*' list-colors &quot;${(@s.:.)LS_COLORS}&quot; autoload -Uz compinit compinit source ~/.zshrc Docker 方式1：直接用WSL2的虚拟机安装Docker curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh 此过程中会出现 sleep 20， 并推荐你使用方式2，但是我偏不想使用方式2 所以，只需等20秒，便可成功通过方式1来安装 Note: 此期间不要按Ctrl + C, 否则会退出方式1的安装 方式2：Docker Desktop for windows https://www.docker.com/products/docker-desktop 查看Docker服务 sudo service docker start sudo service docker status sudo service docker stop 使用Docker sudo docker run hello-world Docker语法个人笔记 =&gt; Docker语法 Docker所用的虚拟机占用内存过高的解决方案 cd $UserProfile sublime .wslconfig 方式1(限制内存，我的占用了400M暂时看不出效果)：写入如下内容: [wsl2] memory=4GB swap=0 localhostForwarding=true 方式2：不用的时候，关闭WSL， 输入如下命令，关闭 wsl ，即可： wsl --shutdown 以后还是老样子直接 wsl 即可。 ","link":"https://cythonlin.github.io/post/pr-greater-windows-terminal-ge-xing-hua-pei-zhi/"},{"title":"PY => 音声合成","content":"背景 音声合成 基于 很久之前写的文章 音声分离 一些 Light Music 的 Electronic Drum 太吵了。 于是突发奇想，如何 N v 1 分离出 Drum 并且 Drop 音声分离（更新为5stems-16kHz Model） 2stems (vocals / accompaniment) spleeter separate -o audio_output -i audio_example.mp3 4stems (vocals / bass / drums / other ) spleeter separate -o audio_output -p spleeter:4stems -i audio_example.mp3 5stems (vocals / bass / drums / piano / other) spleeter separate -o audio_output -p spleeter:5stems-16kHz -i audio_example.mp3 这次用的是 5stems预训练模型， 得到了如下5个文件： bass.wav drums.wav other.wav piano.wav vocals.wav 寻找解决方案 最开始不知道从何搜起，后来直接索性Github贴了一个 Question Issues。 有人给出stack的解决方案， 个人简化使用如下： ffmpeg -i other.wav -i vocals.wav -i bass.wav -i piano.wav -filter_complex amix=inputs=4:duration=longest output.mp3 结果 最终成功把 Electronic Drum 声 Drop。 唯一美中不足的就是，5个Stems预训练模型分的不够细致, Github Wiki最新方案的就是5stems~ ","link":"https://cythonlin.github.io/post/py-greater-yin-sheng-he-cheng/"},{"title":"RS => 推荐系统（四）实时计算解决冷启动","content":"实时计算业务需求 实时（在线）计算： 解决用户冷启动问题 实时计算能够根据用户的点击实时反馈，快速跟踪用户的喜好 Flume收集日志到Kafka 目的：收集本地实时日志行为数据，到kafka 步骤： 开启zookeeper以及kafka测试 创建flume配置文件，开启flume 开启kafka进行日志写入测试 脚本添加以及supervisor管理 开启zookeeper,需要在一直在服务器端实时运行，以守护进程运行 /root/bigdata/kafka/bin/zookeeper-server-start.sh -daemon /root/bigdata/kafka/config/zookeeper.properties 以及kafka的测试： /root/bigdata/kafka/bin/kafka-server-start.sh /root/bigdata/kafka/config/server.properties kafka测试过程： 开启消息生产者 /root/bigdata/kafka/bin/kafka-console-producer.sh --broker-list 192.168.19.137:9092 --sync --topic click-trace 开启消费者 /root/bigdata/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.19.137:9092 --topic click-trace 正式配置 修改原来收集日志的文件，添加flume收集日志行为到kafka的source, channel, sink a1.sources = s1 a1.sinks = k1 k2 a1.channels = c1 c2 a1.sources.s1.channels= c1 c2 a1.sources.s1.type = exec a1.sources.s1.command = tail -F /root/logs/userClick.log a1.sources.s1.interceptors=i1 i2 a1.sources.s1.interceptors.i1.type=regex_filter a1.sources.s1.interceptors.i1.regex=\\\\{.*\\\\} a1.sources.s1.interceptors.i2.type=timestamp # channel1 a1.channels.c1.type=memory a1.channels.c1.capacity=30000 a1.channels.c1.transactionCapacity=1000 # channel2 a1.channels.c2.type=memory a1.channels.c2.capacity=30000 a1.channels.c2.transactionCapacity=1000 # k1 a1.sinks.k1.type=hdfs a1.sinks.k1.channel=c1 a1.sinks.k1.hdfs.path=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d a1.sinks.k1.hdfs.useLocalTimeStamp = true a1.sinks.k1.hdfs.fileType=DataStream a1.sinks.k1.hdfs.writeFormat=Text a1.sinks.k1.hdfs.rollInterval=0 a1.sinks.k1.hdfs.rollSize=10240 a1.sinks.k1.hdfs.rollCount=0 a1.sinks.k1.hdfs.idleTimeout=60 # k2 a1.sinks.k2.channel=c2 a1.sinks.k2.type=org.apache.flume.sink.kafka.KafkaSink a1.sinks.k2.kafka.bootstrap.servers=192.168.19.137:9092 a1.sinks.k2.kafka.topic=click-trace a1.sinks.k2.kafka.batchSize=20 a1.sinks.k2.kafka.producer.requiredAcks=1 开启flume新的配置进行测试, 开启之前关闭之前的flume程序 supervisor&gt; status collect-click RUNNING May 26 09:43 AM offline STOPPED May 27 06:03 AM supervisor&gt; stop collect-click #!/usr/bin/env bash export JAVA_HOME=/root/bigdata/jdk export HADOOP_HOME=/root/bigdata/hadoop export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin /root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1 添加之后，开启supervisor里面的flume进程 start collect-click 开启kafka进行日志写入测试 开启kafka脚本进行测试，为了保证每次能够正常把zookeeper也放入脚本中，关闭之前的zookeeper，统一在kafka的开启脚本中加入zookeeper，名为 start_kafka.sh #!/usr/bin/env bash /root/bigdata/kafka/bin/zookeeper-server-start.sh -daemon /root/bigdata/kafka/config/zookeeper.properties /root/bigdata/kafka/bin/kafka-server-start.sh /root/bigdata/kafka/config/server.properties /root/bigdata/kafka/bin/kafka-topics.sh --zookeeper 192.168.19.137:2181 --create --replication-factor 1 --topic click-trace --partitions 1 supervisor添加脚本 =&gt; supervisor详情配置 [program:kafka] command=/bin/bash /root/toutiao_project/scripts/start_kafka.sh user=root autorestart=true redirect_stderr=true stdout_logfile=/root/logs/kafka.log loglevel=info stopsignal=KILL stopasgroup=true killasgroup=true supervisor进行update 测试配置后的Kafka 开启Kafka消费者 /root/bigdata/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.19.137:9092 --topic click-trace 写入一次点击数据： echo {\\&quot;actionTime\\&quot;:\\&quot;2019-04-10 21:04:39\\&quot;,\\&quot;readTime\\&quot;:\\&quot;\\&quot;,\\&quot;channelId\\&quot;:18,\\&quot;param\\&quot;:{\\&quot;action\\&quot;: \\&quot;click\\&quot;, \\&quot;userId\\&quot;: \\&quot;2\\&quot;, \\&quot;articleId\\&quot;: \\&quot;14299\\&quot;, \\&quot;algorithmCombine\\&quot;: \\&quot;C2\\&quot;}} &gt;&gt; userClick.log 观察消费者结果 [root@hadoop-master ~]# /root/bigdata/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.19.137:9092 --topic click-trace {&quot;actionTime&quot;:&quot;2019-04-10 21:04:39&quot;,&quot;readTime&quot;:&quot;&quot;,&quot;channelId&quot;:18,&quot;param&quot;:{&quot;action&quot;: &quot;click&quot;, &quot;userId&quot;: &quot;2&quot;, &quot;articleId&quot;: &quot;14299&quot;, &quot;algorithmCombine&quot;: &quot;C2&quot;}} 总结数据流通的两条路径 Flume -&gt; Hadoop -&gt; Hive Flume -&gt; Kafka生产者主题(click-trace) -&gt; 消费者（Spark Streaming 或 Storm） 实时召回 Spark Streaming 目的 使用Spark Streaming 读取 Kafka生产者topic里面的行为日志数据（kafka去flume那里收集的） 根据行为日志找出用户行为操作的文章， 并根据内容把相似文章推荐推给用户，实现实时内容召回。 步骤 配置spark streaming信息 读取点击行为日志数据，获取相似文章列表 过滤历史文章集合 存入召回结果以及历史记录结果 写入日志进行测试 创建spark streaming配置信息以及happybase 导入默认的配置，SPARK_ONLINE_CONFIG # 增加spark online 启动配置 class DefaultConfig(object): &quot;&quot;&quot;默认的一些配置信息 &quot;&quot;&quot; SPARK_ONLINE_CONFIG = ( (&quot;spark.app.name&quot;, &quot;onlineUpdate&quot;), # 设置启动的spark的app名称，没有提供，将随机产生一个名称 (&quot;spark.master&quot;, &quot;yarn&quot;), (&quot;spark.executor.instances&quot;, 4) ) 配置StreamingContext，在online的__init__.py文件添加，导入模块时直接使用 # 添加sparkstreaming启动对接kafka的配置 from pyspark import SparkConf from pyspark.sql import SparkSession from pyspark import SparkContext from pyspark.streaming import StreamingContext from pyspark.streaming.kafka import KafkaUtils from setting.default import DefaultConfig import happybase # 用于读取hbase缓存结果配置 pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090) # 1、创建conf conf = SparkConf() conf.setAll(DefaultConfig.SPARK_ONLINE_CONFIG) # 建立spark session以及spark streaming context sc = SparkContext(conf=conf) # 创建Streaming Context stream_c = StreamingContext(sc, 60) Kafka 消费者组 配置streaming 读取Kafka的配置,在配置文件中增加KAFKAIP和端口 下面groupby代表分组，Kafka分组的作用就是把数据copy n份，放到n个组中 这样，即使Topic内的数据被消费了一个，还有其他组的数据可供使用 # KAFKA配置 KAFKA_SERVER = &quot;192.168.19.137:9092&quot; # 基于内容召回配置，用于收集用户行为，获取相似文章实时推荐 similar_kafkaParams = {&quot;metadata.broker.list&quot;: DefaultConfig.KAFKA_SERVER, &quot;group.id&quot;: 'similar'} SIMILAR_DS = KafkaUtils.createDirectStream(stream_c, ['click-trace'], similar_kafkaParams) 创建online_update文件，建立在线召回类 import os import sys BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) sys.path.insert(0, os.path.join(BASE_DIR)) from online import stream_sc, SIMILAR_DS, pool from datetime import datetime from setting.default import DefaultConfig import redis import json import time import setting.logging as lg import logging 注意添加运行时环境 # 注意，如果是使用jupyter或ipython中，利用spark streaming链接kafka的话，必须加上下面语句 # 同时注意：spark version&gt;2.2.2的话，pyspark中的kafka对应模块已被遗弃，因此这里暂时只能用2.2.2版本的spark os.environ[&quot;PYSPARK_SUBMIT_ARGS&quot;] = &quot;--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.2 pyspark-shell&quot; Kafka读取点击行为日志数据，获取相似文章列表 传入Kafka的数据： {&quot;actionTime&quot;:&quot;2019-04-10 21:04:39&quot;,&quot;readTime&quot;:&quot;&quot;,&quot;channelId&quot;:18,&quot;param&quot;:{&quot;action&quot;: &quot;click&quot;, &quot;userId&quot;: &quot;2&quot;, &quot;articleId&quot;: &quot;116644&quot;, &quot;algorithmCombine&quot;: &quot;C2&quot;}} Time taken: 3.72 seconds, Fetched: 1 row(s) 过滤历史文章集合 存入召回结果以及历史记录结果 class OnlineRecall(object): &quot;&quot;&quot;在线处理计算平台 &quot;&quot;&quot; def __init__(self): pass def _update_content_recall(self): &quot;&quot;&quot; 在线内容召回计算 :return: &quot;&quot;&quot; # {&quot;actionTime&quot;:&quot;2019-04-10 21:04:39&quot;,&quot;readTime&quot;:&quot;&quot;,&quot;channelId&quot;:18, # &quot;param&quot;:{&quot;action&quot;: &quot;click&quot;, &quot;userId&quot;: &quot;2&quot;, &quot;articleId&quot;: &quot;116644&quot;, &quot;algorithmCombine&quot;: &quot;C2&quot;}} # x [,'json.....'] def get_similar_online_recall(rdd): &quot;&quot;&quot; 解析rdd中的内容，然后进行获取计算 :param rdd: :return: &quot;&quot;&quot; # rdd---&gt; 数据本身 # [row(1,2,3), row(4,5,6)]-----&gt;[[1,2,3], [4,5,6]] import happybase # 初始化happybase连接 pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090) for data in rdd.collect(): # 进行data字典处理过滤 if data['param']['action'] in [&quot;click&quot;, &quot;collect&quot;, &quot;share&quot;]: logger.info( &quot;{} INFO: get user_id:{} action:{} log&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), data['param']['userId'], data['param']['action'])) # 读取param当中articleId，相似的文章 with pool.connection() as conn: sim_table = conn.table(&quot;article_similar&quot;) # 根据用户点击流日志涉及文章找出与之最相似文章(基于内容的相似)，选取TOP-k相似的作为召回推荐结果 _dic = sim_table.row(str(data[&quot;param&quot;][&quot;articleId&quot;]).encode(), columns=[b&quot;similar&quot;]) _srt = sorted(_dic.items(), key=lambda obj: obj[1], reverse=True) # 按相似度排序 if _srt: topKSimIds = [int(i[0].split(b&quot;:&quot;)[1]) for i in _srt[:10]] # 根据历史推荐集过滤，已经给用户推荐过的文章 history_table = conn.table(&quot;history_recall&quot;) _history_data = history_table.cells( b&quot;reco:his:%s&quot; % data[&quot;param&quot;][&quot;userId&quot;].encode(), b&quot;channel:%d&quot; % data[&quot;channelId&quot;] ) history = [] if len(_history_data) &gt; 1: for l in _history_data: history.extend(l) # 根据历史召回记录，过滤召回结果 recall_list = list(set(topKSimIds) - set(history)) # 如果有推荐结果集，那么将数据添加到cb_recall表中，同时记录到历史记录表中 logger.info( &quot;{} INFO: store online recall data:{}&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), str(recall_list))) if recall_list: recall_table = conn.table(&quot;cb_recall&quot;) recall_table.put( b&quot;recall:user:%s&quot; % data[&quot;param&quot;][&quot;userId&quot;].encode(), {b&quot;online:%d&quot; % data[&quot;channelId&quot;]: str(recall_list).encode()} ) history_table.put( b&quot;reco:his:%s&quot; % data[&quot;param&quot;][&quot;userId&quot;].encode(), {b&quot;channel:%d&quot; % data[&quot;channelId&quot;]: str(recall_list).encode()} ) conn.close() # x可以是多次点击行为数据，同时拿到多条数据 # 😀 注意：kafka 生产出的数据 的 数据类型为 RDD操作，所以用了for RDD SIMILAR_DS.map(lambda x: json.loads(x[1])).foreachRDD(get_similar_online_recall) 开启实时运行,同时增加日志打印 if __name__ == '__main__': # 启动日志配置 lg.create_logger() op = OnlineRecall() op._update_online_cb() stream_c.start() # 使用 ctrl+c 可以退出服务 _ONE_DAY_IN_SECONDS = 60 * 60 * 24 try: while True: time.sleep(_ONE_DAY_IN_SECONDS) except KeyboardInterrupt: pass 添加文件打印日志 # 添加到需要打印日志内容的文件中 logger = logging.getLogger('online') # 在线更新日志 # 离线处理更新打印日志 trace_file_handler = logging.FileHandler( os.path.join(logging_file_dir, 'online.log') ) trace_file_handler.setFormatter(logging.Formatter('%(message)s')) log_trace = logging.getLogger('online') log_trace.addHandler(trace_file_handler) log_trace.setLevel(logging.INFO) 添加日志数据，进行测试 echo {\\&quot;actionTime\\&quot;:\\&quot;2019-04-10 21:04:39\\&quot;,\\&quot;readTime\\&quot;:\\&quot;\\&quot;,\\&quot;channelId\\&quot;:18,\\&quot;param\\&quot;:{\\&quot;action\\&quot;: \\&quot;click\\&quot;, \\&quot;userId\\&quot;: \\&quot;2\\&quot;, \\&quot;articleId\\&quot;: \\&quot;116644\\&quot;, \\&quot;algorithmCombine\\&quot;: \\&quot;C2\\&quot;}} &gt;&gt; userClick.log 2019-05-29 07:37:00 INFO: get user_id:2 action:click log 2019-05-29 07:37:02 INFO: store online recall data:[118560, 18530, 59747, 118370, 49962, 17613, 117199, 15315, 118550, 140376] 结果查询 hbase(main):028:0&gt; get 'cb_recall', 'recall:user:2' COLUMN CELL als:13 timestamp=1558041569201, value=[141431] als:18 timestamp=1558041572176, value=[19200, 17665, 19476, 16151, 16411, 19233, 13 090, 15140, 16421, 19494, 14381, 17966, 17454, 14125, 16174, 14899, 44339, 1 6437, 18743, 44090, 18238, 13890, 14915, 15429, 15944, 44371, 18005, 15196, 13410, 13672, 44137, 18795, 19052, 44652, 44654, 44657, 14961, 17522, 43894, 44412, 16000, 14208, 44419, 17802, 14223, 18836, 140956, 18335, 13728, 1449 8, 44451, 44456, 18609, 18353, 44468, 18103, 13757, 14015, 13249, 44739, 444 83, 17605, 14021, 15309, 18127, 43983, 44754, 43986, 19413, 14805, 18904, 44 761, 17114, 13272, 14810, 18907, 13022, 14299, 17120, 17632, 43997, 17889, 1 7385, 18156, 15085, 13295, 44020, 14839, 44024, 14585, 18172, 44541] als:5 timestamp=1558041564163, value=[141440] als:7 timestamp=1558041568776, value=[141437] online:18 timestamp=1559086622107, value=[118560, 18530, 59747, 118370, 49962, 17613, 117199, 15315, 118550, 140376] 热门文章与新文章实时召回 热门文章与新文章是存到Redis中，Redis存储结构： 新文章召回 结构 示例 new_article ch:{}:new ch:18:new 热门文章召回 结构 示例 popular_recall ch:{}:hot ch:18:hot redis实现基本思路 # 新文章存储 # ZADD ZRANGE # ZADD key score member [[score member] [score member] ...] # ZRANGE page_rank 0 -1 client.zadd(&quot;ch:{}:new&quot;.format(channel_id), {article_id: time.time()}) # 热门文章存储 # ZINCRBY key increment member # ZSCORE # 为有序集 key 的成员 member 的 score 值加上增量 increment 。 client.zincrby(&quot;ch:{}:hot&quot;.format(row['channelId']), 1, row['param']['articleId']) # ZREVRANGE key start stop [WITHSCORES] client.zrevrange(ch:{}:new, 0, -1) 添加热门文章kafka配置信息 # 添加sparkstreaming启动对接kafka的配置 # 配置KAFKA相关，用于热门文章KAFKA读取 click_kafkaParams = {&quot;metadata.broker.list&quot;: DefaultConfig.KAFKA_SERVER} HOT_DS = KafkaUtils.createDirectStream(stream_c, ['click-trace'], click_kafkaParams) 编写热门文章收集程序 在setting中增加redis的配置 # redis的IP和端口配置 REDIS_HOST = &quot;192.168.19.137&quot; REDIS_PORT = 6379 然后添加初始化信息 class OnlineRecall(object): &quot;&quot;&quot;实时处理（流式计算）部分 &quot;&quot;&quot; def __init__(self): self.client = redis.StrictRedis(host=DefaultConfig.REDIS_HOST, port=DefaultConfig.REDIS_PORT, db=10) # 在线召回筛选TOP-k个结果 self.k = 20 收集热门文章代码： def _update_hot_redis(self): &quot;&quot;&quot;更新热门文章 click-trace :return: &quot;&quot;&quot; client = self.client def updateHotArt(rdd): for row in rdd.collect(): logger.info(&quot;{}, INFO: {}&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), row)) # 如果是曝光参数，和阅读时长选择过滤 if row['param']['action'] == 'exposure' or row['param']['action'] == 'read': pass else: # 解析每条行为日志，然后进行分析保存点击，喜欢，分享次数，这里所有行为都自增1 client.zincrby(&quot;ch:{}:hot&quot;.format(row['channelId']), 1, row['param']['articleId']) HOT_DS.map(lambda x: json.loads(x[1])).foreachRDD(updateHotArt) return None 查看Redis热门文章 zrevrange表示从高到低遍历： client.zrevrange(ch:{}:new, 0, -1) 编写新文章收集程序 新文章如何而来，黑马头条后台在文章发布之后，会将新文章ID以固定格式传到KAFKA的new-article topic当中。 新文章的Kafka的Topic配置 类似 上面热门文章的配置： # new-article，新文章的读取 KAFKA配置 NEW_ARTICLE_DS = KafkaUtils.createDirectStream(stream_c, ['new-article'], click_kafkaParams) 并且导入相关包 from online import HOT_DS, NEW_ARTICLE_DS 然后，并且在kafka启动脚本start-kafka.sh中添加，先在supervisor中关闭flume与kafka的进程 /root/bigdata/kafka/bin/kafka-topics.sh --zookeeper 192.168.19.137:2181 --create --replication-factor 1 --topic new-article --partitions 1 然后supervisor start重新启动 supervisor配置见下方 👇 添加supervisor在线实时运行进程管理 supervisor配置文件增加以下配置 [program:online] environment=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python ,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python,PYSPARK_SUBMIT_ARGS='--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.2 pyspark-shell' command=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/reco_sys/online/online_update.py directory=/root/toutiao_project/reco_sys/online user=root autorestart=true redirect_stderr=true stdout_logfile=/root/logs/onlinesuper.log loglevel=info stopsignal=KILL stopasgroup=true killasgroup=true 通过supervisor运行新文章更新的进程 supervisor&gt; update online: added process group supervisor&gt; status collect-click RUNNING pid 97209, uptime 6:46:53 kafka RUNNING pid 105159, uptime 6:20:09 offline STOPPED Apr 16 04:31 PM online RUNNING pid 124591, uptime 0:00:02 supervisor&gt; 增加一个新文章kafka的topic，这里会与后台对接 新文章代码 def _update_new_redis(self): &quot;&quot;&quot;更新频道新文章 new-article :return: &quot;&quot;&quot; client = self.client def computeFunction(rdd): for row in rdd.collect(): channel_id, article_id = row.split(',') logger.info(&quot;{}, INFO: get kafka new_article each data:channel_id{}, article_id{}&quot;.format( datetime.now().strftime('%Y-%m-%d %H:%M:%S'), channel_id, article_id)) client.zadd(&quot;ch:{}:new&quot;.format(channel_id), {article_id: time.time()}) NEW_ARTICLE_DS.map(lambda x: x[1]).foreachRDD(computeFunction) return None 测试: python安装kafka: pip install kafka-python 查看所有本地topic情况 from kafka import KafkaClient client = KafkaClient(hosts=&quot;127.0.0.1:9092&quot;) for topic in client.topics: print(topic) from kafka import KafkaProducer # kafka消息生产者 kafka_producer = KafkaProducer(bootstrap_servers=['192.168.19.137:9092']) # 构造消息并发送 msg = '{},{}'.format(18, 13891) kafka_producer.send('new-article', msg.encode()) 可以得到redis结果 127.0.0.1:6379[10]&gt; keys * 1) &quot;ch:18:hot&quot; 2) &quot;ch:18:new&quot; 127.0.0.1:6379[10]&gt; ZRANGE &quot;ch:18:new&quot; 0 -1 1) &quot;13890&quot; 2) &quot;13891&quot; 流程回顾 协商好新文章发送格式以及Topic 定义一个新的Topic 配置好Kafka的新的Topic，重新开启Kafka Streaming程序对接读取逻辑，存入Redis，实时运行程序，检测Kafka是否有新数据 实时计算脚本，封装为一个函数，并作为.py脚本，通过supervisor 管理py脚本相关运行命令 ","link":"https://cythonlin.github.io/post/go-greater-goland-chang-yong-dai-ma-pian-duan-zi-ding-yi/"},{"title":"PY => Github-Cli(1.6.2)","content":"下载 选个OS版本（我用的Win）：https://github.com/cli/cli/releases WSL一步安装(新增) sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-key C99B11DEB97541F0 &amp;&amp; sudo apt-add-repository https://cli.github.com/packages &amp;&amp; sudo apt update &amp;&amp; sudo apt install gh 初次安装git需： git config --global user.name &quot;hacker-lin&quot; git config --global user.email &quot;Cython_lin@cklin.top&quot; 设置SSH Key 正常生成密钥命令 ssh-keygen 进入用户家目录，把id_rsa.pub公钥复制出来 cat ~/.ssh/id_rsa.pub 粘贴到-&gt; 公钥存储 cli查看密钥 gh ssh-key list 使用上面这条命令时，需要👇更新添加一个token权限(并引导重新登录)： gh auth refresh -s read:public_key cli添加密钥文件(通常不需要这条命令，用ssh-keygen即可) gh ssh-key add 公钥文件路径 更新 token gh auth refresh -s read:public_key 设置编辑器 通过gh设置 gh config set editor &quot;vim&quot; gh config set editor &quot;code --wait&quot; # 由于sublime.exe有路径BUG，所以还是用上面的vim好了，总比nano顺手 # gh config set editor &quot;/mnt/d/ide/Sublime/sublime.exe&quot; 通过git设置 git config --global core.editor &quot;IDE文件的执行路径/xxxx.exe&quot; git config --global core.editor &quot;code --wait&quot; 便捷小技巧预设.zshrc alias ga=&quot;git add&quot; alias gb=&quot;git branch&quot; # 删除已经被merge到master的branch (此别名是默认提供的） alias gbda='git branch --no-color --merged | command grep -vE &quot;^(\\+|\\*|\\s*($(git_main_branch)|development|develop|devel|dev)\\s*$)&quot; | command xargs -n 1 git branch -d' alias gc=&quot;git checkout&quot; alias gcb=&quot;git checkout -b&quot; alias gcm=&quot;git checkout $(git_main_branch)&quot; # 默认拉取master alias gl=&quot;git pull&quot; # -a参数表示（如果修改文件后，忘记git add， -a参数会在commit的时候会自动帮你add） alias gi='git init &amp;&amp; git add . &amp;&amp; git commit -am &quot;Init&quot;' # push：$(git_current_branch) 表示当前所在分支（master 或 其他分支） alias gp='git push --set-upstream origin $(git_current_branch)' # add+commit alias gcam='ga . &amp;&amp; git commit -a -m' # remote alias grv='git remote -v' 上面技巧组合使用 cd 本地代码目录 (进入的目录里面 就是要上传的代码) gi （生成 .git 并把本地代码添加到本地仓） gh repo create (创建远程仓库，并把远程仓库信息 追加 到当前的 .git中) gp (推送到远程仓库) 给shell配置自动补全提示 zsh 命令： gh completion -s zsh &gt; /usr/local/share/zsh/site-functions/_gh vi .~/.zshrc: autoload -U compinit compinit -i bash vi ~/.bash_profile eval &quot;$(gh completion -s bash)&quot; 列出配置 git config --list 由于我github2个号切换，导致，push的时候有403错误混淆， 所以删除了家目录的 .gitconfig(应该是这个有些记不清了) 设置代理 为了加速clone，这里先设置，若没有PROXY, 那此步可略过 git config --global http.proxy &quot;socks5://127.0.0.1:7890&quot; git config --global https.proxy &quot;socks5://127.0.0.1:7890&quot; 查看当前代理： git config --global --get http.proxy git config --global --get https.proxy 清除代理也很简单 git config --global --unset http.proxy git config --global --unset https.proxy 登录 gh auth login 提前声明，遇到选项，都是用上下箭头选择 👍1.选择Github.com（也就是个人用户） 👍2.选择SSH（HTTPS也可以） 👍3.Token or Web Browser， 2选1 浏览器方式很简单了： Command中会给一串代码，复制代码-&gt;CMD回车-&gt;自动跳转到Web-&gt;粘贴代码-&gt;确认-&gt;确认授权 另外说下token方式: =&gt; 生成Token 点击 Generate bew token ，新建一个新 token： Note： 需要把 repo的所有权限勾上 外加一个admin:org下面的 read:org 选项 Note: read:org 必须勾上，不然创建失败。 Note: gist （create gist 也勾上， gist这项不是必须的，建议勾上） 👏查看token+用户名+协议(ssh/http): cat ~/.config/gh/hosts.yml 查看登录状态 gh auth status 退出登录 gh auth logout issue 创建 issue gh issue create 写body的时候，由于我们上面通过gh已经设置好了默认编辑器，所以按e直接用vim 并且里面可以直接写markdown语法 查看 issue 查看未被关闭的issue (--repo 或者 -R) gh issue list --repo cli/cli 当已经clone cli/cli，并进入此cli仓库，则可以直接使用 gh issue list 😶查看 已关闭 的issue gh pr list --state closed 查看3015编号的issue（view表示用markdown格式） gh issue view --repo cli/cli 3015 用web打开 ( -- web 或者 -w): gh issue view --repo cli/cli 3015 --web 也可以用 view 或 --web 应用到其他选项，例如repo gh repo view cli/cli --web 编辑 issue 注意：执行后要按一下右方向键，才选中，不然改不动 gh issue edit 2 关闭 issue / 重新打开 issue gh issue cluse 2 gh issue reopen 2 评论 issue gh issue comment 2 删除 issue gh issue delete 2 Note: 执行后会提示你验证一下，issue编号，输入一下编号即可，这个例子是2 仓库 创建仓库 gh repo create my-gh -&gt; Public -&gt; xxx in your current directory（Y/N） y (回车默认就是yes，下同) -&gt; Create a local project directory for xxx （Y/N）y 查看远程权限 git remote -v origin https://github.com/Cythonlin/my-gh.git (fetch) origin https://github.com/Cythonlin/my-gh.git (push) 克隆 gh repo clone gin-gonic/gin cd gin git remote -v origin https://github.com/gin-gonic/gin.git (fetch) origin https://github.com/gin-gonic/gin.git (push) # 我们可以发现，这分支并不是我们的自己的 # 所以我们可以 fork 下来 fork fork 指的是，把克隆到自己的仓库，作为上游（upstream）项目，然后自己就可自由同步它 cd gin # 上面已经进此路径，这步可省 gh repo fork -&gt; Would you like to add a remote for the fork? (Y/n) 回车yes 上面是先clone,然后进入路径，再fork 如果事先未clone， 也可以用gh repo fork + 用户名/仓库名， 直接 fork+clone一步到位 # 这就不需要像上面先 cd进入clone的目录下再fork了，这种方式直接fork即可 gh repo fork pytorch/pytorch 查看仓库 gh repo view PR (Pull Request） PR概念 &quot;我fork了你们的代码，现在我发送一个请求，请你们回收我的代码&quot;😂 PR流程 fork别人仓库（先fork在clone， 前面已经提到 gh 可以直接fork一步到位了） 切换分支（也可以在 master 下），add,commit (gcam)修改代码。 在你fork后的仓库主页点击右上角的 Compare &amp; pull request 提交合并申请 等待别人合并你的请求 用cli代替如上流程 一、用当前的号，去 fork另一个号（另外那个也是自己的号方便做实验）的仓库。 gh repo fork hacker-lin/bio2bioes cd bio2bioes 二、 创建+切换分支+add+commit gcb dev echo 111 &gt; 1.txt gcam &quot;mytest&quot; # 到这里即可，不需要push， 因为我们是 request 三、用GitHUB-Cli命令 代替 点击Compare &amp; pull request 按钮提交合并申请 创建PR gh pr create -&gt; Where should we push the 'dev' branch # 选第一个 -&gt; Title # 随便写个 some update -&gt; Body # 直接 回车 跳过就行。 -&gt; What's next? # 选 Submit 提交即可 Merge PR 列出 pr 信息 列出当前未关闭的PR信息： gh pr list 😶列出已关闭的pr: gh pr list --status closed 查看具体某一ID的PR信息： gh pr view &lt;id&gt; 查看所有PR详细信息 gh pr status Merge PR gh pr merge -&gt; What merge method would you like to use # Create a merge commit即可 # 如果提示 是否删除 local and github branch: # 如果输入y， （合并+删除 本地分支） # 如果输入n, （合并+保留 本地分支） # Note: 无论选 y/n ，只会对本地分支操作保留/删除。 Closed PR gh pr close 3 Reopen PR gh pr reopen 😑Note: 被merge的PR不能 reopen 比较某PR前后信息(+-) gh pr diff &lt;ID&gt; 转到某ID的PR的分支 gh pr checkout &lt;ID&gt; 在PR下面评论（同级评论） review 和 comment 平陵功能一样，不过 review多了2个额外功能 建议用 comment(因为issue也是 comment) gh pr comment &lt;id&gt; gh pr review &lt;id&gt; gist =&gt; 用户gist地址 创建 gist 😄默认私有存储 gh gist create test.py 😏公开存储 gh gist create lin.txt --public 🙂打包存储 (一个gist里面放2个文件，方便下载，默认以第一个文件名来命名此gist) gh gist create lin.txt test.py 😌也可以标准输入字符串（而不是上传文件）（同样默认私有） 无名文件（默认名为 gistfile0.txt） # - 后面不跟任何参数（同样默认私有） gh gist create - # 公开 gh gist create --public - # 可以配合cat使用 (这样就不用写 - 这个符号了) cat xxx.txt | gh gist create 输入后，回车换个行，然后ctrl+d保存创建 列出 gist + 内容查看 列出全部 gh gist list 列出私有 gh gist list --secret 列出公有 gh gist list --public 查看具体某一gist具体文件内容： gh gist view 28e6f15f41b3644740aa6549abd318e2 修改 gist （web一体修改） gh gist edit dab6dd9482e6c5b5b69a2422b30c2b7e 下载 gist文件 gh gist clone dab6dd9482e6c5b5b69a2422b30c2b7e 删除 gist gh gist delete 28e6f15f41b3644740aa6549abd318e2 release create release gh release create v1.3 test.txt -t &quot;xxapp1.3&quot; -n &quot;Fix xxxx for xxx #3019&quot; v1.3：表示左侧竖条版本 test.txt：表示上传文件（给别人下载的） -t：表示release标题 -n：表示标题下面的小说明信息 查看所有release版本信息(粗略) gh release list 查看具体版本信息 默认不跟版本号，则看最新版本 gh release view 看具体某一版本 gh release view v1.1 给某版本上传追加文件 gh release upload v1.1 lin.txt 下载release文件 只写版本号，不加正则模式，则默认下载此版本下的全部文件 gh release download v1.1 加正则，下载所有 txt格式 文件 gh release download v1.1 -p &quot;*.txt&quot; 加正则，下载多种格式文件(txt)（就是叠加写法） gh release download v1.1 -p &quot;*.txt&quot; -p &quot;*.7z&quot; ","link":"https://cythonlin.github.io/post/py-greater-github-cli-quan-tao/"},{"title":"RS => 推荐系统（三）离线召回 ","content":"召回设计 召回排序流程 匿名用户： 通常使用用户冷启动方案，区别在于user_id为匿名用户手机识别号(不允许匿名用户) 所有只正针对于登录用户： 用户冷启动（前期点击行为较少情况） 非个性化推荐 热门召回：自定义热门规则，根据当前时间段热点定期更新维护人点文章库 新文章召回：为了提高新文章的曝光率，建立新文章库，进行推荐 个性化推荐： 基于内容的协同过滤在线召回：基于用户实时兴趣画像相似的召回结果用于首页的个性化推荐 后期离线部分（用户点击行为较多，用户画像完善） 建立用户长期兴趣画像（详细）：包括用户各个维度的兴趣特征 训练排序模型 LR模型、FTRL、Wide&amp;Deep 离线部分的召回： 基于模型协同过滤推荐离线召回：ALS 基于内容的离线召回：或者称基于用户画像的召回 召回表设计与模型召回 召回表设计 我们的召回方式有很多种。 多路召回结果存储模型召回 与 内容召回的结果 需要进行相应频道推荐合并。 方案：基于模型与基于内容的召回结果存入同一张表，避免多张表进行读取处理 由于HBASE有多个版本数据功能存在的支持 TTL=&gt;7776000, VERSIONS=&gt;999999 如下： create 'cb_recall', {NAME=&gt;'als', TTL=&gt;7776000, VERSIONS=&gt;999999} alter 'cb_recall', {NAME=&gt;'content', TTL=&gt;7776000, VERSIONS=&gt;999999} alter 'cb_recall', {NAME=&gt;'online', TTL=&gt;7776000, VERSIONS=&gt;999999} # 例子（多版本）： put 'cb_recall', 'recall:user:5', 'als:1',[45,3,5,10] put 'cb_recall', 'recall:user:5', 'als:1',[289,11,65,52,109,8] put 'cb_recall', 'recall:user:5', 'als:2',[1,2,3,4,5,6,7,8,9,10] put 'cb_recall', 'recall:user:2', 'content:1',[45,3,5,10,289,11,65,52,109,8] put 'cb_recall', 'recall:user:2', 'content:2',[1,2,3,4,5,6,7,8,9,10] hbase(main):084:0&gt; desc 'cb_recall' Table cb_recall is ENABLED cb_recall COLUMN FAMILIES DESCRIPTION {NAME =&gt; 'als', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false' , KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; '7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE _INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_ OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'} {NAME =&gt; 'content', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'fa lse', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; '7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', C ACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS _ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'} {NAME =&gt; 'online', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'fal se', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; '7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CA CHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'} 3 row(s) （几乎不用）在HIVE用户数据数据库下建立HIVE外部表,若hbase表有修改，则进行HIVE 表删除更新 create external table cb_recall_hbase( user_id STRING comment &quot;userID&quot;, als map&lt;string, ARRAY&lt;BIGINT&gt;&gt; comment &quot;als recall&quot;, content map&lt;string, ARRAY&lt;BIGINT&gt;&gt; comment &quot;content recall&quot;, online map&lt;string, ARRAY&lt;BIGINT&gt;&gt; comment &quot;online recall&quot;) COMMENT &quot;user recall table&quot; STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,als:,content:,online:&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;cb_recall&quot;); 增加一个历史召回结果表 create 'history_recall', {NAME=&gt;'channel', TTL=&gt;7776000, VERSIONS=&gt;999999} put 'history_recall', 'recall:user:5', 'als:1',[1,2,3] put 'history_recall', 'recall:user:5', 'als:1',[4,5,6,7] put 'history_recall', 'recall:user:5', 'als:1',[8,9,10] 为什么增加历史召回表？ 1、直接在存储召回结果部分进行过滤，比之后排序过滤，节省排序时间 2、防止Redis缓存没有消耗完，造成重复推荐，从源头进行过滤 基于模型召回集合计算 ALS模型推荐实现 步骤： 1、数据类型转换,clicked以及用户ID与文章ID处理 2、ALS模型训练以及推荐 3、推荐结果解析处理 4、推荐结果存储 数据类型转换,clicked( bool 转 int) ur.spark.sql(&quot;use profile&quot;) user_article_click = ur.spark.sql(&quot;select * from user_article_basic&quot;).\\ select(['user_id', 'article_id', 'clicked']) # 更换类型 def change_types(row): return row.user_id, row.article_id, int(row.clicked) user_article_click = user_article_click.rdd.map(change_types).toDF(['user_id', 'article_id', 'clicked']) 这步处理结果格式如下： user_id article_id clicked 0 1 用户ID与文章ID处理，编程ID索引（原用户ID和文章ID是长字符串，ALS模型不能处理，要重新编排ID索引） from pyspark.ml.feature import StringIndexer from pyspark.ml import Pipeline # 用户和文章ID超过ALS最大整数值，需要使用StringIndexer进行转换 user_id_indexer = StringIndexer(inputCol='user_id', outputCol='als_user_id') article_id_indexer = StringIndexer(inputCol='article_id', outputCol='als_article_id') pip = Pipeline(stages=[user_id_indexer, article_id_indexer]) pip_fit = pip.fit(user_article_click) als_user_article_click = pip_fit.transform(user_article_click) ALS 模型训练与推荐（ALS模型需要输出用户ID列，文章ID列以及点击列） from pyspark.ml.recommendation import ALS # 模型训练和推荐默认每个用户固定文章个数 als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1) model = als.fit(als_user_article_click) recall_res = model.recommendForAllUsers(100) 结果： als_user_id recommendations 1 [[article_id, 分数]] 推荐结果处理 通过StringIndexer变换后的下标知道原来的和用户ID # recall_res得到需要使用StringIndexer变换后的下标 # 保存原来的下表映射关系 refection_user = als_user_article_click.groupBy(['user_id']).max('als_user_id').withColumnRenamed( 'max(als_user_id)', 'als_user_id') refection_article = als_user_article_click.groupBy(['article_id']).max('als_article_id').withColumnRenamed( 'max(als_article_id)', 'als_article_id') # Join推荐结果与 refection_user映射关系表 # +-----------+--------------------+-------------------+ # | als_user_id | recommendations | user_id | # +-----------+--------------------+-------------------+ # | 8 | [[163, 0.91328144]... | 2 | # | 0 | [[145, 0.653115], ... | 1106476833370537984 | recall_res = recall_res.join(refection_user, on=['als_user_id'], how='left').select( ['als_user_id', 'recommendations', 'user_id']) 对推荐文章ID后处理：得到推荐列表,获取推荐列表中的ID索引 # Join推荐结果与 refection_article映射关系表 # +-----------+-------+----------------+ # | als_user_id | user_id | als_article_id | # +-----------+-------+----------------+ # | 8 | 2 | [163, 0.91328144] | # | 8 | 2 | [132, 0.91328144] | import pyspark.sql.functions as F recall_res = recall_res.withColumn('als_article_id', F.explode('recommendations')).drop('recommendations') # +-----------+-------+--------------+ # | als_user_id | user_id | als_article_id | # +-----------+-------+--------------+ # | 8 | 2 | 163 | # | 8 | 2 | 132 | def _article_id(row): return row.als_user_id, row.user_id, row.als_article_id[0] 进行索引对应文章ID获取 als_recall = recall_res.rdd.map(_article_id).toDF(['als_user_id', 'user_id', 'als_article_id']) als_recall = als_recall.join(refection_article, on=['als_article_id'], how='left').select( ['user_id', 'article_id']) # 得到每个用户ID 对应推荐文章 # +-------------------+----------+ # | user_id | article_id | # +-------------------+----------+ # | 1106476833370537984 | 44075 | # | 1 | 44075 | 获取每个文章对应的频道，推荐给用户时按照频道存储: ur.spark.sql(&quot;use toutiao&quot;) news_article_basic = ur.spark.sql(&quot;select article_id, channel_id from news_article_basic&quot;) als_recall = als_recall.join(news_article_basic, on=['article_id'], how='left') als_recall = als_recall.groupBy(['user_id', 'channel_id']).agg(F.collect_list('article_id')).withColumnRenamed( 'collect_list(article_id)', 'article_list') als_recall = als_recall.dropna() 召回结果存储 HBASE表设计概览： put 'cb_recall', 'recall:user:5', 'als:1',[45,3,5,10,289,11,65,52,109,8] put 'cb_recall', 'recall:user:5', 'als:2',[1,2,3,4,5,6,7,8,9,10] 存储代码如下： def save_offline_recall_hbase(partition): &quot;&quot;&quot;离线模型召回结果存储 &quot;&quot;&quot; import happybase pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090) for row in partition: with pool.connection() as conn: # 获取历史看过的该频道文章 history_table = conn.table('history_recall') # 多个版本 data = history_table.cells('reco:his:{}'.format(row.user_id).encode(), 'channel:{}'.format(row.channel_id).encode()) history = [] if len(data) &gt;= 2: for l in data[:-1]: history.extend(eval(l)) else: history = [] # 过滤reco_article与history reco_res = list(set(row.article_list) - set(history)) if reco_res: table = conn.table('cb_recall') # 默认放在推荐频道 table.put('recall:user:{}'.format(row.user_id).encode(), {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()}) # 放入历史推荐过文章 history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(), {'channel:{}'.format(row.channel_id): str(reco_res).encode()}) conn.close() als_recall.foreachPartition(save_offline_recall_hbase) 离线用户基于内容召回集 目标 知道离线内容召回的概念 知道如何进行内容召回计算存储规则 应用 应用spark完成离线用户基于内容的协同过滤推荐 基于内容召回实现（文章向量之前已经弄好了） 过滤用户点击的文章 # 基于内容相似召回（画像召回） ur.spark.sql(&quot;use profile&quot;) user_article_basic = self.spark.sql(&quot;select * from user_article_basic&quot;) user_article_basic = user_article_basic.filter(&quot;clicked=True&quot;) def save_content_filter_history_to__recall(partition): &quot;&quot;&quot;计算每个用户的每个操作文章的相似文章，过滤之后，写入content召回表当中（支持不同时间戳版本） &quot;&quot;&quot; import happybase pool = happybase.ConnectionPool(size=10, host='hadoop-master') # 进行为相似文章获取 with pool.connection() as conn: # key: article_id, column: similar:article_id similar_table = conn.table('article_similar') # 循环partition for row in partition: # 获取相似文章结果表 similar_article = similar_table.row(str(row.article_id).encode(), columns=[b'similar']) # 相似文章相似度排序过滤，召回不需要太大的数据， 百个，千 _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True) if _srt: # 每次行为推荐10篇文章 reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10] # 获取历史看过的该频道文章 history_table = conn.table('history_recall') # 多个版本 data = history_table.cells('reco:his:{}'.format(row.user_id).encode(), 'channel:{}'.format(row.channel_id).encode()) history = [] if len(data) &gt;= 2: for l in data[:-1]: history.extend(eval(l)) else: history = [] # 过滤reco_article与history reco_res = list(set(reco_article) - set(history)) # 进行推荐，放入基于内容的召回表当中以及历史看过的文章表当中 if reco_res: # content_table = conn.table('cb_content_recall') content_table = conn.table('cb_recall') content_table.put(&quot;recall:user:{}&quot;.format(row.user_id).encode(), {'content:{}'.format(row.channel_id).encode(): str(reco_res).encode()}) # 放入历史推荐过文章 history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(), {'channel:{}'.format(row.channel_id).encode(): str(reco_res).encode()}) conn.close() user_article_basic.foreachPartition(save_content_filter_history_to__recall) 离线用户召回定时更新 定时更新代码 import os import sys # 如果当前代码文件运行测试需要加入修改路径，否则后面的导包出现问题 BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) sys.path.insert(0, os.path.join(BASE_DIR)) from pyspark.ml.feature import StringIndexer from pyspark.ml import Pipeline from pyspark.ml.recommendation import ALS from offline import SparkSessionBase from datetime import datetime import time import numpy as np class UpdateRecall(SparkSessionBase): SPARK_APP_NAME = &quot;updateRecall&quot; ENABLE_HIVE_SUPPORT = True def __init__(self, number): self.spark = self._create_spark_session() self.N = number def update_als_recall(self): &quot;&quot;&quot; 更新基于模型（ALS）的协同过滤召回集 :return: &quot;&quot;&quot; # 读取用户行为基本表 self.spark.sql(&quot;use profile&quot;) user_article_click = self.spark.sql(&quot;select * from user_article_basic&quot;).\\ select(['user_id', 'article_id', 'clicked']) # 更换类型 def change_types(row): return row.user_id, row.article_id, int(row.clicked) user_article_click = user_article_click.rdd.map(change_types).toDF(['user_id', 'article_id', 'clicked']) # 用户和文章ID超过ALS最大整数值，需要使用StringIndexer进行转换 user_id_indexer = StringIndexer(inputCol='user_id', outputCol='als_user_id') article_id_indexer = StringIndexer(inputCol='article_id', outputCol='als_article_id') pip = Pipeline(stages=[user_id_indexer, article_id_indexer]) pip_fit = pip.fit(user_article_click) als_user_article_click = pip_fit.transform(user_article_click) # 模型训练和推荐默认每个用户固定文章个数 als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1) model = als.fit(als_user_article_click) recall_res = model.recommendForAllUsers(self.N) # recall_res得到需要使用StringIndexer变换后的下标 # 保存原来的下表映射关系 refection_user = als_user_article_click.groupBy(['user_id']).max('als_user_id').withColumnRenamed( 'max(als_user_id)', 'als_user_id') refection_article = als_user_article_click.groupBy(['article_id']).max('als_article_id').withColumnRenamed( 'max(als_article_id)', 'als_article_id') # Join推荐结果与 refection_user映射关系表 # +-----------+--------------------+-------------------+ # | als_user_id | recommendations | user_id | # +-----------+--------------------+-------------------+ # | 8 | [[163, 0.91328144]... | 2 | # | 0 | [[145, 0.653115], ... | 1106476833370537984 | recall_res = recall_res.join(refection_user, on=['als_user_id'], how='left').select( ['als_user_id', 'recommendations', 'user_id']) # Join推荐结果与 refection_article映射关系表 # +-----------+-------+----------------+ # | als_user_id | user_id | als_article_id | # +-----------+-------+----------------+ # | 8 | 2 | [163, 0.91328144] | # | 8 | 2 | [132, 0.91328144] | import pyspark.sql.functions as F recall_res = recall_res.withColumn('als_article_id', F.explode('recommendations')).drop('recommendations') # +-----------+-------+--------------+ # | als_user_id | user_id | als_article_id | # +-----------+-------+--------------+ # | 8 | 2 | 163 | # | 8 | 2 | 132 | def _article_id(row): return row.als_user_id, row.user_id, row.als_article_id[0] als_recall = recall_res.rdd.map(_article_id).toDF(['als_user_id', 'user_id', 'als_article_id']) als_recall = als_recall.join(refection_article, on=['als_article_id'], how='left').select( ['user_id', 'article_id']) # 得到每个用户ID 对应推荐文章 # +-------------------+----------+ # | user_id | article_id | # +-------------------+----------+ # | 1106476833370537984 | 44075 | # | 1 | 44075 | # 分组统计每个用户，推荐列表 # als_recall = als_recall.groupby('user_id').agg(F.collect_list('article_id')).withColumnRenamed( # 'collect_list(article_id)', 'article_list') self.spark.sql(&quot;use toutiao&quot;) news_article_basic = self.spark.sql(&quot;select article_id, channel_id from news_article_basic&quot;) als_recall = als_recall.join(news_article_basic, on=['article_id'], how='left') als_recall = als_recall.groupBy(['user_id', 'channel_id']).agg(F.collect_list('article_id')).withColumnRenamed( 'collect_list(article_id)', 'article_list') als_recall = als_recall.dropna() # 存储 def save_offline_recall_hbase(partition): &quot;&quot;&quot;离线模型召回结果存储 &quot;&quot;&quot; import happybase pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090) for row in partition: with pool.connection() as conn: # 获取历史看过的该频道文章 history_table = conn.table('history_recall') # 多个版本 data = history_table.cells('reco:his:{}'.format(row.user_id).encode(), 'channel:{}'.format(row.channel_id).encode()) history = [] if len(data) &gt;= 2: for l in data[:-1]: history.extend(eval(l)) else: history = [] # 过滤reco_article与history reco_res = list(set(row.article_list) - set(history)) if reco_res: table = conn.table('cb_recall') # 默认放在推荐频道 table.put('recall:user:{}'.format(row.user_id).encode(), {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()}) conn.close() # 放入历史推荐过文章 history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(), {'channel:{}'.format(row.channel_id): str(reco_res).encode()}) conn.close() als_recall.foreachPartition(save_offline_recall_hbase) def update_content_recall(self): &quot;&quot;&quot; 更新基于内容（画像）的推荐召回集, word2vec相似 :return: &quot;&quot;&quot; # 基于内容相似召回（画像召回） ur.spark.sql(&quot;use profile&quot;) user_article_basic = self.spark.sql(&quot;select * from user_article_basic&quot;) user_article_basic = user_article_basic.filter(&quot;clicked=True&quot;) def save_content_filter_history_to__recall(partition): &quot;&quot;&quot;计算每个用户的每个操作文章的相似文章，过滤之后，写入content召回表当中（支持不同时间戳版本） &quot;&quot;&quot; import happybase pool = happybase.ConnectionPool(size=10, host='hadoop-master') # 进行为相似文章获取 with pool.connection() as conn: # key: article_id, column: similar:article_id similar_table = conn.table('article_similar') # 循环partition for row in partition: # 获取相似文章结果表 similar_article = similar_table.row(str(row.article_id).encode(), columns=[b'similar']) # 相似文章相似度排序过滤，召回不需要太大的数据， 百个，千 _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True) if _srt: # 每次行为推荐10篇文章 reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10] # 获取历史看过的该频道文章 history_table = conn.table('history_recall') # 多个版本 data = history_table.cells('reco:his:{}'.format(row.user_id).encode(), 'channel:{}'.format(row.channel_id).encode()) history = [] if len(_history_data) &gt; 1: for l in _history_data: history.extend(l) # 过滤reco_article与history reco_res = list(set(reco_article) - set(history)) # 进行推荐，放入基于内容的召回表当中以及历史看过的文章表当中 if reco_res: # content_table = conn.table('cb_content_recall') content_table = conn.table('cb_recall') content_table.put(&quot;recall:user:{}&quot;.format(row.user_id).encode(), {'content:{}'.format(row.channel_id).encode(): str(reco_res).encode()}) # 放入历史推荐过文章 history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(), {'channel:{}'.format(row.channel_id).encode(): str(reco_res).encode()}) conn.close() user_article_basic.foreachPartition(save_content_filter_history_to__recall) if __name__ == '__main__': ur = UpdateRecall(500) ur.update_als_recall() ur.update_content_recall() 定时更新代码，在main.py和update.py中添加以下代码： from offline.update_recall import UpdateRecall from schedule.update_profile import update_user_profile, update_article_profile, update_recall def update_recall(): &quot;&quot;&quot; 更新用户的召回集 :return: &quot;&quot;&quot; udp = UpdateRecall(200) udp.update_als_recall() udp.update_content_recall() main中添加 scheduler.add_job(update_recall, trigger='interval', hour=3) 排序设计 排序模型 宽模型 + 特征⼯程 LR/MLR + 非ID类特征(⼈⼯离散/GBDT/FM) spark 中可以直接使用 宽模型 + 深模型 wide&amp;deep,DeepFM 使用TensorFlow进行训练 深模型： DNN + 特征embedding 使用TensorFlow进行训练 特征处理原则 离散数据 one-hot编码 连续数据 归一化 图片/文本 文章标签/关键词提取 embedding 优化训练方式 使用Batch SGD优化 加入正则化防止过拟合 spark LR 进行预估 目的：通过LR模型进行CTR预估 步骤： 1、需要通过spark读取HIVE外部表，需要新的sparksession配置 增加HBASE配置 2、读取用户点击行为表，与用户画像和文章画像，构造训练样本 3、LR模型进行训练 4、LR模型预测、结果评估 创建环境 import os import sys # 如果当前代码文件运行测试需要加入修改路径，避免出现后导包问题 BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd())) sys.path.insert(0, os.path.join(BASE_DIR)) PYSPARK_PYTHON = &quot;/miniconda2/envs/reco_sys/bin/python&quot; # 当存在多个版本时，不指定很可能会导致出错 os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHON os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHON from pyspark.ml.feature import OneHotEncoder from pyspark.ml.feature import StringIndexer from pyspark.ml import Pipeline from pyspark.sql.types import * from pyspark.ml.feature import VectorAssembler from pyspark.ml.classification import LogisticRegression from pyspark.ml.classification import LogisticRegressionModel from offline import SparkSessionBase class CtrLogisticRegression(SparkSessionBase): SPARK_APP_NAME = &quot;ctrLogisticRegression&quot; ENABLE_HIVE_SUPPORT = True def __init__(self): self.spark = self._create_spark_hbase() ctr = CtrLogisticRegression() 这里注意的是_create_spark_hbase，我们后面需要通过spark读取HIVE外部表，需要新的配置 def _create_spark_hbase(self): conf = SparkConf() # 创建spark config对象 config = ( (&quot;spark.app.name&quot;, self.SPARK_APP_NAME), # 设置启动的spark的app名称，没有提供，将随机产生一个名称 (&quot;spark.executor.memory&quot;, self.SPARK_EXECUTOR_MEMORY), # 设置该app启动时占用的内存用量，默认2g (&quot;spark.master&quot;, self.SPARK_URL), # spark master的地址 (&quot;spark.executor.cores&quot;, self.SPARK_EXECUTOR_CORES), # 设置spark executor使用的CPU核心数，默认是1核心 (&quot;spark.executor.instances&quot;, self.SPARK_EXECUTOR_INSTANCES), (&quot;hbase.zookeeper.quorum&quot;, &quot;192.168.19.137&quot;), (&quot;hbase.zookeeper.property.clientPort&quot;, &quot;22181&quot;) ) conf.setAll(config) # 利用config对象，创建spark session if self.ENABLE_HIVE_SUPPORT: return SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate() else: return SparkSession.builder.config(conf=conf).getOrCreate() 读取用户点击行为表，与用户画像和文章画像，构造训练样本 目标值：clicked 特征值： 用户画像关键词权重：权重值排序TOPK，这里取10个 文章频道号：channel_id, ID类型通常要做one_hot编码，变成25维度(25个频道) 这里由于我们的历史点击日志测试时候是只有18号频道，所以不进行转换 文章向量：articlevector 进行行为日志数据读取 ctr.spark.sql(&quot;use profile&quot;) # +-------------------+----------+----------+-------+ # | user_id|article_id|channel_id|clicked| # +-------------------+----------+----------+-------+ # |1105045287866466304| 14225| 0| false| user_article_basic = ctr.spark.sql(&quot;select * from user_article_basic&quot;).select( ['user_id', 'article_id', 'channel_id', 'clicked']) 用户画像读取处理与日志数据合并 user_profile_hbase = ctr.spark.sql( &quot;select user_id, information.birthday, information.gender, article_partial, env from user_profile_hbase&quot;) user_profile_hbase = user_profile_hbase.drop('env') # +--------------------+--------+------+--------------------+ # | user_id|birthday|gender| article_partial| # +--------------------+--------+------+--------------------+ # | user:1| 0.0| null|Map(18:Animal -&gt; ...| _schema = StructType([ StructField(&quot;user_id&quot;, LongType()), StructField(&quot;birthday&quot;, DoubleType()), StructField(&quot;gender&quot;, BooleanType()), StructField(&quot;weights&quot;, MapType(StringType(), DoubleType())) ]) def get_user_id(row): return int(row.user_id.split(&quot;:&quot;)[1]), row.birthday, row.gender, row.article_partial 读取用户画像HIVE的外部表，构造样本: user_profile_hbase_temp = user_profile_hbase.rdd.map(get_user_id) user_profile_hbase_schema = ctr.spark.createDataFrame(user_profile_hbase_temp, schema=_schema) train = user_article_basic.join(user_profile_hbase_schema, on=['user_id'], how='left').drop('channel_id') 文章频道与向量读取合并，删除无用的特征 # +-------------------+----------+-------+--------+------+--------------------+ # | user_id|article_id|clicked|birthday|gender| weights| # +-------------------+----------+-------+--------+------+--------------------+ # |1106473203766657024| 13778| false| 0.0| null|Map(18:text -&gt; 0....| ctr.spark.sql(&quot;use article&quot;) article_vector = ctr.spark.sql(&quot;select * from article_vector&quot;) train = train.join(article_vector, on=['article_id'], how='left').drop('birthday').drop('gender') # +-------------------+-------------------+-------+--------------------+----------+--------------------+ # | article_id| user_id|clicked| weights|channel_id| articlevector| # +-------------------+-------------------+-------+--------------------+----------+--------------------+ # | 13401| 10| false|Map(18:tp2 -&gt; 0.2...| 18|[0.06157120217893...| 结果格式(删除了gender和birthday)： article_id use_id channel_id articlevector weight clicked -------------- article_id: n user_id: n channel_id: n articlevector: [] weight: Map(channel_id: keyword -&gt; weight) clicked: bool 合并文章画像的权重特征 ctr.spark.sql(&quot;use article&quot;) article_profile = ctr.spark.sql(&quot;select * from article_profile&quot;) def article_profile_to_feature(row): try: weights = sorted(row.keywords.values())[:10] except Exception as e: weights = [0.0] * 10 return row.article_id, weights article_profile = article_profile.rdd.map(article_profile_to_feature).toDF(['article_id', 'article_weights']) article_profile.show() train = train.join(article_profile, on=['article_id'], how='left') 结果格式： article_id use_id channel_id articlevector weight article_weight clicked -------------- article_id: n user_id: n channel_id: n articlevector: [] weight: Map(channel_id: keyword -&gt; weight) article_weight: [] clicked: bool 进行用户的权重特征筛选处理，类型处理 用户权重排序筛选，缺失值: 获取用户对应文章频道号的关键词权重 若无：生成默认值 columns = ['article_id', 'user_id', 'channel_id', 'articlevector', 'user_weights', 'article_weights', 'clicked'] def get_user_weights(row): from pyspark.ml.linalg import Vectors try: user_weights = sorted([row.article_partial[key] for key in row.article_partial.keys() if key.split(':')[0] == str(row.channel_id)])[ :10] except Exception: user_weights = [0.0] * 10 return row.article_id, row.user_id, row.channel_id, Vectors.dense(row.articlevector), Vectors.dense( user_weights), Vectors.dense(row.article_weights), int(row.clicked) train_vector = train_user_article.rdd.map(get_user_weights).toDF(columns) 结果格式：Note: Vectors.dense要根据列的数据来指定字段 article_id use_id channel_id articlevector weight article_weight clicked -------------- article_id: n user_id: n channel_id: n articlevector: [] weight: [] # 和上面对比，这里从Map变成了 [] article_weight: [] clicked: bool LR点击率预测 输入模型的特征格式指定，通过VectorAssembler()收集 只要channel_id，articlevector，weights，article_weights作为输入特征 train_version_two = VectorAssembler().setInputCols(cols[2:6]).setOutputCol(&quot;features&quot;).transform(train) 把输入特征平铺，再输入到LR中 合并特征向量(channel_id1个+用户特征权重10个+文章向量100个+文章关键词权重10个) = 121个特征 按理说，把channel做one-hot会更多： 121-1+25=145 lr = LogisticRegression() model = lr.setLabelCol(&quot;clicked&quot;).setFeaturesCol(&quot;features&quot;).fit(train_version_two) model.save(&quot;hdfs://hadoop-master:9000/headlines/models/logistic_ctr_model.obj&quot;) 使用model模型加载预估 online_model = LogisticRegressionModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/logistic_ctr_model.obj&quot;) res_transfrom = online_model.transform(train_version_two) res_transfrom.select([&quot;clicked&quot;, &quot;probability&quot;, &quot;prediction&quot;]).show() probability结果中有对某个文章点击(1为目标)的概率，和不点击（0为目标）的概率，是个列表 eg: probability = [不点击的概率， 点击的概率] def vector_to_double(row): return float(row.clicked), float(row.probability[1]) score_label = res_transfrom.select([&quot;clicked&quot;, &quot;probability&quot;]).rdd.map(vector_to_double) 模型评估-Accuracy与AUC ROC 曲线图 画出ROC图,使用训练的时候的模型model中会有 import matplotlib.pyplot as plt plt.figure(figsize=(5,5)) plt.plot([0, 1], [0, 1], 'r--') plt.plot(model.summary.roc.select('FPR').collect(), model.summary.roc.select('TPR').collect()) plt.xlabel('FPR') plt.ylabel('TPR') plt.show() 计算AUC值 方式1：spark内置方法 from pyspark.mllib.evaluation import BinaryClassificationMetrics # score_label包括[&quot;clicked&quot;, &quot;probability&quot;] metrics = BinaryClassificationMetrics(score_label) metrics.areaUnderROC 0.7364334522585716 方式2：sklearn方法 from sklearn.metrics import roc_auc_score, accuracy_score import numpy as np arr = np.array(score_label.collect()) #评估AUC与准确率 accuracy_score(arr[:, 0], arr[:, 1].round()) 0.9051438053097345 roc_auc_score(arr[:, 0], arr[:, 1]) 0.719274521004087 离线ctr特征中心创建 特征服务中心可以作为离线计算用户与文章的高级特征。 不仅仅提供给离线使用。还可以作为实时的特征供其他场景读取进行 用户，文章能用到的特征都进行处理进行存储，便于实时推荐进行读取 存储到HBASE 创建User特征Hbase表： create 'ctr_feature_user', 'channel' 4 column=channel:13, timestamp=1555647172980, value=[] 4 column=channel:14, timestamp=1555647172980, value=[] 4 column=channel:15, timestamp=1555647172980, value=[] 4 column=channel:16, timestamp=1555647172980, value=[] 4 column=channel:18, timestamp=1555647172980, value=[0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073] 4 column=channel:19, timestamp=1555647172980, value=[] 4 column=channel:20, timestamp=1555647172980, value=[] 4 column=channel:2, timestamp=1555647172980, value=[] 4 column=channel:21, timestamp=1555647172980, value=[] 创建Article特征Hbase表 create 'ctr_feature_article', 'article' COLUMN CELL article:13401 timestamp=1555635749357, value=[18.0,0.08196639249252607,0.11217275332895373,0.1353835167902181,0.16086650318453152,0.16356418791892943,0.16740082750337945,0.18091837445730974,0.1907214431716628,0.2........................-0.04634634410271921,-0.06451843378804649,-0.021564142420785692,0.10212902152136256] 使用Hive关联特种中心（基本不需要） 关联用户 create external table ctr_feature_user_hbase( user_id STRING comment &quot;user_id&quot;, user_channel map comment &quot;user_channel&quot;) COMMENT &quot;ctr table&quot; STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,channel:&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;ctr_feature_user&quot;); 关联文章 create external table ctr_feature_article_hbase( article_id STRING comment &quot;article_id&quot;, article_feature map comment &quot;article&quot;) COMMENT &quot;ctr table&quot; STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,article:&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;ctr_feature_article&quot;); 用户特征中心更新 目的：计算用户特征更新到HBASE 步骤： 获取特征进行用户画像权重过滤 特征批量存储 获取特征进行用户画像权重过滤 # 构造样本 ctr.spark.sql(&quot;use profile&quot;) user_profile_hbase = ctr.spark.sql( &quot;select user_id, information.birthday, information.gender, article_partial, env from user_profile_hbase&quot;) # 特征工程处理 # 抛弃获取值少的特征 user_profile_hbase = user_profile_hbase.drop('env', 'birthday', 'gender') def get_user_id(row): return int(row.user_id.split(&quot;:&quot;)[1]), row.article_partial user_profile_hbase_temp = user_profile_hbase.rdd.map(get_user_id) from pyspark.sql.types import * _schema = StructType([ StructField(&quot;user_id&quot;, LongType()), StructField(&quot;weights&quot;, MapType(StringType(), DoubleType())) ]) user_profile_hbase_schema = ctr.spark.createDataFrame(user_profile_hbase_temp, schema=_schema) def frature_preprocess(row): from pyspark.ml.linalg import Vectors channel_weights = [] for i in range(1, 26): try: _res = sorted([row.weights[key] for key in row.weights.keys() if key.split(':')[0] == str(i)])[:10] channel_weights.append(_res) except: channel_weights.append([0.0] * 10) return row.user_id, channel_weights res = user_profile_hbase_schema.rdd.map(frature_preprocess).collect() 结果格式： [ # weight 代表用户的标签权重 ( user_id, [ [weight], [], [], ..., 25个[]，代表25个channel ] ] 特征批量存储，保存用户每个频道的特征 import happybase # 批量插入Hbase数据库中 pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090) with pool.connection() as conn: ctr_feature = conn.table('ctr_feature_user') with ctr_feature.batch(transaction=True) as b: for i in range(len(res)): for j in range(25): b.put(&quot;{}&quot;.format(res[i][0]).encode(),{&quot;channel:{}&quot;.format(j+1).encode(): str(res[i][1][j]).encode()}) conn.close() 文章特征中心更新 文章特征： 关键词权重 文章的频道 文章向量结果 存储这些特征以便于后面实时排序时候快速使用特征 步骤： 读取相关文章画像 进行文章相关特征处理和提取 合并文章所有特征作为模型训练或者预测的初始特征 文章特征存储到HBASE 读取相关文章画像 ctr.spark.sql(&quot;use article&quot;) article_profile = ctr.spark.sql(&quot;select * from article_profile&quot;) 进行文章相关特征处理和提取 def article_profile_to_feature(row): try: weights = sorted(row.keywords.values())[:10] except Exception as e: weights = [0.0] * 10 return row.article_id, row.channel_id, weights article_profile = article_profile.rdd.map(article_profile_to_feature).toDF(['article_id', 'channel_id', 'weights']) article_profile.show() 格式如下： article_id channel_id weights [] 再把文章向量join进来 article_vector = ctr.spark.sql(&quot;select * from article_vector&quot;) article_feature = article_profile.join(article_vector, on=['article_id'], how='inner') def feature_to_vector(row): from pyspark.ml.linalg import Vectors return row.article_id, row.channel_id, Vectors.dense(row.weights), Vectors.dense(row.articlevector) article_feature = article_feature.rdd.map(feature_to_vector).toDF(['article_id', 'channel_id', 'weights', 'articlevector']) 指定上面特征进行合并（atricle_id,channel_id, weights, articlevector） # 保存特征数据 cols2 = ['article_id', 'channel_id', 'weights', 'articlevector'] # 做特征的指定指定合并 article_feature_two = VectorAssembler().setInputCols(cols2[1:4]).setOutputCol(&quot;features&quot;).transform(article_feature) 最终结果格式（可给LR使用）： article_id channel_id weights articlevector features # 格式样式说明 article_id: n channel_id: n weights: [] articlevector: [] features: [] # features就是我们最终合并成一个list的结果，它和 channel_id 用于给 lr 使用 保存到特征数据库HBASE中: # 保存到特征数据库中 def save_article_feature_to_hbase(partition): import happybase pool = happybase.ConnectionPool(size=10, host='hadoop-master') with pool.connection() as conn: table = conn.table('ctr_feature_article') for row in partition: table.put('{}'.format(row.article_id).encode(), {'article:{}'.format(row.article_id).encode(): str(row.features).encode()}) article_feature_two.foreachPartition(save_article_feature_to_hbase) 离线特征中心定时更新 添加update.py更新程序 def update_ctr_feature(): &quot;&quot;&quot; 定时更新用户、文章特征 :return: &quot;&quot;&quot; fp = FeaturePlatform() fp.update_user_ctr_feature_to_hbase() fp.update_article_ctr_feature_to_hbase() 添加apscheduler定时运行 # 添加定时更新用户文章特征结果的程序，每个4小时更新一次 scheduler.add_job(update_ctr_feature, trigger='interval', hours=4) 完整代码： import os import sys # 如果当前代码文件运行测试需要加入修改路径，否则后面的导包出现问题 BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) sys.path.insert(0, os.path.join(BASE_DIR)) from offline import SparkSessionBase # from offline.utils import textrank, segmentation import happybase import pyspark.sql.functions as F from datetime import datetime from datetime import timedelta import time import gc class FeaturePlatform(SparkSessionBase): &quot;&quot;&quot;特征更新平台 &quot;&quot;&quot; SPARK_APP_NAME = &quot;featureCenter&quot; ENABLE_HIVE_SUPPORT = True def __init__(self): # _create_spark_session # _create_spark_hbase用户spark sql 操作hive对hbase的外部表 self.spark = self._create_spark_hbase() def update_user_ctr_feature_to_hbase(self): &quot;&quot;&quot; :return: &quot;&quot;&quot; clr.spark.sql(&quot;use profile&quot;) user_profile_hbase = self.spark.sql( &quot;select user_id, information.birthday, information.gender, article_partial, env from user_profile_hbase&quot;) # 特征工程处理 # 抛弃获取值少的特征 user_profile_hbase = user_profile_hbase.drop('env', 'birthday', 'gender') def get_user_id(row): return int(row.user_id.split(&quot;:&quot;)[1]), row.article_partial user_profile_hbase_temp = user_profile_hbase.rdd.map(get_user_id) from pyspark.sql.types import * _schema = StructType([ StructField(&quot;user_id&quot;, LongType()), StructField(&quot;weights&quot;, MapType(StringType(), DoubleType())) ]) user_profile_hbase_schema = self.spark.createDataFrame(user_profile_hbase_temp, schema=_schema) def frature_preprocess(row): from pyspark.ml.linalg import Vectors channel_weights = [] for i in range(1, 26): try: _res = sorted([row.weights[key] for key in row.weights.keys() if key.split(':')[0] == str(i)])[:10] channel_weights.append(_res) except: channel_weights.append([]) return row.user_id, channel_weights res = user_profile_hbase_schema.rdd.map(frature_preprocess).collect() # 批量插入Hbase数据库中 pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090) with pool.connection() as conn: ctr_feature = conn.table('ctr_feature_user') with ctr_feature.batch(transaction=True) as b: for i in range(len(res)): for j in range(25): b.put(&quot;{}&quot;.format(res[i][0]).encode(), {&quot;channel:{}&quot;.format(j + 1).encode(): str(res[i][1][j]).encode()}) conn.close() def update_article_ctr_feature_to_hbase(self): &quot;&quot;&quot; :return: &quot;&quot;&quot; # 文章特征中心 self.spark.sql(&quot;use article&quot;) article_profile = self.spark.sql(&quot;select * from article_profile&quot;) def article_profile_to_feature(row): try: weights = sorted(row.keywords.values())[:10] except Exception as e: weights = [0.0] * 10 return row.article_id, row.channel_id, weights article_profile = article_profile.rdd.map(article_profile_to_feature).toDF( ['article_id', 'channel_id', 'weights']) article_vector = self.spark.sql(&quot;select * from article_vector&quot;) article_feature = article_profile.join(article_vector, on=['article_id'], how='inner') def feature_to_vector(row): from pyspark.ml.linalg import Vectors return row.article_id, row.channel_id, Vectors.dense(row.weights), Vectors.dense(row.articlevector) article_feature = article_feature.rdd.map(feature_to_vector).toDF( ['article_id', 'channel_id', 'weights', 'articlevector']) # 保存特征数据 cols2 = ['article_id', 'channel_id', 'weights', 'articlevector'] # 做特征的指定指定合并 article_feature_two = VectorAssembler().setInputCols(cols2[1:4]).setOutputCol(&quot;features&quot;).transform( article_feature) # 保存到特征数据库中 def save_article_feature_to_hbase(partition): import happybase pool = happybase.ConnectionPool(size=10, host='hadoop-master') with pool.connection() as conn: table = conn.table('ctr_feature_article') for row in partition: table.put('{}'.format(row.article_id).encode(), {'article:{}'.format(row.article_id).encode(): str(row.features).encode()}) article_feature_two.foreachPartition(save_article_feature_to_hbase) 离线总结 至此，离线部分已全部完成，包括如下内容 文章画像 文章信息拼接 文章分词 对分词结果TF-IDF 对分词结果TextRank （作为关键词，带权重） 将TF-IDF和TextRank结果交集，（作为 主题词） 对分词结果Word2Vec得到词向量 把文章的所有词向量的均值，（作为文章向量） LSH计算相似文章，得到所有文章相似文章向量（为下面内容召回打下基础） 用户画像 对用户行为表的埋点日志的Json格式转到Hive的Map 对用户行为的map格式 explode爆炸处理 对用户与文章画像合并处理 根据用户对文章的喜好程度（行为权重打分）把文章画像的标签取TopK打在用户身上 最终得到用户的每个uid, channel的特征标签+权重 用户召回 模型ALS（模型结果自动推荐TopK个，及权重） 内容召回（文章相似的文章的TopK个） 召回的Item推荐用户 根据用户/文章画像抽取各自的特征组成特征中心平台 用户画像抽取用户特征，使用VectorAssembler拼进一个列表中，供后续实时排序等使用。 文章画像抽取文章特征，使用VectorAssembler拼进一个列表中，供后续实时排序等使用。 ","link":"https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-san-chi-xian-zhao-hui-yu-pai-xu/"},{"title":"PY => HBase","content":"报错 若list 或其他命令 有如下错误： ERROR: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing 则使用如下命令： cd $HBASE_HOME/bin ./hbase zkcli ls / rmr /hbase ls / 退出 zookeeper cli， 删除hdfs中的 /hbase hdfs dfs -rm -r /hbase 然后重启hbase: cd $HBASE_HOME/bin ./stop-hbase.sh ./start-hbase.sh 若stop hbase的时候出现 ..... 停止不掉， 则： cd $HBASE_HOME/bin ./hbase-daemons.sh stop regionserver # ./start-hbase.sh # kill -9 pid来终止hbase的进程 HBase命令 https://blog.csdn.net/vbirdbest/article/details/88236575 ","link":"https://cythonlin.github.io/post/py-greater-hbase/"},{"title":"RS => 推荐系统（二）离线画像构建 ","content":"文章离线画像构建 Spark配置基类抽取 from pyspark import SparkConf from pyspark.sql import SparkSession import os class SparkSessionBase(object): SPARK_APP_NAME = None SPARK_URL = &quot;yarn&quot; SPARK_EXECUTOR_MEMORY = &quot;2g&quot; SPARK_EXECUTOR_CORES = 2 SPARK_EXECUTOR_INSTANCES = 2 ENABLE_HIVE_SUPPORT = False def _create_spark_session(self): conf = SparkConf() # 创建spark config对象 config = ( (&quot;spark.app.name&quot;, self.SPARK_APP_NAME), # 设置启动的spark的app名称，没有提供，将随机产生一个名称 (&quot;spark.executor.memory&quot;, self.SPARK_EXECUTOR_MEMORY), # 设置该app启动时占用的内存用量，默认2g (&quot;spark.master&quot;, self.SPARK_URL), # spark master的地址 (&quot;spark.executor.cores&quot;, self.SPARK_EXECUTOR_CORES), # 设置spark executor使用的CPU核心数，默认是1核心 (&quot;spark.executor.instances&quot;, self.SPARK_EXECUTOR_INSTANCES), (&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;), ) conf.setAll(config) # 利用config对象，创建spark session if self.ENABLE_HIVE_SUPPORT: return SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate() else: return SparkSession.builder.config(conf=conf).getOrCreate() 主应用导入基类 # pip install pyspark # pip install findspark import findspark findspark.init() import os import sys # 如果当前代码文件运行测试需要加入修改路径，避免出现后导包问题 BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd())) sys.path.insert(0, os.path.join(BASE_DIR)) print(BASE_DIR) PYSPARK_PYTHON = &quot;/miniconda2/envs/reco_sys/bin/python&quot; # 当存在多个版本时，不指定很可能会导致出错 os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHON os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHON from offline import SparkSessionBase class OriginArticleData(SparkSessionBase): SPARK_APP_NAME = &quot;mergeArticle&quot; SPARK_URL = &quot;yarn&quot; ENABLE_HIVE_SUPPORT = True def __init__(self): self.spark = self._create_spark_session() oa = OriginArticleData() # oa就是带有配置的 sparkSession的实例化对象 文章表合并 文章基本信息表+文章内容表+频道表： titlce_content = oa.spark.sql(&quot;select a.article_id, a.channel_id, a.title, b.content from news_article_basic a inner join news_article_content b on a.article_id=b.article_id) 因为得到的是 DF类型，想要用SQL，可以把DF注册为临时表 titlce_content.registerTempTable('temptable') 再把 频道表 的 频道名 合并进来 channel_title_content = oa.spark.sql(&quot;select t.*, n.channel_name from temptable t left join news_channel n on t.channel_id=n.channel_id&quot;) 文章 字段 合并 将 文章标题+文章内容+文章频道 的列，拼接成一个大字符串 import pyspark.sql.functions as F import gc # 增加channel的名字，后面会使用 basic_content.registerTempTable(&quot;temparticle&quot;) channel_basic_content = oa.spark.sql( &quot;select t.*, n.channel_name from temparticle t left join news_channel n on t.channel_id=n.channel_id&quot; ) # 利用concat_ws方法，将多列数据合并为一个长文本内容（频道，标题以及内容合并） oa.spark.sql(&quot;use article&quot;) sentence_df = channel_basic_content.select(&quot;article_id&quot;, &quot;channel_id&quot;, &quot;channel_name&quot;, &quot;title&quot;, &quot;content&quot;, \\ F.concat_ws( &quot;,&quot;, # 指定大字符串分隔符 channel_basic_content.channel_name, channel_basic_content.title, channel_basic_content.content ).alias(&quot;sentence&quot;) # 新列 大字符串 取名 ) del basic_content del channel_basic_content gc.collect() # sentence_df.write.insertInto(&quot;article_data&quot;) # 写入提前创建好的Hive表中 分词 def segmentation(partition): # 就这一行的缩进需要调整下 import os import re import jieba import jieba.analyse import jieba.posseg as pseg import codecs abspath = &quot;/root/words&quot; # 结巴加载用户词典 userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;) jieba.load_userdict(userDict_path) # 停用词文本 stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;) def get_stopwords_list(): &quot;&quot;&quot;返回stopwords列表&quot;&quot;&quot; stopwords_list = [i.strip() for i in codecs.open(stopwords_path).readlines()] return stopwords_list # 所有的停用词列表 stopwords_list = get_stopwords_list() # 分词 def cut_sentence(sentence): &quot;&quot;&quot;对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词&quot;&quot;&quot; # print(sentence,&quot;*&quot;*100) # eg:[pair('今天', 't'), pair('有', 'd'), pair('雾', 'n'), pair('霾', 'g')] seg_list = pseg.lcut(sentence) seg_list = [i for i in seg_list if i.flag not in stopwords_list] filtered_words_list = [] for seg in seg_list: # print(seg) if len(seg.word) &lt;= 1: continue elif seg.flag == &quot;eng&quot;: if len(seg.word) &lt;= 2: continue else: filtered_words_list.append(seg.word) elif seg.flag.startswith(&quot;n&quot;): filtered_words_list.append(seg.word) elif seg.flag in [&quot;x&quot;, &quot;eng&quot;]: # 是自定一个词语或者是英文单词 filtered_words_list.append(seg.word) return filtered_words_list for row in partition: sentence = re.sub(&quot;&lt;.*?&gt;&quot;, &quot;&quot;, row.sentence) # 替换掉标签数据 words = cut_sentence(sentence) yield row.article_id, row.channel_id, words 计算 TF-IDF TF: ktt.spark.sql(&quot;use article&quot;) article_dataframe = ktt.spark.sql(&quot;select * from article_data limit 20&quot;) words_df = article_dataframe.rdd.mapPartitions(segmentation).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;words&quot;]) from pyspark.ml.feature import CountVectorizer # 总词汇的大小，文本中必须出现的次数 cv = CountVectorizer(inputCol=&quot;words&quot;, outputCol=&quot;countFeatures&quot;, vocabSize=200*10000, minDF=1.0) # 训练词频统计模型 cv_model = cv.fit(words_df) cv_model.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;) # cv_model.vocabulary 查看统计词表（相当于groupby结果的 key, 但不包括value） 训练TF-IDF: # 词语与词频统计 from pyspark.ml.feature import CountVectorizerModel cv_model = CountVectorizerModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;) # 得出词频向量结果 cv_result = cv_model.transform(words_df) # 训练IDF模型 (把 tf结果传进去，其实说是 IDF模型，计算结果得出的就是 TF-IDF) from pyspark.ml.feature import IDF idf = IDF(inputCol=&quot;countFeatures&quot;, outputCol=&quot;idfFeatures&quot;) idfModel = idf.fit(cv_result) idfModel.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/IDF.model&quot;) # idfModel.idf.toArray()[:20] 查看逆文档频率矩阵 TF-IDF结果数据格式： 列1， 列...， 列 TF-IDF (1000,[804,1032],[6.349777077,7.0761797]) 。。。 使用TF-IDF模型，取Top-K个词： from pyspark.ml.feature import CountVectorizerModel cv_model = CountVectorizerModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model&quot;) from pyspark.ml.feature import IDFModel idf_model = IDFModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model&quot;) cv_result = cv_model.transform(words_df) tfidf_result = idf_model.transform(cv_result) def func(partition): TOPK = 20 for row in partition: # 找到索引与IDF值并进行排序 _ = list(zip(row.idfFeatures.indices, row.idfFeatures.values)) # [ (indexes,values)， (indexes,values)] _ = sorted(_, key=lambda x: x[1], reverse=True) result = _[:TOPK] for word_index, tfidf in result: yield row.article_id, row.channel_id, int(word_index), round(float(tfidf), 4) # yield这句注定了返回结果格式 (多层for循环 yield， 原本一行数据按每个单词爆炸展开) # article_id, channel_id, word_index, tfidf # 1 100 40 15.5 # 1 100 14 10.3 # 1 100 23 13.2 _keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;index&quot;, &quot;tfidf&quot;]) 我们的目标是： 构成 词+TFIDF值, 而不是索引+TFIDF cv_model.vocabulary 结果是所有单词的列表。 上面的 index就是对应这个列表的索引 最终构建一个词典+索引表： index word .. .. 然后将 主表（文章id,频道id,索引，tfidf）与 词典表（index+word） 合并 得到 （文章id,频道id, 词， tfidf） 计算 TextRank TextRank和核心就是设定一个固定窗口来滑动 把每个窗口内的每个词， 设为字典的Key, value就是他附近的n个词的列表 然后每个词都这样做， 遇到相同的词就追加到字典的value 列表中 # 分词 def textrank(partition): import os import jieba import jieba.analyse import jieba.posseg as pseg import codecs abspath = &quot;/root/words&quot; # 结巴加载用户词典 userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;) jieba.load_userdict(userDict_path) # 停用词文本 stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;) def get_stopwords_list(): &quot;&quot;&quot;返回stopwords列表&quot;&quot;&quot; stopwords_list = [i.strip() for i in codecs.open(stopwords_path).readlines()] return stopwords_list # 所有的停用词列表 stopwords_list = get_stopwords_list() class TextRank(jieba.analyse.TextRank): def __init__(self, window=20, word_min_len=2): super(TextRank, self).__init__() self.span = window # 窗口大小 self.word_min_len = word_min_len # 单词的最小长度 # 要保留的词性，根据jieba github ，具体参见https://github.com/baidu/lac self.pos_filt = frozenset( ('n', 'x', 'eng', 'f', 's', 't', 'nr', 'ns', 'nt', &quot;nw&quot;, &quot;nz&quot;, &quot;PER&quot;, &quot;LOC&quot;, &quot;ORG&quot;)) def pairfilter(self, wp): &quot;&quot;&quot;过滤条件，返回True或者False&quot;&quot;&quot; if wp.flag == &quot;eng&quot;: if len(wp.word) &lt;= 2: return False if wp.flag in self.pos_filt and len(wp.word.strip()) &gt;= self.word_min_len \\ and wp.word.lower() not in stopwords_list: return True # TextRank过滤窗口大小为5，单词最小为2 textrank_model = TextRank(window=5, word_min_len=2) allowPOS = ('n', &quot;x&quot;, 'eng', 'nr', 'ns', 'nt', &quot;nw&quot;, &quot;nz&quot;, &quot;c&quot;) for row in partition: tags = textrank_model.textrank(row.sentence, topK=20, withWeight=True, allowPOS=allowPOS, withFlag=False) for tag in tags: yield row.article_id, row.channel_id, tag[0], tag[1] # 计算textrank textrank_keywords_df = article_dataframe.rdd.mapPartitions(textrank).toDF( [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;textrank&quot;] ) # textrank_keywords_df.write.insertInto(&quot;textrank_keywords_values&quot;) textrank运行结果如下： hive&gt; select * from textrank_keywords_values limit 20; OK 文章ID channel word textrank 98319 17 var 20.6079 98323 17 var 7.4938 98326 17 var 104.9128 然后和 tfidf一样 根据 textrank值， 取TOP-K个词 计算 主题词 和 关键词 关键词：TEXTRANK计算出的结果TOPK个词以及权重 主题词：TEXTRANK的TOPK词 与 ITFDF计算的TOPK个词的交集 格式如下： hive&gt; desc article_profile; OK article_id int article_id channel_id int channel_id keywords map keywords topics array topics hive&gt; select * from article_profile limit 1; # 这里把结果按行排列开方便观看 article_id 26 channel_id 17 关键词字典 {&quot;策略&quot;:0.3973770571351729,&quot;jpg&quot;:0.9806348975390871,&quot;用户&quot;:1.2794959063944176,&quot;strong&quot;:1.6488457985625076,&quot;文件&quot;:0.28144603583387057,&quot;逻辑&quot;:0.45256526469610714,&quot;形式&quot;:0.4123994242601279,&quot;全自&quot;:0.9594604850547191,&quot;h2&quot;:0.6244481634710125,&quot;版本&quot;:0.44280276959510817,&quot;Adobe&quot;:0.8553618185108718,&quot;安装&quot;:0.8305037437573172,&quot;检查更新&quot;:1.8088946300014435,&quot;产品&quot;:0.774842382276899,&quot;下载页&quot;:1.4256311032544344,&quot;过程&quot;:0.19827163395829256,&quot;json&quot;:0.6423301791599972,&quot;方式&quot;:0.582762869780791,&quot;退出应用&quot;:1.2338671268242603,&quot;Setup&quot;:1.004399549339134} 主题词列表 [&quot;Electron&quot;,&quot;全自动&quot;,&quot;产品&quot;,&quot;版本号&quot;,&quot;安装包&quot;,&quot;检查更新&quot;,&quot;方案&quot;,&quot;版本&quot;,&quot;退出应用&quot;,&quot;逻辑&quot;,&quot;安装过程&quot;,&quot;方式&quot;,&quot;定性&quot;,&quot;新版本&quot;,&quot;Setup&quot;,&quot;静默&quot;,&quot;用户&quot;] 增量更新 离线文章画像 更新流程： 1、toutiao 数据库中，news_article_content 与news_article_basic—&gt;更新到article数据库中article_data表，方便操作 2. 第一次：所有更新，后面增量每天的数据更新26日：1：00~2：00，2：00~3：00，左闭右开,一个小时更新一次 3、刚才新更新的文章，通过已有的idf计算出tfidf值以及hive 的textrank_keywords_values 4、更新hive的article_profile 离线更新文章画像 代码组装：Pycharm 注意在Pycharm中运行要设置环境： PYTHONUNBUFFERED=1 JAVA_HOME=/root/bigdata/jdk SPARK_HOME=/root/bigdata/spark HADOOP_HOME=/root/bigdata/hadoop PYSPARK_PYTHON=/root/anaconda3/envs/reco_sys/bin/python PYSPARK_DRIVER_PYTHON=/root/anaconda3/envs/reco_sys/bin/python 具体代码如下： import os import sys BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) sys.path.insert(0, os.path.join(BASE_DIR)) from offline import SparkSessionBase from datetime import datetime from datetime import timedelta import pyspark.sql.functions as F import pyspark import gc class UpdateArticle(SparkSessionBase): &quot;&quot;&quot; 更新文章画像 &quot;&quot;&quot; SPARK_APP_NAME = &quot;updateArticle&quot; ENABLE_HIVE_SUPPORT = True SPARK_EXECUTOR_MEMORY = &quot;7g&quot; def __init__(self): self.spark = self._create_spark_session() self.cv_path = &quot;hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model&quot; self.idf_path = &quot;hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model&quot; def get_cv_model(self): # 词语与词频统计 from pyspark.ml.feature import CountVectorizerModel cv_model = CountVectorizerModel.load(self.cv_path) return cv_model def get_idf_model(self): from pyspark.ml.feature import IDFModel idf_model = IDFModel.load(self.idf_path) return idf_model @staticmethod def compute_keywords_tfidf_topk(words_df, cv_model, idf_model): &quot;&quot;&quot;保存tfidf值高的20个关键词 :param spark: :param words_df: :return: &quot;&quot;&quot; cv_result = cv_model.transform(words_df) tfidf_result = idf_model.transform(cv_result) # print(&quot;transform compelete&quot;) # 取TOP-N的TFIDF值高的结果 def func(partition): TOPK = 20 for row in partition: _ = list(zip(row.idfFeatures.indices, row.idfFeatures.values)) _ = sorted(_, key=lambda x: x[1], reverse=True) result = _[:TOPK] # words_index = [int(i[0]) for i in result] # yield row.article_id, row.channel_id, words_index for word_index, tfidf in result: yield row.article_id, row.channel_id, int(word_index), round(float(tfidf), 4) _keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;index&quot;, &quot;tfidf&quot;]) return _keywordsByTFIDF def merge_article_data(self): &quot;&quot;&quot; 合并业务中增量更新的文章数据 :return: &quot;&quot;&quot; # 获取文章相关数据, 指定过去一个小时整点到整点的更新数据 # 如：26日：1：00~2：00，2：00~3：00，左闭右开 self.spark.sql(&quot;use toutiao&quot;) _yester = datetime.today().replace(minute=0, second=0, microsecond=0) start = datetime.strftime(_yester + timedelta(days=0, hours=-1, minutes=0), &quot;%Y-%m-%d %H:%M:%S&quot;) end = datetime.strftime(_yester, &quot;%Y-%m-%d %H:%M:%S&quot;) # 合并后保留：article_id、channel_id、channel_name、title、content # +----------+----------+--------------------+--------------------+ # | article_id | channel_id | title | content | # +----------+----------+--------------------+--------------------+ # | 141462 | 3 | test - 20190316 - 115123 | 今天天气不错，心情很美丽！！！ | basic_content = self.spark.sql( &quot;select a.article_id, a.channel_id, a.title, b.content from news_article_basic a &quot; &quot;inner join news_article_content b on a.article_id=b.article_id where a.review_time &gt;= '{}' &quot; &quot;and a.review_time &lt; '{}' and a.status = 2&quot;.format(start, end)) # 增加channel的名字，后面会使用 basic_content.registerTempTable(&quot;temparticle&quot;) channel_basic_content = self.spark.sql( &quot;select t.*, n.channel_name from temparticle t left join news_channel n on t.channel_id=n.channel_id&quot;) # 利用concat_ws方法，将多列数据合并为一个长文本内容（频道，标题以及内容合并） self.spark.sql(&quot;use article&quot;) sentence_df = channel_basic_content.select(&quot;article_id&quot;, &quot;channel_id&quot;, &quot;channel_name&quot;, &quot;title&quot;, &quot;content&quot;, \\ F.concat_ws( &quot;,&quot;, channel_basic_content.channel_name, channel_basic_content.title, channel_basic_content.content ).alias(&quot;sentence&quot;) ) del basic_content del channel_basic_content gc.collect() sentence_df.write.insertInto(&quot;article_data&quot;) return sentence_df def generate_article_label(self, sentence_df): &quot;&quot;&quot; 生成文章标签 tfidf, textrank :param sentence_df: 增量的文章内容 :return: &quot;&quot;&quot; # 进行分词 words_df = sentence_df.rdd.mapPartitions(segmentation).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;words&quot;]) cv_model = self.get_cv_model() idf_model = self.get_idf_model() # 1、保存所有的词的idf的值，利用idf中的词的标签索引 # 工具与业务隔离 _keywordsByTFIDF = UpdateArticle.compute_keywords_tfidf_topk(words_df, cv_model, idf_model) keywordsIndex = self.spark.sql(&quot;select keyword, index idx from idf_keywords_values&quot;) keywordsByTFIDF = _keywordsByTFIDF.join(keywordsIndex, keywordsIndex.idx == _keywordsByTFIDF.index).select( [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;tfidf&quot;]) keywordsByTFIDF.write.insertInto(&quot;tfidf_keywords_values&quot;) del cv_model del idf_model del words_df del _keywordsByTFIDF gc.collect() # 计算textrank textrank_keywords_df = sentence_df.rdd.mapPartitions(textrank).toDF( [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;textrank&quot;]) textrank_keywords_df.write.insertInto(&quot;textrank_keywords_values&quot;) return textrank_keywords_df, keywordsIndex def get_article_profile(self, textrank, keywordsIndex): &quot;&quot;&quot; 文章画像主题词建立 :param idf: 所有词的idf值 :param textrank: 每个文章的textrank值 :return: 返回建立号增量文章画像 &quot;&quot;&quot; keywordsIndex = keywordsIndex.withColumnRenamed(&quot;keyword&quot;, &quot;keyword1&quot;) result = textrank.join(keywordsIndex, textrank.keyword == keywordsIndex.keyword1) # 1、关键词（词，权重） # 计算关键词权重 _articleKeywordsWeights = result.withColumn(&quot;weights&quot;, result.textrank * result.idf).select( [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;weights&quot;]) # 合并关键词权重到字典 _articleKeywordsWeights.registerTempTable(&quot;temptable&quot;) articleKeywordsWeights = self.spark.sql( &quot;select article_id, min(channel_id) channel_id, collect_list(keyword) keyword_list, collect_list(weights) weights_list from temptable group by article_id&quot;) def _func(row): return row.article_id, row.channel_id, dict(zip(row.keyword_list, row.weights_list)) articleKeywords = articleKeywordsWeights.rdd.map(_func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keywords&quot;]) # 2、主题词 # 将tfidf和textrank共现的词作为主题词 topic_sql = &quot;&quot;&quot; select t.article_id article_id2, collect_set(t.keyword) topics from tfidf_keywords_values t inner join textrank_keywords_values r where t.keyword=r.keyword group by article_id2 &quot;&quot;&quot; articleTopics = self.spark.sql(topic_sql) # 3、将主题词表和关键词表进行合并，插入表 articleProfile = articleKeywords.join(articleTopics, articleKeywords.article_id == articleTopics.article_id2).select( [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keywords&quot;, &quot;topics&quot;]) articleProfile.write.insertInto(&quot;article_profile&quot;) del keywordsIndex del _articleKeywordsWeights del articleKeywords del articleTopics gc.collect() return articleProfile if __name__ == '__main__': ua = UpdateArticle() sentence_df = ua.merge_article_data() if sentence_df.rdd.collect(): rank, idf = ua.generate_article_label(sentence_df) articleProfile = ua.get_article_profile(rank, idf) 使用工具：Supervisor+Apscheduler # pip install APScheduler from apscheduler.schedulers.blocking import BlockingScheduler from apscheduler.executors.pool import ProcessPoolExecutor from scheduler.update import update_article_profile # 创建scheduler，多进程执行 executors = { 'default': ProcessPoolExecutor(3) } scheduler = BlockingScheduler(executors=executors) # 添加定时更新任务更新文章画像,每隔一小时更新， trigger还有其他定时方式 scheduler.add_job(update_article_profile, trigger='interval', hours=1) scheduler.start() 自定义Logger: import logging import logging.handlers import os logging_file_dir = '/root/logs/' def create_logger(): # 离线处理更新打印日志 log_trace = logging.getLogger('offline') trace_file_handler = logging.FileHandler( os.path.join(logging_file_dir, 'offline.log') ) trace_file_handler.setFormatter(logging.Formatter('%(message)s')) log_trace.addHandler(trace_file_handler) log_trace.setLevel(logging.INFO) supervisor管理apscheduler: [program:offline] environment=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python command=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/scheduler/main.py directory=/root/toutiao_project/scheduler user=root autorestart=true redirect_stderr=true stdout_logfile=/root/logs/offlinesuper.log loglevel=info stopsignal=KILL stopasgroup=true killasgroup=true Word2Vec与文章相似度 w2v.spark.sql(&quot;use article&quot;) article = w2v.spark.sql(&quot;select * from article_data where channel_id=18 limit 2&quot;) words_df = article.rdd.mapPartitions(segmentation).toDF(['article_id', 'channel_id', 'words']) Spark Word2Vec API介绍： 模块：from pyspark.ml.feature import Word2Vec API：class pyspark.ml.feature.Word2Vec(vectorSize=100, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, seed=None, inputCol=None, outputCol=None, windowSize=5, maxSentenceLength=1000) 参数说明： vectorSize=100: 词向量长度 minCount：过滤次数小于默认5次的词 windowSize=5：训练时候的窗口大小 inputCol=None：输入列名 outputCol=None：输出列名 Spark Word2Vec训练保存模型： new_word2Vec = Word2Vec(vectorSize=100, inputCol=&quot;words&quot;, outputCol=&quot;model&quot;, minCount=3) new_model = new_word2Vec.fit(words_df) new_model.save(&quot;hdfs://hadoop-master:9000/headlines/models/test.word2vec&quot;) 上传历史数据训练的模型： hadoop dfs -put ./word2vec_model /headlines/models/ 增量更新-文章向量计算 有了词向量之后，我们就可以得到一篇文章的向量了，为了后面快速使用文章的向量，我们会将每个频道所有的文章向量保存起来。 目的：保存所有历史训练的文章向量 步骤： 1、加载某个频道模型，得到每个词的向量 2、获取频道的文章画像，得到文章画像的关键词(接着之前增量更新的文章article_profile) 3、计算得到文章每个词的向量 4、计算得到文章的平均词向量即文章的向量 加载某个频道模型，得到每个词的向量 from pyspark.ml.feature import Word2VecModel channel_id = 18 channel = &quot;python&quot; wv_model = Word2VecModel.load( &quot;hdfs://hadoop-master:9000/headlines/models/word2vec_model/channel_%d_%s.word2vec&quot; % (channel_id, channel)) vectors = wv_model.getVectors() 获取新增的文章画像，得到文章画像的关键词： # 选出新增的文章的画像做测试，上节计算的画像中有不同频道的，我们选取Python频道的进行计算测试 # 新增的文章画像获取部分 profile = w2v.spark.sql(&quot;select * from article_profile where channel_id=18 limit 10&quot;) # profile = articleProfile.filter('channel_id = {}'.format(channel_id)) profile.registerTempTable(&quot;incremental&quot;) articleKeywordsWeights = w2v.spark.sql( &quot;select article_id, channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight&quot;) _article_profile = articleKeywordsWeights.join(vectors, vectors.word==articleKeywordsWeights.keyword, &quot;inner&quot;) 计算得到文章的平均词向量即文章的向量 def avg(row): x = 0 for v in row.vectors: x += v # 将平均向量作为article的向量 return row.article_id, row.channel_id, x / len(row.vectors) articleKeywordVectors.registerTempTable(&quot;tempTable&quot;) articleVector = w2v.spark.sql( &quot;select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from tempTable group by article_id&quot;).rdd \\ # 分组之后， 求map的平均之前， 结果是 artile_id, channel_id, vector_list # vector_list 是二维数组。 .map(avg).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;articleVector&quot;]) # 求map的平均之后结果是 article_id, channel_id, article_vector # article_vector 代表文章向量 文章相似度计算 存在的问题：以进行某频道全量所有的两两相似度计算。但是事实当文章量达到千万级别或者上亿级别，特征也会上亿级别，计算量就会很大。以下有两种类型解决方案： 每个频道的文章先进行聚类（缺点，（分成几个簇）也是个超参数） 局部敏感哈希LSH(Locality Sensitive Hashing) 基本思想1：LSH算法基于一个假设，如果两个文本在原有的数据空间是相似的，那么分别经过哈希函数转换以后的它们也具有很高的相似度 基本思想2: 经常使用的哈希函数，冲突总是难以避免。LSH却依赖于冲突，在解决NNS(Nearest neighbor search )时，我们期望： 离得越近的对象，发生冲突的概率越高 离得越远的对象，发生冲突的概率越低 局部敏感哈希LSH(Locality Sensitive Hashing) LSH过程： mini hashing(略) Random Projection（特征压缩）： Random Projection是一种随机算法.随机投影的算法有很多，如PCA、Gaussian random projection - 高斯随机投影。 随机桶投影是用于欧几里德距离的 LSH family。其LSH family将x特征向量映射到随机单位矢量v，并将映射结果分为哈希桶中。哈希表中的每个位置表示一个哈希桶。 使得： 离得越近的对象，发生冲突的概率越高 离得越远的对象，发生冲突的概率越低 代码实现： 读取数据，进行类型处理(数组转换类型为Vector)： from pyspark.ml.linalg import Vectors # 选取部分数据做测试 article_vector = w2v.spark.sql(&quot;select article_id, articlevector from article_vector where channel_id=18 limit 10&quot;) train = articlevector.select(['article_id', 'articleVector']) def _array_to_vector(row): return row.article_id, Vectors.dense(row.articleVector) train = train.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector']) 相似度计算（BRP进行FIT）： 函数参数说明： class pyspark.ml.feature.BucketedRandomProjectionLSH(inputCol=None, outputCol=None, seed=None, numHashTables=1, bucketLength=None) inputCol=None：输入特征列 outputCol=None：输出特征列 numHashTables=1：哈希表数量，几个hash function对数据进行hash操作 bucketLength=None：桶的数量，值越大相同数据进入到同一个桶的概率越高 method: # 计算df1每个文章相似的df2数据集的数据 approxSimilarityJoin(df1, df2, 2.0, distCol='EuclideanDistance') # 转为向量 # 代码调用： from pyspark.ml.feature import BucketedRandomProjectionLSH brp = BucketedRandomProjectionLSH(inputCol='articleVector', outputCol='hashes', numHashTables=4.0, bucketLength=10.0) model = brp.fit(旧文章向量) 计算相似的文章以及相似度 # 计算文章和文章之间的相似度 similar = model.approxSimilarityJoin(新增文章向量, 新增文章向量, 2.0, distCol='Similarity') # 输出列名 similar.sort(['EuclideanDistance']).show() 计算结果： datasetA(新增), datasetB（旧的）, Similarity [2,[文章向量]] [5,[文章向量]] 0.0051 [1,[文章向量]] [3,[文章向量]] 0.0054 [2,[文章向量]] [8,[文章向量]] 0.0053 [1,[文章向量]] [4,[文章向量]] 0.0052 [2,[文章向量]] [7,[文章向量]] 0.0055 ... 文章相似度存储 HBase 存储目标：存储 文章，相似文章， 相似度 调用foreachPartition： foreachPartition不同于map和mapPartition。 无返回结果，主要用于离线分析之后的数据（数据库存储等）落地 如果想要返回新的一个数据DF，就使用map后者。 我们需要建立一个HBase存储文章相似度的表： create 'article_similar', 'similar' # 存储格式如下： 表 row_key column_family value put 'article_similar', '1', 'similar:1', 0.2 put 'article_similar', '1', 'similar:2', 0.34 HBase 开启失败可能的原因的： 时间未同步的解决办法： ntpdate 0.cn.pool.ntp.org 或 ntpdate ntp1.aliyun.com thrift服务未开启的解决办法： hbase-daemon.sh start thrift happybase代码实现： def save_hbase(partition): import happybase pool = happybase.ConnectionPool(size=3, host='hadoop-master') with pool.connection() as conn: # 建议表的连接 table = conn.table('article_similar') for row in partition: if row.datasetA.article_id == row.datasetB.article_id: pass else: table.put(str(row.datasetA.article_id).encode(), {&quot;similar:{}&quot;.format(row.datasetB.article_id).encode(): b'%0.4f' % (row.EuclideanDistance)}) # 手动关闭所有的连接 conn.close() similar.foreachPartition(save_hbase) 文章相似度增量更新代码整理 def compute_article_similar(self, articleProfile): &quot;&quot;&quot; 计算增量文章与历史文章的相似度 word2vec :return: &quot;&quot;&quot; # 得到要更新的新文章通道类别(不采用) # all_channel = set(articleProfile.rdd.map(lambda x: x.channel_id).collect()) def avg(row): x = 0 for v in row.vectors: x += v # 将平均向量作为article的向量 return row.article_id, row.channel_id, x / len(row.vectors) for channel_id, channel_name in CHANNEL_INFO.items(): profile = articleProfile.filter('channel_id = {}'.format(channel_id)) wv_model = Word2VecModel.load( &quot;hdfs://hadoop-master:9000/headlines/models/channel_%d_%s.word2vec&quot; % (channel_id, channel_name)) vectors = wv_model.getVectors() # 计算向量 profile.registerTempTable(&quot;incremental&quot;) articleKeywordsWeights = ua.spark.sql( &quot;select article_id, channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight where channel_id=%d&quot; % channel_id) articleKeywordsWeightsAndVectors = articleKeywordsWeights.join(vectors, vectors.word == articleKeywordsWeights.keyword, &quot;inner&quot;) articleKeywordVectors = articleKeywordsWeightsAndVectors.rdd.map( lambda r: (r.article_id, r.channel_id, r.keyword, r.weight * r.vector)).toDF( [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;weightingVector&quot;]) articleKeywordVectors.registerTempTable(&quot;tempTable&quot;) articleVector = self.spark.sql( &quot;select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from tempTable group by article_id&quot;).rdd.map( avg).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;articleVector&quot;]) # 写入数据库 def toArray(row): return row.article_id, row.channel_id, [float(i) for i in row.articleVector.toArray()] articleVector = articleVector.rdd.map(toArray).toDF(['article_id', 'channel_id', 'articleVector']) articleVector.write.insertInto(&quot;article_vector&quot;) import gc del wv_model del vectors del articleKeywordsWeights del articleKeywordsWeightsAndVectors del articleKeywordVectors gc.collect() # 得到历史数据, 转换成固定格式使用LSH进行求相似 train = self.spark.sql(&quot;select * from article_vector where channel_id=%d&quot; % channel_id) def _array_to_vector(row): return row.article_id, Vectors.dense(row.articleVector) train = train.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector']) test = articleVector.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector']) brp = BucketedRandomProjectionLSH(inputCol='articleVector', outputCol='hashes', seed=12345, bucketLength=1.0) model = brp.fit(train) similar = model.approxSimilarityJoin(test, train, 2.0, distCol='EuclideanDistance') def save_hbase(partition): import happybase for row in partition: pool = happybase.ConnectionPool(size=3, host='hadoop-master') # article_similar article_id similar:article_id sim with pool.connection() as conn: table = connection.table(&quot;article_similar&quot;) for row in partition: if row.datasetA.article_id == row.datasetB.article_id: pass else: table.put(str(row.datasetA.article_id).encode(), {b&quot;similar:%d&quot; % row.datasetB.article_id: b&quot;%0.4f&quot; % row.EuclideanDistance}) conn.close() similar.foreachPartition(save_hbase) 添加函数到主函数中文件中，修改update更新代码： ua = UpdateArticle() sentence_df = ua.merge_article_data() if sentence_df.rdd.collect(): rank, idf = ua.generate_article_label(sentence_df) articleProfile = ua.get_article_profile(rank, idf) ua.compute_article_similar(articleProfile) 用户画像构建与更新 组成成分 用户基本信息+用户行为(历史+新增) 用户行为包括： hive&gt; select * from user_action limit 1; OK 2019-03-05 10:19:40 0 {&quot;action&quot;:&quot;exposure&quot;,&quot;userId&quot;:&quot;2&quot;,&quot;articleId&quot;:&quot;[16000, 44371, 16421, 16181, 17454]&quot;,&quot;algorithmCombine&quot;:&quot;C2&quot;} 2019-03-05 我们需要对用户行为（字典）数据格式平铺处理 user_id action_time article_id share click collected exposure read_time 步骤： 1、创建HIVE基本数据表 2、读取固定时间内的用户行为日志 3、进行用户日志数据处理 4、存储到user_article_basic表中 创建HIVE基本数据表 create table user_article_basic( user_id BIGINT comment &quot;userID&quot;, action_time STRING comment &quot;user actions time&quot;, article_id BIGINT comment &quot;articleid&quot;, channel_id INT comment &quot;channel_id&quot;, shared BOOLEAN comment &quot;is shared&quot;, clicked BOOLEAN comment &quot;is clicked&quot;, collected BOOLEAN comment &quot;is collected&quot;, exposure BOOLEAN comment &quot;is exposured&quot;, read_time STRING comment &quot;reading time&quot;) COMMENT &quot;user_article_basic&quot; CLUSTERED by (user_id) into 2 buckets STORED as textfile LOCATION '/user/hive/warehouse/profile.db/user_article_basic'; 读取增量用户行为数据-固定时间内的用户行为日志 关联历史日期文件 # 在进行日志信息的处理之前，先将我们之前建立的user_action表之间进行所有日期关联，spark hive不会自动关联 import pandas as pd from datetime import datetime def datelist(beginDate, endDate): date_list=[datetime.strftime(x,'%Y-%m-%d') for x in list(pd.date_range(start=beginDate, end=endDate))] return date_list dl = datelist(&quot;2019-03-05&quot;, time.strftime(&quot;%Y-%m-%d&quot;, time.localtime())) fs = pyhdfs.HdfsClient(hosts='hadoop-master:50070') for d in dl: try: _localions = '/user/hive/warehouse/profile.db/user_action/' + d if fs.exists(_localions): uup.spark.sql(&quot;alter table user_action add partition (dt='%s') location '%s'&quot; % (d, _localions)) except Exception as e: # 已经关联过的异常忽略,partition与hdfs文件不直接关联 pass sqlDF = uup.spark.sql( &quot;select actionTime, readTime, channelId, param.articleId, param.algorithmCombine, param.action, param.userId from user_action where dt={}&quot;.format(time_str) ) 原始数据格式与目标数据格式 原始数据格式：（行为参数都算在 action列内） actionTime readTime channelID articleId 算法名称 action userId 123 1 exposure 1 321 1 click 1 目标数据格式（行为参数1拆4，action列被爆炸为4列，均为bool类型） user_id action_time article_id shared clicked collected expore read_time 1 1 false false false true 1 2 false true false true 进行用户日志数据格式处理 if sqlDF.collect(): def _compute(row): # 进行判断行为类型 _list = [] if row.action == &quot;exposure&quot;: for article_id in eval(row.articleId): _list.append( [row.userId, row.actionTime, article_id, row.channelId, False, False, False, True, row.readTime]) return _list else: class Temp(object): shared = False clicked = False collected = False read_time = &quot;&quot; _tp = Temp() if row.action == &quot;share&quot;: _tp.shared = True elif row.action == &quot;click&quot;: _tp.clicked = True elif row.action == &quot;collect&quot;: _tp.collected = True elif row.action == &quot;read&quot;: _tp.clicked = True else: pass _list.append( [row.userId, row.actionTime, int(row.articleId), row.channelId, _tp.shared, _tp.clicked, _tp.collected, True, row.readTime]) return _list # 进行处理 # 查询内容，将原始日志表数据进行处理 _res = sqlDF.rdd.flatMap(_compute) data = _res.toDF([&quot;user_id&quot;, &quot;action_time&quot;,&quot;article_id&quot;, &quot;channel_id&quot;, &quot;shared&quot;, &quot;clicked&quot;, &quot;collected&quot;, &quot;exposure&quot;, &quot;read_time&quot;]) 将上述目标格式的数据按照 userid 和 articleid 分组 先合并历史数据，存储到user-article-basic表中 # 合并历史数据，插入表中 old = uup.spark.sql(&quot;select * from user_article_basic&quot;) # 由于合并的结果中不是对于user_id和article_id唯一的，一个用户会对文章多种操作 new_old = old.unionAll(data) HIVE目前支持hive终端操作ACID，不支持python的pyspark原子性操作，并且开启配置中开启原子性相关配置也不行。 new_old.registerTempTable(&quot;temptable&quot;) # 按照用户，文章分组存放进去 uup.spark.sql( &quot;insert overwrite table user_article_basic select user_id, max(action_time) as action_time, &quot; &quot;article_id, max(channel_id) as channel_id, max(shared) as shared, max(clicked) as clicked, &quot; &quot;max(collected) as collected, max(exposure) as exposure, max(read_time) as read_time from temptable &quot; &quot;group by user_id, article_id&quot;) 用户画像标签权重计算 如何存储 用户画像，作为特征提供给一些算法排序，方便与快速读取使用 选择存储在Hbase当中。 然后用 Hive 外表关联 hbase 如果离线分析也想要使用我们可以建立HIVE到Hbase的外部表。 HBase表设计 table_name column1 column2 column3 create 'user_profile', 'basic','partial','env' row_key column_family value put 'user_profile', 'user:2', 'partial:{channel_id}:{topic}': weights put 'user_profile', 'user:2', 'basic:{info}': value put 'user_profile', 'user:2', 'env:{info}': value Hive表设计 create external table user_profile_hbase( user_id STRING comment &quot;userID&quot;, information map&lt;string, DOUBLE&gt; comment &quot;user basic information&quot;, article_partial map&lt;string, DOUBLE&gt; comment &quot;article partial&quot;, env map&lt;string, INT&gt; comment &quot;user env&quot;) COMMENT &quot;user profile table&quot; STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,basic:,partial:,env:&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;user_profile&quot;); Spark SQL关联表读取问题 创建关联表之后，离线读取表内容需要一些依赖包。解决办法： 拷贝/root/bigdata/hbase/lib/下面hbase-*.jar 到 /root/bigdata/spark/jars/目录下 拷贝/root/bigdata/hive/lib/h*.jar 到 /root/bigdata/spark/jars/目录下 上述操作三台虚拟机都执行一遍。 用户画像频道关键词获取与权重计算 目标：获取用户1~25频道(不包括推荐频道)的关键词，并计算权重 1、读取user-article-basic表，合并行为表与文章画像中的主题词 2、进行用户权重计算公式、同时落地存储 # 获取基本用户行为信息，然后进行文章画像的主题词合并 uup.spark.sql(&quot;use profile&quot;) # 取出日志中的channel_id user_article_ = uup.spark.sql(&quot;select * from user_article_basic&quot;).drop('channel_id') uup.spark.sql('use article') article_label = uup.spark.sql(&quot;select article_id, channel_id, topics from article_profile&quot;) # 合并使用文章中正确的channel_id click_article_res = user_article_.join(article_label, how='left', on=['article_id']) 将关键词字段的列表爆炸 import pyspark.sql.functions as F click_article_res = click_article_res.withColumn('topic', F.explode('topics')).drop('topics') 爆炸后格式如下： user_id article_id topic ... 1 1 python 1 1 golang 1 1 linux ... ... ... 用户画像之标签权重算法 用户标签权重 =( 行为类型权重之和) × 时间衰减 行为类型权重 的 分值 的确定需要整体协商 行为 分值 阅读时间&lt;1000ms 1 阅读时间&gt;=1000ms 2 收藏 2 分享 3 点击 5 时间衰减: 1/(log(t)+1) ,t为时间发生时间距离当前时间的大小。 # 计算每个用户对每篇文章的标签的权重 def compute_weights(rowpartition): &quot;&quot;&quot;处理每个用户对文章的点击数据 &quot;&quot;&quot; weightsOfaction = { &quot;read_min&quot;: 1, &quot;read_middle&quot;: 2, &quot;collect&quot;: 2, &quot;share&quot;: 3, &quot;click&quot;: 5 } import happybase from datetime import datetime import numpy as np # 用于读取hbase缓存结果配置 pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090) # 读取文章的标签数据 # 计算权重值 # 时间间隔 for row in rowpartition: t = datetime.now() - datetime.strptime(row.action_time, '%Y-%m-%d %H:%M:%S') # 时间衰减系数 time_exp = 1 / (np.log(t.days + 1) + 1) if row.read_time == '': r_t = 0 else: r_t = int(row.read_time) # 浏览时间分数 is_read = weightsOfaction['read_middle'] if r_t &gt; 1000 else weightsOfaction['read_min'] # 每个词的权重分数 weigths = time_exp * ( row.shared * weightsOfaction['share'] + row.collected * weightsOfaction['collect'] + row. clicked * weightsOfaction['click'] + is_read) # with pool.connection() as conn: # table = conn.table('user_profile') # table.put('user:{}'.format(row.user_id).encode(), # {'partial:{}:{}'.format(row.channel_id, row.topic).encode(): json.dumps( # weigths).encode()}) # conn.close() click_article_res.foreachPartition(compute_weights) 落地Hbase中之后，在HBASE中查询，happybase或者hbase终端 import happybase # 用于读取hbase缓存结果配置 pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090) with pool.connection() as conn: table = conn.table('user_profile') # 获取每个键 对应的所有列的结果 data = table.row(b'user:2', columns=[b'partial']) conn.close() # 等价于 hbase(main):015:0&gt; get 'user_profile', 'user:2' 基础信息画像更新 def update_user_info(self): &quot;&quot;&quot; 更新用户的基础信息画像 :return: &quot;&quot;&quot; self.spark.sql(&quot;use toutiao&quot;) user_basic = self.spark.sql(&quot;select user_id, gender, birthday from user_profile&quot;) # 更新用户基础信息 def _udapte_user_basic(partition): &quot;&quot;&quot;更新用户基本信息 &quot;&quot;&quot; import happybase # 用于读取hbase缓存结果配置 pool = happybase.ConnectionPool(size=10, host='172.17.0.134', port=9090) for row in partition: from datetime import date age = 0 if row.birthday != 'null': born = datetime.strptime(row.birthday, '%Y-%m-%d') today = date.today() age = today.year - born.year - ((today.month, today.day) &lt; (born.month, born.day)) with pool.connection() as conn: table = conn.table('user_profile') table.put('user:{}'.format(row.user_id).encode(), {'basic:gender'.encode(): json.dumps(row.gender).encode()}) table.put('user:{}'.format(row.user_id).encode(), {'basic:birthday'.encode(): json.dumps(age).encode()}) conn.close() user_basic.foreachPartition(_udapte_user_basic) logger.info( &quot;{} INFO completely update infomation of basic&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))) # hbase(main):016:0&gt; get 'user_profile', 'user:2' 用户画像增量更新定时开启: 用户画像增量更新代码整理 添加定时任务以及进程管理 from offline.update_user import UpdateUserProfile def update_user_profile(): &quot;&quot;&quot; 更新用户画像 &quot;&quot;&quot; uup = UpdateUserProfile() if uup.update_user_action_basic(): uup.update_user_label() uup.update_user_info() scheduler.add_job(update_user_profile, trigger='interval', hours=2) ","link":"https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-er-chi-xian-hua-xiang-gou-jian/"},{"title":"PY => Spark mapPartition","content":"正解 map()：每次处理一条数据 mapPartition()：每次处理一个分区的数据，这个分区的数据处理完后，原RDD中分区的数据才能释放，可能导致OOM 当内存空间较大的时候建议使用mapPartition()，以提高处理效率 分隔 下面所有是自己的思路，推到最后给自己推蒙了，可以掠过 正文实验（可忽略，直奔结果） 为了方便，用把组装的数据类型灌入 map 来模拟 mapPartitions 功能是模拟计算 tf-idf class Article: '''文章类''' def __init__(self,id, indexex, tfidfs): self.id = id self.indexex = indexex # 文章分词后的所有词索引列表 self.tfidfs = tfidfs # 每个词对应的TF-IDF值 列表 def f(partition): for row in partition: # row 代表每个文章 # row.indexex 代表 文章分词后的所有词索引列表 # row.tfidfs 代表 每个词对应的TF-IDF值 列表 word_list = list(zip(row.indexex, row.tfidfs)) for index, tfidf in word_list: # 遍历 &quot;每个&quot;词语 的 index与tfidf ########### 这里 yield 是重点 ########### yield f'文章{row.id}', index, tfidf c = map(f, [ # &lt;-为了模拟分区，这一层的列表代表partition [ # &lt;- 这一层模拟的是每个分区里面的文章列表 Article(0, [1,2],[0.1,0.4] ), # &lt;-文章0 Article(1, [3,4],[3.4,3.7] ) # &lt;-文章1 ] ] ) ######################## 执行 ######################## for x in c: # 解zip print(list(x)) # 解yield 结果： 如果使用 return 关键词，得出的最终打印结果: （不满足） ['文章0', 1, 0.1] 如果使用 yield 关键词，得出的最终打印结果: （满足） [('文章0', 1, 0.1), ('文章0', 2, 0.4), ('文章1', 3, 3.4), ('文章1', 4, 3.7)] 这里就出现了一个问题： 正常用法都是用 return，时常用 lambda（lambda默认也是隐式 return。） 是何原因让我们不得不用 yield? 一点一点往下推： map: 核心是 &quot;按单个数据映射&quot; mapPartition： 核心是&quot;把数据分组，按组映射&quot; 按组映射是没错，但我们的目的是想操作组内的每条数据。 所以我们必须需要每次对组内数据 for循环遍历出来单独处理。 然后 返回回去。 那我们先用正常的 return 返回试试： def(partation): for x in partition: return x.name, x.age 也许看到这里你觉得没什么问题。。。 但是不要忘了最基础的内容， return 是直接跳出 for 循环和函数的。 再次强调，mapPartition是按组映射，所以仔细看上面代码: 最终的mapPartition是按组映射结果就是： 每组的第一个元素的集合 （因为for被return了，每组的函数也被return了） 解决这种问题，有两种方式： 最简单将源代码for循环内部的 return 改为 yield 新建临时列表过渡，return放在for外面，如下案例： def f(partition): _ = [] for x in partition: _.append(x) return _ b = [[1,2,3], [4,5,6]] c = map(f,b) print(list(c)) ","link":"https://cythonlin.github.io/post/py-greater-spark-mappartition/"},{"title":"RS => 推荐系统（一）环境配置+数据收集","content":"Python环境 miniconda创建虚拟环境： conda create -n reco_sys python=3.6.7 激活/退出 虚拟环境： conda activate spider-venv conda deactivate 2个slave需要安装依赖： yum -y install gcc 安装模块： pip install redis supervisor apscheduler chardet jieba jupyter numpy pandas scipy scikit-learn pyspark findspark happybase pyhdfs -i https://pypi.douban.com/simple 大数据环境 Lambda环境启动脚本配置 禁用+关闭防火墙： systemctl disable firewalld.service systemctl stop firewalld.service 同步系统时间（不这样做 hbase可能启动失败）（3台都运行命令）： yum install ntpdate -y ntpdate 0.cn.pool.ntp.org 创建综合启动脚本 vi start.sh /root/bigdata/hadoop/sbin/start-all.sh start-hbase.sh /root/bigdata/spark/sbin/start-all.sh vi stop.sh /root/bigdata/spark/sbin/stop-all.sh stop-hbase.sh /root/bigdata/hadoop/sbin/stop-all.sh 开启 /root/scripts/start.sh 停止 /root/scripts/stop.sh UI地址（查log也可）： Hadoop UI: http://192.168.19.137:8088 YARN UI： http://192.168.19.137:50070 Hbase UI: http://192.168.19.137:16010 Spark UI： http://192.168.19.137:8080/ 数据库运行： MySQL启动： systemctl start docker docker start mysql Hive元数据服务开启: nohup hive --service metastore &amp; spark相关问题 spark on yarn 启动巨慢，解决办法： hadoop fs -mkdir -p /system/spark-lib hadoop fs -put /root/bigdata/spark-2.2.2-bin-hadoop2.7/jars/* /system/spark-lib hadoop fs -chmod -R 755 /system/spark-lib cd $SPARK_HOME/conf cp spark-defaults.conf.template spark-defaults.conf vi spark-defaults.conf spark.yarn.jars hdfs://192.168.19.137:9000//system/spark-lib/* 如果用的是 jupyter, 记得重启 jupyter服务 jupyter notebook --allow-root --ip 0.0.0.0 HDFS-Hive相关问题 内部表修改为外部表： alter table user_profile SET TBLPROPERTIES('EXTERNAL'='TRUE'); 数据构成 数据库1： toutiao news_article_basic # 文章标题 news_article_content # 文章内容 news_channel # 文章频道（类别） user_basic # 用户业务数据 user_profile # 用户私人信息 数据库2： profile user_action # 用户行为日志 数据库2： article article_data # 合并文章标题+内容+频道后的存储结果 ... 数据迁移（Sqoop） 耗时 4000w (10+g): 30+ min 检测Sqoop是否能连通MySQL, 并列出MySQL所有数据库: sqoop list-databases --connect jdbc:mysql://192.168.19.137:3306/ --username root -P 全量导入方式（不推荐）： #!/bin/bash array=(user_profile user_basic news_user_channel news_channel user_follow user_blacklist user_search news_collection news_article_basic news_article_content news_read news_article_statistic user_material) for table_name in ${array[@]}; do sqoop import \\ --connect jdbc:mysql://192.168.19.137/toutiao \\ --username root \\ --password password \\ --table $table_name \\ --m 5 \\ --hive-home /root/bigdata/hive \\ --hive-import \\ --create-hive-table \\ --hive-drop-import-delims \\ --warehouse-dir /user/hive/warehouse/toutiao.db \\ --hive-table toutiao.$table_name done 增量导入方式 方式1： 通过指定递增的字段来导入（不推荐，因为某些字段的值不是递增的） append：即通过指定一个递增的列，如：--incremental append --check-column num_iid --last-value 0 方式2：incremental： 时间戳 --incremental lastmodified \\ --check-column column \\ --merge-key key \\ --last-value '2012-02-01 11:0:00' 就是只导入check-column的列比'2012-02-01 11:0:00'更大（新）的数据,按照key合并 增量导入位置 直接sqoop导入到hive(–incremental lastmodified 模式不支持导入 Hive ) sqoop导入到hdfs，然后建立hive表关联 --target-dir /user/hive/warehouse/toutiao.db/ sqoop导入到hdfs，hive表关联到hdfs填坑 现象： 查出一堆 null 原因： sqoop 导出的 hdfs 分片数据，都是使用逗号 , 分割的。 由于 hive 默认的分隔符是 /u0001（Ctrl+A）,为了平滑迁移，需要在创建表格时指定数据的分割符号。 解决方式： 导入数据到hive中，需要在创建HIVE表加入 row format delimited fields terminated by ',' sqoop迁移到hdfs后，Hive创建表并指定关联位置实例 create table user_profile( user_id BIGINT comment &quot;userID&quot;, gender BOOLEAN comment &quot;gender&quot;) COMMENT &quot;toutiao user profile&quot; row format delimited fields terminated by ',' LOCATION '/user/hive/warehouse/toutiao.db/user_profile'; 注： 5个表中，只有 news_article_content （因为这个表奇怪字符太多，是全量导入的，hive不需要手动创建表，会自动创建的） 而这个表从 hdfs 拿到的数据是已经做过过滤的，所以不需要 加分隔符了，也就是不需要下面这行代码： row format delimited fields terminated by ',' Flume日志收集到Hive中 新建数据库 create database if not exists profile comment &quot;use action&quot; location '/user/hive/warehouse/profile.db/'; 创建表的格式语法实例如下： create table user_action( actionTime STRING comment &quot;user actions time&quot;, readTime STRING comment &quot;user reading time&quot;, channelId INT comment &quot;article channel id&quot;, param map&lt;string, string&gt; comment &quot;action parameter&quot;) COMMENT &quot;user primitive action&quot; PARTITIONED BY(dt STRING) ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' LOCATION '/user/hive/warehouse/profile.db/user_action'; 文档中：Hive建表，有个问题 map 需要指定数据类型: param map&lt;string, string&gt; comment &quot;action parameter&quot;) map 要向上面一样指定 &lt;string, string&gt; 才可以， 不然会报如下错误： '''mismatched input 'comment' expecting &lt; near 'map' in map type''' 疑难参数解读： PARTITIONED BY(dt STRING)： hive按照 dt 字段分区 为什么要分区： Hive适合处理大的文件内容量，少的文件数量。 Flume收集日志可能来一点日志就加到一个新文件中。 如此一来，文件零散的特别多。 Hive处理的会很慢。 所以，Hive指定个分区，来把小文件们分成几大块（就是几个分区），这样处理会更快 ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'： 处理Json格式数据 数据导入步骤如下（虚代替Flume）（操作完成后是查不到数据的，需要关联，下面会解释）： hadoop fs -put /root/data/backup/profile.db/user_action/* /user/hive/warehouse/profile.db/user_action/ # 删除: hadoop fs -rmr /user/hive/warehouse/profile.db/* Flume 收集配置 进入flume/conf目录 创建一个collect _ click.conf的文件，写入flume的配置： a1.sources = s1 a1.sinks = k1 a1.channels = c1 a1.sources.s1.channels= c1 a1.sources.s1.type = exec a1.sources.s1.command = tail -F /root/logs/userClick.log a1.sources.s1.interceptors=i1 i2 a1.sources.s1.interceptors.i1.type=regex_filter a1.sources.s1.interceptors.i1.regex=\\\\{.*\\\\} a1.sources.s1.interceptors.i2.type=timestamp # channel1 a1.channels.c1.type=memory a1.channels.c1.capacity=30000 a1.channels.c1.transactionCapacity=1000 # k1 a1.sinks.k1.type=hdfs a1.sinks.k1.channel=c1 a1.sinks.k1.hdfs.path=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d a1.sinks.k1.hdfs.useLocalTimeStamp = true a1.sinks.k1.hdfs.fileType=DataStream a1.sinks.k1.hdfs.writeFormat=Text a1.sinks.k1.hdfs.rollInterval=0 a1.sinks.k1.hdfs.rollSize=10240 a1.sinks.k1.hdfs.rollCount=0 a1.sinks.k1.hdfs.idleTimeout=60 参数说明： sources：为实时查看文件末尾，interceptors解析json文件 channels：指定内存存储，并且制定batchData的大小，PutList和TakeList的大小见参数，Channel总容量大小见参数 指定sink：形式直接到hdfs，以及路径，文件大小策略默认1024、event数量策略、文件闲置时间 开始收集： /root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1 Hive 关联分区： 如果不关联分区，无论是 Flume收集到HDFS的分区数据，还是我们传进去HDFS模拟的分区数据 通过Hive是查不到的 关联分区如下操作： alter table user_action add partition (dt='2018-12-11') location &quot;/user/hive/warehouse/profile.db/user_action/2018-12-11/&quot; 进程管理 Supervisor 正常配置流程 安装: pip install supervisor 创建配置文件（当前目录执行，或者主目录执行都可，总配置文件就会生成到当前目录下）： echo_supervisord_conf &gt; supervisord.conf 创建自定义配置文件目录： mkdir /etc/supervisor vim 打开编辑supervisord.conf文件，修改最后1行： [include] files = relative/directory/*.ini 为 [include] files = /etc/supervisor/*.conf 将最开始生成的 supervisord.conf 复制到 /etc/ 下， 然后主文件就不用动了： cp supervisord.conf /etc/ 最后在 /etc/supervisor 这个目录中，自定义我们自己需要的 启动程序的配置文件模板，这里为 vi reco.conf： 见下面Flume案例 Flume+Supervisor 配置案例 flume启动需要相关hadoop,java环境，可以在shell脚本汇总添加: 先创建一个存放此shell脚本的目录： mkdir /root/toutiao_project/scripts 打开文件: vi /root/toutiao_project/scripts/collect_click.sh 并写入: #!/usr/bin/env bash export JAVA_HOME=/root/bigdata/jdk export HADOOP_HOME=/root/bigdata/hadoop export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin /root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1 并在 /etc/supervisor 的reco.conf添加: [program:flume] command=/bin/bash /root/toutiao_project/scripts/collect_click.sh user=root autorestart=true redirect_stderr=true stdout_logfile=/root/logs/collect.log loglevel=info stopsignal=KILL stopasgroup=true killasgroup=true 启动 supervisor服务: supervisord -c /etc/supervisord.conf 查看 supervisor是否运行： ps aux | grep supervisord 管理 supervisor进程管理界面，输入命令： supervisorctl 管理界面可通过如下 命令+进程名 来管理进程： start flume stop flume restart flume status # 查看所有进程状态 update # 重启配置文件修改过的程序 ","link":"https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-yi-huan-jing-pei-zhi-shu-ju-shou-ji/"},{"title":"PY => PySpark-Spark SQL","content":"Spark SQL Spark SQL 分为三类： SQL DataFrame (参考pandas，但略有不同) Datasets (由于python是动态的，所以不支持python) 初始环境： import findspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession.builder.appName('myspark').getOrCreate() # 初始化session # spark.sparkContext.parallelize([1,2,3,4]).collect() # 里面包含之前说过的sparkContext ... 中间这部分留给下面写 ... spark.stop() # 关闭 session 从json导入为df: df = spark.read.json(&quot;file:///home/lin/data/user.json&quot;,multiLine=True) 打印DF字段信息： df.printSchema() root |-- age: long (nullable = true) |-- gender: string (nullable = true) |-- name: string (nullable = true) CRUD 增 from pyspark.sql import functions as f # schema就相当于 pandas 指定的 columns, 双层序列， [2x4] 的样本 df1 = spark.createDataFrame([[1,2,3,4],[5,6,7,8]],schema=['1_c','2_c', '3_c', '4_c']) +-----+-----+-----+-----+ |1\\_col|2\\_col|3\\_col|4\\_col| +-----+-----+-----+-----+ | 1| 2| 3| 4| | 5| 6| 7| 8| +-----+-----+-----+-----+ # lit可以在指定空列的时候，指定 null值， 或者 int型（里面有很多类型，可以发现） # df2 = df1.withColumn('null_col', f.lit(None)).withColumn('digit_col', f.lit(2)) df2 = df1.withColumn('5_col', df1['4_col']+1) # 在原来列字段基础上。 df2.show() 删 df2 = df1.drop('age') # 删除 age列 df2 = df1.dropna() # 删除空行 df2 = df1.drop_duplicates() # 删除重复-行 改 和&quot;增&quot;，差不多，只不过字段，指定为原有字段字符串即可。 查 下面的讲的（投影、过滤、排序、分组），几乎都是查。 投影 投影所有： df.show(n=20) # 默认就是 n=20 只返回 前20条记录 +---+------+--------+ |age|gender| name| +---+------+--------+ | 18| man|zhangsan| +---+------+--------+ 选中某列投影： df.select('name','age').show() # 若直接写 '*', 和直接 df.show()是一个效果 +--------+---+ | name|age| +--------+---+ |zhangsan| 18| +--------+---+ 或者用另两种方式投影（投影过程可计算）： df.select(df['name'],df['age']+20).show() # 同上，这是另一种写法，注意一下列名 df.select(df.name, df.age+20).show() # 同上，这是另二种写法，注意一下列名 +--------+----------+ | name|(age + 20)| +--------+----------+ |zhangsan| 38| +--------+----------+ 取出前N条DF，并转化为 [ {} , {} ] 格式 df_user.take(1) # [Row(age=18, name='张三')] df_user.head(1) # [Row(age=18, name='张三')] df_user.first() # Row(age=18, name='张三') # 注意,无列表 排序 df_user.head(1) df_user.sort(df_user.name.desc()).show() # 另外说明一点, df的每个熟悉都有,一些操作符函数, desc()就是一种操作符函数 过滤： df.filter( df['age'] &gt; 15).show() +---+------+----+ |age|gender|name| +---+------+----+ +---+------+----+ 分组： df.groupBy('name').count().show() +--------+-----+ | name|count| +--------+-----+ |zhangsan| 1| +--------+-----+ Join df_user.join(df_user, on=df_user.name==df_user.name, how='inner').show() +----+---+----+---+ |name|age|name|age| +----+---+----+---+ |李四| 20|李四| 20 | |张三| 18|张三| 18 | +----+---+----+---+ # 特别提醒， 此 Join， 只要都进来是 DF格式的任何数据库，都可 Join # 比如： MySQL 和 Hive , Json 也可。 储存为临时视图（表), 并调用sql语句： df.createOrReplaceTempView('user') # 创建为 user临时视图 df_sql = spark.sql('select * from user').show() # spark.sql返回的还是df, 所以要show() +---+------+--------+ |age|gender| name| +---+------+--------+ | 18| man|zhangsan| +---+------+--------+ RDD 与 DF互转 RDD -&gt; DF ### RDD -&gt; DF 需要把RDD做成两种格式(任选其一) ### 第一种 Row 格式 from pyspark import Row rdd_user = spark.sparkContext.parallelize( [('张三',18), ('李四',20)] ) rdd_user_row = rdd_user.map(lambda x:Row(name=x[0], age=x[1])) print(rdd_user_row.collect()) # [Row(age=18, name='张三'), Row(age=20, name='李四')] df_user = spark.createDataFrame(rdd_user_row) ### 第二种 [('张三', 18),('李四', 20)] rdd_user = spark.sparkContext.parallelize( [('张三',18), ('李四',20)] ) df_user = rdd_user.toDF(['name', 'age']) # 给定列名 df_user.show() DF -&gt; RDD rdd_row = df_user.rdd.map(lambda x: x.asDict()) # 或者 x.name, x.age取值 rdd_row.collect() # [{'age': 18, 'name': '张三'}, {'age': 20, 'name': '李四'}] CSV读写 从HDFS中读取(我们先新建一个CSV并扔到HDFS中), vi mydata.csv: name,age zhangsan,18 lisi, 20 hadoop fs -mkdir /data # 在HDFS中新建一个目录 /data hadoop fs -put mydata.csv /data # 并把本地 mydata.csv扔进去 (-get可拿出来) 在代码中读取 HDFS数据: df = spark.read.csv(&quot;hdfs:///data/mydata.csv&quot;, header=True) df.show() # header=True 代表, csv文件的第一行作为csv的抬头(列名) # df.write.csv(&quot;hdfs:///data/mydata.csv&quot;, header=True) # read改为write就变成了写 Hive读写 Hive的配置与依赖之前讲过了（最值得注意的是需要先启动一个 metadata的服务） 先验传送门：https://segmentfault.com/a/1190000020841646 import findspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession.builder\\ .appName(&quot;Spark Hive Example&quot;)\\ .master(&quot;local[*]&quot;)\\ .config(&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;)\\ .enableHiveSupport()\\ .getOrCreate() spark.sql(&quot;use mydatabase&quot;) # 执行Hive 的 SQL, 切换数据库（前提你得有） 读： df = spark.table('person').show() # 直接对表操作 (注意，sql语句也可) 写： df = spark.table('person') df2 = df.withColumn('nickname', df.name) # 稍微变动一下，添一个字段 df2.write.saveAsTable(&quot;new_person&quot;) # 写入新表 MySQL读写 读： # 注意0：有好几种方式，我只列举一个 成对的读写配置。 # 注意1: url中 &quot;hive&quot;是数据库名. 你也可以起为别的名 # 注意2：table的值--&quot;TBLS&quot;, 它是 MySQL中&quot;hive库&quot;中的一个表。 # 注意3：特别注意！ TBLS不是我们想要的表。他只是一个大表，管理了我们hive的信息 # TBLS中的 一个列属性 &quot;TBL_NAME&quot; 才是真正我们需要的表！！ df = spark.read.jdbc( url='jdbc:mysql://localhost:3306/hive?user=root&amp;password=123', table=&quot;TBLS&quot;, properties={&quot;driver&quot;:&quot;com.mysql.jdbc.Driver&quot;}, ) df.show() df.select(&quot;TBL_NAME&quot;).show() 写： df.write.jdbc( url='jdbc:mysql://localhost:3306/hive?user=root&amp;password=123', table=&quot;new_table&quot;, mode='append', properties={&quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;} ) # 同是特别注意： 和读一样， 它写入的新表，也是一个整体的表结构。 # 此表的一个列&quot;TBL_NAME&quot;，它才对应了我们真正要操作的表 ","link":"https://cythonlin.github.io/post/py-greater-pyspark-spark-sql/"},{"title":"PY => PySpark-Spark Core（RDD）","content":"前言 第一篇传送门：https://segmentfault.com/a/1190000020841646 RDD认知 RDD是什么？ RDD: 弹性分布式数据集（Resiliennt Distributed Datasets） 转为格式RDD的几种方式： 1. parallelize: rdd = sc.parallelize([1,2,3,4,5]) # 里面传的就是普通python类型 2. 读文件/读数据库/读ES等各种方式，此处以读文件为例： rdd = sc.textFile('file:///home/lin/data/hello.txt') RDD核心概念 Application: application: 一个app 就是一个自定义的 py脚本（被 spark-submit提交的）或一个spark-shell app = 1个 driver + 多个executors(相当于多个进程) 注意：数据在不同的 app之间 不能被共享， 若想要共享（需要考虑外部存储） Driver: 每一个.py脚本中都有一个 sparkcontext，它就是driver Worker Node: 相当于standalone 的 slave节点 Executor: Executor(进程)：每个Driver中都有多个 Executors 并且可以运行多个 Tasks Job: job: 对应下面即将要说的 action : collect() 等 一个 task 对应 一个 job (一个 transformation 对应 一个 action) 一个 job 对应 多个 task (多个 transformations链式调用之后，再调用一个action) Task: task: 对应下面即将要说的 transformation :map() 等 每个task可用一个线程执行。多个task可并行 Stage: 一个job被切分为多份 Cluster Manager: 管理 从 Standalone, YARN, Mesos 中获取的资源 就是 --master 指定的参数 其中 还包括 空间 内存等参数配置 Cache: 缓存： ### persist &amp; cache &amp; unpersist 三种API可供选择 Lineage(依赖,血缘关系)： 依赖： 父 子 孙 RDD1 -&gt; map-&gt; RDD2 -&gt; filter-&gt; RDD3 服务器1： part1 -&gt; part1-&gt; part1 服务器2： part2 -&gt; part2-&gt; part2 服务器3： part3 -&gt; part3-&gt; part3 如上图: 假如 RDD3 的 part2 挂了， 那么就会退回到 RDD2的part2再计算一遍。 而不是回到&quot;最初&quot;的起点。 窄依赖（Narrow, 依赖的很少，很窄）： 重点: '子part' 只依赖一个 '父part'。 map, filter 等: 元素被摊分在每一个part中， 子part出错就找&quot;对应&quot;（一个） 父part即可。 宽依赖（Wide, 依赖的很多，很宽）： 重点: '子part' 依赖多个 '父part' 同时计算得到。 shuffle操作: xxBy, join等： 子part出错 找&quot;对应&quot;（多个） 父part 重新共同计算。 stage: 遇到 1个宽依赖， 就会做 shuffle操作。 然后就会把&quot;之前&quot;的 “所有窄依赖”划分为 &quot;1个stage&quot;。 最后，整体全部，也当作 &quot;1个stage&quot;。 官档图 传送门：http://spark.apache.org/docs/latest/cluster-overview.html RDD两大算子 Transformation （Lazy） 主要机制：各种操作不会被立刻执行，但这些操作之间的关系会被记录下来，等待下面action调用。 直观理解举例： 1. 像 sqlalchemy 中的 filter(), groupby(), page()等操作 2. 像 tensorflow1.x 中的 sess.run() 之前的各种操作 3. 像 数据库的事务，在提交之前的各种操作 接下来介绍，Transformation 的各种操作。 map 同 python 的 map。 你只需记住RDD类型里面包裹的就是我们熟悉的python类型 所以： python 的 map 怎么用， RDD对象的 map 就怎么用， 下面filter同理 只举一个语法格式例子：（下面同理） rdd.map(lambda x:x+1) filter 同上，同python flatMap 和 map 几乎差不多。 唯一有一点区别： map 每次基于单个元素，返回什么，那最终结果就是什么（最后拼成序列）。 flatMap 每次基于单个元素，若返回的是序列（列表等），那么会自动被解包，并一字排开返回。 groupBy 和 groupByKey 说一下 没有key, 和 带有key的区别（后面同理，就不啰嗦了）： 没有key: 1. 一般必须需要一个 函数句柄 (lambda), 而这个句柄是针对（操作后新形成的key）使用的 2. 针对一层序列 [, , ...] 带有key 1. 一般无参 2. 针对双层序列 [(),(),...] 直接上例子了（对比着看）： rdd1 = sc.parallelize(['a','b','c','a']) # 一层序列 rdd2 = sc.parallelize( [('a',1),('b',2), ('c',3), ('a',4)] ) # 双层序列 group1 = rdd1.groupBy(lambda x:x) # 针对 一层序列， 注意这里，必须写 函数句柄 group2 = rdd2.groupByKey() # 针对 双层序列 print( group1.collect() ) print( group2.collect() ) # 可以这样告诉你， 他们俩的最外层结果是一样的: [{key:value}, ...], 结果如下 ~~~~ [ ('a', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb7e4384c88&gt;), ('b', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb7e43848d0&gt;), ('c', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb7e4384940&gt;) ] # 如果加了count(), 那么它们的结果就是一样的了，返回统计的个数， 等到 action再说。 reduceByKey 照应双层或多层序列，或者 承接 groupByKey() rdd = sc.parallelize(['Tom', 'Jerry', 'Tom', 'Putch']) rdd.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).collect() # 结果（可以忽略上面的 collect(), 它属于action，放在这里方便贴个结果） &gt;&gt; [('Tom', 2), ('Jerry', 1), ('Putch', 1)] sortBy 和 sortByKey sortBy：根据元素排序（这里的例子是根据key排序， a[1]代表根据value排序） &gt;&gt;&gt; a = sc.parallelize([['z',1], ['b',4],['h',3]]) &gt;&gt;&gt; a.sortBy(lambda a:a[0]).collect() [['b', 4], ['h', 3], ['z', 1]] &gt;&gt;&gt; a.sortBy(lambda a:a[1]).collect() [['z', 1], ['h', 3], ['b', 4]] sortByKey：根据key排序 &gt;&gt;&gt; a.sortByKey().collect() [('b', 4), ('h', 3), ('z', 1)] 可选参数： ascending=False (默认为True升序) union rdd1.union(rdd2) # 相当于 python的 &quot;列表加法&quot; 或者 python的 &quot;extend&quot; distinct rdd.distinct() # 去重 join 前提： （我的理解就是，能转化成 python 字典的列表格式即可） eg: [ [1,2], [3,4], [5,6] ] 两层列表 每层列表的每个元素中， 只有2个元素 错误格式示例： [['a','b','c'], ['d','e','f']] 也不能说错误吧，不过若是这种3个-多个子元素的格式， join时默认会取前2个元素。其余丢弃。 内连接（innerJoin）： 左外连接（leftOuterJoin）： 右外连接（rightOuterJoin）： 全外连接（fullOuterJoin）： 完整示例： rdd1 = sc.parallelize( [['a','b'], ['d','e']] ) # 左 rdd2 = sc.parallelize( [['a','c'], ['e','f']] ) # 右 # 开头说过:能转化成字典的列表格式即可，或者你可以写成这样（但是不能传原生字典进去）： rdd1 = sc.parallelize( list({'a': 'b', 'd': 'e'}.items()) ) rdd2 = sc.parallelize( list({'a': 'c', 'e': 'f'}.items()) ) # 内连接（交集） print( rdd1.join(rdd2).collect() ) # [('a', ('b', 'c'))] # 左连接（左并集） print( rdd1.leftOuterJoin(rdd2).collect() ) # [('d', ('e', None)), ('a', ('b', 'c'))] # 右连接（右并集） print( rdd1.rightOuterJoin(rdd2).collect() ) # [('a', ('b', 'c')), ('e', (None, 'f'))] # 全连接（并集） print( rdd1.fullOuterJoin(rdd2).collect() ) # [('d', ('e', None)), ('a', ('b', 'c')), ('e', (None, 'f'))] persist &amp; cache &amp; unpersist cache(): 缓存 persist(): 持久化 unpersist(): 清空缓存 （他属于 action-立即触发， 为了方便对比，我就一起放到了这里） 官档：http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence Action (Commit) 主要机制：拿到 transformation 记录的关系， 用 action的各种操作来真正触发、执行、返回结果。 对应上面，继续直观举例： 1. 像 sqlalchemy 中的 commit() 2. 像 tensorflow1.x 中的 sess.run() 3. 像 数据库的事务的 &quot;提交&quot; 接下来介绍，Action 的各种操作。 collect 执行transformation记录的关系 并 返回结果， 在Pyspark中就是RDD类型 转 Python数据类型。 (中间你可以链式调用各种 transformation方法，结尾调用一个 collect(), 就可以出结果了) rdd1.xx().xx().collect() count 统计元素项的个数，同上语法, 同上理念，触发返回结果 rdd2 = sc.parallelize( [['a','c','d'], ['e','f','g']] ) rdd2.count() # 无参 &gt;&gt; 2 reduce rdd2 = sc.parallelize( [['a','c','d'], ['e','f','g']] ) rdd2.reduce(lambda x,y:x+y) # 参数为2个参数的函数句柄，做&quot;累&quot;的操作，（累加，累乘）等 &gt;&gt; ['a', 'c', 'd', 'e', 'f', 'g'] take 相当于mysql的limit操作，取前n个 rdd2 = sc.parallelize( [['a','c','d'], ['e','f','g']] ) rdd2.take(0) # [] rdd2.take(1) # [['a', 'c', 'd']] rdd2.take(2) # [['a', 'c', 'd'], ['e', 'f', 'g']] 再次强调： take的参数是，个数的意思，而不是索引，不要混淆额 top 返回最大的n个元素（会自动给你排序的） rdd2 = sc.parallelize( [1,2,3,8,5,3,6,8]) rdd2.top(3) &gt;&gt; [8, 8, 6] foreach 遍历每个元素，对子元素做-对应函数句柄的操作，下面说这个action的两点注意事项： 注意1： 无返回值（返回None） 注意2： 通常用作 print(), 但是它不会在notebook中打印， 而是在你后台开启的spark中打印。 rdd2 = sc.parallelize( [['a','c','d'], ['e','f','g']] ) rdd2.foreach(lambda x:print(x)) &gt;&gt; ['a', 'c', 'd'] ['e', 'f', 'g'] saveAsTextFile rdd = sc.textFile('file:///home/lin/data') rdd.saveAsTextFile('file:///home/lin/mydata') # 这里有个注意事项： saveAsTextFile的参数路径不能在都进来的路径范围内。 # 或者说，读是从这个文件夹A（这是最后一级的目录）读进来的， 写就不能写入文件夹A了 # 另外， mydata是目录名， 进去你会看见 part-00000 这样的文件名，这才是真数据文件。 Spark优化相关 序列化： 好处1：网络传输必备 好处2：节省内存 两种方式序列化方式： 1. Java内部序列化（默认，较慢，但兼容性好） 2. Kryo （较快，但兼容性不太好） 内存管理： 可分为 execution（进程执行） 和 storage（存储） execution相关操作: shuffle, join, sort, aggregation storage相关操作 : cache， 特点： execution 和 storage 共享整体内存： execution起到 &quot;存霸&quot; 的角色: 1. 若 execution区域内存 不够用了， 它会去抢夺 storage 区域的内存（不归还） 2. 当然，可以为 storage 设置阈值 （必须给 storage留下多少） 具体分配多少： 总内存 = n execution内存 = (总内存 - 300M) * 50% storage内存 = (总内存 - 300M) * 50% 说白了，就是留给JVM 300M， 然后 execution 和 storage 各分一半。 查看内存占用情况 可通过WebUI查看 （序列化后存储，通常会节省内存） Broadcasting Variable 情景：正常来说，每个 task（map, filter等） 都会占用1份数据，100个task就会拿100份数据。 这种情况造成了数据的冗余， BroadCasting Variable（广播变量）就是解决这一问题的。 ","link":"https://cythonlin.github.io/post/py-greater-pyspark-spark-corerdd/"},{"title":"PY => Ubuntu-Hadoop-YARN-HDFS-Hive-Spark安装配置","content":"环境条件 Java 8 Python 3.7 Scala 2.12.10 Spark 2.4.4 hadoop 2.7.7 hive 2.3.6 mysql 5.7 mysql-connector-java-5.1.48.jar R 3.1+（可以不安装） 安装Java 先验传送门：https://segmentfault.com/a/1190000020746647#articleHeader0 安装Python 用Ubuntu自带Python3.7 安装Scala 下载：https://downloads.lightbend.com/scala/2.12.10/scala-2.12.10.tgz 解压： tar -zxvf 下载好的Scala 配置： vi ~/.bashrc export SCALA_HOME=/home/lin/spark/scala-2.12.10 export PATH=SCALAHOME/bin:{SCALA_HOME}/bin:SCALAH​OME/bin:PATH 保存退出 激活配置： source ~/.bashrc 安装Hadoop 提前说明： 若不使用HDFS 和 YARN，整个 Hadoop安装可忽略，可以直接安装spark。 下载：https://archive.apache.org/dist/hadoop/common/ 详细地址: https://archive.apache.org/dist/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz 解压： tar -zxvf 下载好的 hadoop 配置hadoop: vi ~/.bashrc export HADOOP_HOME=/home/lin/hadoop/hadoop-2.7.7 export PATH=HADOOPHOME/bin:{HADOOP_HOME}/bin:HADOOPH​OME/bin:PATH 激活配置： source ~/.bashrc HDFS配置： 进入解压后的 etc/hadoop (注意这不是根目录的etc， 而是解压后的hadoop目录下的etc) echo $JAVA_HOME # 复制打印出的路径 vi hadoop-env.sh: （找到 export JAVA_HOME 这行，并替换为如下） export JAVA_HOME=/home/lin/spark/jdk1.8.0_181 vi core-site.xml: （hdfs后面为 主机名:端口号） （主机名就是终端显示的 @后面的~~~） &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://lin:8020&lt;/value&gt; &lt;/property&gt; vi hdfs-site.xml： (同样在 &lt;configuration&gt; 之间加入) (/home/lin/hdfs是我已经有的目录) &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/lin/hdfs/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/lin/hdfs/dfs/data&lt;/value&gt; &lt;/property&gt; 格式化HDFS： hadoop namenode -format # 然后去刚才上面配置的这个路径里面是否有新东西出现： /home/lin/hdfs 开启HDFS (先进入 sbin目录下， sbin 和 etc bin是同级的， 这里说的都是hadoop里的目录): ./start-dfs.sh # 一路yes, 若让你输入密码， 则输入对应服务器的密码。（我这里都是本机） # 若提示权限错误，继续往下看（支线） sudo passwd root # 激活ubuntu的root用户，并设置密码 vi /etc/ssh/sshd_config： PermitRootLogin yes # 任意位置添加这个（此选项之前可能是存在的注释后再添加） service ssh restart 查看HDFS里面根目录 / 的内容： hadoop fs -ls / 向HDFS里面根目录 / 中 传入文件： echo test &gt; test.txt # 首先，随便建立一个文件 hadoop fs -put test.txt / # 向HDFS里面根目录 / 中 传入文件 hadoop fs -ls / # 再次查看，就发现有 test.txt 文件了。 从HDFS里面根目录 / 中 读取文件test.txt： hadoop fs -text /test.txt 从Hadoop WebUI 中查看刚才的文件是否存在： http://192.168.0.108:50070/ # 50070是默认端口 点击右侧下拉框 &quot;Utilities&quot; -&gt; &quot;Browser the file system &quot; 清晰可见， 我们的test.txt 躺在那里~ YARN配置 还是etc/hadoop这个目录: cp mapred-site.xml.template mapred-site.xml vi mapred-site.xml： &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; vi yarn-site.xml: &lt;property&gt; &lt;name&gt;yarn.nodemanager.services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 启动 YARN: （还是 sbin目录下） ./start-yarn.sh # 同样，若有密码，输入机器的密码即可 从Hadoop WebUI 中查看YARN： http://192.168.0.108:8088/ 安装MySQL 下面要用MySQL, 所以单独提一下MySQL 的 安装与配置: 其实MySQL是不需要单独说的, (但我装的时候出现了和以往的不同经历), 所以还是说一下吧: apt-get install mysql-server-5.7 安装容易, 不同版本的MySQL配置有些鸡肋 (我用的是 Ubuntu19): vi /etc/mysql/mysql.conf.d/mysqld.cnf: bind-address 0.0.0.0 # 找到修改一下即可 修改密码+远程连接权限 (默认无密码): mysql # 啥参数也不用加, 直接就能进去 use mysql update mysql.user set authentication_string=password(&quot;123&quot;) where user=&quot;root&quot;; update user set plugin=&quot;mysql_native_password&quot;; flush privileges; select host from user; update user set host ='%' where user ='root'; flush privileges; 重启服务: systemctl restart mysql 服务端连接测试: mysql -uroot -p # 密码 123 远程连接测试 (Navicat): 成功 安装Hive 下载: https://archive.apache.org/dist/hive/hive-2.3.6/apache-hive-2.3.6-bin.tar.gz 解压： tar -zxvf apache-hive-2.3.6-bin.tar.gz 配置Hive： vi ~/.bashrc export HIVE_HOME=/home/lin/hive/apache-hive-2.3.6-bin export PATH=HIVEHOME/bin:{HIVE_HOME}/bin:HIVEH​OME/bin:PATH 激活配置： source ~/.bashrc Hive其他相关配置(同理进入hive解压目录的 conf目录中): cp hive-env.sh.template hive-env.sh vi hive-env.sh: HADOOP_HOME=/home/lin/hadoop/hadoop-2.7.7 Hive-MySQL相关配置(同是在 conf目录下): vi hive-site.xml: (特别注意后两个 里面的内容, 自己修改一下用户名和密码) javax.jdo.option.ConnectionURL jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false javax.jdo.option.ConnectionDriverName com.mysql.jdbc.Driver javax.jdo.option.ConnectionUserName root javax.jdo.option.ConnectionPassword 123 下载 jdbc-mysql驱动,并放入Hive中,操作如下 (因为我们上面hive-site.xml 用的是mysql): 首先下载: http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.48/mysql-connector-java-5.1.48.jar 将此jar文件放入 hive的lib目录中(和 conf同级): 将此jar文件再copy一份放入 spark的 jar目录下（为了后续jupyter直连MySQL(不通过Hive)使用） 初始化(先确保之前的HDFS和MySQL已经启动): schematool -dbType mysql -initSchema 注意事项1: 这个用命令初始化的 步骤是 Hive 2.+ 才需要做的 注意事项2: 初始化一次即可, 多次初始化,会使得MySQL有重复的键. 报错. 开启 metastore 服务： nohup hive --service metastore &amp; 检测是否初始化(去MySQL表中查看): use hive show tables; # 若有数据,则说明初始化成功 启动hive: hive 建库, 建表测试 (注意,千万不要用 user这种关键字当作表名等): HIVE中输入: create database mydatabase; use mydatabase; create table person (name string); MySQL中查看Hive表的相关信息: select * from TBLS; # 查看所有表结构信息 select * from COLUMNS_V2; # 查看所有列的信息 向Hive导入文件: vi hive_data.txt: (写入如下两行) tom catch jerry every one can learn AI load data local inpath '/home/lin/data/hive_data.txt' into table person; 查询: select * from person; PySpark客户端配置连接代码: import findspark findspark.init() from pyspark.sql import SparkSession spark = SparkSession.builder\\ .appName(&quot;Spark Hive Example&quot;)\\ .master(&quot;local[*]&quot;)\\ .config(&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;)\\ .enableHiveSupport()\\ .getOrCreate() spark.sql(&quot;use mydatabase&quot;).show() spark.sql('show tables').show() 安装Spark 下载：spark-2.4.4-bin-hadoop2.7.tgz： 粗糙传送门：https://spark.apache.org/downloads.html 详细传送门：http://ftp.riken.jp/net/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz 解压： tar -zxvf 下载好的spark-bin-hadoop 配置spark： vi ~/.bashrc export SPARK_HOME=home/lin/spark/spark-2.4.4-bin-hadoop2.7 export PATH=SPARKHOME/bin:{SPARK_HOME}/bin:SPARKH​OME/bin:PATH 激活配置： source ~/.bashrc 最后一步（Python环境可能出错） pyspark脚本默认调用的是 &quot;python&quot; 这个名， 而ubuntu默认只有&quot;python&quot; 和 &quot;python3&quot;。 所以我们需要做如下软连接，来使得可以输入python， 直接寻找python3.7命令（不要用alias） ln -s /usr/bin/python3.7 /usr/bin/python 测试 服务端直接输入命令： pyspark 或远程浏览器输入: http://192.xx.xx.xx:4040/jobs 远程使用Jupyter连接 安装 Jupyter Notebook: pip3 install jupyter # 若新环境，需要安pip: apt-get install python3-pip pip 安装 findspark 和 pyspark pip install pyspark pip install findspark （Linux服务端） 启动 Jupyter Notebook 服务（--ip指定 0.0.0.0），(--allow-root若不加上可能会报错) jupyter notebook --allow-root --ip 0.0.0.0 （Linux服务端） 下面说的是Jupyter Notebook 客户端（Windows10） 下面两行findspark代码必须放在每个py脚本的第一行 import findspark findspark.init() PYSPARK_PYTHON = &quot;/usr/bin/python&quot; os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHON os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHON 然后才可正常写其他代码 from pyspark import SparkConf, SparkContext sc = SparkContext( master='local[*]', # 下面会讲这个参数 appName='myPyspark', # 随便起名 ) # 这句话，就把 spark启动起来了，然后才可以通过浏览器访问了。 4040 # 如果你 python魔法玩的6，那么提到上下文，你应该会自动想到 with语句 （__enter__,__exit__） # 不写参数，本地运行，这样也是可以的， sc = SparkContext() raw_data = [1,2,3] rdd_data = sc.parallelize(raw_data) # python列表类型 转 spark的RDD print(rdd_data) raw_data = rdd_data.collect() # spark的RDD 转回到 python列表类型 print(raw_data) sc.stop() # 关闭spark, 同理，浏览器也就访问不到了。 解释 SparkContext 的 master参数： &quot;local&quot; : 表示只用一个线程，本地运行。 &quot;local[*]&quot; : 表示用(cpu的个数)个线程，本地运行。 &quot;local[n]&quot; : 表示用n个线程，本地运行。 &quot;spark://ip:host&quot; : 连其他集群 回顾环境问题 并 解释 &quot;本地&quot; 的概念： 在 Linux 中 安装了 Spark全套环境。 在 Linux 中 安装了 Jupyter， 并启动了 Jupyter Notebook 服务。 在 Win10 中 远程连接 Linux中的 &quot;Jupyter Notebook&quot;服务 写业务代码（相当于客户端连接） 所以， 之前所说的 &quot;本地&quot;, 这个词归根结底是相对于 Linux来说的，我们写代码一直操作的是Linux。 通常使用spark-submit 首先：我们自己编写一个包含各种 pyspark-API 的 xx.py 脚本 如果：你用了我上面推荐的 Jupyter Notebook，你会发现文件是.ipynb格式，可以轻松转.py 最后提交py脚本： spark-submit --master local[*] --name myspark /xx/xx/myspark.py # 你会发现 --master 和 --name 就是上面我们代码中配置的选项，对号入座写入即可。 # /xx/xx/myspark.py 就是 py脚本的绝对路径。 喂给spark，让他去执行。即可。 Standalone部署Spark 介绍： Standalone部署需要同时启动： master端 slave 端 按着下面配置，最后一条 ./start-all.sh 即可同时启动。 查看 JAVA_HOME环境变量。 echo $JAVA_HOME # 记住结果，复制出来 进入conf目录，做一些配置（conf和spark中的bin目录同级）： cp spark-env.sh.template spark-env.sh vi spark-env.sh：（里面写） JAVA_HOME=上面的结果 cp slaves.template slaves vi slaves: （localhost改成本机机器名） lin 上面配置完毕后，进入sbin目录（和上面的conf在一个目录中） ./start-all.sh # 启动 # 若提示权限错误，继续往下看（支线） sudo passwd root # 激活ubuntu的root用户，并设置密码 vi /etc/ssh/sshd_config： PermitRootLogin yes # 任意位置添加这个（此选项之前可能是存在的注释后再添加） service ssh restart 启动没报错，会给你弹出一条绝对路径的日志文件 xxx cat xxx # 即可看见启动状态 ，各种日志信息 其中有几条信息: Successfully started service 'WorkerUI' on port 8082 （浏览器访问 8082端口） Successfully registered with master spark://lin:7077 （代码上下文访问） 其中，有些信息可能未打印出来： 建议浏览器中：( 8080-8082 )端口都可以尝试一下。 输入命令，查看启动状态： jps # 若同时有 worker 和 master 说明启动成功 测试： pyspark --master spark://lin:7077 # WebUI 的 Worker端，就可看见有 一个Job被添加了进来 YARN部署Spark 配置： echo $HADOOP_HOME # 我的是 /home/lin/hadoop/hadoop-2.7.7 进入spark解压包的路径的 conf 目录中: vi spark-env.sh: ( etc/hadoop前面就是刚才 echo出来的， etc/hadoop大家都是一样的) HADOOP_CONF_DIR=/home/lin/hadoop/hadoop-2.7.7/etc/hadoop 启动spark： spark-submit --master yarn --name myspark script/myspark.py # 注意 --master 的值改成了 yarn ， 其他不变。 或者你可以： pyspark --master yarn 看到启动成功，说明配置成功 Spark历史服务配置 痛点：有时我们的spark上下文 stop后，WebUI就不可访问了。 若有未完成，或者历史信息, 也就看不到了。 这时我们配置 history 服务就可在 context stop后，仍可查看 未完成job。 预新建一个HDFS目录myhistory (根路径下),下面用得到： hadoop fs -mkdir /myhistory 首先，进入 spark解压包的 conf目录下： cp spark-defaults.conf.template spark-defaults.conf vi spark-defaults.conf: (解开如下注释, lin本机名称, 放在HDFS的根路径下的myhistory) spark.eventLog.enabled true spark.eventLog.dir hdfs://lin:8020/myhistory vi spark-env.sh: (我们之前 把template 复制过一次，所以这次直接编辑即可) SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://lin:8020/myhistory&quot; 启动（进入 spark解压包的 sbin目录下）： ./start-history-server.sh # cat 输入的信息（日志文件）。 即可查看是否启动成功 # WebUI默认是 ：http://192.168.0.108:18080/ 测试： 浏览器中访问History WebUI： http://192.168.0.108:18080/ 发现啥也没有： 这是正常的，因为我们还没运行 spark context主程序脚本。 --------------------------------------------------------------------- 运行spark-context主程序脚本： spark-submit script/myspark.py # 这个脚本是随便写的，没什么意义。 不过里面有个我们常用的一个注意事项！！！ # 我的这个脚本的 context 用完，被我 stop了 # 所以我们访问不到它的运行状态的 Spark Context 的 WebUI # 但是我们刚才辛辛苦苦配置Spark history 服务，并启动了它。 # 所以 context的信息，被写入了我们刚才配置的 Spark history 中 # 所以 我们再次访问 Spark history WebUI 即可看到有内容被写入进来。 --------------------------------------------------------------------- 再次访问History WebUI： http://192.168.0.108:18080/ 你就会发现，里面有内容了(spark history服务已经为我们干活了)~~~~ 免密码登录 环境Ubuntu(CentOS应该也可，很少用) 免密码登录设置： cd ~ ssh-keygen -t rsa -P &quot;&quot; cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys chmod 600 .ssh/authorized_keys 注意几种情况： 你如果是root用户，那么你需要切换到 /root/ 执行上面的命令 如果是普通用户， 那么你需要切换到 /home/xxx/ 执行上面的命令 这个要特别注意一下，有时候用 sudo -s ,路径是没有自动切换的。 需要我们自己手动切换一下 &quot;家&quot; 路径 自定义脚本启动服务 下面内容仅供个人方便， shell不熟，用py脚本了, 你随意。 vi start.py: (此脚本用于启动上面配置的 HDFS,YARN,SparkHistory 和 Jupyter Notebook) import os import subprocess as sub ###### 启动 HDFS + YARN ############### hadoop_path = os.environ['HADOOP_HOME'] hadoop_sbin = os.path.join(hadoop_path, 'sbin') os.chdir(hadoop_sbin) sub.run('./start-dfs.sh') sub.run('./start-yarn.sh') ###### 启动 SparkHistory ############## spark_path = os.environ['SPARK_HOME'] spark_sbin = os.path.join(spark_path, 'sbin') os.chdir(spark_sbin) sub.run('./start-history-server.sh') ###### 启动 Jupyter Notebook ############### # home_path = os.environ['HOME'] home_path = '/home/lin' os.chdir(home_path) sub.run('jupyter notebook --allow-root --ip 0.0.0.0'.split()) 之后每次重启，就不用进入每个目录去启动了。直接一条命令： sudo python start.py nohup hive --service metastore &amp; 查看本脚本启动相关的WebUI： HDFS: http://192.168.0.108:50070/ YARN: http://192.168.0.108:8088/ SparkHistory: http://192.168.0.108:18080/ 另附其他 WebUI： spark: http://192.168.0.108:4040/ standalone启动指定的端口(如果你使用的 standalone方式，而不是local,可能用到如下端口): pyspark --master spark://lin:7077 ","link":"https://cythonlin.github.io/post/py-greater-ubuntu-hadoop-yarn-hdfs-hive-spark-an-zhuang-pei-zhi/"},{"title":"AI => Seq2Seq+Attention+Transformer(简)","content":"数据预处理（TF20-Keras-Preprocessing） 我们自己的普通数据集（常用） 主要使用tensorflow.keras.preprocessing这个库中的（image, text，sequence）这三个模块。 text： 可以用来 （统计词频，分字，word_2_id, id_2_word等操作。） sequence 可以（给句子做结构化操作（填充0，裁剪长度）） from tensorflow.keras.preprocessing.text import Tokenizer # 主干，句子编码 from tensorflow.keras.preprocessing.sequence import pad_sequences # 辅助，填充，剪枝 q1 = '欢 迎 你 你 你' q2 = '我 很 好' q_list = [q1,q2] # 需要特别注意，因为此API对英文友好，所以，我们必须把句子用 空格 隔开输入 token = Tokenizer( num_words=2, # num_words代表设置过滤num_words-1个词频， 例如num_words=2， # 那么过滤掉2-1=1个词频， 所以一会你会看到下面词频为1的都被过滤掉了 ) # 这里面参数很多，还有标点符号过滤器等 token.fit_on_texts(q_list) # 把原始句子集合，放进去拟合一下（封装成一个类） print(token.document_count) # 2 # 句子个数 print(token.word_index) # {'你': 1, '欢': 2, '迎': 3, '我': 4, '很': 5, '好': 6} # word_2_id print(token.index_word) # {1: '你', 2: '欢', 3: '迎', 4: '我', 5: '很', 6: '好'} # id_2_word print(token.word_counts) # OrderedDict([('欢', 1), ('迎', 1), ('你', 3), ('我', 1), ('很', 1), ('好', 1)]) # 统计词频 seq = token.texts_to_sequences(q_list) # 先把所有的输入，变成一一变成编码化 print(seq) # [[1, 1, 1], []] # 会不会好奇？数据怎么没了？因为我们上面设置了过滤词频为1的都过滤了 pad_seq = pad_sequences( seq, # 输入编码化后的 句子 maxlen=2, # 统一句子最大长度 padding='pre', # 不足的补0， 从前面补0， （也可以用 post，代表后面） truncating='pre' # 多余的长度裁剪，从前面裁剪 ) print(pad_seq) # 打印一下我们填充后的句子形状。 # [ # [1 1], # 如你所愿,最大长度为2，[1,1,1] 已经裁剪成了 [1,1] # [0 0], # 如你所愿，之前[] ，已经都填满了0 # ] 虽然我们用不到 image这个模块数据增强模块，但是我把了解的API也写出来。 train_datagen = keras.preprocessing.image.ImageDataGenerator( # 数据增强生成器（定义） rescale=1. / 255, # 数据归一化 rotation_range = 40, # -40-40 随机角度 （数据增强） width_shift_range = 0.2, # 宽度位移（0-20%随机选个比例去平移） （数据增强） height_shift_range = 0.2, # 高度位移（同上） （数据增强） shear_range=0.2, # 图片剪切（0.2） （数据增强） zoom_range=0.2, # 图片缩放（0.2） （数据增强） horizontal_flip=True, # 图片随机水平反转 （数据增强） fill_mode='nearest', # 图片填充像素（放大后失帧）用附近像素值来填充 （数据增强） ) # train_generator = train_datagen.flow_from_dataframe() # 如果你用Pandas，你可以选这个 train_generator = train_datagen.flow_from_directory( # 从文件中读取（Kaggle） train_dir, # 图片目录 target_size = (height, width), # 图片读取进来后缩放大小 batch_size = batch_size, # 就是批次 seed=6, # 随机化种子 shuffle=True, # 样本随机打散训练，增强模型泛化能力 class_mode='categorical', # label格式，是否需要one_hot， 是 ) ... ... train_num = train_generator.samples # 打印样本形状 history = model.fit_generator( # 注意我们上面是用的数据生成器，所以这要用 fit_generator train_generator, steps_per_epoch=train_num//batch_size, # 每个epoch多少 step(因为数据增强API是生成器方式，所以需要自己手动计算一下) epochs=epochs, validation_data=valid_generator, # 如果你有验证集，你也可以用这个。否则可以不用 validation_steps=valid_num//batch_size # 同上 ) Seq2Seq 思想 语言不同，那么我们可以搭建桥梁。 即使我们表面上不相同。 但是我们映射到这个桥梁上的结果是几乎类似的。 样本句子长度统一 为什么每个句子的长度需要统一？ 因为，每个句子for循环操作会很耗算力， 而转化为矩阵/向量化操作，会节约太多算力。 因为矩阵运算严格要求样本的形状，所以每个句子的长度需要一致 如何做到句子长度统一？ 填0， 对应TF操作就是padding， 不过TF20 的keras预处理包中已经有 成品的数据统一化操作。 并且还具有 word_2_id，词向量编码操作。 组成 编码器 （输入每条样本句子的每个单词， 编码器的最后一个RNN单元，浓缩了整个句子的信息） 中间向量 （作为中间特征桥梁， 用来保存，输入进来的整个句子） 解码器 （中间向量作为解码器第一个RNN单元的输入，而每个单元的输出y,作为下一个单元的输入） 其中解码器部分的输出y会用 softmax 对 词库（词典）求多分类概率。 然后求损失（MSE或者CrossEntropy） 注意了： softmax求出的概率该如何选择，这是个问题: 假如: 每个单元的输出y的概率都取最大值, 那么可能一步错，步步错。 太极端了（贪心搜索） 接下来，聊一聊一周 集束搜索的算法 BeamSearch BeamSearch 由于贪心搜索（只取概率的一个最大值，的结果不尽人意。所以 BeamSearch来啦） BeamSearch的主要思想: 只取一个太冒险了，所以: BeamSearch 取每个经过softmax输出概率集合的 Top-N个 Top-N: 的 N 代表你保留几个概率 （举一反三理解: 贪心算法就是 Top-1） 假如我们取Top-3个 那么你一个RNN节点的预测y将会保留3个概率值， 并将这3个概率值作为 下一个节点的输入。 具体流程看:下图 (可能有点丑) 然后，我们会选择出: 3 个 &quot;红线&quot; 最优路径。 最终: 我们通过单独的语言模型，来从这 3 个 &quot;红线&quot; 较优路径中，选出一个 最优路径。 Attention(注意力机制) 前情回顾 Seq2Seq 的 Encoder部分虽然用的是 高效的 LSTM，并且也很好的解决了，记忆的问题。 但是他不能很好的解决每个单词的权重分配问题。 虽然: Encoder的所有单元都会通过LSTM的记忆传递， 输入进“中间桥梁向量”。 但是: 还是有&quot;偏心&quot;成分, 最后一个LSTM单元信息一定是最浓的。 （新鲜的，热乎的） 所以: 你第1个LSTM单元的信息，或者说前面的LSTM单元的信息，这些记忆到最后可能会被稀释。 为了解决上面的问题, Attention就出来帮忙了~~~ Attentioin原理 我觉得墨迹半天不如自己画一张图~~~ （只会mspaint画图） 上图中计算权重那里&quot;通过一个函数，可以是求相似度&quot;， 我简写了。 其实有两种常用的方式： Bahdanau注意力: weight = FC层( tanh ( FC层(Encoder的每个输出y) + FC层(Decoder的一个H) ) ) luong注意力: weight = Encoder的每个输出y @ W随机权重矩阵 @ Decoder的一个H # @是TF20的矩阵乘法操作符 无论使用上面哪种: 都要套一层 Softmax weight = softmax(weight, axis=1) 注意力向量C = sum( weight * Encoder的每个输出y , axis=1) # 加权求和，最终得到一个向量 Decoder的下一个输入 = concat( 注意力向量C, 上一个预测y4 ) Transformer 第一印象挑明： 他是一种无RNN的一种特殊的 Seq2Seq 模型。 RNN-LSTM-GRU虽然这些NN的主要特色就是&quot;时间序列&quot;。（缺点：慢，记忆弥散） 但是我们上面说了，要想取得好的效果。那么需要加Attention。 于是有人想到了，既然Attention效果这么好，为什么不直接用Attention呢？ Attention效果虽好，关联性强，但是它不能保证时间序列模式。 于是后来出现了 Transformer。（既能保证记忆注意力，又能保证时间序列）。具体如下！ Transformer整体结构组成 Self-Attention self-attention原理就是各种链式矩阵乘法（并行计算，可用GPU加速） self-attention计算过程如下：（假设输入句子切分单词为：矩阵X = [&quot;早&quot;,&quot;上&quot;,&quot;好&quot;]） 矩阵X @ 权重矩阵Q（Q1，Q2，Q3）=&gt; Q矩阵（Q1，Q2，Q3） 矩阵X @ 权重矩阵K（Q1，Q2，Q3）=&gt; K矩阵（Q1，Q2，Q3） 矩阵X @ 权重矩阵V（Q1，Q2，Q3）=&gt; V矩阵（Q1，Q2，Q3） α = softmax( (Q矩阵 @ K矩阵) / q^0.5 ) self_attention = α @ V矩阵 # 单词1 = Q1*K1*V1 + Q1*K2*V2 + Q1*k3*V3 # 用自己的Q，查别人的KV，加权求和，最终得出的就是自己（自身单词的注意力） Multi-Head Self-Attention Multi-Head Attention 对 Self-Attention 对了如下扩展： self-attention: 一组 Q矩阵，K矩阵，V矩阵 Multi-Head Self-Attention: 多组 Q矩阵，K矩阵，V矩阵 扩张为多头注意力的过程： Q @ W ====&gt; [Q1, Q2, Q3] K @ W ====&gt; [K1, K2, K3] V @ W ====&gt; [V1, V2, V3] 可理解为，多个卷积核的意思能提取不同特征的意思。 Position Encoder 上述的self-attention有个问题， 我们没有用到RNN等序列NN，那么矩阵相乘的过程中。 单词的计算顺序可能是不同的。 那么如何保证让他们位置有条不紊？ 可以使用位置编码，融入到Embedding，形成带有时间序列性质的模型。 可自行查找计算位置编码的博文。 传送门 至于Transformer，现在官方已经有TF20和Pytorch的库了。 传送门如下。 https://github.com/huggingface/transformers Transformer延申的各种模型，像Bert等也有可调用的API https://huggingface.co/transformers/ ","link":"https://cythonlin.github.io/post/ai-greater-seq2seqattentiontransformerjian/"},{"title":"AI => Pytorch与Tersorflow2.0简单对比","content":"前言 目前一些模型API尚未迁移到TF20中。 eg: CRF，Seq2Seq等 如果退回TF10，有些伤。 倒不如转至Torch。 Pytorch的大部分思想和TF20大致相似。 至于安装，GPU我前面说过TF20。这里不赘述。 官档安装：https://pytorch.org/get-started/locally/#start-locally 注意 本文几乎通篇以代码案例 和 注释标注 的方式解释API。(模型的训练效果不做考虑。只看语法) 你如果懂Tensorflow2.0（Stable），那么你看本文一定不费劲。 Torch和TF20 很像！！！ 因此一些地方，我会列出 TF20 与 Torch的细节对比。 开门案例1-MNIST 模块导入 import torch from torch import nn, optim from torchvision import datasets, transforms from torch.utils.data import DataLoader 数据预处理 data_preprocess = transforms.Compose([ # 顶预定数据处理函数，类似map()里的函数句柄 transforms.Resize(28,28), # 变形 transforms.ToTensor(), # numpy 转 Tensor ]) trian_dataset = datasets.MNIST( # TF20在keras.datasets中,未归一化（0-255） '.', # 下载至当前目录， （图片0-1，已经被归一化了） train=True, # train=True， 代表直接给你切出 训练集 download=True, # True，若未下载，则先下载 transform=data_preprocess, # 指定数据预处理函数。第一行我们指定的 ) test_dataset = datasets.MNIST( '.', train=False, # False代表测试集 # 就说下这里， False代表 给你切出测试集 download=True, transform=data_preprocess, ) train = DataLoader( # 对应TF20中的 tf.data.Dataset对数据二次预处理（分批，乱序） trian_dataset, # 把上面第一次预处理的数据集 加载进来 batch_size=16, # mini-batch shuffle=True, # 乱序，增强模型泛化能力 ) test = DataLoader( test_dataset, batch_size=16, shuffle=True, ) MNIST模型（定义-训练代码） # 模型定义部分 class MyModel(nn.Module): # TF20是 tk.models.Model def __init__(self): # TF20 也是 __init__() super().__init__() self.model = nn.Sequential( # tk.models.Sequential , 并且 TF里面 需要加一个 [] nn.Linear(28*28, 256), # tk.layers.Dense(256) nn.ReLU(), # tk.layers.Relu() nn.Linear(256, 128), # tk.layers.Dense(128) nn.ReLU(), nn.Linear(128, 10), # tk.layers.Dense(10) ) def forward(self, x): # TF20是 __call__() x = x.view( x.size(0), 28*28 ) # x.view ==&gt; tf.reshape x.size ==&gt; x.shape[0] y_predict = self.model(x) return y_predict # -------------------------------华丽分割线--------------------------------- # 模型训练部分 def main(): vis = visdom.Visdom() model = MyModel() loss_ = nn.CrossEntropyLoss() # 会将 y_predict自动加一层 softmax optimizer = optim.Adam(model.parameters()) # TF20: model.trainable_variables # visdom可视化 # 这步是初始化坐标点，下面loss会用这个直接更新 vis.line( [0], # x坐标 [0], # y坐标 win='loss', # 窗口名称 opts={'title': 'loss'}, # 窗口标题 ) for epoch in range(10): # epochs for step, (x, y_true) in enumerate(train): y_predict = model(x) loss = loss_(y_predict, y_true) optimizer.zero_grad() # 优化器清零 loss.backward() # 梯度计算 optimizer.step() # 梯度下降更新 tp.gradient(loss, variables)。 # 在上面的定义的基础上更新追加画点-连成线 vis.line( [loss.item()], [step], win='loss', update='append', # 追加画点，而不是更新覆盖 ) print(loss.item()) # .item() =&gt; 相当于 tensorflow 的 numpy() if epoch % 2 == 0: total_correct_samples = 0 # 用于记录（预测正确的样本的 总数量） total_samples = 0 # 用于记录（样本的 总数量） for x_test, y_test in test: y_pred = model(x_test) y_final_pred = y_pred.argmax(dim=1) # TF20的坐标轴参数是 axis # 每一批是 batch_size=16，我们要把它们都加在一起 total_correct_samples += torch.eq(y_final_pred, y_test).float().sum().item() # 这里提一下 eq() 和 equal() 的返回值的区别， 自己看，我们通常用 eq # print( torch.equal( torch.Tensor([[1,2,3]]), torch.Tensor([[4,5,6]] ) ) ) #结果: False # print( torch.eq( torch.Tensor([[1,2,3]]), torch.Tensor([[4,5,6]] ) ) ) #结果: tensor([[0, 0, 0]], dtype=torch.uint8) per_sample = x_test.size(0) # 再说一次， size(0) 相当于TF xx.shape[0] # 获取每批次样本数量, 虽然我们知道是 16 # 但是最后一个batch_size 可能不是16，所以要准确获取。 total_samples += per_sample acc = total_correct_samples / total_samples print(f'epoch: {epoch}, loss: {loss}, acc: {acc}') # 测试部分 vis.line( [acc], [step], win='acc', update='append', # 追加画点，而不是更新覆盖 ) x, label = iter(test).next() target_predict = model(x).argmax(dim=1) # 画出测试集图片 viz.images(x, nrow=16, win=&quot;test_x&quot;, opts={'title': &quot;test_x&quot;}) vis.text( # 显示预测标签文本 str(target_predict.detach().numpy() ), win = 'target_predict', opts = {&quot;title&quot;: target_predict} ) vis.text( # 显示真值文本 str(label.detach().numpy() ), win = 'target_true', opts = {&quot;target_true&quot;: target_predict} ) main() 模型可视化（visdom) 安装 和 运行 和 使用 安装 pip install visdom 运行 python -m visdom.server （第一次可能会有点慢） # 语法和Tensorboard很像 使用 import visdom 见上代码 vis.xxxxx 案例2-CIFAR10+CNN 说明 模块导入和数据预处理部分和案例1的 MNIST一模一样。 只要稍稍修改 datasets.MNIST ==&gt; datasets.CIFAR10 即可， 简单的不忍直视~~ 代码如下： 模型定义部分： class MyModel(nn.Module): # 温馨提示， 这是 Mmodule, 不是model def init(self): &quot;&quot;&quot; 先注明一下： TF中输入图片形状为 (样本数, 高，宽，图片通道) PyTorch中输入图片形状为 (样本数, 图片通道，高，宽) &quot;&quot;&quot; super().__init__() self.conv = nn.Sequential( # 再强调一遍，没有 [] nn.Conv2d( in_channels=3, # 对应TF 图片通道数（或者上一层通道） out_channels=8, # 对应TF filters, 卷积核数量 kernel_size=3, # 卷积核大小 stride=1, # 步长, TF 是 strides， 特别注意 padding=0, # no padding, 默认 ), nn.ReLU(), nn.MaxPool2d( kernel_size=3, # 滑动窗口大小 stride=None, # 默认为None， 意为和 kernel_size相同大小 ), nn.Conv2d( in_channels=8, # 对应TF 图片通道数（或者上一层通道） out_channels=16, # 对应TF filters, 卷积核数量 kernel_size=3, # 卷积核大小 stride=1, # 步长, TF 是 strides， 特别注意 padding=0, # no padding, 默认 ), nn.ReLU(), nn.MaxPool2d( kernel_size=2, # 滑动窗口大小 stride=None, # 默认为None， 意为和 kernel_size相同大小 ), ) self.dense = nn.Sequential( nn.Linear(16*4*4, 128), # 对应TF Dense nn.Linear(128, 64), nn.Linear(64, 10), ) def forward(self, x): conv_output = self.conv(x) # (16, 16, 4.4) conv_output_reshape = conv_output.view(-1, 16*4*4) dense_output = self.dense(conv_output_reshape) return dense_output 模型训练（模型调用+模型训练的定义） def main(): vis = visdom.Visdom() epochs = 100 device = torch.device('cuda') # 预定义 GPU 槽位（一会往里面塞 模型和数据。） model = MyModel().to(device) # 模型转为 GPU 计算 # CrossEntropyLoss 会自动把下面的 dense_output ，也就是y_predict 加一层 softmax loss_ = nn.CrossEntropyLoss().to(device) optimizer = optim.Adam( model.parameters() ) for epoch in range(epochs): for step, (x_train, y_train) in enumerate(train): x_train, y_train = x_train.to(device), y_train.to(device) dense_output = model(x_train) loss = loss_(dense_output, y_train) optimizer.zero_grad() # 上一个例子提到过，梯度清零 loss.backward() # 反向传播， 并将梯度累加到 optimizer中 optimizer.step() # 相当于做了 w = w - lr * 梯度 print(loss.item()) # item() 意思就是 tensor转numpy,TF中的 API是 xx.numpy() sample_correct_numbers = 0 sample_total_numbers = 0 with torch.no_grad(): # 测试部分不需要计算梯度，因此可以包裹在上下文中。 for x_test, y_test in test: x_test, y_test = x_test.to(device), y_test.to(device) # softmax 的 y_predict 与 y_test的 one-hot做交叉熵 y_predict = model(x_test).argmax(dim=1) sample_correct_numbers += torch.eq(y_predict, y_test).float().sum().item() sample_total_numbers += x_test.size(0) # 每批样本的总数加在一起 acc = sample_correct_numbers / sample_total_numbers print(acc) main() 案例3：CIFAR10+ResNet-18 ###结构图体系： 上述结构说明： 1conv + (2+2+2+2)*2 + 1 fc = 18层 1conv + (3+4+6+3)*2 + 1 fc = 34层 1conv + (3+4+6+3)*3 + 1 fc = 50层 1conv + (3+4+23+3)*3 + 1 fc = 101层 1conv + (3+8+36+3)*3 + 1 fc = 152层 代码实现 模块导入 import cv2 import torch from torch import nn, optim from torchvision import datasets, transforms from torch.utils.data import DataLoader import visdom import torch.nn.functional as F 数据导入预处理 data_preprocess = transforms.Compose([ transforms.Resize(32,32), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]) train_dataset = datasets.CIFAR10( '.', train=True, download=True, transform=data_preprocess, ) test_dataset = datasets.CIFAR10( '.', train=False, # False代表测试集 download=True, transform=data_preprocess, ) train = DataLoader( train_dataset, batch_size=16, shuffle=True, ) test = DataLoader( test_dataset, batch_size=16, shuffle=True, ) 基础块定义（BasicBlock）： class BasicBlock(nn.Module): &quot;&quot;&quot;单个残差块 2个卷积+2个BN&quot;&quot;&quot; def init(self, input_channel, output_channel, stride=1): super().init() self.major = nn.Sequential( # 第一个Conv的步长为指定步长，允许降采样，允许输出输出通道不一致 nn.Conv2d(input_channel,output_channel,kernel_size=3,stride=stride, padding=1), nn.BatchNorm2d(output_channel), nn.ReLU(inplace=True), # 第二个Conv的步长为定长1， 输入输出通道不变（缓冲输出） nn.Conv2d(output_channel, output_channel, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(output_channel), # 第二个Conv就不用ReLU了， 因为一会需要和 x加在一起，最后最一层大的Relu ) # 若输入通道==输出通道，且步长为1，意味着图片未被降采样，则残差网络课直接为普通网络 self.shortcut = nn.Sequential() # 若输入输出通道不匹配，这时需要将图片做同样的变换，才能加在一起。 if input_channel != output_channel or stride != 1: self.shortcut = nn.Sequential( nn.Conv2d( input_channel, output_channel, kernel_size=(1,1), stride = stride ), nn.BatchNorm2d(output_channel) ) def forward(self, x): major_out = self.major(x) # 主干网络的输出 shotcut_out = self.shortcut(x) # 残差网络的输出 # 上面这两个网络是平行的关系， 因为 它们的输出不是链式的， 而是 都是同样的 x。 # 拼接主干网络+残差网络，F 相当于TF20的 tf.nn 里面单独有各种 loss函数 return F.relu(major_out + shotcut_out) # 最后在拼接后的网络外面加一层relu ResNet+ResBlock定义： class ResNet(nn.Module): def init(self, layers): # layers用来接受，用户想要指定 ResNet的形状 super().init() self.conv1 = nn.Sequential( nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True), ) self.res_net = nn.Sequential( *self.ResBlock(32,64, layers[0],stride=2), # 16 *self.ResBlock(64,128, layers[1],stride=2), # 8 *self.ResBlock(128,256, layers[2],stride=2), # 4 *self.ResBlock(256,512, layers[3],stride=2), # 2 ) # 因为我们一会需要展平，里面填&quot;通道*宽度*高度&quot;, &quot;输出通道&quot; self.dense = nn.Linear(512 * 2 * 2, 10) def forward(self, x): out = self.conv1(x) out = self.res_net(out) out = out.view(x.size(0), -1)# 卷积展平操作 ， torch中没有flatten所以我们就得手工 out = self.dense(out) return out def ResBlock(self, input_channel, output_channel, block_nums=2, stride=2): # 自定义规定，第一个block缩小的(对应通道翻倍)，其余block大小不变 # 通道翻倍，步长*2，特征减半 all_block = [BasicBlock(input_channel, output_channel,stride=stride)] for x in range(1,block_nums): all_block.append(BasicBlock(output_channel, output_channel,stride=1)) return all_block # resnet = ResNet(layers=[2,2,2,2]) # out = resnet(torch.randn(4,3,32,32)) # print(out.shape) 模型训练： def main(): vis = visdom.Visdom() epochs = 5 device = torch.device('cuda') model = ResNet(layers=[2,2,2,2]).to(device) # 会自动把下面的 dense_output ，也就是y_predict 加一层 softmax，y_true做one-hot loss_ = nn.CrossEntropyLoss().to(device) optimizer = optim.Adam( model.parameters(), lr=0.0001) for epoch in range(epochs): total_loss = 0.0 for step, (x_train, y_train) in enumerate(train): x_train, y_train = x_train.to(device), y_train.to(device) dense_output = model(x_train) loss = loss_(dense_output, y_train) optimizer.zero_grad() # 上一个例子提到过，梯度清零 loss.backward() # 反向传播， 并将梯度累加到 optimizer中 optimizer.step() # 相当于做了 w = w - lr * 梯度 total_loss += loss.item() # item()就是 tensor转numpy, TF中的 API是 xx.numpy() if step % 50 == 49: print('epoch:',epoch, 'loss:', total_loss / step) sample_correct_numbers = 0 sample_total_numbers = 0 with torch.no_grad(): # 测试部分不需要计算梯度，因此可以包裹在上下文中。 for x_test, y_test in test: x_test, y_test = x_test.to(device), y_test.to(device) # softmax 的 y_predict 与 y_test的 one-hot做交叉熵 y_predict = model(x_test).argmax(dim=1) sample_correct_numbers += torch.eq(y_predict, y_test).float().sum().item() sample_total_numbers += x_test.size(0) # 每批样本的总数加在一起 acc = sample_correct_numbers / sample_total_numbers print(acc) torch.save(model, 'model.pkl') # 保存整个模型 main() 测试数据预处理(我随便在网上下载下来的 1 张图片)： # 这是Cifar-10数据的标准标签 label = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck'] plane = cv2.imread('plane.jpg') # 我用的opencv plane = cv2.cvtColor(plane, cv2.COLOR_BGR2RGB) # opencv读的数据格式是BGR，所以转为RGB plane = (plane - 127.5) / 127.5 # 二话不说，保持模型输入数据的概率分布，先做归一化 plane = cv2.resize(plane, (32,32)) # 图片缩小到32x32,和模型的输入保持一致 plane = torch.Tensor(plane) # 转换成 tensor plane = plane.view(1,32,32,3) # 增加一个维度 plane = plane.repeat(16,1,1,1) # 我就用一张图片，为了满足模型的形状16，我复制了16次 plane = plane.permute([0,3,1,2]) # 虽然torch也有 像TF那样的transpose，但是只能操作2D device = torch.device('cuda') # 先定义一个cuda设备对象 plane = plane.to(device) # 我们训练集用的cuda， 所以预测数据也要转为cuda 正式输入模型预测： model = torch.load('model.pkl') # 读取出 我们训练到最后整个模型 # 说明一下，如果你的预测是另一个脚本中，class ResNet 的代码定义部分也要复制过来 out = model(plane) # 预测结果，形状为[16,10] 16个样本，10个预测概率， label_indexes = out.argmax(dim=1) # 取10个概率最大值的索引。 （1轴），形状为 [16,1] print(label_indexes) for i in label_indexes: # i为每个样本预测的最大概率值 的 索引位置。 print(label[i]) # 拿着预测标签的索引 去 真实标签中找到真实标签 ","link":"https://cythonlin.github.io/post/ai-greater-pytorch-yu-tersorflow20-jian-dan-dui-bi/"},{"title":"AI => Tensorflow2.0语法 - keras_API的使用(三)","content":"前言 keras接口大都实现了 _call_ 方法。 母类 _call_ 调用了 call()。 因此下面说的几乎所有模型/网络层 都可以在定义后，直接像函数一样调用。 eg: 模型对象(参数) 网络层对象(参数) 我们还可以实现继承模板 导入 from tensorflow import keras metrics (统计平均) 里面有各种度量值的接口 如：二分类、多分类交叉熵损失容器，MSE、MAE的损失值容器， Accuracy精确率容器等。 下面以Accuracy伪码为例： acc_meter = keras.metrics.Accuracy() # 建立一个容器 for _ in epoches: for _ in batches: y = ... y_predict = ... acc_meter.update_state(y, y_predict) # 每次扔进去数据，容器都会自动计算accuracy，并储存 if times % 100 == 0: # 一百次一输出, 设置一个阈值/阀门 print(acc_meter.result().numpy()) # 取出容器内所有储存的数据的，均值准确率 acc_meter。reset_states() # 容器缓存清空， 下一epoch从头计数。 激活函数+损失函数+优化器 导入方式： keras.activations.relu() # 激活函数：以relu为例，还有很多 keras.losses.categorical_crossentropy() # 损失函数：以交叉熵为例，还有很多 keras.optimizers.SGD() # 优化器：以随机梯度下降优化器为例 keras.callbacks.EarlyStopping() # 回调函数： 以‘按指定条件提前暂停训练’回调为例 Sequential(继承自Model)属于模型 模型定义方式 定义方式1： model = keras.models.Sequential( [首层网络,第二层网络。。。] ) 定义方式1： model = keras.models.Sequential() model.add(首层网络) model.add(第二层网络) 模型相关回调配置 logdir = 'callbacks' if not os.path.exists(logdir): os.mkdir(logdir) save_model_file = os.path.join(logdir, 'mymodel.h5') callbacks = [ keras.callbacks.TensorBoard(logdir), # 写入tensorboard keras.callbacks.ModelCheckpoint(output_model_file, save_best_only=True), # 模型保存 keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3) # 按条件终止模型训练 # 验证集，每次都会提升，如果提升不动了，提升小于这个min_delta阈值，则会耐心等待5次。 # 5次过后，要是还提升这么点。就提前结束。 ] # 代码写在这里，如何传递调用， 下面 “模型相关量度配置” 会提到 模型相关量度配置：(（损失，优化器，准确率等) 说明，下面的各种量度属性，可通过字符串方式，也可通过上面讲的导入实例化对象方式。 model.compile( loss=&quot;sparse_categorical_crossentropy&quot;, # 损失函数，这是字符串方式 optimizer= keras.optimizers.SGD() # 这是实例化对象的方式，这种方式可以传参 metrics=['accuracy'] # 这项会在fit()时打印出来 ) # compile() 操作，没有真正的训练。 model.fit( x,y, epochs=10, # 反复训练 10 轮 validation_data = (x_valid,y_valid), # 把划分好的验证集放进来（fit时打印loss和val） validation_freq = 5, # 训练5次，验证一次。 可不传，默认为1。 callbacks=callbacks, # 指定回调函数， 请衔接上面‘模型相关回调配置’ ) # fit()才是真正的训练 模型 验证&amp;测试 一般我们会把 数据先分成三部分（如果用相同的数据，起不到测试和验证效果，参考考试作弊思想）： 训练集: （大批量，主体） 测试集: （模型所有训练结束后， 才用到） 验证集: （训练的过程种就用到） 说明1：（如何分离？） 它们的分离是需要（x,y）组合在一起的，如果手动实现，需要随机打散、zip等操作。 但我们可以通过 scikit-learn库，的 train_test_split() 方法来实现 （2次分隔） 可以使用 tf.split()来手动实现 具体分离案例：参考上一篇文章： https://segmentfault.com/a/1190000020447666 说明2：（为什么我们有了测试集，还需要验证集？） 测试集是用来在最终，模型训练成型后（参数固定），进行测试，并且返回的是预测的结果值！！！！ 验证集是伴随着模型训练过程中而验证） 代码如下： loss, accuracy = model.evaluate( (x_test, y_test) ) # 度量， 注意，返回的是精度指标等 target = model.predict( (x_test, y_test) ) # 测试， 注意，返回的是 预测的结果！ 可用参数 model.trainable_variables # 返回模型中所有可训练的变量 # 使用场景： 就像我们之前说过的 gradient 中用到的 zip(求导结果, model.trainable_variables) 自定义Model Model相当于母版， 你继承了它，并实现对应方法，同样也能简便实现模型的定义。 自定义Layer 同Model， Layer也相当于母版， 你继承了它，并实现对应方法，同样也能简便实现网络层的定义。 模型保存与加载 ###方法1：之前callback说的 ###方法2：只保存weight(模型不完全一致) 保存： model = keras.Sequential([...]) ... model.fit() model.save_weights('weights.ckpt') 加载： 假如在另一个文件中。（当然要把保存的权重要复制到本地目录） model = keras.Sequential([...]) # 此模型构建必须和保存时候定义结构的一模一样的！ model.load_weights('weights.ckpt') model.evaluate(...) model.predict(...) ###方法3：保存整个模型（模型完全一致） 保存： model = keras.Sequential([...]) ... model.fit() model.save('model.h5') # 注意 这里变了，是 save 加载:（直接加载即可，不需要重新复原建模过程） 假如在另一个文件中。（当然要把保存的模型要复制到本地目录） model = keras.models.load_model('model.h5') # load_model是在 keras.models下 model.evaluate(...) model.predict(...) ###方法4：导出可供其他语言使用（工业化） 保存： （使用tf.saved_model模块） model = keras.Sequential([...]) ... model.fit() tf.saved_model.save(model, '目录') 加载：（使用tf.saved_model模块） model = tf.saved_model.load('目录') ","link":"https://cythonlin.github.io/post/ai-greater-tensorflow20-yu-fa-keras_api-de-shi-yong-san/"},{"title":"AI => 知识图谱之Neo4j-Cypher","content":"CRUD 创建 普通无属性创建（默认给你创建一个ID） create (p:person)-[:eat]-&gt;(f:food) 带有属性的创建（ {} ） create (p:person{name:'zhangsan'})-[:eat]-&gt;(f:food{name:'apple'}) 给两个孤独的实体创建关系： match (a:animal),(c:color) create （如果这里改为 merge 则是 “有则查询，无则创建”） (a)-[h:have]-&gt;(c) return h 对应查询： match (a:animal),(c:color) return a,c 删除 (delete) match (a:animal)-[h:have]-&gt;(c:color) delete a,h,c 更新修改（set） match (f:food) set f.age=20 查询 主体查询结构 match (p:)-[:关系名]-[别名2:实体名] return 别名1,别名2 普通条件查询1（whree） match (n:User) where n.name='Tom' return n 普通条件查询2：（ {} ） match (p:person{name:'zhangsan'})-[:eat{level:1}]-&gt;(f:food{name:'apple'}) return p,f 正则条件查询(~) match (n:User) where n.name=~'T.*' return n 包含条件查询（contains） match (n:User) where n.name contains 'T' return n 多度查询 match (t:teacher)-[]-(s:student)-[]-(ss:score) return t,s,ss # 注意1： [] 里面不写，代表所有关系 # 注意2： - 没有箭头，代表任意方向 # 注意3： 别名不可以重复指定， 所以我设置了 ss 多度关系： （通常是基于人脉来讲的） 1度关系：我 -&gt; 你 2度关系：我 -&gt; 你 -&gt; 他 理解技巧： 算几度关系时，把自己（节点）捂住不看， 然后剩下几个人员节点，就是几度关系 特别注意： 多度查关系时，比如你查 3度关系的结果。 neo4j的图可能会把， 2度关系也画出来，why? 因为他通过2度关系也可直接得出结果。 （可理解为 条条大路通罗马。） ！！！但是最终有效的返回路径只是你最初想要的 3度。 （2度就不算了） 查询最短路径： match (t:teacher), (s:student), p=shortestpath( (t)-[*..]-(s) ) return p # 注意： p= 之前有个逗号 ， 查询所有最短路径： match (t:teacher), (s:student), p=allshortestpaths( (t)-[*..]-(s) ) return p # 注意1： 前面多个 all ，后面多个s # 注意2： 所有最短路径的都会列出来。 人人平等~ 索引 创建索引 (create) create index on :food(name) # food为实体名，name为属性名， 同时注意这个 : 删除索引（drop） drop index on :food(name) 约束 创建约束 create constraint on (gf:girlfriend) assert (gf.name) is unique 删除约束 create constraint on (o:others) assert (o.name) is unique 聚合 统计个数（count） match ... return count(别名) 限制取多少条 （limit） match ... return 别名 limit 5 # 只取5条 知识图谱流程 数据抓取 知识模型设计 NER （远程监督） 关系抽取（Bootstrap） 知识推理 图谱存储（Neo4j Cypher） 检索/问答/推荐 实体抽取 BILSTM+CRF 关系抽取 Bootstrap方法： 1. 构建种子实体： &quot;猫&quot;， &quot;老鼠&quot;。 2. 寻找包含 &quot;猫&quot; &quot;老鼠&quot; 的 句子： 找到句子：&quot;猫和老鼠是好朋友&quot; 可抽取关系: 和...是好朋友 3. 拿着抽取的关系再次寻找新句子： 找到新句子：&quot;张三和里李四是好朋友&quot; 提取出新实体: &quot;张三&quot;, &quot;李四&quot; 4. 寻找包含 &quot;张三&quot; &quot;李四&quot; 的 句子： 找到句子：&quot;张三经常和李四一起玩&quot; 可抽取关系: 经常和... 一起玩 5. 拿着抽取的关系再次寻找新句子： 找到新句子：&quot;王五和赵六是好朋友&quot; 提取出新实体: &quot;王五&quot;, &quot;赵六&quot; ... ... 循环反复 ","link":"https://cythonlin.github.io/post/ai-greater-zhi-shi-tu-pu-zhi-neo4j-cypher/"},{"title":"AI => NER之BIO转BIOES","content":"BIO B: 命名实体的起始 或 单个字命名实体 I: 命名实体的中间位置 或 结束位置 O：非命名实体 BIOES B: 命名实体的起始标注（Only哦） I: 命名实体的中间标注（Only哦） E: 命名实体的结尾标注（Only哦） O: 非命名实体 S: 单个字命名实体 BIO转BIOES规则 个人代码实现 传送门：https://github.com/hacker-lin/bio2bioes/ ","link":"https://cythonlin.github.io/post/ai-greater-ner-zhi-bio-zhuan-bioes/"},{"title":"AI => 安装neo4j (Linux)","content":"依赖java,先装java 官网：https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 找到： jdk-8u231-linux-x64.tar.gz 下载（可能会比较慢，还需要注册） 个人网盘分享：https://pan.baidu.com/s/1EMFM_Y_HT3bHFwncKj2orw&amp;shfl=shareset 提取码: w24a 通过XFTP传送到服务器。 任意目录（自己记住即可）： tar -zxvf jdk-8u231-linux-x64.tar.gz # 解压出来一个目录，记住找个名字，和当前路径，下面用 # 我这里是 jdk1.8.0_181 相关配置: vi ~/.bashrc export JAVA_HOME=/root/kg/neo4j/java/jdk1.8.0_181/ export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib export PATH=${JAVA_HOME}/bin:$PATH 保存退出 激活配置： source ~/.bashrc 至此，java安装完成！ 安装Neo4j: 下载连接：https://neo4j.com/download-center/ 选个Linux版本的（需要上网）（记得下社区版的） 同样传到Linux解压即可。 个人网盘分享：https://pan.baidu.com/s/1FnbZW0n2-w8yCZEXG3J6zw&amp;shfl=shareset 提取码: 451c 因为我是云端运行。所以需要改下配置，支持远程访问。 进入解压后的目录，我这里是 neo4j-community-3.5.11 cd neo4j-community-3.5.11 cd conf vi neo4j.conf 添加如下配置（或者你能找到这行，直接解除注释也行）： dbms.connectors.default_listen_address=0.0.0.0 至此，neo4j安装完成。 启动neo4j: cd neo4j-community-3.5.11 cd bin ./neo4j console 看启动信息的url:port， 复制出来直接访问即可。 xxxxxxxx:7474 # 若是阿里云，需要放通防火墙7474 和 7687 成功进去后，会提示你输入默认用户名 和 密码： 默认用户名为 ：neo4j 默认密码也为 ：neo4j 密码忘记了？？ 进入 xx/data/dbms 删除 auth 文件，重启服务 ","link":"https://cythonlin.github.io/post/ai-greater-an-zhuang-neo4j-linux/"},{"title":"AI => CNN之kernel_size/strides/padding/same","content":"前言 先强调一点： 涉及到参数，都是针对Tensorflow来讲的。 本文主要说一些CNN最常用的几个参数。 以及各种组合情况下卷积之后计算形状的规律公式。 filters (必填， 这个本文不说) kernel_size （必填） strides (选填， 默认为 (1,1) ) padding （选填，默认为 valid） 这里先提一下，通用公式： (宽高都用这个公式，我只列出了高度的) h图片输出 = （h图片输入 - h卷积核 + 2padding） / strides + 1 也许你看起来没什么问题，但是Tensorflow中的API可不告诉你padding是几。 而是提供给你2个可选参数： padding='valid' (默认参数) # 代表不做padding，滑动不能越界，多余就裁剪。 padding='same' # same我会在下面展开说明。 无same情况 strides = 1 你完全可以按照上面的通用公式计算，但是通常情况下我会用另一个简便的公式： h图片输出 = h图片输入 - h卷积核 + 1 strides != 1 这个就得使用上面的通用公式了： h图片输出 = （h图片输入 - h卷积核 + 2padding） / strides + 1 有same情况 加了same，情况就比较复杂一些了。 same 就是 让你在不同的 strides滑动情况下，帮你把原图片虚拟补全。 使得filter在图片中完全滑动，(而不至于，最后剩下一点越界滑不到给无情的裁剪掉)。 strides = 1（重点） strides=1的情况下，如果你使用了 same。 那么same会帮你补充 padding, 使得你滑动后的特征形状与滑动之前的形状 &quot;保持一致&quot;。 补充的padding大小为: (敲黑板)： padding = h图片输入 % h卷积核 # 看清楚，是取模运算 举个例子，验证一下： 假如: 图片（输入） = 7 x 7, kernel_size = (5,5), strides = 1。 （通过我们上面给出的公式，你先求 padding = 7 % 5） ，很显然是2。 我们再用最开始讲的通用公式验证一遍： h输出长度 = (h输入 - h卷积核 + 2 * padding) / 1 + 1 = 7-5 + 2*2 + 1 = 7 巧了，我们的输入长度也是 7。 所以综上所述： padding = h图片输入 % h卷积核。 可以使得：输入形状 与卷积后的输出形状一致 strides != 1（重点） 当步长strides为多步时，如果你使用了 same。 敲黑板：那么same会帮你补充 padding, 并且卷积后的特征图缩小为原来的 strides倍。 补充的padding大小为: (敲黑板)： padding = h图片输入 % h卷积核 # 看清楚，是取模运算 我为了演示， 我突来灵感，随便找了一个 五子棋棋盘（笑了）， 见下图。 小技巧（有无same都适用） 当你预期想要把特征形状卷积之后形状大小&quot;成倍缩小&quot;时，你可以 &quot;同时&quot; 设置： kernel_size == strides == (n,n) # n就是缩小的倍数 说明，只要你把 kernel_size 和 strides 设置为相等，设置为n。那么图像一定会缩小n倍。 举个栗子： n设为3 kernel_size = (3,3) strides = (3,3) 那么: 你卷积出来的特征尺寸，一定比上一层缩小3倍。 结束语（聊一下Pooling2D） pooling的通用公式和 CNN中的是一模一样的。 Tensorflow 中 xxxPooling API参数如下： pool_size （选填，默认为 (2, 2) ） strides （选填，默认为 None） （这里你会疑问，None是什么鬼？？？） 我也不知道，API参数也没给说明。当我跳进源码，我发现如下代码，瞬间豁然开朗！ if strides is None: strides = pool_size （这应该就不用我解释了吧~） padding （选填，默认为 valid， 通常没必要用 same ） 我们上面说过小技巧： 卷积层中： kernel_size == strides == (n,n) # n就是缩小的倍数 池化层也同理： pool_size == strides == (n,n) # n就是缩小的倍数 又因为刚才又说了 ， 池化层中 strides 默认值等于 pool_size 的长度。 所以: 我们只设置一个 pool_size = (n,n)。 即可直接让原图缩小n倍。 所以: 池化层通常用来做降采样。 (起到变换形状的作用，无权重参数更新) ","link":"https://cythonlin.github.io/post/ai-greater-cnn-zhi-kernel_sizestridespaddingsame/"},{"title":"AI => CNN-RNN（Ng）","content":"前言 看Andrew Ng视频，总结的学习心得。 虽然本篇文章可能不是那么细致入微，甚至可能有了解偏差。 但是，我喜欢用更直白的方式去理解知识。 上一篇文章传送门： https://segmentfault.com/a/1190000020588580 端到端 首先聊一个面试经历 我最开始接触的是ML （但只限于Sklearn的简单应用，工程化的内容当时一点都不了解。） 后来有幸了解到DL （这个了解比较多） 我面的是普通Python岗， 因为我的小项目中涉及到 （聊天机器人）。所以第二个面试官揪着这个聊了聊。 与面试官交谈时，我也直接挑明了，模型是Github找的，当时自己爬了些问答对，处理后放入模型自己训练的。 面试官一顿(特征提取，语义）等各种 ML-NLP工程化的过程，把我直接问懵了。。 怎么提取特征（问号脸，难道是TF-IDF，分词之类的？？）？？？？？ 我也不知道说啥，以仅有的能力，和他聊聊（LSTM、Embedding, Seq2Seq的 Encoder-Vector-Decoder）。。 面试官说：“你说了这些， 那你特征工程是怎么做的？？？” 我感觉已经没有任何反驳的能力了。。。接下来的事情，我不说，大家也应该清楚了。 反思 我回来后也反思过， 做了什么特征工程？？ 我看视频中 也是，数据简单预处理下，然后分词，词频过滤，构建词典 然后，直接就是构建NN层（包括Embedding层）。 直到最后了解了&quot;端到端这个概念&quot; 与 传统ML的区别。 才清楚， 当时面试的场景是怎么个情况。。。 正式开篇端到端 传统ML： 原数据 -&gt; 数据特征工程(各种复杂的人工处理) ---&gt; 模型 端到端DL：原数据 -----------------------------------------------------&gt; 模型 端到端：（一步到位）： 传统的ML做的中间层人工&quot;手动&quot;特征工程处理出来的特征。 这些特征，端到端的NN都可能&quot;自动学习&quot;的到。 这也可能是当时为什么面试官一直追问我&quot;特征如何处理&quot;的原因吧。也肯能他另有目的QAQ... 或者我们真的不在一个频道上。。。但是交流的过程真的使我受益匪浅，有了更广阔的视野（3Q！） 强调一点： 虽然端到端 模型很便捷。但是需要大量的数据，才能训练出好的效果。 CNN （卷积神经网络） 构成 卷积层（激活函数） + 池化层 + 全连接层 Convolution + Pooling + Dense 至于一些术语: 有人喜欢把: 卷积层 + 池化层 作为一层网络 （因为池化层无训练训练，后面会提到） 也有人喜欢把: 卷积层 和 池化层 各自单独算一个层（也是没问题的。Tensorflow的API就是这样设计的） 卷积层（Convolution Layer） 卷积过程 卷积计算过程就不说了。没有案例图。 但你可以理解为: 两个 正方体 的 对应位置的元素 （相乘再相加）的结果。。。 （互相关，，，） 卷积的输出计算 输出图像大小计算公式： h图片输出 = （h图片输入 - h卷积核 + 2padding） / strides + 1 w图片输出 = （w图片输入 - w卷积核 + 2padding） / strides + 1 首先声明: 这个式子由于不一定能够整除， 因此除不尽的情况下，向下取整, 也叫地板除 因为有个原则: 卷积核滑动的时候(通常是，步长&gt;1的情况下) 如果越界了一部分。则舍弃掉 根据上面的公式，求一个输出图像大小的例子(此处，不做paddding， 并且步长为1) eg: 输入8x8 的图像 , 并使用3x3的卷积核 输出图像高度为: h图片输出 = (8-3 + 2x0) / 1 + 1 = 6 输出图像宽度为: w图片输出 = (8-3 + 2x0) / 1 + 1 = 6 所以输出图像为: 6x6 很明显: 卷积一次，图像变小了。 如果再卷积几次， 图像就看不到了。。。 所以: 我们需要解决这个问题 原则上: 增加 padding 能解决步长为1时，卷积后的图片缩放问题。 假如我们希望输出图像的大小 等于 输出图像的大小，而我们想要求padding需要设置为多少。 一般这种场景适用于 步长strides = 1, 所以参考开始的公式，可写出如下公式： 因为: w 和 h是一样的公式，此处只写出一个h,来推导： h图片输出 = （h图片输入 - h卷积核 + 2padding） / strides + 1 化简: padding =（ h图片输出 - h图片输入 + h卷积核 - 1 ） / 2 因为我们希望的条件: h图片输出 等于 h图片输入， 所以可继续化简： padding =（ h卷积核 - 1 ） / 2 所以步长为1的情况下卷积, 并且想让图片不变形，你的padding的取值，就需要受到 卷积核大小的影响。 现在常用的卷积核大多都是 1x1、 3x3、 5x3 。所以看上面化简好的公式： padding =（ h卷积核 - 1 ） / 2 &lt;============= 1x1, 3x3, 5x5 奇数-1总等于偶数。 所以不用担心除不尽的情况 还需要注意一下: 填充padding一般是环形填充， 假如padding=1, 那么上下左右 都会添加一层。 当然: tensorflow的padding是可以设置形状的 padding的种类（tensorflow） valid: 不填充 same: 自动去填充，使得输入图像 与 输出图像 的大小相等 温馨提示：关于三通道卷积 和 多个卷积核的区别 三通道卷积： 假如你只有一个卷积核 即使你图片是3通道的（三层） 即使你卷积核也是三通道的（三层） 但是: 卷积输出结果是依然是一个 m x n 的形状 (一层&quot;薄纸&quot;) 你的疑惑: 不是三层嘛？ 最后怎么变成一层了 ？？？ 我的解释: 每滑动一次,3层通道，各自都会计算各自层的卷积，然后求总和，并填入一层&quot;薄纸&quot;的对应位置 多个卷积核： 上面说了: 1个卷积核，无论图片是什么形状的、有几个通道，卷积后输出的形状 是 一层薄纸: m x n 而如果你有多个卷积核: 那么你就会有多层薄纸摞起来。 就像 一个长方形 摞成了一个 长方体 明确点: 假如你用了 6个卷积核, 那么你的输出就变成了 m x n x 6 (三维的一个长方体了) 上面说的：就是我最开始学的CNN时候经常理解不清楚的地方。我相信你也一样, qaq .... 下面做个总结: C个通道: 一点都不会影响输出维度。 注意我说的是维度。 假如你的输入是 m x n , 那么你的输出依然是 p x q （注意维度即可，维度没变，二维) f个卷积核: 会影响输出的维度。 输出的维度将会增加一个维度f 假如你的输入是 m x n ， 那么你的输出依然是 p x q x f (增加一个维度F，变成了) 也许你当你学TF的时候会有另一个理解障碍:那就是TF数据格式（以图片为例）： 通常TF数据格式是这样的： [图片数量B, 图片高度H, 图片宽度W, 通道数C] 假如你使用 F 个卷积核做了卷积： 那么他的卷积结果的特征的形状就变变成： [B, H, W, F] 发现没输出结果和通道数C，没有关系的。 只和 卷积核的个数 f有关系。 但是注意: 虽然结果和C没关系。但是 需要卷积核中具有 C的数量，还做唯独匹配。桥梁运算。 对应上例， 我们的卷积核的形状应该是这样的 : [F, C, H, W] 注意一下： 这里面有 卷积核数量f， 也有通道数量C。 如果最后一步的卷积核形状不理解: 没关系。以后是TF20的天下。 对应API不需要你指定卷积核的形状。 因此，你没必要记住卷积核的形状。 你只需要 传递，卷积核的个数， 和 宽高 和 步长 即可。 当然这些都是独立的命名参数。 摘一小段Conv2D的源码: def init(self, filters, # 你只需要传递这个参数， 卷积核的个数 kernel_size, # 卷积核的宽高，假如 3x3 你只需写 [3,3] 即可 strides=(1, 1), # 这是步长， 你不传，他也会给你填默认值, 步长为1 padding='valid', # 这时 padding策略，前面说过，这个一般都会设为 &quot;same&quot; 或许你还有些疑问: 刚才上面不是提到了卷积核应该设置 通道数C么。 原则上是的。因为要和 输出的样本做卷积。要匹配才行。 但是在Tensorflow中。 特别是 Tenosrflow.Keras中，定义模型层 我们只需要把整个模型，从上到下连接起来。（就像先后排队一样） 而对于一些前后流动贯通的参数，比如刚才提到的通道C。 这些参数，Tensorflow会自动帮我们上下文识别管理。 所以我们做的只是需要，把原始数据的形状设置好传 给第一层（给排头发数据） 至于你这些在中间层流动的参数，Tensorflow会自动帮你计算，你不用传。 虽然不用传，但你最好清楚每层是什么结构（当然这时后话，可能需要一些时间理解） 到最后，我再给你设置一个输出形状，你能给我输出出来即可 （队尾接数据） 基本TF参数流动机制讲到这里，刚开始学的时候，也是各种苦思冥想，想不明白qaq... 透过现象看本质（卷积 =&gt; 线性） 其实我们做的每一步 (每一个)卷积就相当于一个矩阵线性操作： x1 @ w1 之后，基于常理话， 我会还会给它一个偏差： b 变成 ===&gt; x1 @ w1 + b 我们说过，可能会给出很多个卷积核进行运算。 上面 x1 @ w1 + b 是每一个卷积核的卷积结果 我们还需要讲所有卷积核计算结果堆叠在一起： 记为 X @ W + b # m x n x f 最后将堆叠在一起的结果，做一层非线性变换 relu ( X @ W + b ) # CNN 通常用 relu eg： 现有图片 5 x 5 x 3 的图像 （暂时不考虑样本个数，就认为是一个样本）. 我们用的是 2 x 2 x 20 的卷积核 (步长为1，不做padding) 那么输出结果就是 (5-2+1) x (5-2+1) x 20 === 4 x 4 x 20 忘记说了，还有一个公式，用来计算 每层卷积的权重参数量的个数的： 公式: 每层权重参数量(W) = 卷积核个数 x 卷积核高 x 卷积核宽 x 卷积核通道数 公式: 每层偏差数量(b) = 1 x 卷积核的个数 # 因为每个卷积核只有一个偏差b 温馨提示: 有太多人喜欢把卷积核个数 与 卷积核通道称作:&quot;输入/输出&quot;通道。 这样的称呼是没问题的， 但我在计算参数量的时候，不喜欢这样的称呼，易混淆。 前情回顾: 记不记得普通神经网络了。每个神经元节点，都有它们自己的参数。因此它们的参数量是巨大的 回归正文: 而卷积核是共享的， 因为它是在一张图片上滑动的。（挨个服务）所以权重参数也是共享的。 池化层 (Pooling Layer) 卷积层(激活函数) =&gt; 池化层 池化层主要分两种： MaxPooling 和 AvgPooling 池化层输出图片形状计算公式： 声明： 池化层也有滑动窗口，并且输出形状计算公式，和 卷积的输出形状计算公式一样： h图片输出 = （h图片输入 - h卷积核 + 2padding） / strides + 1 w图片输出 = （w图片输入 - w卷积核 + 2padding） / strides + 1 因为池化层，的基本都是放在卷积层之后，因此池化层的通道数 也就顺理成章的 和 卷积层通道一样 举个例子: 卷积层数据形状为: m x n x f 那么池化层形状同为: p x q x f 我想主要强调的是: 通道数不变，变得是 宽高。 池化层 滑动窗口参数相关配置 还是，把Tensorflow, 源码搬过来，标注一下: def init(self, pool_size=(2, 2), # 滑动窗口大小 2x2 strides=None, # 步长，通常设为2 padding='valid', # Maxpooling 通常不用padding 一般都是使用组合 pool_size=(2, 2) 和 stride = 2 所以，公式来了: 输入h 滑动窗口h 输出h = (输入h - 滑动窗口h) / stride + 1 = ---------- - -------- + 1 stride stride 通常我们把 pooling层作称作数据的降采样: 所以大多数经验者，都会把 滑动窗口 和 stride步长 设为相等大小。 所以带入上面公式： 输入h 1 输入h 输出h = (输入h - 滑动窗口h) / stride + 1 = ---------- - ----- + 1 = ------- stride 1 步长 简化一下: （当 pool_size 和 strides 设置相等大小时）： 输出 = 输入 / 步长 所以当我们: 步长设为2时， 输出就是输出的一半。 步长设为3时， 输出就是输出的1/3。 ... 不知道有没有这样一个疑问：”为什么滑动窗口没有设置 窗口数量 （就像设置卷积核数量）“ 再次说一下Tensorflow的原理。 因为Pooling的上一层基本完全是 Conv卷积层， 卷积层的 卷积核的个数已经设置好了。 卷积层对接池化层的时候， Tensorflow会自动判断，并设置: 池化层滑动窗口的个数===卷积核个数 池化层通道个数的个数===卷积层通道个数===图片的原始通道个数 MaxPooling （最大池化，常用） 卷积操作：之前我们卷积不是拿着滑动窗口，对应元素相乘再相加么？ 池化操作：池化层也是拿着滑动窗口一样滑，但是不做运算，而是只取每个窗口内最大值。放在一层&quot;薄纸&quot;上 AvgPooling （平均池化，不常用） 一样滑动窗口，各种滑， 然后取每个窗口内的数据的&quot;平均值&quot;, 其他就不说了，同 MaxPooling 额外提醒（池化层的参数是否训练） 池化层的是&quot;没有&quot;参数可以训练的。所以，反向传播，也不为所动~~~ 全连接层 （Dense Layer） 什么是全连接层？？ 你很熟悉的， 全连接层其实就是之前讲的普通的NN（神经网络），所以并没有什么好说的。 只是拼接在池化层之后罢了。 但其实还是有一些细节需要注意。尤其之前的东西没搞懂，那么这里的参数形状你会垮掉~~~ 展平 及 参数 之前为了图方便，参数我都没怎么提到样本参数。 下面我要把样本参数也加进来一起唠唠了。我感觉讲这里，直接上例子比较直观。 好了，现在我们有个需求， 想要做一个10分类的任务： 卷积层-池化层: 这个照常做， 设置你还可以堆叠 卷积层1 + 池化层1 + 卷积层2 + 池化层2 ... 等堆叠的差不多了: (你自我感觉良好了。。。)，我们需要做一层展平处理！ 展平处理（特意拿出来说） 假如你叠加到最后一层池化层数据形状是:(1000,4,4,8)==&gt; 1000个样本，宽高8 x 8, 输出通道为8 你展平后的形状为: (1000, 448) == (1000, 128) 展平操作第一种API: tf.keras.Flatten() # tensorflow2.0的Flatten被作为网络层使用 展平操作第一种API: tf.reshape(x, [-1, 128]) # 手动变换， -1是补位计算的意思 然后在加全连接层，形状为: (1000, 50) # 50代表输出，起到过渡作用 然后在加全连接层，形状为: (1000, 10) # 最终，10代表输出，因为我们说了，要做10分类嘛 1. 其实你中间可以加很多全连接层，我这里只加了一层，控制最后一层全连接是你想要的输出就行。 2. 特别注意， 这里的每一层全连接计算，是需要有激活函数跟着的。 除了最后一层全连接，其他层的全连接都设置为 Relu 激活函数即可。 3. 因为我们做的是10分类（多分类自然应想到 softmax参数， 如果是其他业务，你也可以不加激活函数） 没做，也就是最后一层。我们要再添加一层激活函数 Softmax。 1 x 1 卷积的作用 降采样（控制输出通道数量）： 假如,前一个卷积层参数为: (1000,32,32,256) 如果你下一层使用1x1x128的卷积，则对应参数为: (1000，32，32，128) # 256通道变成了128通道 CNN文本分类（也许你看完下面的RNN再回来看这个会更好） 通常CNN大多数都是用来做CV工作。对于某些文本分类。CNN也可以完成。如下变通概念： 句子的长度 看作 (图片的高度) embedding的维度， 看作 (图片的宽度) 卷积核是铺满一行(或者多行)，然后沿着高度竖着滑下来的。 你也可以有多个卷积核 eg: 一个句子 10个词语，20dim， 这个句子的输入形状就是（10 x 20） 我们准备3个卷积核分别是（3x20),(2x20), (1x20） 每个卷积核竖着滑下来，最后按次序得到向量形状为（10，3） 你可以看作输出三通道（对应卷积核个数，这和之前讲的CNN原理一模一样） 最终提取出来这个（10，3）是，一个句子3个通道的特征信息。 将10x3特征矩阵，通过maxpooling压缩成 （1x3)的特征矩阵 放入Dense层，构建多输出单元的n分类模型。 ResNet (残差网络 Residual Networks) 问题引入： 是否网络层数越多越好，虽然堆叠更多的网络，可以使得参数丰富，并且可以学到更多更好的特征。 但是实际效果并非如此，而是出现，过拟合等现象。 ResNet作者 何凯明：有感而发：按理说模型是应该越丰富越好的。可是出现了过拟合这种现象。 最少，更深层的网络的效果，应该比浅层网络的效果好吧。不至于差了那么多。 因此，他将此问题转换为一个深度模型优化的问题。 ResNet相关配置 batch-size: 256 optimizer: SGD+Momentum(0.9) learning_rate: 初始化为0.1， 验证集出现梯度不下降的情况下，learning_rate每次除以10衰减 每一层卷积层之后，都做 Batch Normalization 不使用 Dropout （其实应该是用了 BN,所以就没有 Dropout） RNN (循环神经网络) ###直接引用 Andrew Ng的降解图 可以看到，上图中有一些输入和输出：慢慢捋清。 第一个输入 x&lt;1&gt; 代表 （你分词后的每一个句子中的第一个单词） x&lt;2&gt;就是第二个单词喽 第二个输入 a&lt;0&gt; 代表 初始输入， （一般初始化为0矩阵） 前面两个输入， 各会乘上各自的 权重矩阵，然后求和 得出 a&lt;1&gt; (这是临时输出) a&lt;1&gt; 乘上 一个权重参数 得到输出一: y&lt;1&gt; （这是终极输出一） （这就是图中黑框顶部的输出分支） a&lt;1&gt; 乘上 又一个新权重参数后， 再加上 x&lt;2&gt; 乘以自己的权重参数得到 a&lt;2&gt; ......你会发现 1-5步是个循环的过程， 到第5步， a&lt;1&gt; 就相当于 最开始a&lt;0&gt;的地位，作为下一层的输入 题外话。其实每层的输出y1 会替代下一层的x作为下一层的输入。 （我会放到下面&quot;防坑解释&quot;中说） 然后将上述途中最后2行的公式化简，可得到如下形式： 防坑解释 (RNN语言模型) 如果你看过了上面的图，你会很清楚， 有多少个x, 就会输出多少个y。 上面第7点说过 : &quot;其实每层的输出y1 会替代下一层的x作为下一层的输入&quot;, 该如何理解这句话？？？ 假如你有这样一段文本: &quot;我精通各种语言&quot; =&gt; 分词后的结果会变成 &quot;我&quot;,&quot;精通&quot;,&quot;各种&quot;,&quot;语言&quot; 一般的问答对这种的句子，处理流程是: （这里只先说一个）： 那就是：在句子的末尾添加一个 标识符 所以句子变成了: &quot;我&quot;,&quot;精通&quot;,&quot;各种&quot;,&quot;语言&quot;, &quot;END&quot; 这些单词都会预先转为 （One-Hot编码 或者Embedding编码） x1(初始值0) =&gt; y1(我) y1有一定概率输出 &quot;我&quot;， 下面所有的y同理，只是概率性。 x2(y1) =&gt; y2(精通) 如此每一层嵌套下来，相当于条件概率 P(精通|我) x3(y2) =&gt; y3(各种) P(各种|(我，精通)) x4(y3) =&gt; y4(语言) ... x5(y4) =&gt; y5(&lt;END&gt;) ... 不知道看了上例，你会不会有下面一连串的问号脸？？： 为什么y1-y5 输出都是精准的文字？ 答：我只是方便书写表示，其实每个输出的Y都是一个从词典选拔出来的词的概率。（多分类） 不是说x1-x5每个x应该输入固定句子的每个单词么？？？为什么变成了输入y1-y5 答：的确是这样的，但是我们的 y1-y5 都是朝着预测x1-x5的方向前进的。（这也是我们要训练的目标） 所以: 可以近似把y1-y5等价于x1-x5。 所以用 y1-y5 替代了 x1-x5 这样: 也可以把前后单词串联起来，让整个模型具有很强的关联性。 比如: 你第一个y1就预测错了。那么之后的y很可能都预测错。（我的例子是：双色球概率） 但是: 假如我们预测对了。那说明我们的模型的效果，已经特别好了（双色球每个球都预测对了~） 那我们就靠这x和y就能把前后语义都关联起来吗？？？ 答： 当然不仅于此。 你别忘了我们还有贯穿横向的输入啊，如最开始RNN图的 a&lt;0&gt;. a&lt;1&gt; 这些 既然你说y 是从词典选拔出来的词的概率属性。 那么这个概率怎么算？ 答： 这问得太好了~~~ 前面说了: 一般都会预先给数据做 One-Hot或 Embedding编码。 所以数据格式为: [0,0,....,1,...] # 只有一个为1 基本上我们最后给输出都会套一层: softmax激活函数， softmax应该知道吧：e^x /(ex1+..+ex) 所以: softmax结果就是一个 和One-Hot形状一样的概率列表集合: [.....,最高概率,...] softmax的结果（概率列表） :（代表着预测 在词典中每一个单词的可能性） 那么损失函数怎么算呢？？ 答：没错，损失函数也是我们最关注的。 前面:我们已经求出了softmax对应的结果列表 (....,最高概率，...) 损失函数: 我们使用的是交叉熵。 交叉熵知道吧: -（ Σplogq ） # p为真实值One-hot, q为预测值 简单举个例子: 假如 softmax预测结果列表为 :[0.01,0.35, 0.09, 0.55] # 温馨提示，softmax和为1 你的真实标签One-Hot列表为: [0, 0, 0, 1] 那么交叉熵损失就等于: -( 0log0.01 + 0log0.35 + 0log0.09 + 1*log0.55 ) = ... 到此为止，我们第一层NN的输出的损失函数就已经计算完毕了。 而我们训练整个网络需要计算整体的损失函数。 所以，我们需要把上面的交叉熵损失求和， 优化损失。 梯度爆炸 &amp; 梯度消失 RNN 的梯度是累乘，所以NN层如果很多，可能会达到 指数级的梯度。 你应该听过一个小关于指数的小案例吧~~ (学如逆水行舟，不进则退~) &gt;&gt;&gt; 1.01 ** 365 37.78343433288728 # 每天进步0.01 ，一年可以进步这些 （对应梯度爆炸） &gt;&gt;&gt; 0.99 ** 365 0.025517964452291125 # 每天退步0.01 ， 一年可以沦落至此（对应梯度消失） 梯度爆炸： 就是上面例子的原理。 就不多说了。 解决方式：梯度裁剪 梯度消失： 同上例， 不好解决（于是LSTM网络出现， 和LSTM） Tensorflow2.0（Stable） API import tensorflow.keras as tk # 注意我用的是TF20标准版，所以这样导入 tk.layers.SimpleRNN( units=单元层, # units单元数，对应着你每个单词的个数 return_sequences=False # 默认值就是False ) GRU GRU比RNN的每一层的多了一个 记忆信息(相当于RNN的 h)，这个记忆信息就像传送带一样，一直流通各层RNN 然后还多了2个门 (r门和U门)， 这2个门就是负责控制（是否从传送带上取记忆， 且取多少记忆） ####注明： GRU 只有一个 c(横向， 传送带) ， 没有h 简化版（只有U门）： C新' = tanh( w @ [C旧, x新] + b ) # 根据传动带的旧信息， 生产出 传送带的新信息 u门 = sigmoid (w @ [c旧, x新] + b) # 一个门控单元，起到过滤信息的作用 C新 = u门 * C新' + (1-u门) * C旧 # 经过u门控单元的控制过滤后， 最终放到传送带的信息 如果: u门为1，则传送带上全是新信息（旧的全忘掉） 如果: u门为0，则传送带上全是旧信息（新的不要） 强调一下: 我不方便写公式负号，于是用了 &quot;新&quot;,&quot;旧&quot; 代替 新: 代表当前 t 旧: 代表前一时刻 t-1 完整版（同时具有r门和u门） 添加这一行： r门 = sigmoid (w @ [c旧, x新] + b) # 和下面的U门几乎相似，只不过换了一下权重和偏差 C新' = tanh( w @ [r门 @ C旧, x新] + b ) # 修改这一行: C旧 变为=&gt; r门 @ C旧 u门 = sigmoid (w @ [c旧, x新] + b) # 一个门控单元，起到过滤信息的作用 C新 = u门 * C新' + (1-u门) * C旧 # 经过u门控单元的控制过滤后， 最终放到传送带的信息 Tensorflow2.0（Stable） API import tensorflow.keras as tk # 注意我用的是TF20标准版，所以这样导入 tk.layers.GRU( # 参数同上面RNN我就不解释了 units=64, return_sequences=False # 这些参数看下面LSTM我会讲到 ) LSTM LSTM和GRU很像，但是比GRU复杂。 LSTM结构包括: u门（更新门）+ f门（遗忘门）+ o门（输出门） 注明： LSTM 不仅有个传送带C（横向） ， 他还有个RNN的 h 信息（横向） f门 = sigmoid (w @ [c旧, x新] + b) # 和下面的U门几乎相似，只不过换了一下权重和偏差 o门 = sigmoid (w @ [c旧, x新] + b) # 和下面的U门几乎相似，只不过换了一下权重和偏差 C新' = tanh( w @ [C旧, x新] + b ) # 注意，这里没有r门了 u门 = sigmoid (w @ [c旧, x新] + b) # 一个门控单元，起到过滤信息的作用 C新 = u门 * C新' + f门 * C旧 # &quot;(1-u门)&quot; 换成了 f门 h = o门 * tanh( C新 ) Tensorflow2.0（Stable） API import tensorflow.keras as tk # 注意我用的是TF20标准版，所以这样导入 keras.layers.LSTM( units=64, return_state=True # 占坑，下面剖析 return_sequences=False # 占坑，下面源码剖析 recurrent_initializer='glorot_uniform', # 均匀分布的权重参数初始化 # stateful=True, # API文档：若为True,则每一批样本的state的状态，都会继续赋予下一批样本 ) return_state 和 return_sequences 这两个参数到底有什么用？？？ 我的另一篇文章单独源码分析这两个参数：https://segmentfault.com/a/1190000020603328 总结对比 GRU 和 LSTM GRU有2个门: u门 和 r门 LSTM有3个门: u门 和 f门 和 o门 GRU有一个C: # 就有一条传送带c, 他的前后单元信息仅靠这一条传送带来沟通（舍弃与保留） LSTM有一个C和一个h: # 不仅有传送带c, 还有h， 他的前后单元信息 靠 c 和 h 联合沟通。 再说一下每个门控单元: 不管你是什么门， 都是由 Sigmoid() 包裹着的。 所以: 说是 0 和 1 ， 但严格意义上，只是无穷接近。但是微乎其微，所以我们理解为近似相等0和1 RNN-LSTM-GRU拓展 双向（Bidirection） 首先说明: 双向模型，对于 RNN/LSTM/GRU 全部都适用 由于单向的模型，不能关联之后信息。比如：你只能根据之前的单词预测下一个单词。 而双向的模型，可以根据前后上下文的语境，来预测当前的下一个单词。 或者举一个更直白的例子（我自己认为）： 比如说: 你做英语完型填空题， 你是不是 需要 把空缺部分的 前面 和 后面 都得读一遍，才能填上。 单向与双向结构对比如下： 单向: 1 -&gt; 2 -&gt; 3 -&gt; 4 双向: 1 -&gt; 2 -&gt; 3 -&gt; 4 | 1 &lt;- 2 &lt;- 3 &lt;- 4 注意: 上下对齐，代表一层。 Tensorflow2.0（Stable） API import tensorflow.keras as tk # 注意我用的是TF20标准版，所以这样导入 tk.layers.Bidirectional( # 就在上上面那些API的基础上,外面嵌套一个 这个层即可。 tk.layers.GRU( units=64, return_sequences=False ) ), 模型深层堆叠（纵向堆叠） 首先说明: 层叠模型对于 RNN/LSTM/GRU 同样全部都适用 之前单层单向模型是这种结构 1 -&gt; 2 -&gt; 3 计算公式是: 单元 = tanh ( W @ (x, h左) ) 而多层单向是这种结构（我们以2层为例）： y1 y2 y3 输出层 ^ ^ ^ | | | 7 -&gt; 8 -&gt; 9 二层单元 ^ ^ ^ | | | 4 -&gt; 5 -&gt; 6 一层单元 ^ ^ ^ | | | x1-&gt; x2 -&gt;x3 输入层 你 好 啊 计算公式是： （我写的可能只按自己的意思了~） 一层每个单元 = tanh ( W @ (x, h左) ) # 因为是第一层嘛：所以输入为 x 和 左边单元h 二层每个单元 = tanh ( W @ (h下, h左) ) # 第二层就没有x了:而是下边单元h 和 左边单元h 词嵌入(Word Embedding) 单词之间相似度计算 c1 @ c2 余弦定理，求 cosθ = ------------------ ||c1|| * ||c2|| 或者你可以使用欧氏距离。 原始词嵌入并训练 假如我们通过一个句子的一部分来预测，这个句子的最后一个单词。 把词典的每个词做成One-Hot便是形式，记作矩阵 O 随机高维权重矩阵， 记为 E E @ O 矩阵乘积后记为词向量 W 可见如下案例： 如果: 我们分词后词典总大小为1000 那么: 他的 One-Hot 矩阵形状为 [6, 1000] （假如我们这里通过句子6个词来预测最后一个词） 并且: 随机高维权重矩阵 形状为 [1000, 300] （注意，这个300是维度，可自行调整选择） 注意: 上面权重矩阵是随机初始化的， 后面训练调节的。 最后: E @ O 后得到词向量W 的形状为 [6, 300] 送进NN(打成1000类) 作为输出 加一层 Softmax （算出1000个单词的概率） 作为最终输出y_predict y_predict 与 y真正的单词标签（one-hot后的） 做交叉熵loss 优化loss，开始训练。 Word2Vec 的 skip-grams(不是太懂， Pass) 说下个人的理解，可能不对 skip-grams： 拿出中间 一个词，来预测若干（这是词距，自己给定）上下文的单词。 例子如下： seq = 今天去吃饭 给定单词 标签值（y_true） 去 今 去 天 去 吃 去 饭 训练过程就是上面说过的小节 &quot;原始词嵌入并训练&quot;，你只需把 y_true改为 &quot;今&quot;,&quot;天&quot;,&quot;吃&quot;,&quot;饭&quot;训练即可。 Word2Vec 除了 skip-grams, 还有 CBOW 模型。 它的作用是 给定上下文，来预测中间的词。 据说效率等某种原因(softmax计算慢，因为分母巨大)，这两个都没看。（Pass~） 负采样（Negative Sampling） 解决Word2Vec 的 softmax计算慢。 负采样说明（假如我们有1000长度的词典）： 从上下文（指定词距）: 随机，选择一个正样本对，n个负样本对（5-10个即可） 主要机制: 将 Word2Vec的 softmax（1000分类） 换成 1000个 sigmoid做二分类。 因为: 是随机采样（假设，采样1个正样本 和 5个负样本）。 所以: 1000个 sigmoid二分类器，每次只用到 6 个对应分类器（1个正样本分类器，5个负样本分类器） 负采样，样本随机选择公式： 单个词频 ^ (3/4) ----------------- Σ(所有词频 ^ (3/4)) ","link":"https://cythonlin.github.io/post/ai-greater-cnn-rnnng/"},{"title":"AI => TF20的LSTM与GRU(return_sequences与return_state)参数源码","content":"前言 温馨提示： 本文只适用于: 了解LSTM 和 GRU的结构，但是不懂Tensorflow20中LSTM和GRU的参数的人) 额外说明 看源码不等于高大上。 当你各种博客翻烂，发现内容不是互相引用，就是相互&quot;借鉴&quot;。。。且绝望时。 你可能会翻翻文档，其实有些文档写的并不是很详细。 这时，看源码是你最好的理解方式。（LSTM 和 GRU 部分源码还是比较好看的） 标题写不下了: TF20 ==&gt; Tensorflow2.0（Stable） tk ===&gt; tensorflow.keras LSTM 和 GRU 已经放在 tk.layers模块中。 return_sequences = True return_state = True 这两个参数是使用率最高的两个了， 并且LSTM 和 GRU 中都有。 那它们究竟是什么意思呢??? 来，开始吧！ 进入源码方式： import tensorflow.keras as tk tk.layers.GRU() tk.layers.LSTM() 用pycharm ctrl+左键 点进源码即可~~~ LSTM源码 我截取了部分主干源码： ... ... states = [new_h, new_c] # 很显然，第一个是横向状态h, 另一个是记忆细胞c if self.return_sequences: # 如果return_sequences设为True output = outputs # 则输出值为所有LSTM单元的 输出y，注意还没return else: # 如果return_sequences设为False output = last_output # 则只输出LSTM最后一个单元的信息, 注意还没return if self.return_state: # 如果return_state设为True return [output] + list(states) # 则最终返回 上面的output + [new_h, new_c] else: # 如果return_state设为False return output # 则最终返回 只返回上面的output 小技巧: 瞄准 return 关键词。 你就会非常清晰，它会返回什么了。 GRU源码 ... ... ######## 我们主要看这一部分 ######################################### last_output, outputs, runtime, states = self._defun_gru_call( inputs, initial_state, training, mask) ##################################################################### ... ... ######### 下面不用看了， 这下面代码和 LSTM是一模一样的 ################### if self.return_sequences: output = outputs else: output = last_output if self.return_state: return [output] + list(states) else: return output 现在我们的寻找关键点只在于， states 是怎么得到的？？？ 你继续点进去 &quot;self._defun_gru_call&quot; 这个函数的源码， 你会发现 states 就直接暴露在里面 states = [new_h] return ..., states 现在源码几乎全部分析完毕。 我们回头思考总结一下： LSTM 和 GRU 中的 return_sequences 和 return_state 部分的源码是一模一样的！！！ return_sequences: 只管理 output变量的赋值，（最后一个单元 或 全部单元） return_state： 负责返回 output变量，并且按条件决定是否再一并多返回一个 states变量 进而我们把问题关注点转换到 output变量， 和 states变量： LSTM 和 GRU 的 output变量: 大致相似，不用管。 LSTM 和 GRU 的 ststes变量： LSTM的 states变量: [H, C] # 如果你了解LSTM的结构，看到这里你应该很清楚，LSTM有C和H GRU的 states变量: [H] # 如果你了解GRU的结构，看到这里你应该很清楚，GRU就一个H 最终使用层总结： LSTM: 有四种组合使用： return_sequences = False 且 return_state = False (默认) 返回值: 只返回 最后一个 LSTM单元的输出Y return_sequences = True 且 return_state = False 返回值: 只返回 所有 LSTM单元的输出Y return_sequences = False 且 return_state = True 返回值: 返回最后一个LSTM单元的输出Y 和 C + H 两个（隐层信息） return_sequences = True 且 return_state = True 返回值: 返回所有LSTM单元的输出Y 和 C + H 两个（隐层信息） (适用于Atention) GRU： 有四种组合使用： return_sequences = False 且 return_state = False (默认) 返回值: 同LSTM return_sequences = True 且 return_state = False 返回值: 同LSTM return_sequences = False 且 return_state = True 返回值: 返回 最后一个 LSTM单元的输出Y 和 一个H（隐层信息） return_sequences = True 且 return_state = True 返回值: 返回 所有 LSTM单元的输出Y 和 一个H（隐层信息） (适用于Atention) ","link":"https://cythonlin.github.io/post/ai-greater-tf20-de-lstm-yu-grureturn_sequences-yu-return_statecan-shu-yuan-ma/"},{"title":"AI => DeepLearning+Metrics（Ng）","content":"前言 看Andrew Ng视频，总结的学习心得。 虽然本篇文章可能不是那么细致入微，甚至可能有了解偏差。 但是，我喜欢用更直白的方式去理解知识。 数据划分 传统机器学习数据的划分 传统机器学习一般都是小规模数据（几万条） 那么可以 训练集：验证集：测试集 = 6:2:2 若是大规模深度学习一般都是大规模数据（几百万条） 训练集: 验证机：测试集 = 9:0.5:0.5 划分 验证集 可以过早的为我们预测 指标和精度 偏差 与 方差 高偏差： 训练集和测试集 loss 都比较高 （比人预测的loss高很多） （欠拟合） 高方差： 训练集Loss低， 测试集 Loss高。 所以训练集和测试集 loss相差太大， 也成为（过拟合） 防止过拟合的几种方法 损失函数 (惩罚项系数) 正则化（regularization） 可分两种 （L1正则化惩罚 和 L2正则化惩罚）下面只以 L2为例，L2用的也是比较多的） 正则化系数公式： loss = ... new_loss = loss + (λ/2m) * w^2 w = w - learning_rate * 梯度 上面公式的单调变换解释： 求梯度的时候 λ越大， new_loss越大， 求得的梯度越大（正比） w 减去的值就越大。 w变得就越小。 w 越小， 一定程度上特征就衰减了许多。 就有效的放置了过拟合哦 对应API（有两种方式）： L1 = keras.regularizers.l2(0.01) # TF2 当作 keras 的 Layers定义来使用 L1 = tf.nn.l2_loss(w_b) # 看到里面传递 w和b了吧， 这种是偏手动方式实现的API 如果你想使用手撕实现，下面有个例子（伪代码）： for with tf.GradientTape() as tape: ... loss_reg = [tf.nn.l2_loss(w_b) for w_b in model.trainable_variables] # [w1,b1,w2,b2] print(tf.reduce_sum(loss_reg)) # tf.Tensor(2.98585, shape=(), dtype=float32) # 就是这个形状 loss = loss + loss_reg 另一种正则化方式（regularization） -- DropOut &quot;随机&quot;剪枝， 略， TF框架一步到位 还有一种防止过拟合的方式（数据增强） 防止过拟合的另一种方式，就是需要的大量的数据来支撑，那么数据就那么点，怎么办？ 数据增强（ 其原理就是增大样本数量，小幅度翻转等） 某种程度上，就是增加了样本。 最后一种防止过拟合的方法 （earlystopping ） earlystopping （tf.keras.callback模块中，有这个 callback函数，注释部分有解释） callbacks = [ keras.callbacks.TensorBoard(logdir), keras.callbacks.ModelCheckpoint(output_model_file, save_best_only=True), # 在这里 keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3) # 验证集，每次都会提升，如果提升不动了，小于这个min_delta阈值，则会耐性等待5次。 # 5次过后，要是还提升这么点。就提前结束。 ] 数据预处理 标准化 和 归一化能起到什么作用： 做了标准化 和 归一化 可以 让函数收敛的 更快速（梯度下降的快） 参考 （圆形下降的快， 和 椭圆下降的慢） 其次，在神经网络中,BN层还有额外的效果。每层网络的不同参数，可能会导致&quot;数据分布&quot;散乱、变形 因此，BN可以有效 防止数据分布变形。 （其实说白了也是&quot;加速函数收敛&quot; ,加速NN训练） 注意点 训练集 和 测试集的样本特征 （&quot;要么都做处理，要么都不做处理&quot;）（就是相同待遇的意思。。。） 专业点叫做 ：&quot;保证训练集 和 测试集 相同分布&quot; 数据 随机分布有什么影响（Andrew ng解释）？ 假如训练集的数据分布 训练的目标是正中靶心。 而预测时，假如你的预测集数据分布 和 训练集数据分布的不同。 那么很可能会 预测集 预测的时候（是另一个靶心）。 所以你训练的再好，到预测集的靶子上 也是脱靶。。。 所以 训练集 和 测试集 的 相同分布很重要 ###数据预处理大体分2类： 1. 中心化处理 2. 缩放处理 zero-centered (中心化处理) 平移 --- 减去固定值 ###scale (缩放处理) 归一化：除以 最大值-最小值 标准化：除以 标准差 下面的操作都是通过 中心化处理+缩放处理 联合组成的 normalization（归一化） sklearn那里也提到过 目标： 将数据收敛到 [0,1] 公式 x - min(x) # 中心化 ------------------ max(x) - min(x) # 缩放处理 ###StandardScaler（标准化） 目标： 将数据转化为标准 正态分布（均值为0，方差为1） 公式： x - 平均值 ----------------- 标准差 标准化 和 归一化 选哪个？？？ 视觉图片： 归一化 其他： 标准化较好 数据集不足怎么办？ 我自己也遇到过这样的问题。 我之前做一个QA聊天机器人时。 数据是百度知道的爬的。 但是（用过的应该清楚。 百度知道有很多用户的垃圾回答，是用户刷分的。） 问了解决这一问题。我的解决思路是：通过Pandas, 筛选答案长度至最小。 但是这样。就可能筛选除了 大量大量的 原生数据 再加上，把（原数据中 &quot;有问无答&quot; 的问答对）过滤扔掉。 那么弄下来的源数据，几乎没剩多少了。。（我记得我当时弄了400W+问答对） 筛选到最后（问答长度 15个汉字， 筛选掉空回答）(只剩下 几万条了。。。) 后来，我急中生智。在网上找了一些 中文语料库（我用的青云中文语料库） 把它融合到 我自己的 语料库中。。。 但是训练后的结果， 全是人家 青云语料库的 问答内容。。。 后来也没去继续深究了。。。 后来正好看到 Ng。提到这一问题，记录一下相应的应对措施! 训练集：青云语料+ 1/2 自己的语料 测试集: 1/4 自己的语料 验证集：1/4 自己的语料 随机初始化权重 随机初始化认知 为什么不是初始化为0？？？ 因为神经网络中， W初始化为0的话， 会导致反向传播后， 所有神经元会训练同一个网络。一点效果没有 为什么不初始化很大的值或者很小的值？？？ 这是分情况来定的。 比如你用的 tanh 或者 sigmoid函数 由脑海中的图像可知（求导 或 斜率） ，当 初始值过大，或者过小。 都会可能导致，y直接落在 sigmoid的 顶部和底部（就是斜率水平，近乎为0） 落在了水平的梯度。这样的梯度，猴年马月也降不下去啊。。。。。 如果落在了 倾斜陡峭的梯度。 那么梯度下降的一定很快啦。 如果做了BatchNormalization，那么可使用 高斯 x 0.01 正态分布 * 拉低值 np.random.randn(2,2) * 0.01 # 2,2是形状， 这个0.01 可以自己调节。 总之，小一点 最好， 但不要太小 如果使用了Relu激活函数，对应 初始化方法 np.random.randn(shapex, shapey) * np.sqrt( 2/shapex ) # 系数为2 如果使用了Tanh激活函数，对应 初始化方法（NG推荐， 也叫 Xavier） np.random.randn(shapex, shapey) * np.sqrt( 1/shapex ) # 系数为1 激活函数 激活函数认知 学习Andrew Ng课更深刻了解了激活函数。 神经网络中，为什么我们需要激活函数，甚至需要非线性激活函数？ 首先挑明，我们使用神经网络的目的，就是想训练出更多，更丰富的特征。 所以。 一直用线性激活函数，或者不用激活函数。会使得你整个网络训练到头，还是线性的。就没意思了。 它学不到丰富的特征的。 因为神经网络多层是需要拿前一层的结果作为下一层的 x，所以有了如下公式： w3 (w2 (w1x+b) +b) +b 展开后， w3 * w2 * w1 * x + ...... 很明显它依然是线性的。 所以，无论你用多少层 神经网络。 到最后它依然是线性的。。。。 这样倒不如 一层网络也不用。 直接上个 逻辑回归模型，效果估计也是一样的。。。。。。 当然有一些场合也需要使用 线性激活函数，比如 房价预测。身高预测。（这些都是线性回归模型） 这些情况，就可以使用 线性激活函数了。 但是不妨想一想， 就像上面 身高预测这些。是线性回归，并且 y预测都是正数值。 某种程度上，其实我们也可以使用 relu激活函数， （因为 relu的右半侧（就是大于0的部分） 也是线性的哦） 我们NN隐层就大多数都使用非线性激活函数。 隐层： relu 或者 leakly relu 或者 tanh 输出层： sigmoid 或者 softmax 或者 tanh 等等 sigmoid 公式 1 --------- 1 + e**(-x) 每个out: (0, 1) 二分类out之和为 1 对应API： 1. tf.sigmoid(y) 2. 或函数参数 xxxxx (activations='sigmoid') 3. tf.keras.activations.sigmoid() softmax e**x --------------------------------- e**(x1) + e**(x2) + ... + e**(xn) 每个out: (0,1) 多分类 out之和为 1 对应API： 1. tf.nn.softmax() 2. 函数参数 xxxxx (activations='softmax') 3. tf.keras.activations.softmax() softmax特点： 输出的是什么形状的张量，输出的就是什么形状的张量 也是有线性决策边界（线性 多 分类器） tanh coshx ex - e(-x) ------------- 2 sinhx ex + e(-x) -------------- 2 tanhx ex - e(-x) ------------- ex + e(-x) 每个out: (0,1) * 2 -1 ===&gt; (-1,1) LSTM 对应API： 1. tf.tanh(y) 2. 函数参数 xxxxx (activations='tanh') 3. tf.keras.activations.tanh() relu 公式： y = 0 if x &lt; 0 else x # 大于0，梯度为1 对应API 1. tf.nn.relu() 2. 或函数参数 xxxxx (activations='relu') 3. tf.keras.activations.relu() leaky_relu: (小扩展) y = kx if x &lt; 0 else x tf.nn.leaky_relu() 损失函数 MSE （均方误差） 公式 Σ( (y-y_predict)**2 ) -------------------- n 对应API 公式实现： tf.reduce_mean( tf.square( y-y_predict ) ) tf.API: tf.reduce_mean( tf.loss.MSE(y, y_predict) ) CrossEntropy （交叉熵） 熵公式： -Σ（plogp） 交叉熵公式：-（ Σplogq ） p为真实值One-hot, q为预测值 p: [1,0,0] q: [0.9, 0,0.1] H = -( 1log0.9 + 0log0 + 0*log0.1) = -log0.9 = -ln0.9 ≈ 0.1053.... tf的 tf.math.log相当于 ln 交叉熵API： 交叉熵越小（y与y-predict差距越小，预测较准确） 交叉熵越大（y与y_predict差距越大，交叉相乘累加后值大，说明预测错位了。。。所以交叉起来变大了） tf.API: （方式1：直接是函数调用） loss = tf.losses.categorical_crossentropy([1,0,0], [0.9, 0, 0.1],from_logits=True) # 第一个参数y, 第二个参数 y_predict loss = tf.reduce_mean(loss) tf.API: （方式2：用类的call调用 , 这次以 二分类交叉熵为例） loss = tf.losses.BinaryCrossentropy(from_logits=True)( [1], [0.1] ) # 结果为2.+ 。 因为 真实值是1类， 而预测值概率是0.1太小了。所以肯定预测错了。 loss = tf.reduce_mean(loss) 说明：categorical_crossentropy( ) # 第一个参数必须 one_hot, （第二个参数按理来说需要做 softmax，但是你传了 from_logigs=True，就不必softmax了） 梯度 SGD（Stochastic Gradent Descent）: 解释 各种梯度下降的区别： Mini-Batch Gradent Descent: 指定每次 mini-batch个 来做梯度下降 （就是每次指定多少个样本 来做GD的意思） 这种介于 1-全部样本之间的。 是最优的 Batch gradent descent: mini-batch 为全部样本 Stochastic gradent descent: mini-batch 为 1个样本 缺点： 每次 1个样本做SGD， 那么就失去了 向量化（矩阵乘代替循环）的 加速快感。。。。。 减去梯度，代表朝着梯度方向走 w新 = w当前 - learning_rate * 梯度 使用方式： model.compile(..... ,optimizer=keras.optimizers.SGD(learning_rate=0.01)) 再记录其他优化器之前， 先补一个 指数加权平均 的知识 公式： y = β * X之前 + （1-β）* X当前 图形曲线表现： β越小：（小到0.5） ：曲线越抖动频繁（锯齿 越厉害）（0.5左右已经，严重上下跳动了） β越大：（大至1.0） ：曲线越光滑（无锯齿） 所以 β： 越大越好 （涉及到一个技术--偏差修正， 如果你不修正。 可能训练会稍微慢一些。无伤大雅） Momentum（动量） 公式大概： dw' = β * dw-1 + ( 1-β ) * dw # 用 dw-1 掰弯 dw db' = β * db-1 + ( 1-β ) * db # 用 db-1 掰弯 db 公式理解： 在原来的梯度基础上， 用 上一次的梯度方向， 把当前将要计算的梯度掰弯 RMSProp model.compile(..... ,optimizer=keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.9)) Adam(强烈推荐) TF-API: 默认原参数 model.compile(..... ,optimizer=keras.optimizers.Adam( learning_rate=0.001, beta_1=0.9, # 学习率衰减参数 beta_2=0.999, epsilon=1e-7, ), ) 其实这个API参数，我们只稍微调整一下 learning _ rate 即可，其他不用怎么。 学习率衰减 其实大多数 优化器内都有 学习率衰减参数，例如： SGD(decay) Adam(beta_1) 当然你也可以自己实现（按照样本已训练的批次，动态衰减） learning rate = learning rate * 1/（epoch轮数 * 衰减率 + 1） 其实还有更多 可调节参数，就像Adam中的 那么多参数似。当然我压根也没想自己实现衰减。。 可知 decay越小， 学习率衰减的越慢， 当衰减率为0时。 学习率压根就不衰减 而 decay越大， 学习率衰减的越快， 当衰减率为1时。 那衰减的就太夸张了~~ 迁移学习 （我想到一个词：移花接木） 应用场景 假如已经有现成的 狗类 识别的 神经网络模型 那么假如你现在想要 做一个 猫类的 识别 你完全可以把 狗识别 网络模型拿过来 然后把最后 输出层 扔掉，自己加一个新输出层（当然中间你也可以加一些新的NN层） 然后 旁敲侧击，只在最后一层提供输入，只对 新的输出层（或者你额外加的NN）层训练。 应用条件 当你迁移后的数据有特别特别多的时候， 那么你完全可以把 搬过来的 模型参数 从头到尾训练一遍。 就像你 狗模型， 前面的网络学到了很多 毛，特征。 （这猫也有嘛，所以正好可以用得上） 然后你 在狗模型的基础上 ，训练猫模型 （我不太了解猫~~~， 比如说可以新学到猫的胡须之类的新特征） 总结来说： 新模型 = NN层（狗）参数 + NN层（猫）参数 + 输出层（猫）参数 当然， 如果你迁移支持的数据，只有很少，根本不够强大的神经网络训练 那么，你就可以直接把，搬过来的模型参数固定住， 直接只在 最后输出层，提供输入，进行训练 总结来说： 新模型 = NN层（狗）参数 + 输出层（猫）参数 迁移学习的主要目的思想： 当你 有很少的小数据集A， 但是你想训练成一个 NN 来 达到目的。 可想而知，少量数据集A 还不够 NN 塞牙缝的。。。 所以，你需要找一些其他类似的数据集B（量多的，好收集的） 然后这些大量数据集B，足以 驰骋于 NN ， 得到一个模型。（并且带着 训练好的参数） 数据集A说： &quot; 大哥，你训练好的网络借我用用呗。 你用了那么多数据，训练出的特征一定有我想要的。 我把整个模型拿过来，只改一下最后一层的输入。然后只训练最后一层的参数。 其他层的参数都用你的。 &quot;。 数据集B大哥说： &quot;可以&quot; 迁移学习API （Tensorflow2.0） 温馨提示： TF20的Keras Layers 是可以 用切片语法 选取具体网络层的，举个例子： # from tensorflow import keras # cut_resnet = keras.applications.DenseNet121( # 使用现有ResNet模型 # include_top=False, # 不要最后一层，而是使用我们自己定义的全连接层 # pooling='avg', # weights='imagenet', # 初始化权重（从imagenet训练好模型参数来初始化） # ) # for layer in cut_resnet.layers[0:-3]: # 部分可训练（fine-tune分割） # trainable=False # 0 到 倒数第三层，参数不可训练 # # new_model = keras.models.Sequential() # new_model.add(cut_resnet) # new_model.add(其他层) 迁移学习 适用场景 统一使用领域（要么文本迁移要文本， 要门图像迁移到图像。） 假如 A 迁移到 B （那么 A的样本最好远大于 B的样本） 假如 A 迁移到 B （最好A的许多特征信息，B正好可以用得到。比如 猫狗，都有毛发，胡须，四条腿） 多任务学习（了解，用的少） 直接感观：我认为就像（类的继承 ， 或者封装为一个函数， 这样的概念。。） 你想训练 预测 各种各样类别的图片。 你可以首先 用一个任务 训练一下 共有特征 的 NN。 然后其他任务 用这个 训练好的 共有的特征的 NN。 Ng提示： 你需要有庞大的神经网络支撑，不然效果不好。 ","link":"https://cythonlin.github.io/post/ai-greater-deeplearningmetricsng/"},{"title":"PY => Tensorflow2.0 GPU管理与分布式","content":"Nvidia命令 nvidia-smi # 查看GPU占用情况 watch -n 0.1 -x nvidia-smi # 动态实时0.1秒间隔，查看GPU占用情况。 GPU管理 为什么需要管理GPU？ 默认TF程序运行会沾满耗尽GPU 如何管理GPU 使用内存增长式 API 内存增长解释： 按需分配 # 查看物理GPU信息 gpus = tf.config.experimental.list_physical_devices('GPU') # 获取所有物理GPU信息 tf.config.experimental.set_visible_devices(gpus[2]) # 只使用 第3个GPU， 默认不设置就是使用所有GPU for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) # 这行代码必须放在前面 print(len(gpus)) GPU 逻辑切分 （分配大小） 就像划分CDE盘符一样，实际上还是一个整体的GPU tf.config.experimental.set_virtual_device_configuration( gpus[2], # 拿第3块物理 GPU来切分 [ tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048), # 实例化第一个蛋糕，并分为2G tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048), # 实例化第二个蛋糕，并分为2G ] ) 使用逻辑切分好的GPU # 查看逻辑GPU logical_gpus = tf.config.experimental.list_logical_devices('GPU') # 获取所有物理GPU信息 print(len(logical_gpus)) c = [] for gpu in logical_gpus: with tf.device(gpu.name): # 指定GPU名字作为上下文环境，（将此GPU应用到上下文变量当中） a = xxx b = xxx c.append( a @ b ) # 矩阵乘法，应用到了CPU，遍历一次，切换一个GPU with tf.device('/CPU:0'): # 指定CPU名字作为上下文环境，（将此CPU应用到上下文变量当中） tf.add_n(c) 分布式策略 MirroredStrategy 镜像式策略： 0. 一台机器， 多GPU 参数同步式，分布式训练 同步的意思是， 每个GPU的不同的参数会 按次序同步处理。 主要机制： 数据集 分发 给 不同的 GPU处理 （参考多线程） CentralStorageStrategy MirroredStrategy 变种 注意这里： 参数转变为 存储在一个设备上集中管理（可CPU，可GPU） 而计算，依然是在所有GPU上运行 MultiWorkerMirroredStrategy 同 MirroredStrategy，只不过 可扩展为 在多台机器 参数同步式 ParameterServerStrategy 参数，异步分布 机器分为 Parameter Server 和 Worker两类 (可理解为，生产者，消费者) Parameter Serve： 负责管理，更新 参数和梯度 Worker: 负责计算，训练网络 将输入数据转发给 多个 Worker Woker流程： 3.1 Worker们， 训练后 将参数 push 回给 Server 3.2 Worker们， 训练后 将 Server的参数 pull 拉过来 Server流程： 3.1 梯度聚合 3.2 梯度更新 总结同步式 vs 异步式 同步式： 就像分布式爬虫一样， 同步式，相当于在 Server中 加了一个缓冲管道。 等所有GPU齐了，再全部聚合更新。 缺点： 有的机器计算快，有的机器计算慢，浪费时间 (短板效应，，拖后腿) 适用于： 一台机器，多个GPU， 避免过多网络IO通信 异步式： 缺点+ 也算是优点： 就像多线程对全局变量的不可控一样。可能参数错乱。 但是这样训练出的模型，更有泛化能力（更能容忍错误， 所以这也算是个小优点） 适用于： 多机器，多GPU 分布式实例（以MirroredStrategy为例，其他也一样） keras中使用分布式： strategy = tf.distribute.MirroredStrategy() with strategy.scope(): # 同样制造一个上下文 model = keras.models.Sequential([...]) # 模型定义部分放在上下文意味着，参数需要分布式管理 model.add() ... ... model.add() model.compile(...) # compile 放在上下文中，意味着，训练环节，同样也需要分布式训练 # 至此结束 estimator使用分布式 原始，没有添加分布式的 estimator 代码 model = keras.Sequential([]) estimator = keras.estimator.model_to_estimator(model) # 偷个懒，直接用Keras转过来的， 其实estimator 有很多内置模型（比如线性回归，逻辑回归）。和SKlearn用法差不多 # 所以在训练时，把 model.fit替换为 estimator.train from sklearn.linear_model.base import make_dataset estimator.train( input_fn= lambda: 自己处理数据的函数(参数), # 此函数返回结果通常是 tf.data.Dataset对象() max_steps=1000 # 最多训练1000步数 ) # 训练结果的精度值等指标，通常输出到一个文件中， 可通过 tensorflow_board 打开查看 使用 分布式的 estimator 代码： 只需按照上面便跟几行即可： strategy = tf.distribute.MirroredStrategy() config = tf.estimator.RunConfig( # 主要加了这里 train_distribute=strategy ) model = keras.Sequential([]) estimator = keras.estimator.model_to_estimator(model, config=config) 下面同上 自定义模型+训练 的 分布式改造 分布式效果不好？？ 可以考虑 把 batch _ size 增大 可以尝试数据分布式 数据分布式代码： strategy = tf.distribute.MirroredStrategy() with strategy.scope(): ... train_dataset = strategy.experimental_distribute_dataset(train_dataset) test_dataset = strategy.experimental_distribute_dataset(test_dataset) ","link":"https://cythonlin.github.io/post/py-greater-tensorflow20-gpu-guan-li-yu-fen-bu-shi/"},{"title":"PY => Tensorflow2.0模型保存与部署","content":"模型保存 tf1: checkpoint（主力） tf2: keras(hdf5) 或 SavedModel(tf2.0) 推荐格式 模型部署 TFLite TFLite Converter 将上面的 hdf5 或 SavedModel 转化为 TFLite格式 TFLite Interpreter 用来加载 TFLite Converte 转化后的模型 （支持 IOS Andriod） （支持C++等多种语言） TFLite FlatBuffer TFLite 是一种 FlatBuffer 格式 FlatBuffer： Google开源 跨平台数据序列化库 TFLite 量化 量化： 参数从 float32 变为 int8。 （损失精度，提升速度） 模型可以降低为原来的 1/4 实战 保存模型 tf2的模型保存，有两大派别： 方式1. keras的保存方式 方式2. saved _ model的保存方式 方式1具体使用： 使用callback keras.callbacks.ModelCheckpoint( 'xxx.h5', save_best_only=True, save_weights_only=False # 默认就是False, 意为保存 模型+参数 ) 方式2具体使用： 使用 tf.saved _ model() 将 keras转为 saved _ model tf.saved_model.save(model, './文件夹名') # 结果目录里面有个 saved_model.pd 有用 模型 转化为 Lite格式 keras格式的模型对应API： keras_model = tf.lite.TFLiteConverter.from_keras_model(model) # 普通 keras_tflite = keras_model.convert() with open('keras目录', 'wb') as f: f.write(keras_tflite) saved _ model 格式的模型对应API： saved_model = tf.lite.TFLiteConverter.from_saved_model('./原始模型保存目录') saved_model_tflite = saved_model.convert() with open('saved_model目录', 'wb') as f: f.write(saved_model_tflite) TFLite Interpreter继续操作Lite with open('keras目录', 'rb') as f: # 读取 TFLite keras_model = f.read() interpreter = tf.lite.Interpreter( model_content=keras_model ) interpreter.allocate_tersors() # 给 TFLite中的所有 Tensor 分配内存 模型量化 实操 其实就是在上面的基础上，中间部分加一行代码 keras_model = tf.lite.TFLiteConverter.from_keras_model(model) # 普通 # 就在这里加 keras_model.optimizations = [ tf.lite.Optimize.OPTIMIZE_FOR_SIZE ] keras_tflite = keras_model.convert() 模型转换（转换为 JS为例） 首先需要下载一个工具 tensorflowjs pip install tensorflowjs -i https://pypi.douban.com/simple 注意一下， tensorflowjs目前最高只支持 tf1.14 。 所以你运行上述安装命令，你的tf会自动降级到 tf1.14...... 查看帮助文档，看看参数： tensorflowjs_converter --help 正式转换：(CMD or Shell命令行式转换方法) （keras保存版） tensorflowjs_converte --input_format keras --output_format tfjs_layers_model ./xxx/xxx.h5 ./keras_to_tfjs_layers # 倒数第二个参数是 输入 # 倒数第一个参数是 输出 # 运行成功后， JS保存目录中 会有2个文件 1. xx.bin # 真正的模型 2. xx.json # 模型相关配置信息 （model _ saved保存版） tensorflowjs_converte --input_format tf_saved_model --output_format tfjs_graph_model ./xxx/xxx.h5 ./keras_to_tfjs_layers # 上面的 saved_model 版本 主要改了这两个参数: --input_format tf_saved_model \\ --output_format tfjs_graph_model \\ 其次还有 代码式 转换方法： import tensorflowjs as tfjs model = ... tfjs.converters.save_keras_model(model,'新JS路径') tfjs.converters.convert_tf_saved_model(model,'新JS路径') JS端调用 需要安装Node等启动服务。 Node也需要安装 tensorflowjs 模块。 使用方法，略。。。。。， ","link":"https://cythonlin.github.io/post/py-greater-tensorflow20-mo-xing-bao-cun-yu-bu-shu/"},{"title":"AI => Tensorflow2.0语法 - dataset数据封装+训测验切割（二）","content":"训练集-测试集-验证集切割 ###方法1：（借用三方sklearn库） 因为sklearn的train_test_split只能切2份，所以我们需要切2次： from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split( x, y, # x,y是原始数据 test_size=0.2 # test_size默认是0.25 ) # 返回的是 剩余训练集+测试集 x_train, x_valid, y_train, y_valid = train_test_split( x_train, y_train, # 把上面剩余的 x_train, y_train继续拿来切 test_size=0.2 # test_size默认是0.25 ) # 返回的是 二次剩余训练集+验证集 切分好的数据，一般需要做 batch_size， shuffle等， 可以使用 tf.keras模型的 fit() 一步传递！ eg: model.compile( loss=keras.losses.mean_squared_error, optimizer=keras.optimizers.SGD(), metrics=['acc'] # 注意这个metrics参数，下面一会就提到 ) history = model.fit( x_train, y_train, validation_data=(x_valid, y_valid), # 验证集在这里用了！！！ epochs=100, batch_size = 32 # batch_size 不传也行，因为默认就是32 shuffle=True, # shuffle 不传也行，因为默认就是True # callbacks=callbacks, # ) 度量指标 = model.evaluate(x_test, y_test) # 返回的是指标（可能包括loss,acc） # 这里说一下，为什么我说可能包括。 # 因为这个返回结果取决于 你的 model.compile() 传递的参数 # 如果你传了 metrics=['acc']， 那么这个度量指标的返回结果就是 (loss, acc) # 如果你没传 metrics ， 那么这个度量指标的返回结果就是一个 loss y_predict = model.predict(x_test) # 返回的是预测结果 ###方法2：（tf.split） 自己封装的代码：功能包括： 3切分，乱序数据集，分批操作 一体化！！！ （可能有瑕疵） 已上传至Github : https://github.com/hacker-lin/train_test_valid_split 定义部分： class HandlerData: def init(self, x, y): &quot;&quot;&quot;我封装的类，数据通过实例化传进来保存&quot;&quot;&quot; self.x = x self.y = y def shuffle_and_batch(self, x, y, batch_size=None): &quot;&quot;&quot;默认定死乱序操作，batch_size可选参数， 其实乱序参数也应该设置可选的。懒了&quot;&quot;&quot; data = tf.data.Dataset.from_tensor_slices((x, y)) # 封装 dataset数据集格式 data_ = data.shuffle( # 乱序 buffer_size=x.shape[0], # 官方文档说明 shuffle的buffer_size 必须大于或等于样本数量 ) if batch_size: data_ = data_.batch(batch_size) return data_ def train_test_valid_split(self, test_size=0.2, # 测试集的切割比例 valid_size=0.2, # 验证集的切割比例 batch_size=32, # batch_size 默认我设为了32 is_batch_and_shuffle=True # 这个是需不需要乱序和分批，默认设为使用乱序和分批 ): sample_num = self.x.shape[0] # 获取样本总个数 train_sample = int(sample_num * (1 - test_size - valid_size)) # 训练集的份数 test_sample = int(sample_num * test_size) # 测试集测份数 valid_train = int(sample_num * valid_size) # 验证集的份数 # 这三个为什么我用int包裹起来了，因为我调试过程中发现，有浮点数计算精度缺失现象。 # 所以必须转整形 # tf.split() 此语法上一篇我讲过，分n份，每份可不同数量 x_train, x_test, x_valid = tf.split( self.x, num_or_size_splits=[train_sample, test_sample, valid_train], axis=0 ) y_train, y_test, y_valid = tf.split( self.y, [train_sample, test_sample, valid_train], axis=0 ) # 因为份数是我切割x,y之前计算出来的公共变量。所以不用担心 x,y不匹配的问题。 if is_batch_and_shuffle: # 是否使用乱序和分批，默认是使用的，所以走这条 return ( self.shuffle_and_batch(x_train, y_train, batch_size=batch_size), self.shuffle_and_batch(x_test, y_test, batch_size=batch_size), self.shuffle_and_batch(x_valid, y_valid, batch_size=batch_size), ) else: # 如果你只想要切割后的原生数据，那么你把is_batch_and_shuffle传False就走这条路了 return ( (x_train, y_train), (x_test, y_test), (x_valid, y_valid) ) 调用案例： x = tf.ones([1000, 5000]) y = tf.ones([1000, 1]) data_obj = HandlerData(x,y) # x是原生的样本数据，x是原生的label数据 # 方式1：使用乱序，使用分批，就是一个参数都不用传，全是默认值 train, test, valid = data_obj.train_test_valid_split( # test_size=0.2, # valid_size=0.2, # batch_size=32, # is_batch_and_shuffle=True ) # 这些参数你都可以不传，这都是设置的默认值。 print(train) print(test) print(valid) # 结果 &gt;&gt;&gt; &lt;BatchDataset shapes: ((None, 5000), (None, 1)), types: (tf.float32, tf.float32)&gt; &gt;&gt;&gt; &lt;BatchDataset shapes: ((None, 5000), (None, 1)), types: (tf.float32, tf.float32)&gt; &gt;&gt;&gt; &lt;BatchDataset shapes: ((None, 5000), (None, 1)), types: (tf.float32, tf.float32)&gt; # 虽然你看见了样本数为None，但是没关系，因为你还没使用，遍历一下就明白了 for x_train,y_train in train: print(x_train.shape,y_train.shape) # 结果 600 // 32 == 18 （你可以查一下正好18个） # 结果 600 % 32 == 24 （你可以看一下最后一个就是24） (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (32, 5000) (32, 1) (24, 5000) (24, 1) # 32个一批，最后一个就是余数 24个了。 # 方式2：不使用乱序，使用分批，只要原生数据， (x_train, y_train), (x_test, y_test), (x_valid, y_valid) = data_obj.train_test_valid_split( # test_size=0.2, # valid_size=0.2, # batch_size=32, is_batch_and_shuffle=False # 这个改为False即可，其他参数可选 ) print(x_train.shape, y_train.shape) print(x_test.shape, y_test.shape) print(x_valid.shape, y_valid.shape) # 结果 &gt;&gt;&gt; (600, 5000) (600, 1) &gt;&gt;&gt; (200, 5000) (200, 1) &gt;&gt;&gt; (200, 5000) (200, 1) 方式3（训验分割） history = model.fit( ..... validation_split=0.2 # 训练集分出0.2给验证集 ) 数据处理 （dataset） 这个模块的作用就是，将我们的数据，或者 TF张量，封装成数据集。 这个数据集具有成品API，比如：可以帮助我们，分批，乱序，制作迭代，等一些列操作。 基本理解 dataset = tf.data.Dataset.from_tensor_slices(np.arange(16).reshape(4,4)) 按理来说（先不取），数据形状应该是这样的。 （一个大列表里面，有4个小列表） [ [0, 1, 2 ,3 ], [4, 5, 6 ,7 ], [8, 9, 10,11], [12,13,14,15], ] for data in dataset: # 封装的数据集需要遍历（或者 iter() 改变为迭代器类型），才能返回值 print(data) # 每遍历一条就是里面的小列表。 eg:第一条形状： [0, 1, 2 ,3 ] # 但是别忘了。我们这是Tensorflow，因此每层数据集都被封装为Tensor。 # 因此，我们每遍历出一条数据，都是一条Tensor 输出： &gt;&gt; tf.Tensor([0 1 2 3], shape=(4,), dtype=int32) tf.Tensor([4 5 6 7], shape=(4,), dtype=int32) tf.Tensor([ 8 9 10 11], shape=(4,), dtype=int32) tf.Tensor([12 13 14 15], shape=(4,), dtype=int32) 前面说了，这个数据的格式就是（一个大列表里面，有4个小列表） 对应来看， （一个大Tensor里面， 有4个小Tensor）。 记住这个理念 数据来源参数类型 参数传元组： question = [[1, 0], [1, 1]] answer = ['encode', 'decoder'] dataset = tf.data.Dataset.from_tensor_slices( (question, answer) ) # 用元组包起来了 for data in dataset: print(data[0],'=&gt;' ,data[1]) 输出: &gt;&gt; tf.Tensor([1 0], shape=(2,), dtype=int32) =&gt; tf.Tensor(b'encode', shape=(), dtype=string) tf.Tensor([1 1], shape=(2,), dtype=int32) =&gt; tf.Tensor(b'decoder', shape=(), dtype=string) 你可以看出它自动把我们传递的 question 和 answer 两个大列表。 &quot;相当于做了zip()操作&quot;。 # 我的实验经历：训练 Encoder-Decoder模型的，&quot;问答对数据&quot;，做编码后，就可以这样用元组传。 参数传字典： data_dict = { 'encoder': [1, 0], 'decoder': [1, 1] } dataset = tf.data.Dataset.from_tensor_slices(data_dict) for data in dataset: # 其实每一个元素就是一个字典 print(data) # 其实就是把你的 value部分，转成了Tensor类型。 总体结构没变 链式调用 Dataset API 大多数操作几乎都是链式调用（就像python字符串的 replace方法） 用上面的数据作为案例数据， 介绍几种API： batch (分批) for data in dataset.batch(2): # 若设置 drop_remainder=True，则最后余下一批会被丢弃 print(data) 输出： &gt;&gt; tf.Tensor([[0 1 2 3] [4 5 6 7]], shape=(2, 4), dtype=int32) tf.Tensor([[ 8 9 10 11] [12 13 14 15]], shape=(2, 4), dtype=int32) 上面说过，默认就是 遍历出的每个子项，就是一个Tensor， 如上数据，遍历出 4个Tensor 而调用 batch(2) 后， 把2个子项分成一批， 然后再包装成为Tensor。 so, 4/2 = 2批 ， 包装成2个Tensor repeat（重复使用数据：epoch理念， 重复训练n轮次） 注意（传的就是总重复数，算自身）： 1. 如果repeat() 不传参数，那就是无限重复。。。 2. 如果传参数 = 0, 那么代表不取数据 3. 如果传参数 = 1, 那么代表一共就一份数据 4. 如果传参数 = 2, 那么代表一共就2份数据（把自己算上，一共2份，就这么个重复的意思） for data in dataset.repeat(2).batch(3): # 重复2次。 3个一组 （这就是链式调用） print(data) 结果 &gt;&gt; tf.Tensor([[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]], shape=(3, 4), dtype=int32) tf.Tensor([[12 13 14 15] [ 0 1 2 3] [ 4 5 6 7]], shape=(3, 4), dtype=int32) tf.Tensor([[ 8 9 10 11] [12 13 14 15]], shape=(2, 4), dtype=int32) 原数据是4个子项， 重复2次 ： 4*2=8 然后链式调用分3批： 8/3=2 ..... 2 （整批3个一组， 最后一批余数一组） # 还要注意一下， 它们重复是顺序重复拼接。 分批时，可以首尾相连的 （eg:就像小时候吃的一连串棒棒糖， 拽不好，会把上一个的糖皮连着拽下来） 未完待续 ","link":"https://cythonlin.github.io/post/ai-greater-tensorflow20-yu-fa-dataset-shu-ju-feng-zhuang-xun-ce-yan-qie-ge-er/"},{"title":"AI => Tensorflow2.0语法 - 张量&基本函数(一)","content":"前言 TF2.0 是之前学习的内容，当时是写在了私有的YNote中，重写于SF。 之前接触过 TF1, 手动session机制，看着很是头疼。 TF2.0不需要做这些 TF2.0 理解起来更容易（逐渐 Pythonic and Numpic） TF2.0 后端采用keras接口 (构建网络层)，更方便。 TF2.0 的keras接口定义的模型层，都实现了 call方法。意味大多数实例对象可以当作函数来直接调用 行列轴 以列表为例（抽象举例，摞起来的面包片。。。。） [ # 最外层，无意义不用记 [1,2,3], # 面包片1 (第一个样本) [4,5,6], # 面包片2 (第二个样本) ] 每个 次内层列表 代表一个样本， 比如 [1,2,3] 整体代表 第一个样本 最内层元素代表属性值。 eg: 1,2,3 单个拿出来都是属性值。 例子： 元素5 单独拿出来，它就被看做 &quot;第二个样本的，属性值5&quot; （当然横纵索引依然都是从0取的） 以刚才的数据为例： t = tf.constant( [ [1., 2., 3.], [4., 5., 6.] ] ) print(tf.reduce_sum(t, axis=0)) # 求和操作，上下压扁， 聚合样本 &gt;&gt; tf.Tensor([5. 7. 9.], shape=(3,), dtype=float32) print(tf.reduce_sum(t, axis=1)) # 求和操作，左右压扁， 聚合属性 &gt;&gt; tf.Tensor([ 6. 15.], shape=(2,), dtype=float32) 注：Numpy轴也是这样的，我最初用x，y轴方式 抽象 去记忆， 基本上是记不住的。。太多概念混淆。 但如果你记不住，你每次使用各种操作和聚合API时，都会自己在心理重新花大量时间理一遍。浪费时间。 所以你一定要练习理解，要做到：“瞄一眼，就能知道这种维度的数据的意义，以及轴操作的意义” 我自己的记忆方式（axis=0, axis=1）： 0轴通常代表，样本（上下压扁） 1轴通常代表，属性（左右压扁） 常需要用 axis参数 的相关聚合函数： tf.reduce_sum() # 求和 tf.reduce_mean() # 平均值 tf.reduce_max() # 最大值 tf.reduce_min() # 最小值 tf.square() # 平方 tf.concat() # 拼接 注： 如果 axis参数 &quot;不传&quot;， 那么&quot;所有维度&quot;都会被操作。 常用导入 # 基本会用到的 import numpy as np import tensorflow as tf from tensorflow import keras # 可选导入 import os, sys, pickle import scipy import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler # 标准化 from sklearn.model_selection import train_test_split # 训测分离 张量&amp;操作符 常量（普通的张量） 定义： c = tf.constant( [[1., 2., 3.], [4., 5., 6.]] ) # 数字后面加个点代表 转float32 类型 print(c) &gt;&gt; tf.Tensor([[1. 2. 3.] [4. 5. 6.]], shape=(2, 3), dtype=float32) 六则运算（加减乘除，矩阵乘, 矩阵转置） 先说矩阵乘法（大学都学过的，运算过程不说了）： 语法格式： a @ b 条件要求： a的列数 === b的行数 （必须相等） eg: （5行2列 @ 2行10列 = 5行10列） 特例： (第0维度，必须相等) t1 = tf.ones([2, 20, 30]) t2 = tf.ones([2, 30, 50]) print( (t1@t2).shape ) &gt;&gt; (2, 20, 50) # 第0维没变， 后2维照常按照矩阵乘法运算 矩阵转置： tf.transpose(t) # 不仅可以普通转置，还可以交换维度 t2 = tf.transpose(t,[1,0]) # 行变列，列变行。 和基本的转置差不多（逆序索引，轴变逆序） # 或假如以 （2,100，200，3）形状 为例 t = tf.ones([2, 100, 200, 3]) print(tf.transpose(t, [1, 3, 0, 2]).shape) # 轴交换位置 &gt;&gt; (100, 3, 2, 200) # 原1轴 -&gt; 放在现在0轴 # 原3轴 -&gt; 放在现在1轴 # 原0轴 -&gt; 放在现在2轴 # 原2轴 -&gt; 放在现在3轴 加减乘除都具有&quot;广播机制&quot; ： 形象（广播机制解释）解释： 我尝试用白话解释下： 1. 我形状和你不一样， 但我和你运算的时候，我会尽力扩张成 你的形状 来和你运算。 2. 扩张后如果出现空缺， 那么把自己复制一份，填补上 （如果补不全，就说明不能运算） 3. 小形状 服从 大形状 (我比你瘦，我动就行。 你不用动。。。) eg: t = tf.constant( [ [1, 2, 3], [4, 5, 6], ] ) t + [1,2,1] 过程分析： [1,2,1] 显然是 小形状， 它会自动尝试变化成大形状 -&gt; 第一步变形（最外层大框架满足， 里面还有空缺）： [ [1,2,1], ] 第二步变形 (把自己复制，然后填补空缺)： [ [1,2,1], [1,2,1], # 这就是复制的自己 ] 第三步运算（逐位相加） [ + [ = [ [1,2,3], [1,2,1], [2,4,4], [4,5,6], [1,2,1], [5,7,7], ] ] [ 抽象（广播机制）演示： 假如 t1 的 shape为 [5,200,100,50] 假如 t2 的 shape为 [5,200] 注意：我以下的数据演示，全是表示 Tensor的形状，形状，形状！ [5,200,1,50] # 很明显，开始这2行数据 维度没匹配， 形状也没对齐 [5,1] ------------------------ [5,200,1,50] [5,50] # 这行对齐补50 ------------------------ [5,200,5,50] # 这行对齐补5 [5,50] ------------------------ [5,200,5,50] [1, 1, 5,50] # 这行扩张了2个， 默认填1 ------------------------ [5,200,5,50] [1,200, 5,50] # 这行对齐补200 ------------------------ [5,200,5,50] [5,200,5,50] # 这行对齐补5 注意： 1. 每个维度形状：二者必须有一个是1， 才能对齐。 （不然ERROR，下例ERROR-&gt;） [5,200,1,50] [5,20] # 同理开始向右对齐，但是 50与20都不是1，所以都不能对齐，所以ERROR 2. 若维度缺失： 依然是全部贴右对齐 然后先从右面开始，补每个维度的形状 然后扩展维度，并默认设形状为1 然后补扩展后维度的形状（因为默认设为1了，所以是一定可以补齐的） 当然上面说的都是运算时的自动广播机制 你也可以手动广播： t1 = tf.ones([2, 20, 1]) # 原始形状 【2，20，1】 print(tf.broadcast_to(t1, [5,2,20,30]).shape) # 目标形状【5，2，20，30】 [5,2,20,30] [2,20, 1] ----------- [5,2,20,30] [2,20,30] ----------- [5,2,20,30] [1,2,20,30] ----------- [5,2,20,30] [5,2,20,30] 注：因为是手动广播，所以只能 原始形状 自己向 目标形状 ”补充维度，或者补充形状“ 而目标形状是一点也不能动的。 扩充维度（f.expand_dims）+ 复制（tile） 代替 =&gt; 广播（tf.broadcasting） 同样是上面的例子，我想把形状 [2,20,1] ，变成 [5，2，20，30] t1 = tf.ones([2, 20, 1]) a = tf.expand_dims(t1,axis=0) # 0轴索引处插入一个轴， 结果[1,2,20,1] print(tf.tile(a,[5,1,1,30]).shape) # 结果 [5, 2, 20, 30] 流程： [5,2,20,30] [2,20,1] ----------- [5,2,20,30] # tf.expand_dims(t1,axis=0) [1,2,20,1] # 0号索引插入一个新轴（增维） ----------- [5,2,20,30] # tf.tile(5,1,1,30) (形状对齐，tile每个参数代表对应轴的形状扩充几倍) [5,2,30,30] 15 21 201 130 tile 与 broadcasting的区别： tile是物理复制，物理空间增加 而broadcasting是虚拟复制，（为了计算，隐式实现的复制，并没有物理空间增加） tile可以对任意（整数倍复制n*m, mn同为整数） 而broadcasting（原始数据形状只能存在1的情况下才能扩张。 1*n , n为整数） 压缩维度（tf.squeeze）： 就是把每个维度为1的维度都删除掉 （就像数学 a * 1 = a） print(tf.squeeze(tf.ones([2,1,3,1])).shape) (2, 3) 当然你也可以指定维度压缩（默认不指定，所有维度为1的全部压缩）： print(tf.squeeze(tf.ones([2,1,3,1]), axis=-1).shape) &gt;&gt;&gt; (2, 1, 3) 索引&amp;切片 灵魂说明：无论索引还是切片， （行列 是使用 逗号 分隔的）， 并且无论行列，索引都是从0开始的。 索引：取一个值 print(t[1,2]) # 逗号前面代表行的索引， 逗号后面是列的索引 &gt;&gt; tf.Tensor(6.0, shape=(), dtype=float32) 切片：取子结构 （有两种方式） 方式1（冒号切片）： print(t[:, 1:]) # 逗号前面是行。只写: 代表取所有行。逗号后面是列。 1: 代表第二列到最后 &gt;&gt; tf.Tensor([[2. 3.] [5. 6.]], shape=(2, 2), dtype=float32) 方式2（省略号切片）: （我相信不了解Numpy的人都没听说过 python的 Ellipsis ， 就是省略号类） 先自己去运行玩玩这行代码： print(... is Ellipsis) &gt;&gt;&gt; True 回到正题：（省略号 ... 切片，是针对多维度的， 如果是二维直接用:即可） (我们以三维为例，这个就不适合称作行列了) # shape 是 (2, 2, 2) t = tf.constant( [ # 一维 [ # 二维 [1, 2], # 三维 [3, 4], ], [ [5, 6], [7, 8], ], ] ) 伪码：t[1维切片, 二维切片, 三维切片] 代码：t[:, :, 0:1] # 1维不动， 2维不动， 3维 取一条数据 结果: shape为 (2,2,1) [ # 一维 [ # 二维 [1], # 三维 [3], ], [ [5], [7], ], ] 看不明白就多看几遍。 发现没，即使我不对 1维，和 2维切片，我也被迫要写 2个: 来占位 那假如有100个维度，我只想对最后一个维度切片。 前99个都不用动， 那难道我要写 99个 : 占位？？ 不，如下代码即可解决： print(t[..., 0:1]) # 这就是 ... 的作用 （注意，只在 numpy 和 tensorflow中有用） tensor 转 numpy 类型 t.numpy() # tensor 转为 numpy 类型 变量 定义： v = tf.Variable( # 注意： V是大写 [ [1, 2, 3], [4, 5, 6] ] ) 变量赋值(具有自身赋值的性质)： 注意： 变量一旦被定义，形状就定下来了。 赋值（只能赋给同形状的值） v.assign( [ [1,1,1], [1,1,1], ] ) print(v) &gt;&gt; &lt;tf.Variable 'Variable:0' shape=(2, 3) dtype=int32, numpy=array([[1, 1, 1],[1, 1, 1]])&gt; 变量取值（相当于转换为Tensor）： 特别： 变量本身就是 Variable类型， 取值取出得是 Tensor （包括切片取值，索引取值等） print( v.value() ) &gt;&gt; tf.Tensor([[1 2 3] [4 5 6]], shape=(2, 3), dtype=int32) 变量 索引&amp;切片 赋值： 常量:是不可变的。所以只有取值，没有赋值。 变量:取值、赋值都可以 v.assign(xx) 类似于 python的 v=xx v[0, 1].assign(100) # 索引赋值, v.assign 等价于 v[0, :].assign([10, 20, 30]) # 注意，切片赋值传递的需要是容器类型 特别注意： 前面说过，变量 结构形状 是 不可变的，赋值的赋给的是数据。 但是你赋值的时候要时刻注意，不能改变变量原有形状 拿切片赋值为例： 你切多少个，你就得赋多少个。 并且赋的值结构要一致。 举个栗子： 你从正方体里面挖出来一个小正方体。那么你必须填补一块一模一样形状的小正方体） 还有两种扩展API： v.assign_add() # 类似python 的 += v.assign_sub() # 类似python 的 -= 变量 索引&amp;切片 取值 同 常量切片取值（略） Variable 转 Numpy print(v.numpy()) 不规则张量（RaggedTensor） 定义： rag_tensor = tf.ragged.constant( [ [1,2], [2,3,4,5], ] ) # 允许每个维度的数据长度参差不齐 拼接：假如需要&quot;拼接 不规则张量&quot; （可使用 tf.concat(axis=) ） 0轴：竖着拼接（样本竖着摞起来）可随意拼接。 拼接后的依然是&quot;不规则张量&quot; 1轴：横着拼接（属性水平拼起来）这时候需要你样本个数必须相等， 否则对不上，报错 总结： 样本竖着随便拼， 属性横着（必须样本个数相等） 才能拼 RaggedTensor 普通 Tensor： 说明：普通Tensor是必须要求， 长度对齐的。入 对不齐的 末尾补0 tensor = rag_tensor.to_tensor() 稀疏张量 （Sparse Tensor） 特点（可理解为 记录索引）： 只记录非0的坐标位置， indices参数：每个 子列表 表示 一个坐标 虽然只记录坐标，但是转为普通Tensor后，只有坐标位置 有值， 其他位置的值全是0 填充范围，取决于 dense_shape的设定 定义： s = tf.SparseTensor( indices=[[0, 1], [1, 0], [2, 3]], # 注意，这个索引设置需要是（从左到右，从上到下）的顺序设置 values=[1, 2, 3], # 将上面3个坐标值分别设值为 1，2，3 dense_shape=[3, 4] # Tensor总范围 ) print(s) SparseTensor(indices=tf.Tensor([[0 1], [1 0],[2 3]], shape=(3, 2), dtype=int64)。。。 转为普通 Tensor (转为普通Tensor后，看见的才是存储真正的值) tensor = tf.sparse.to_dense(s) print(tensor) tf.Tensor([ [0 1 0 0],[2 0 0 0],[0 0 0 3] ], shape=(3, 4), dtype=int32) 如果上面使用 to_dense() 可能会遇到错误： error: is out of range 这个错误的原因是创建 tf.SparseTensor(indices=) ，前面也说了indices，要按（从左到右，从上到下）顺序写 当然你也可以用排序API，先排序，然后再转： eg: _ = tf.sparse.reorder(s) # 先将索引排序 tensor = tf.sparse.to_dense(_) # 再转 tf.function 这个API作为一个装饰器使用， 用来将 Python 语法转换 尽可能有效的转换为 TF语法、图结构 import tensorflow as tf import numpy as np @tf.function def f(): a = np.array([1, 2, 3]) b = np.array([4, 5, 6]) return a + b print( f() ) &gt;&gt;&gt; tf.Tensor([5 7 9], shape=(3,), dtype=int32) 你应该发现了一个特点，我们定义的 f()函数内部，一个tf语法都没写。 只装饰了一行 @tf.function 而调用结果返回值居然是个 tensor 。 这就是 @tf.function 装饰器的作用了！ 当然函数里面，也可以写 tf的操作，也是没问题的。 但注意一点， 函数里面不允许定义 变量， 需要定义的变量 应 拿到函数外面定义 a = tf.Variable([1,2,3]) # 如需tensor变量，应该放在外面 @tf.function def f(): # a = tf.Variable([1,2,3]) # 这里面不允许定义变量! pass 合并相加（tf.concat） 我的理解就是（基本数学的 合并同类项） # 合并同类项的原则就是有1项不同，其他项完全相同。 # 前提条件：（最多，有一个维度的形状不相等。 注意是最多） t1 = tf.ones([2,5,6]) t2 = tf.ones([6,5,6]) print( tf.concat([t1,t2],axis=0).shape ) # axis=0,传了0轴，那么其他轴捏死不变。只合并0轴 &gt;&gt; (8,5,8) 堆叠降维（tf.stack） 我的理解就是（小学算术的，进位，（进位就是扩充一个维度表示个数）） # 前提条件：所有维度形状必须全部相等。 tf1 = tf.ones([2,3,4]) tf2 = tf.ones([2,3,4]) tf3 = tf.ones([2,3,4]) print(tf.stack([tf1,tf2,tf3], axis=0).shape) # 你可以想象有3组 [2,3,4]，然后3组作为一个新维度，插入到 axis对应的索引处。 &gt;&gt; (3, 2, 3, 4) # 对比理解，如果这是tf.concat(), 那么结果就是 (6,3,4) 拆分降维（tf.unstack） 和tf.stack正好是互逆过程，指定axis维度是几，它就会拆分成几个数据，同时降维。 a = tf.ones([3, 2, 3, 4]) for x in tf.unstack(a, axis=0): print(x.shape) 结果如下（分成了3个 [2,3,4]） &gt;&gt;&gt; (2, 3, 4) &gt;&gt;&gt; (2, 3, 4) &gt;&gt;&gt; (2, 3, 4) 拆分不降维（tf.split） ###语法： 和tf.unstack的区别就是，tf.unstack是均分降维， tf.stack是怎么分都不会降维，且能指定分隔份数 a = tf.ones([2,4,35,8]) for x in tf.split(a, axis=3,num_or_size_splits=[2,2,4]): print(x.shape) 结果： &gt;&gt; (2, 4, 35, 2) # 最后一维2 &gt;&gt; (2, 4, 35, 2) # 最后一维2 &gt;&gt; (2, 4, 35, 4) # 最后一维4 ###使用场景： 假如我们想切分数据集为（train-test-valid） 3分比例为 6：2：2 方法1：（scikit-learn 连续分切2次） x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2) x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train,test_size=0.2) # 源码中显示 test_size如果不传。默认为0.25。 # 思路，因为 scikit-learn 只能切出2个结果： 所以我们需要切2次： # 第一次 从完整训练集 切出来 （剩余训练集， 测试集） # 第二次 从剩余数据集 切出来 （剩余训练集2， 验证集） 方法2： （tf.split） x = tf.ones([1000, 5000]) y = tf.ones([1000, 1]) x_train, x_test, x_valid = tf.split( x, num_or_size_splits=[600,200,200], # 切3份 axis=0 ) y_train, y_test, y_valid = tf.split( y, num_or_size_splits=[600,200,200], # 同样切3份 axis=0 ) print(x_train.shape, y_train.shape) print(x_test.shape, y_test.shape) print(x_valid.shape, y_valid.shape) 结果 &gt;&gt;&gt; (600, 5000) (600, 1) &gt;&gt;&gt; (200, 5000) (200, 1) &gt;&gt;&gt; (200, 5000) (200, 1) 高级索引（tf.gather） numpy这种索引叫做 fancy indexing(如果我没记错的话) data = tf.constant([6,7,8]) # 当作真实数据 index = tf.constant([2, 1, 0]) # 当作索引 print(tf.gather(data, index)) &gt;&gt; tf.Tensor([8 7 6], shape=(3,), dtype=int32) 排序（tf.sort） data = tf.constant([6, 7, 8]) print(tf.sort(data, direction='DESCENDING')) # 'ASCENDING' # 默认是ASCENDING升序 tf.argsort() # 同上， 只不过返回的是排序后的，对应数据的index Top-K(tf.math.top_k) 查找出最大的n个（比先排序然后切片的性能要好） a = tf.math.top_k([6,7,8],2) # 找出最大的两个，返回是个对象 print(a.indices) # 取出最大的两个 索引 （） print(a.values) # 取出最大的两个 值 &gt;&gt; tf.Tensor([2 1], shape=(2,), dtype=int32) &gt;&gt; tf.Tensor([8 7], shape=(2,), dtype=int32) tf.GradientTape （自定义求导） 求偏导 v1, v2 = tf.Variable(1.), tf.Variable(2.) # 变量 会 被自动侦测更新的 c1, c2 = tf.constant(1.), tf.constant(2.) # 常量 不会 自动侦测更新 y = lambda x1,x2: x1**2 + x2**2 with tf.GradientTape(persistent=True) as tape: &quot;&quot;&quot;默认这个 tape使用一次就会被删除， persistent=True 代表永久存在，但后续需要手动释放&quot;&quot;&quot; # 因为常量不会被自动侦测，所以我们需要手动调用 watch() 侦测 tape.watch(c1) # 如果是变量，就不用watch这两步了 tape.watch(c2) f = y(c1,c2) # 调用函数，返回结果 c1_, c2_ = tape.gradient(f, [c1,c2]) # 参数2:传递几个自变量，就会返回几个 偏导结果 # c1_ 为 c1的偏导 # c2_ 为 c2的偏导 del tape # 手动释放 tape 求二阶偏导（gradient嵌套） v1, v2 = tf.Variable(1.), tf.Variable(2.) # 我们使用变量 y = lambda x1,x2: x1**2 + x2**2 with tf.GradientTape(persistent=True) as tape2: with tf.GradientTape(persistent=True) as tape1: f = y(v1,v2) once_grads = tape1.gradient(f, [v1, v2]) # 一阶偏导 # 此列表推导式表示：拿着一阶偏导，来继续求二阶偏导（注意，用tape2） twice_grads = [tape2.gradient(once_grad, [v1,v2]) for once_grad in once_grads] # 二阶偏导 print(twice_grads) del tape1 # 释放 del tape2 # 释放 说明 求导数（一个自变量）：tape1.gradient(f, v1) # gradient传 1个自变量 求偏导（多个自变量）：tape1.gradient(f, [v1,v2]) # gradient传 1个列表， 列表内填所有自变量 SGD（随机梯度下降） 方式1：手撕（不使用优化器） v1, v2 = tf.Variable(1.), tf.Variable(2.) # 我们使用变量 y = lambda x1, x2: x1 ** 2 + x2 ** 2 # 二元二次方程 learning_rate = 0.1 # 学习率 for _ in range(30): # 迭代次数 with tf.GradientTape() as tape: # 求导作用域 f = y(v1,v2) d1, d2 = tape.gradient(f, [v1,v2]) # 求导， d1为 v1的偏导， d2为v2的偏导 v1.assign_sub(learning_rate * d1) v2.assign_sub(learning_rate * d2) print(v1) print(v2) 实现流程总结： 1. 偏导 自变量v1,v2求出来的。 （d1, d2 = tape.gradient(f, [v1,v2])） 2. 自变量v1,v2的衰减 是关联 偏导的（ 衰减值 = 学习率*偏导） 3. 我们把前2步套了一个大循环（并设定迭代次数）， 1-2-1-2-1-2-1-2-1-2 步骤往复执行 方式2：借用 Tensorflow 优化器(optimizer) 实现梯度下降 v1, v2 = tf.Variable(1.), tf.Variable(2.) # 我们使用变量 y = lambda x1, x2: x1 ** 2 + x2 ** 2 # 二元二次函数 , 通常这个函数我们用作计算loss learning_rate = 0.1 # 学习率 optimizer = keras.optimizers.SGD(learning_rate=learning_rate) # 初始化优化器 for _ in range(30): # 迭代次数 with tf.GradientTape() as tape: f = y(v1,v2) d1, d2 = tape.gradient(f, [v1,v2]) # d1为 v1的偏导， d2为v2的偏导 optimizer.apply_gradients( # 注意这里不一样了，我们之前手动衰减 [ # 而现在这些事情， optimizer.SGD帮我们做了 (d1, v1), # 我们只需把偏导值，和自变量按这种格式传给它即可 (d2, v2), ] ) # 通常这种格式，我们用 zip() 实现 # eg: # model = keras.models.Sequential([......]) # ....... # grads = tape.gradient(f, [v1,v2]) # optimizer.apply_gradients( # zip(grads, model.trainable_variables) # ) print(v1) print(v2) 实现流程总结： 1. 偏导 是自变量v1,v2求出来的 （d1, d2 = tape.gradient(f, [v1,v2])） # 此步骤不变 2. 把偏导 和 自变量 传给optimizer.apply_gradients() optimizer.SGD() 自动帮我们衰减。 3. 我们还是把前2步套了一个大循环（并设定迭代次数）， 1-2-1-2-1-2-1-2-1-2 步骤往复执行。 注： 假如你用adam等之类的其他优化器，那么可能有更复杂的公式，如果我们手撕，肯能有些费劲。 这时候我们最好使用 optimizer.Adam ...等各种 成品，优化器。通用步骤如下 1. 先实例化出一个优化器对象 2. 实例化对象.apply_gradients([(偏导,自变量)]) ","link":"https://cythonlin.github.io/post/ai-greater-tensorflow20-yu-fa-zhang-liang-andji-ben-han-shu-yi/"},{"title":"AI => Tensorflow2.0安装","content":"环境推荐 最后成功的配套版本如下： win10 gtx1050 (其他型号的，不保证，应该也能差不多) python3.7 (感觉python问题并不是特别大。 建议 3.7) cuda: cuda_10.0.130_411.31_win10 cudnn: cudnn-10.0-windows10-x64-v7.4.2.24 tensorflow-gpu==2.0.0-beta0 安装CUDA 官链：https://developer.nvidia.com/cuda-downloads 配置环节： 1. 默认自动安装的路径如下： （如果你选的自定义安装，你要记住你的路径） C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin 2. 将此路径配置到 环境变量 中 3. 关掉所有cmd, 重新打开cmd，输入 nvcc -V 4. 没报错说明安装成功。 安装CUDNN 官链：https://developer.nvidia.com/rdp/cudnn-download（如果你不想用我的工具包，可自行官链） (官网下 CUDNN 貌似需要登录) 操作环节： 你可以看到我分享的资源中，有CUDNN，下载下来，解压（任意位置都可，记住就行）。 解压后，进入解压的目录，你会看到有个cuda目录，进去！然后做如下操作！！！ 2.1. 进入 bin 目录， 把里面的文件（应该就一个），复制到 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin 2.2 进入 include 目录，把里面的文件（应该就一个），复制到 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include 2.3 进入 lib 目录, 再继续进入 x64 目录，把里面的文件（应该就一个），复制到 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64 说一下： 上面3个路径，安装cuda提到的（精简版的默认路径， 你的和我的是一模一样的，直接复制） 将此路径添加到环境变量 （同样是默认路径，直接拿去复制，配了就行）： C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\lib\\x64 安装 pip install tensorflow-gpu==2.0.0 -i https://pypi.douban.com/simple 至此，全部安装完成，接下来测试。 测试安装 &amp; 测试是否使用GPU import tensorflow as tf print(tf.__version__) print(tf.test.gpu_device_name()) 查看版本 &gt;&gt;&gt; import tensorflow as tf # 运行后，会输出一段关于CUDA的 INFO &gt;&gt;&gt; tf.__version__ '2.0.0' Pycharm nextversion https://www.jetbrains.com/pycharm/nextversion/ ","link":"https://cythonlin.github.io/post/ai-greater-tensorflow20stablejiang-lin/"},{"title":"PY => HMM 与 CRF 与 Viterbi","content":"HMM(隐马尔可夫模型) 初始权重矩阵 Π 形状：np.zeros(tag_size) # tag_size就是实体标签的总个数 第一个字出现 认名实体标签的次数 计算公式eg：q(人名) --------------------------------- 5000个句子 转移矩阵（transition） 原理：前面的隐含状态 =&gt; 当前隐含状态 形状：np.zeros([tag_size, tag_size]) # tag_size就是实体标签的总个数 （正方形，寻路） 当前实体标签 和 相邻后一个实体标签联合出现的次数 计算公式eg：t = -------------------------------------- 当前实体标签在5000个句子中出现的次数 发射矩阵 ( emission ) 原理：当前隐含状态 =&gt; 当前预测标签（实体标签） （矩形） 形状：np.zeros([tag_size, vocab_size]) # tag_size就是实体标签的总个数 当前实体标签 和 当前（lstm输出值y）联合出现的次数 计算公式eg：e = ------------------------------------------------------- 当前实体标签出现的次数 偏置： 防止乘0，或者防止 log0 取log: 乘法变加法， 防止小数被乘到最后变成了0 HMM总体公式： q = argmax( P(当前实体标签|上一时刻实体标签) * P(检测到的汉字 | 当前时刻实体标签) ) 朝朝所有实体标记，找到一个标记，使得 q最大即可。 CRF(条件随机场) 和HMM类似，CRF也有类似的结构（转移矩阵，发射矩阵） 假如当前为 Y 转移矩阵的：CRF组合形式，分簇的： 1. 第一簇：所有X， 当前时刻Y(t)， 上一时刻Y(t-1) 2. 第二簇：所有X， 当前时刻Y(t)， 上一时刻Y(t-1) (注意，可理解为时间序列是滑动的。) 3. 第三簇：。。。 核心总结： CRF： 转移矩阵 和 发射矩阵几乎和 HMM差不多。 只不过CRF 每一簇（团）都用到了“所有的” lstm的输出Y HMM: 没有簇， 每次用的只是 当前LSTM输出的 y(t) 最终：同样求的是 argmax ( P(Y|X ) ) Viterbi(维特比算法) 使用了动态规划解决 HMM 和 CRF的预测问题 （&quot;转移&quot; 时可找到全局最优） （而 CRF ，HMM 只能找到局部最优） ","link":"https://cythonlin.github.io/post/py-greater-hmm-yu-crf-yu-viterbi/"},{"title":"PY => NLP + ML","content":"文本相似度： 1. 欧氏距离 (反比) 2. 余弦相似度 x1@x2 / ( |x1| * |x2| ) （正比） TF-IDF tf = 某单词 在所处句子中 出现的个数 （词频） idf (文档总数 / 含有该单词的文档数) = 词频 * 每个单词的重要性 = 词频 * log(文档总数 / 含有该单词的文档数) # 分母包含度越高，代表越不相似） Chain Rule P(A,B) = P(A}B) * P(B) = P(B|A) * P(A) P(A,B,C,D) = P(A) * P(B|A) * P(C|AB) * P(D|ABC) 马尔可夫假设： 基于Chain Rule： 1st: 只依赖前一个条件 P(A,B,C,D) = P(A) * P(B|A) * P(C|B) * P(D|C) 对应 1-Gram 也叫 ：(unigram) 2st: 只依赖前两个条件 P(A,B,C,D) = P(A) * P(B|A) * P(C|AB) * P(D|BC) 对应 2-Gram 也叫 ：（bigram） 3st: 只依赖前三个条件 对应 3-Gram 也叫 :(trigram) 如上语言模型都有一个问题： 那么假如预测一个词表中没有的单词，那么就Game Over了， 乘积直接预测为 0 了 平滑处理 如果上面N-Gram。不想出现0，而是变为无穷小，那么需要 做平滑处理 拉普拉斯平滑 add-one (B|A)次数 + 1 P(B|A) = --------- A出现次数 + 词典大小（不包括重复的） add-K (B|A)次数 + K P(B|A) = --------------------------- A出现次数 + K * 词典大小（不包括重复的） K 是一个超参数，需要不断优化 interpolation 仅有的数据集，句子 条件概率为0， 而 未来其他的数据集，可能有这个句子，产生超未来的情况 所以，interpolation就是解决这个情况的。 同时使用 unigram 和 bigram 和 trigram 公式： λ1 * unigram + λ2 * bigram + λ3 * trigram λ1 + λ2 + λ3 = 1 朴素贝叶斯 贝叶斯 P(A|B) = P(B|A) * P(A) -------------- P(B) p(垃圾|邮件内容 ) = P(邮件内容 | 垃圾) * P(垃圾) # 注意这个P（垃圾）就是先验概率， 垃圾文章占总文章的比例 ---------------------------------- P（邮件内容） 同理另一方面（正常方）： p(正常|邮件内容 ) = P(邮件内容 | 正常) * P(正常) # 同样为先验概率 ---------------------------------- P（邮件内容） # 注意1， p(垃圾|邮件内容 ) 和 p(正常|邮件内容 ) 的 分母是可以约分的 # 注意2， P(邮件内容 | 正常) 和 P(邮件内容 | 垃圾) 都是条件独立概率。 可继续拆分为 P（单词1） | P（正常） # 你需要首先计算出每个单词在正常内容里面的概率 联合概率 P(X,Y) = P(x) * P(y|x) = P(y) * P(x|y) 条件独立 P(x,y | z) = P(x|z) * P(y|z) 先验概率 假如你要判断 一封邮件里面的句子，是否为垃圾信息。 这时，你需要计算出 已有 垃圾邮件的概率 和 正常邮件的概率。 准确率 精确率 召回率 F1-Score 准确率（不可靠） 准确率 依赖于 （正、负）平衡的样本 99999个黑猫， 1个白猫， 你怎么预测，几乎都是黑猫（99999/100000）的概率 精确率（precision） 标签+预测数据： 实际标签： T F T T F F 预测标签： T F T F T T 以正样本为例： P(正) = &quot;预测标签中有4个T&quot;， 与 &quot;实际标签&quot;对应位置匹配后，发现只有 &quot;2个T&quot;对应上了。 所以精确率为： 2/4 召回率(recall) R(负数) = &quot;实际标签中有3个T&quot;， 与 &quot;预测标签&quot;对应位置匹配后，发现只有 &quot;2个T&quot;对应上了。 所以召回率为： 2/3 后来又想了一种更好理解 （精确率和召回率）的方式 目标，求小白鼠 “生病” 的 精确率 与 召回率 鼠1 鼠2 鼠3 鼠4 鼠5 鼠6 实际 病 非 病 病 非 非 预测 病 非 病 非 病 病 具体例子： 我 预测结果中有标出 4 个生病的， 与实际对比后， 上下对应之后 鼠1 和 鼠3，标对了，也就是预测对了2个 所以， P（精确） = 2 / 4 = 50% 实际标签有 3个有病的老鼠， 但是（看预测label）， 只有 鼠1 和 鼠3 对齐了，（也就是预测正确了） 所以， P（召回） = 2 / 3 = 66% 上面的例子，是比较容易懂的， 如果不懂，可看下面讲我的复杂理解方式： 首先准备 &quot;真实label&quot; 和 &quot;预测label&quot;： 把这些 &quot;预测label&quot; 放到 &quot;实际 label&quot; 下面一行 一一对齐 位置匹配 例如（看看位置结构就行，下面用得到， 01随便写的不用理会） ： 真实label： 10011 预测label: 11001 精确率：（我预测的label中， 预测对了多少（对齐+交集） ， 可理解为预测的准不准） （注意 &quot;中&quot;字， 谁带中，谁就做分母） 精确率主要以我的 &quot;预测label&quot; 为主， （以谁为主，谁就做分母） （以谁为主，就从谁开始看）（精确率以&quot;预测label&quot;为主，根据上面的结构，先从下面一行开始看） 先从下面这行（也就是我们预测的label） 开始查找正例（也就是我们探索的目标--&quot;有病&quot;） 把有病的个数，作为分母， 然后我们开始把上下2行的label一起对齐比较， 把上下都有 “有病” 的总个(列)数作为分子（就是交集的意思）， 最终精确率就是： 分子 ---- 分母 召回率：（所有实际label中，我预测到了多少（对齐+交集）， 可理解为预测的全不全） （注意 &quot;中&quot;字， 谁带中，谁就做分母） 和上面不同了， 召回率变成以主要以 &quot;实际label&quot; “有病”的个数 为主 （以谁为主，谁就做分母） （以谁为主，就从谁开始看）（精确率以&quot;预测label&quot;为主，根据上面的结构，先从下面一行开始看） 然后从上面这行（也就是实际的label） 开始查找正例（也就是我们探索的目标--&quot;有病&quot;） 把有病的个数，作为分母， 然后我们开始把上下2行的label一起对齐比较， 把上下都有 “有病” 的总个(列)数作为分子（就是交集的意思）， 最终精确率就是： 分子 ---- 分母 F1-Score 上面说明了（P-R）是互斥关系。 所以作为指标，需要考虑二者。不是很方便。 那么有没有一个唯一的指标来 平衡二者呢？ 有---它就是 F1-Score （调和平均值） 1 1 1 1 -- = ---- * （---------- + ----------） F1 2 precision recall 2 * precision * recall F1 = ------------------------------ precision + recall 特点： 有一个值低，另一个值特别高，结果 F1 Score也会特别的低 （或者两个值都低， f1 score也会很低） 只有二者都非常高的话，F1 Score 才会非常的高 Logistic Regression(逻辑回归) 线性回归 + Sigmoid 1 y_predict(类别为1) = -------------- 1+ e^(wx+b) 1 e^(wx+b) y_predict(类别为0) = 1- ------------ = ---------------- 1+ e^(wx+b) 1+ e^(wx+b) 用1-0开关吧上面两个式子合在一起 y_predict = y_predict(类别为1)^(y) + y_predict(类别为0)^(1-y) 两边加上 log，化简一下： log (p(y_predict)) = y * log y_predict类别为1 + (1-y) * log (1-y_predict类别为0) # 上面求得是预测的概率。 我们它希望越大越好。 # 所以额外的整体外面需要加一个 负号 - # 加个负号，就反过来了。 我们就转而求 希望他越小越好。 (或者这里不用理解，最后加个负号，记住即可) log (p(y_predict)) = - ( y * log y_predict类别为1 + (1-y) * log (1-y_predict类别为0) ) # 注意，上面说的都是针对每一个样本的 （记住这行，下面最大似然用到） log (p(每个样本)) = - ( y * log y_predict类别为1 + (1-y) * log (1-y_predict类别为0) ) 最大似然估计： 最大似然估计，就是 考虑所有样本满足&quot;独立分布&quot;： 那么它们的 总概率可写为： P（总） = 累乘( P（所有样本）) 两边再取对数: log P总 = log 累乘( P（每个样本）) 累成转累加： log P总 = Σ log ( P（每个样本）) log ( P（每个样本）)) 带进来： log P总 = 最大似然估计 = - Σ( y * log y_predict类别为1 + (1-y) * log (1-y_predict类别为0) ) 上面就是最大似然估计的公式了 当然，我们不会直接用最大似然当作损失函数，因为我们需要最大似然概率越大越好。 而是把负号去掉，变成损失，让损失越小越好。 并且除以样本的总数，最后损失函数如下： Σ( y * log y_predict类别为1 + (1-y) * log (1-y_predict类别为0) ) loss = --------------------------------------------------------------------- m 逻辑回归是线性分类器： 依据：看决策边界（decision boundary） 求决策边界 上面两个式子y_predict(类别为1) 和 y_predict(类别为0) 相等， 求出的表达式 就是决策边界（纯线性的） 当然，除了Sigmoid，你也可以用 Softmax做多分类 Neural Network (神经网络) 之前说过 逻辑回归 是 线性回归 + Sigmoid拼接而成。 那么现在： 每个逻辑回归 看作一个 神经单元 那么神经网络可以说是 多个 逻辑回归 摞起来的效果。 ","link":"https://cythonlin.github.io/post/py-greater-nlp-ml/"},{"title":" PY => Sklearn","content":"数据预处理 大体分为 （中心化处理+缩放处理） ###zero-centered (中心化处理) 平移 --- 减去固定值 ###scale (缩放处理) 归一化：除以 最大值-最小值 标准化：除以 标准差 下面的操作都是通过 中心化处理+缩放处理 联合组成的 ###normalization（归一化） 目标： 将数据收敛到 [0,1] 公式 x - min(x) # 中心化 ------------------ max(x) - min(x) # 缩放处理 sklearn代码如下 from sklearn.preprocessing import MinMaxScaler mm = MinMaxScaler( feature_range=[0,1] ) # 注意，这里也可以归一化到其他区间 [2,4] ，也是没问题的 # mm.fit([[1,2,3],[40,60,70]]) # result = mm.transform([[1,2,3],[40,60,70]]) # 这步返回的 才是归一化后的数据 result = mm.fit_transform([[1,2,3],[40,60,70]]) raw_data = mm.inverse_transform(result) # 可以还原出 归一化前的 原始数据 # 额外注意 # 如果数据量太大， 导致 fit() 报错，那么可使用 partial_fit() 。 用法和 fit() 一样的 mm.partial_fit(x) ###StandardScaler（标准化） 目标： 将数据转化为标准 正态分布（均值为0，方差为1） 公式： x - 平均值 ----------------- 标准差 sklearn代码如下 from sklearn.preprocessing import StandardScaler standardscaler = StandardScaler() result = standardscaler.fit_transform([[1,2,3],[40,60,70]]) print(result) print(standardscaler.mean_) # 可以查看方差， 注意是 实例化对象的属性 print(standardscaler.var_) # 可以查看均值 # 同样可以 逆向求原始数据 raw_data = standardscaler.inverse_transform(result) # 同样也可以使用 partial_fit standardscaler.partial_fit(x) 标准化 和 归一化 选哪个？？？ 视觉图片： 归一化 其他： 标准化较好 数据缺失值填补 方式1： 用pandas手动fillna 这种方式更简单 df[...].fillna( df.loc[:, '列名'].mean() ) # 用均值来填补 ， 中位数 # median 方式2： 用 sklearn 的Imputer 先说一下查看缺失（通常实在 pandas的dataframe中） df.info() Imputer处理如下 # X 通常是 pandas取出的某一列 from sklearn.impute import SimpleImputer im = SimpleImputer() # 默认 strategy = 'mean'， 用均值填补 im.fit_transform(X) im_meadian = SimpleImputer(strategy='median') # 用中位数填补 im_meadian.fit_transform(X) im_meadian = SimpleImputer(strategy='most_frequent') # 用众数填补 im_meadian.fit_transform(X) im_constant = SimpleImputer(strategy='constant', fill_value=0) # 用常数填补 im_constant.transform(X) 注意了，它们还有个 参数 missing _ values im = SimpleImputer(missing_values=np.nan) # 默认为填补 np.nan # 其实大多数也都是在numpy环境下的。空缺值几乎都是np.nan, 所以这个参数不用动 # 如果你真的有 None ，那么你也可以设置 missing_values=None 标签编码 作用效果 原始标签 ['猫', '狗', '老鼠'] LabelEncoder作用后的标签 [0,1,2] 使用方法 from sklearn.preprocessing import LabelEncoder le = LabelEncoder() encode_label = le.fit_transform(y) print( le.classes_ ) # 查看所有类别 print( le.inverse_transform(encode_label) ) # 获取原生标签数据 特征编码 作用效果 原始特征数据 动物 学历 猫 专科 狗 本科 老鼠 重点 LabelEncoder作用后的标签 动物 学历 0 0 1 1 2 2 使用方法（Sklearn几乎都是一样的） from sklearn.preprocessing import OrdinalEncoder oe = OrdinalEncoder() encode_feature = oe.fit_transform(X) # 传进来的是二维特征 print(oe.categories_) # 查看所有特征 # 结果： [ array['猫','狗','老鼠'], array['专科','本科','重点'] ] oe.inverse_transform(encode_feature) # 获取原生特征 One-Hot特征编码(哑变量) 特征编码和 One-Hot编码有何不同？ One-Hot具有互斥性， 而特征编码是无脑编码 作用效果 原始特征数据 动物 学历 猫 专科 狗 本科 老鼠 重点 LabelEncoder作用后的标签 (它们是拼接在一起的，需要自己甄别) 1,0,0, 1,0,0 0,1,0, 0,1,0, 0,0,1, 0,0,1, 使用方法（Sklearn几乎都是一样的） from sklearn.preprocessing import OneHotEncoder ohe = OneHotEncoder(categories='auto') # 固定参数，这样写即可 encode_label = ohe.fit_transform(X).to_array() # fit_trainsform结果为稀疏矩阵，需要to_array转为DF ohe.inverse_transform(encode_label) # 获取原生特征 print( ohe.get_feature_names() ) # 查看所有特征对应位置的 OneHot # 通常做完One-Hot的结果后，我们需要做善后工作： 1. 拼接到 DF尾部 2. 并且把原 特征删除 3. 把One-Hot后的6个特征列 改个列名 二值化 from sklearn.preprocessing import Binarizer bi = Binarizer(threshold=30) # 30以下设为0， 30以上设为1 result = bi.fit_transform(X) 网格搜索 from sklearn.model_selection import GridSearchCV grid_search = GridSearchCV(knn_clf,dict,cv=3) # cv是几，就进行几折的交叉验证 grid_search.fit(x_train,y_train) grid_search.best_estimator_ # 得出 最佳参数 模型 grid_search.best_score_ # 得出 最佳准确度 grid_search.best_params_ # 得出 params 参数 对应的 最佳 参数组合 grid_search.cv_results_ # 每次 模型 的 结果 交叉验证 from sklearn.model_selection import cross_val_score knn_clf = KNeighborsClassifier() scores = cross_val_score(knn_clf, x_train, y_train, cv=3） 决策树 分类树 from sklearn import tree cls_tree = tree.DecisionTreeClassifier( criterion='gini', # 默认 基尼系数 , 或 &quot;entropy&quot; 信息熵（信息熵可能会过拟合） max_depth=3, # 限制树的深度 # min_samples_leaf=5, # 分支后的 每个‘叶子节点’ 中必须包含样本数必须都大于5.否则不让分支 # min_samples_split=10, # 当 当前节点 的样本数 大于10 ，才允许分支 ) cls_tree.fit() cls_tree.score() cls_tree.predict() print(cls_tree.feature_importances_) # 输出 每个特征对应的概率分布，返回为数组，需要自己zip特证名 回归树 reg_tree = tree.DecisionTreeRegressor(random_state=0) # 比分类树参数少 ... # 下面同分类树API 集成算法 bagging （装袋法） 模型相互独立，最后取平均或聚合等。 代表： 随机森林 分类随机森林 from sklearn.ensemble import RandomForestClassifier rf_clf = RandomForestClassifier( n_estimators = 50, # 50个子树模型 random_state=666, oob_score=True, n_jobs=-1 ) rf_clf.fit() rf_clf.oob_score_ # 求准确率 回归随机森林 rf_clf = RandomForestClassifier( n_estimators = 50, random_state=666, ) boosting (提升法) 等 聚类算法 k-means 客户分类（依据用户各种特征，自动分堆） 簇（堆） 与 质心 簇：是不相交的。 如果有相交的点，那么一定是一个新簇 质心： 相当于每个簇的中心位置坐标 （我理解为&quot;圆&quot;心） 聚类流程 随机选取K个样本点作为 质心 while 循环 （旧质心 != 新质心） 将K个样本点周围的样本 分配到 最近的质心，形成新簇 计算新簇的所有样本点的 （xy坐标和的均值 作为新的质心） 跳回到第二点的 while循环 聚类形成新簇的公式与指标 每个样本点 到 质心距离： distance = 每个样本点 到 质心距离 1.欧拉距离 （默认使用的是欧氏距离） # 根据数学推导，质心位置最终就是 均值 位置 2.曼哈顿距离（绝对值）， # 根据数学推导，质心位置最终就是 中位数 位置 3.余弦距离 # 根据数学推导，质心位置最终就是 均值 位置 Kmeans评估指标（重点） 一、所有点到 质心的距离之和能不能 作为指标？（距离越小，效果越好） 这是严格错误！！ 因为 每调整一次质心 和 调整一次簇， 都可能缩小范围。因此 距离之和 自然就变小了 所以你再用距离判断，就是没有意义的 二、正宗的指标 --- 轮廓系数 (越靠近1 越好) a = 单个点（注意，只是一个点） 与 簇内 其他点的 距离的平均 b = 单个点（注意，只是一个点） 与 簇外 其他点的 距离的平均 b - a s(轮廓系数) = --------------- max(a,b) Sklearn 轮廓系数： 有两种方式（用哪个都行，推荐第一种）： 方式1： from sklearn.metrics import silhouette_score print( silhouette_score(x, y_predict) ) 方式2： from sklearn.metrics import silhouette_samples print( silhouette_samples(x, y_predict).mean() ) # 这还得写个求均值 轮廓系数绘图: from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score import matplotlib.pyplot as plt import matplotlib.cm as cm import numpy as np for n_clusters in [2,3,4,5,6,7]: n_clusters = n_clusters fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7) ax1.set_xlim([-0.1, 1]) ax1.set_ylim([0, X.shape[0] + (n_clusters + 1) * 10]) clusterer = KMeans(n_clusters=n_clusters, random_state=10).fit(X) cluster_labels = clusterer.labels_ silhouette_avg = silhouette_score(X, cluster_labels) print(&quot;For n_clusters =&quot;, n_clusters, &quot;The average silhouette_score is :&quot;, silhouette_avg) sample_silhouette_values = silhouette_samples(X, cluster_labels) y_lower = 10 for i in range(n_clusters): ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i] ith_cluster_silhouette_values.sort() size_cluster_i = ith_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i)/n_clusters) ax1.fill_betweenx(np.arange(y_lower, y_upper) ,ith_cluster_silhouette_values ,facecolor=color ,alpha=0.7 ) ax1.text(-0.05 , y_lower + 0.5 * size_cluster_i , str(i)) y_lower = y_upper + 10 ax1.set_title(&quot;The silhouette plot for the various clusters.&quot;) ax1.set_xlabel(&quot;The silhouette coefficient values&quot;) ax1.set_ylabel(&quot;Cluster label&quot;) ax1.axvline(x=silhouette_avg, color=&quot;red&quot;, linestyle=&quot;--&quot;) ax1.set_yticks([]) ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]) colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters) ax2.scatter(X[:, 0], X[:, 1] ,marker='o' ,s=8 ,c=colors ) centers = clusterer.cluster_centers_ # Draw white circles at cluster centers ax2.scatter(centers[:, 0], centers[:, 1], marker='x', c=&quot;red&quot;, alpha=1, s=200) ax2.set_title(&quot;The visualization of the clustered data.&quot;) ax2.set_xlabel(&quot;Feature space for the 1st feature&quot;) ax2.set_ylabel(&quot;Feature space for the 2nd feature&quot;) plt.suptitle((&quot;Silhouette analysis for KMeans clustering on sample data &quot; &quot;with n_clusters = %d&quot; % n_clusters), fontsize=14, fontweight='bold') plt.show() 聚类时间复杂度 O（k*n*T） ~= O(n) k: 簇数 （超参数） n：样本数量 T：迭代次数 聚类API + 实战 生成分堆点 from sklearn.datasets import make_blobs # 生成分堆点的数据API import matplotlib.pyplot as plt x,y = make_blobs( n_samples=1000, # 1000个样本 n_features=2, # 2个特征。 也可以更多 eg:10个滕峥 centers=4, # 初始化4个质心 random_state=1 # 随机化 ) # x是样本， y是标签， 这里面y用不到，因为是无监督 fig, ax1 = plt.subplots(1) # 生成1个子图。 fig：画布， ax绘画对象 ax1.scatter( x[:, 0], x[:, 1], marker='o', # 点的形状, 这是字母o， 而不是数字0, 钩子字符串了解一下 s=6, # 点的大小 # c='red' # 点的颜色 ) plt.show() 聚类API from sklearn.cluster import KMeans clusters = KMeans( n_clusters=3, # 要求最后 分3堆 数据 random_state=0 ) clusters.fit(x) # fit() 代表求完了所有过程了。 质心-分簇-质心-分簇 # 其实fit() 过后，就已经训练出来了模型的参数（其实最重要参数的就是 质心） labels = clusters.labels_ # 查看每个数据 分出来的 label center = clusters.cluster_centers_ # 返回质心的坐标（x,y）。 有几个簇（类别），就有几个质心 all_distance = clusters.inertia_ # 返回所有点到质心的总距离平方和 y_predict = clusters.fit_predict(x) # 拿着上面训练好的质心去把新数据自动分堆，并返回所有数据的类别label clusters.labels_ 和 y_predict 的结果是一模一样的，用哪个都行 聚类API说明： 因为聚类复杂度为 O（n)+ 所以所有数据喂进去直接 fit() 训练可能会很慢 因此我们可以取部分数据 fit() # 得到质心 然后剩余数据 fit_predict() 去拿着上面的质心， 预测分堆, 并返回 label 画出散点图结果 colors = ['pink', 'orange', 'gray'] fig1, ax2 = plt.subplots(1) for i in range(n_clusters): # 相当于合集 。。。（y_predict值是松散的） ax2.scatter( x[y_predict==i, 0], # 每个预测值对应的样本 的 横坐标 x[y_predict==i, 1], # 每个预测值对应的样本 的 纵坐标 marker='o', s=6, c=colors[i] ) ax2.scatter( center[:, 0], center[:, 1], marker='x', s=6, # c='red' # 点的颜色 ) plt.show() ","link":"https://cythonlin.github.io/post/py-greater-sklearn/"},{"title":"PY => AI笔记","content":"激活函数 sigmoid 公式 1 --------- 1 + e**(-x) 每个out: (0, 1) 二分类out之和为 1 对应API： 1. tf.sigmoid(y) 2. 或函数参数 xxxxx (activations='sigmoid') 3. tf.keras.activations.sigmoid() softmax e**x --------------------------------- e**(x1) + e**(x2) + ... + e**(xn) 每个out: (0,1) 多分类 out之和为 1 对应API： 1. tf.nn.softmax() 2. 函数参数 xxxxx (activations='softmax') 3. tf.keras.activations.softmax() tanh coshx ex - e(-x) ------------- 2 sinhx ex + e(-x) -------------- 2 tanhx ex - e(-x) ------------- ex + e(-x) 每个out: (0,1) * 2 -1 ===&gt; (-1,1) LSTM 对应API： 1. tf.tanh(y) 2. 函数参数 xxxxx (activations='tanh') 3. tf.keras.activations.tanh() relu 公式： y = 0 if x &lt; 0 else x # 大于0，梯度为1 对应API 1. tf.nn.relu() 2. 或函数参数 xxxxx (activations='relu') 3. tf.keras.activations.relu() leaky_relu: (小扩展) y = kx if x &lt; 0 else x tf.nn.leaky_relu() 损失函数 MSE （均方误差） 公式 Σ( (y-y_predict)**2 ) -------------------- n 对应API 公式实现： tf.reduce_mean( tf.square( y-y_predict ) ) tf.API: tf.reduce_mean( tf.loss.MSE(y, y_predict) ) CrossEntropy （交叉熵） 熵公式： -Σ（plogp） 交叉熵公式：-（ Σplogq ） p为真实值One-hot, q为预测值 p: [1,0,0] q: [0.9, 0,0.1] H = -( 1log0.9 + 0log0 + 0*log0.1) = -log0.9 = -ln0.9 ≈ 0.1053.... tf的 tf.math.log相当于 ln 交叉熵API： 交叉熵越小（y与y-predict差距越小，预测较准确） 交叉熵越大（y与y_predict差距越大，交叉相乘累加后值大，说明预测错位了。。。所以交叉起来变大了） tf.API: （方式1：直接是函数调用） loss = tf.losses.categorical_crossentropy([1,0,0], [0.9, 0, 0.1],from_logits=True) # 第一个参数y, 第二个参数 y_predict loss = tf.reduce_mean(loss) tf.API: （方式2：用类的call调用 , 这次以 二分类交叉熵为例） loss = tf.losses.BinaryCrossentropy(from_logits=True)( [1], [0.1] ) # 结果为2.+ 。 因为 真实值是1类， 而预测值概率是0.1太小了。所以肯定预测错了。 loss = tf.reduce_mean(loss) 说明：categorical_crossentropy( ) # 第一个参数必须 one_hot, （第二个参数按理来说需要做 softmax，但是你传了 from_logigs=True，就不必softmax了） 正则化惩罚 L1 L1 = keras.regularizers.l1(0.01) L2 使用方式1: (手撕方式)： for with tf.GradientTape() as tape: ... ... loss_reg = [tf.nn.l2_loss(w_b) for w_b in model.trainable_variables] # [w1,b1,w2,b2] print(tf.reduce_sum(loss_reg)) # tf.Tensor(2.98585, shape=(), dtype=float32) # 就是这个形状 loss = loss + loss_reg 使用方式2: (keras添加层方式)： L2 = keras.regularizers.l2(0.01) keras.layers.Dense(....., kernel_regularizer=l2) # 为每层加上L2 梯度 SGD: 减去梯度，代表朝着梯度方向走 w新 = w当前 - learning_rate * 梯度 使用方式： model.compile(..... ,optimizer=keras.optimizers.SGD(learning_rate=0.01)) Momentum（动量） 公式大概： w新 = w当前 - learning_rate * (梯度 + w旧) # 其中 （梯度+w旧）和为矢量和， 结果和为（平行四边形对角线法则）的方向 公式换算还等价于（另一种思想）： w新 = (w当前 - learning_rate * 梯度) - learning_rate * w旧 # 在直来直去梯度的基础上------------掰弯（就是减去了个东西，改变方向） RMSProp model.compile(..... ,optimizer=keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.9)) Adam(推荐) model.compile(..... ,optimizer=keras.optimizers.Adam(learning_rate=0.01)) RNN ###LSTM 3个门 有两个记忆信息（h c ） ###GRU 有两个门 只有一个记忆信息 c NMS（非极大抑制） 候选假如有ABCD四个候选框！！！！！！ 1. 分类之后，取出最大概率候选框A。 2. 将其余候选框，与这个最大的候选框，做 IOU 交集/并集 3. 如果交集/并集的 结果大于阈值，（筛选出了B，并删除）（这样操作是筛选出 最大概率的点 的 附近的点，并删除） 为什么要删除呢。 因为你有了这个最大概率的点，其他相似的点都是弟弟了。 可以扔了。 4. 剩下的就是 CD了， CD还是重复123操作。 选一个老大（大概率框框） 通过阈值，我们可能会认为， AB是同一层面的东西， 而 CD是预测了另一层面的东西。 所以每个层面选出一个老大即可。！ R-CNN 0. 处理图片（crop + wrap） 1. 图片-&gt;SS(选择性搜索（随便画框）)-&gt;NN-&gt;候选区域（Fine-Tune） (1正样本：3负样本) 2. SVM分类（准确率高） 3. NMS筛选（IOU 交/并，筛选） 4. BoundingBoxReg边框回归 存在问题： 1. 神经网络 慢 2. 图片 变形 SPPNet 图片映射 R-CNN是一大堆图片的 选择性搜索（画框）， 然后塞入 CNN， 得出特征向量。 SPP 是只把一张图片 塞入CNN 训练提取出 众多 &quot;特征向量&quot;。 然后再选择性搜索（SS画框） 得出众多&quot;候选区域&quot;。 然后将 &quot;候选区域&quot; 与 &quot;特征向量&quot; 一一映射。 s = strides卷积 * strides池化 左上角： x特征 = x原图候选/s + 1 y同理 右小角： x特征 = x原图候选/s - 1 y同理 注： 映射后的特征向量大小不固定 所以这些特征向量 需要交给 SPP层处理 spatial pyramid pooling (空间金字塔池化层) 每个 候选区域 ，都会经过如下变换： （256取决于 每个不同的CNN网络模型输出通道） 被分蛋糕为 4x4, 256通道 ====&gt; (16， 256) 被分蛋糕为 2x2, 256通道 ====&gt; (4， 256) 被分蛋糕为 1x1, 256通道 ====&gt; (1， 256) 综上相加 ====&gt; (21, 256) 思想（分隔，再聚合） 改善了R-CNN的什么？ 因为统一用了SPP层格式化（切割），所以就不用了。 Fast R-CNN 改进 相比较于SPP，有如下改动： 1. 新增 ROI pooling层 2. SVM 替换为 softmax( bounding box regresson 依然存在) =&gt; softmax输出为： N个类别+1个背景 3. ROIPooling: 0. 同样先是 SS候选区域 和 CNN特征 的映射 1. 4x4+2x2+1x1 替换成单个 MxN 2. 如果和 fc 层输入不匹配，那么会自动划分（切割）成匹配的大小。 3. 舍弃少量精度，获取更大效率。 4. 损失函数？？ 分类部分=&gt; 交叉熵 回归部分=&gt; MAE Faster R-CNN 改进 相比较于Fast R-CNN，有如下改动： 1. 原图片 -&gt; CNN -&gt; 特征图 2.1 特征图经过RPN生成候选区域 2.2 候选区域 与 特征图 组合 输入到 =&gt; Roi Pooling 得到 每个 候选区域 的 特征图。 RPN网络： 特征图： nxn的大小窗口去扫描特征图， 每个窗口映射到一个低维向量。 低维向量 =&gt; 输入到两个并行的 1x1 卷积中，得出两个结果： 1. reg窗口回归层； (用于修正位置) 2. cls窗口分类层： （表示前景，和 背景的概率） 每个从 CNN出来的 特征图有如下属性（可能有 M*N个特征图）： 9个Anchor: 尺度 ： {128， 256， 512} 长宽比： {1：1， 1：2， 2：1} 9个Anchor相当于（9个粗略的候选区域-&gt; 替代了之前用框框画出的区域） RPN网络训练： 训练样本标记： 求Anchor 与 ground_truth box的IOU: IOU&gt;0.7记为正样本， 0.3&lt;IOU&lt;0.7 标记为负样本 IOU &lt; 0.3的全部删除 RPN: 损失 二分类： 区别出来 （是否有物体 -&gt; 前景 or 背景） 回归： 修正剩余优秀 Anchor的位置作为最终候选特折 Faster R-CNN正式训练： 和 Fast R-CNN一样： 1. 多分类 softmax(N+1) 2. bounding box 回归 优点： 提出RPN网络 端到端网络模型 缺点： 训练参数过大 真实产品还是慢 YOLO GoogleNet+4 Conv + 2 Dense SSD VGG-16: 300 * 300 ","link":"https://cythonlin.github.io/post/py-greater-ai-bi-ji/"},{"title":"PY => Docker配置daemon.json的host选项失败","content":"当我配置Docker让外部连接的时候 我尝试创建 /etc/docker/daemon.json文件， 并写如下内容： { &quot;registry-mirrors&quot;: [&quot;https://rs3ab060.mirror.aliyuncs.com&quot;], &quot;hosts&quot;: [&quot;unix:///var/run/docker.sock&quot;, &quot;tcp://0.0.0.0:6006&quot;] } 然后，重启docker服务，一直失败。 根本无法启动。 经排查，是hosts这个配置项有问题。 终极解决办法： systemctl edit dockerd.service # 复制这条命令执行，并写粘贴如下内容 [Service] ExecStart= ExecStart=/usr/bin/dockerd ctrl + s 保存 ctrl + x 退出 ","link":"https://cythonlin.github.io/post/docker-pei-zhi-daemonjson-de-host-xuan-xiang-shi-bai/"},{"title":"PY => 音声分离","content":"环境安装 自主安装 FFmpeg pip install jupyter pip install spleeter 思路 spleeter工具可在command逐一分离,有些费劲。 并且文件名不许有空格和特殊字符。 本想通过py脚本，批处理（os.system/os.subprocess）做些脚本操作。 但是 spleeter隐式搜索 FFmpeg。所以（os.system/os.subprocess）都搜不到FFmpeg 即使内/外添加环境变量也不行。 后来没办法，想个歪的。 用 jupyter notebook 的 !指令嵌套循环实现。 Jupyter Notebook代码 import re import os import shutil music_list = os.listdir('音乐') r1 = re.compile(r'(\\(.*?\\)|（.*?）|\\[.*?\\]|\\s*?|【.*?】|《.*?》)') new_music_list = [r1.sub('',per_music) for per_music in music_list] %cd 音乐 name_map = zip(music_list, new_music_list) for old_name,new_name in name_map: try: os.rename(old_name,new_name) except: pass new_name_list = os.listdir('.') for filename in new_name_list: if filename.endswith('jpg'): continue print(filename) try: !spleeter separate -i {filename} -p spleeter:2stems -o output except: pass 分离好的音/声都放在各自的歌曲的文件夹内 为了听伴奏，把所有伴奏放一起 music_list = os.listdir('音乐') # 方便复制， output 和 pretrained_models 移动了出来 [shutil.copy(f'output/{music.rsplit(&quot;.&quot;,1)[0]}/accompaniment.wav', f'D:/music/acc/{music}') for music in music_list if music.rsplit(&quot;.&quot;,1)[1] != 'jpg' and music not in ['pretrained_models','output']] # 我只听伴奏，所以人声就免了。。 # [os.rename(f'output/{music.rsplit(&quot;.&quot;,1)[0]}/vocals.wav', f'D:/music/voice/{music}') for music in music_list if music.rsplit(&quot;.&quot;,1)[1] != 'jpg' and music not in ['pretrained_models','output']] os.rename 和 shutil.move使用区别： shutil.move 是先复制后删除 （根本不是cut）(慢) os.rename 可以达到剪切的效果(神速) （也可以用来改名） ","link":"https://cythonlin.github.io/post/py-greater-yin-sheng-fen-chi/"},{"title":"PY => PY38","content":"说明 本篇文章介绍Python3.8的一些较为好用的新特性 赋值表达式 语法格式： 2种格式的区别就是，一个带括号，一个不用带括号 (a:=1) a:=1 其实这两种格式结果是一样的，只是有些情况括号可以省略不写而已。 括号可省的情况 #一、序列类型中 括号可省 [a:=1, 2] 二、作为函数调用的参数 括号可省 # 注意我说的是函数调用，函数定义的时候压根就不能用 := 表达式，更别提可不可省了 print(g:=1) 三、以后碰见再说了，应该也没啥了 括号不可省的情况 一、在while(), if(), for()语句的条件中（虽可省但不要省） while ((b:=1) == 2): # if， for 同理 print(2) print(b) # &gt;&gt; 1 强调1 := 此运算符优先级比 == 还低，虽然可省，所以这里区分语义括号要加上 强调2 := 就算while 或者 if 条件不成立， b依然赋值成功 二、在 f-string 中，括号不可省 （某些情况可省，虽可省但不要省） f'{(p:=1)}' print(p) 假如新变量q是未定义的，不加括号就会出现如下情况（所以这样用就没意义了， f-string括号必须加） f'{q:=1}' print(p) # NameError: name 's' is not defined 三、以后碰见再说了，也没啥了 f-string新增功能 3.8之前这么用: f'{loss}, {acc}' #&gt;&gt; '0.0141, 0.98912' 3.8之后这么用: f'{loss=}, {acc=}' #&gt;&gt; 'loss=0.0141, acc=0.98912' 还有个功能(自己看结果对比的效果把): name = 'zhang' f'{name}' # 'zhang' f'{name=}' # &quot;name='zhang'&quot; # 新增功能1 f'{name=!s}' # 'name=zhang' # 新增功能2 欧氏距离 import math math.dist([1,2,3,4],[5,6,7,8]) f-string回顾(3.6就已经出了，一直在用) 方式0: 提前说明(方式6 和 方式7上档次, 好用) 方式1：变量替换 name = 'zhang' print( f'{name}' ) #&gt;&gt; zhang 方式2：内嵌表达式计算 name = 'zhang' print( f'{name.upper()}' ) #&gt;&gt; ZHANG 方式3：一行写不下，可以多行 f'' (注意，不需要逗号分隔)，并且要用小括号扩起来 name = 'zhang' age = 18 gender = 'man' a = ( f'name={name} ' f'age={age} ' f'gender={gender} ' ) print(a) #&gt;&gt; name=zhang age=18 gender=man 方式4：转义 {} # 注意一下， # f'{ ' 这种会报错的 # 因为 { 在 f''语句里面已经不是字符串了， 而是语法符号 # 如果我们想要 {} 这种字符串， 如下解决办法 # 用 { 来转义 { # 用 } 来转义 } eg: f'{{' 输出为&gt;&gt; '{' f'}}' 输出为 &gt;&gt; '}' 方式5：符号格式 正号： import random b = random.randint(0,5) f'{b:+}' #&gt;&gt; +0 方式6：日期符号格式（特别好用，适用于 time,date,datetime对象） from datetime import datetime e = datetime.now() # 年月日时分秒 f'{e:%Y}' # '2020' 表示2020年 (注意是大写的Y) f'{e:%m}' # '09' 表示9月份 （自动补0）(注意是小写m) f'{e:%d}' # '02' 表示2号 f'{e:%H}' # '21' 表示21点了 (24小时制) f'{e:%M}' # '43' 表示43分了 (注意是大写的M) f'{e:%S}' # '50' 表示50秒了 (注意是大写的S) # 默认组合格式(分隔符是固定的) f'{e:%F}' # '2020-09-02' 年-月-日, 分割符是固定的 '-' f'{e:%T}' # '21:58:39' 时:分:秒, 分割符是固定的 ':' (大写的X一摸一样) # 基于上面2行自己组装一下,形成完整时间 f'{e:%F %T}' # '2020-09-02 22:02:29' 上面2种组装一下 # 星期几+ 每年的第几天 + 每年的第几周 f'{e:%j}' # '246' 表示一年中的第 246 天, 个位数前面补0,至3位 f'{e:%u}' # '3' 表示星期3 （星期日是7） f'{e:%W}' # '35' 表示一年中的第 35周, 个位数前面补0,至2位(注意是大写的W) 方式7: 精度占位符号(:后面不要有空格)(这个较难理解) 情况一、:.n 代表只保留前n位 (并且四舍五入)(总体位数不算小数点所在的位数) f&quot;{1234.5678:.6}&quot; # .6 表示保留6位 (不算小数点, 整数+小数=6位) #&gt;&gt; '1234.57' 情况二、:m 代表只在&quot;最前面&quot;扩充&quot;至&quot;m位(注意这个&quot;至&quot;字) f&quot;{1234.5678:15}&quot; # 扩充至15位(小数点也算位数) 默认用空白字符代替 #&gt;&gt; ' 1234.5678' 情况三、:m前面带0， 补充空字符串 改为 &quot;补0&quot; f&quot;{1234.5678:015}&quot; # 0代表扩充的多余位数补0 (小数点也算位数)15代表最前面扩充至15位 #&gt;&gt; '0000001234.5678' 综合情况、:m.n 结合(只能是 m.n 这种顺序的格式) f&quot;{1234.5678:015.6}&quot; # 15就是上面说的m, 0就是m前面补的0, 6就是上面说的n #&gt;&gt; '000000001234.57' # 重点解读上面这个综合语句(按照下面的顺序才能正确解析): 1. 先看.后面的 n, n是6, 则 整体(整数+小数)(不包括小数点)保留6位 ===&gt; '1234.57' 2. 然后才看 m, m是15, 则在最前面扩充&quot;至&quot;15位置(包括小数点的位数) ===&gt; ' 1234.57' 3. 最后把第2步的空白字符补0 =====&gt; '000000001234.57' 情况四、:m.nf (推荐, 上面&quot;情况一&quot; 和 &quot;综合情况的延申版&quot;, 针对.n设计的, 多了个f,好用多了!) '''加了f, 核心就是 .n规则变为 小数点后保留n位,其余没变 ''' eg1: .nf的情况 (这种是最常用的!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!) f&quot;{1234.5678:.6f}&quot; # 小数点后保留6位, 其余还是那样 #&gt;&gt; '1234.567800' eg2: m.nf的情况 f&quot;{1234.5678:015.6f}&quot; # 还是前面&quot;综合情况&quot;, 先看n, n是6, 所以 &quot;小数点后&quot; 保留6位(四舍五入), # 然后从前面扩张整体位数 &quot;至&quot;15位, 空缺的补0即可 '00001234.567800' 情况五: 百分位(作为训练acc, 也算比较常用) &quot;&quot;&quot; 分析核心思想: 1. 先将原数扩大100倍(也就是小数点向后移2位), 2. 然后关注 .n , 保留小数点后的n位(四舍五入) 3. 最后加个百分号%, 即可) &quot;&quot;&quot; f&quot;{1234.5678:.1%}&quot; #&gt;&gt; '123456.8%' f&quot;{1234.5678:.6%}&quot; #&gt;&gt; '123456.780000%' # 如果想带m,也可以,和上面集中情况,以及解析顺序一摸一样,m不太常用, 我这里就不列了 ","link":"https://cythonlin.github.io/post/py-greater-py38/"},{"title":"GO => JetBrains全家桶初始化默认配置","content":"声明 我只用过 Pycharm 和 Goland且有效。 其他像 IDEA, WS 等血缘产品应该同样有效~~~~。 原因 改语法提示组合键，改崩了。 ","link":"https://cythonlin.github.io/post/go-greater-jetbrains-quan-jia-tong-chu-shi-hua-mo-ren-pei-zhi/"},{"title":"Sublime-Py插件","content":"Sublime-Py插件 localizeMenu # 汉化插件 帮助-&gt; language中就能找到 autofilename # 文件路径提示 SublimeCodeIntel # 基本格式补全 SublimeLinter # 代码pep8格式检查 Python PEP8 Autoformat 插件 # ctrl + shift + R 自动格式化 anaconda # 真正的代码补全 去除白框： Anaconda的settings里面配置 {&quot;anaconda_linting&quot;:false} 即可 Vue+Stylus插件（之前用的） Vuejs Snippets Vue Syntax Highlight VueFormatter Vuetify Stylus-Snippets Stylus ","link":"https://cythonlin.github.io/post/sublime-py-cha-jian/"},{"title":"PY => Anaconda-Jupyter相关配置","content":"Conda虚拟环境命令 查看所有虚拟环境（2种，结果一摸一样） conda info --e 或 conda env list 创建 conda create -n rs python=3.8.5 激活： conda activate rs 退出： conda deactivate 删除虚拟环境： conda remove -n rs --all 查看当前虚拟环境所安装的包 conda list 搜索某包的信息（包括channel） conda search requests 升级当前conda conda update conda 查看镜像 conda config --show channels 更改镜像 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --set show_channel_urls yes 删除镜像 conda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ 更新 某环境的 某个包 conda update -n rs requests 删除 某环境 的 某个包 conda remove -n rs requests 生成Jupyter配置文件 jupyter notebook --generate-config 这条命令过后，会提示你 xxx路径下，创建了一个xxx配置文件，记下来。 更换Jupyter基目录 打开上面的配置文件（windows用Sublime或普通的notebook）（linux用vim即可） 搜索关键词 c.NotebookApp.notebook_dir # 把后面字符串设成你的路径即可 保存退出，重新打开 Jupyter。 Kernel Error locate pywin32_postinstall.py python xxxxx/...../pywin32_postinstall.py -install 调高Jupyter的IO频率（若不调 可能会越界） 打开上面的配置文件（windows用Sublime或普通的notebook）（linux用vim即可） 搜索关键词 c.NotebookApp.iopub_data_rate_limit # 把后面的数字调大（越大越好） 保存退出，重新打开 Jupyter。 修改Jupyter主题样式 安装 pip install jupyterthemes （Anaconda-prompt 的pip） 列出所有样式名称 jt -l 修改样式 jt -t monokai 如果感觉都不好看，可恢复(使用完记得重启jupyter)： jt -r 别人推荐的样式（主题+字体+界面宽度一气呵成）： jt -t chesterish -f fira -fs 13 -nf ptsans -nfs 11 -N -kl -cursw 5 -cursc r -cellw 95% -T 官方样式说明： https://github.com/dunovank/jupyter-themes 或者这篇教程： https://towardsdatascience.com/bringing-the-best-out-of-jupyter-notebooks-for-data-science-f0871519ca29 ipykernel 多个虚拟环境共用一个主体jupyter 创建新的ipynb时也可以选择用哪个虚拟环境的jupyter kernel 安装 ipykernel pip install ipykernel 将虚拟环境添加到jupyter中 python -m ipykernel install --name=torch 结果会生成一个文件: /home/user/.local/share/jupyter/kernels/myenv/kernel.json 在总jupyter列出所有虚拟环境 jupyter kernelspec list 从jupyter中移除某虚拟环境 jupyter kernelspec uninstall torch Jupyter全屏编辑 这个我之前查过，修改配置文件的都不好用 （上面改主题的时候可以该宽度，但貌似必须修改主题，才能修改界面宽度，否则不可以） 只在StackOverflow上找到了一种简陋的解决办法 每个ipynb文件头部加上固定的2条语句即可： from IPython.core.display import display, HTML display(HTML(&quot;.container { width:100% !important; }&quot;)) 有趣的扩展（不是很实用） 内嵌dataframe表格操作: （玩玩可以，慎用， 小型数据就撑不住了，导致整个kernel崩溃） 安装： pip install qgrid jupyter nbextension enable --py --sys-prefix qgrid 导入： from qgrid import show_grid as s 使用案例： df = pd.DataFrame([['bac', 'bcd'],['应用','博客']]) s(df, show_toolbar=True) 结果如下: 笔记无法插入本地图片。。。。。。很易懂的结果。 内嵌小浏览器： from IPython.display import IFrame IFrame('https://www.baidu.com/s?ie=UTF-8&amp;wd=fsd', width=1200, height=1000) Conda各种疑难杂症安装 所有包几乎都可以用 Anaconda-prompt 中的 pip，这种 安装方法是&quot;下下策&quot;。 因为有时用这个装可能出问题。 Opencv conda: conda install -c menpo opencv pip: pip install opencv-python ","link":"https://cythonlin.github.io/post/py-greater-anaconda-jupyter-xiang-guan-pei-zhi/"},{"title":"PY => Celery使用记录","content":"前置环境 pip install redis pip install gevent config.py broker_url = 'redis://ip/0' result_backend = 'redis://ip/1' # tasks返回结果存储地址 result_expires = 10 * 60 # 过期时间 imports = ('tasks') tasks.py import requests from celery import Celery app = Celery() app.config_from_object('config') @app.task(ignore_result=False) def crawl(url): response = requests.get(url) return {'data': response.text} # 返回并储存的结果 spider.py import tasks def main(url_list): for per_url in url_list: tasks.crawl.delay(per_url) if __name__ == '__main__': main( ['http://www.baidu.com','http://www.douyu.com/'] ) worker.py import os os.system('celery worker -A tasks -l info -P gevent') # -A tasks 即为 tasks.py对应模块 # -P gevent 即为异步指定环境为 gevent 使用 先执行 spider.py 将任务存入到redis队列中， 然后执行 worker.py，正式启动任务 总结 celery 大体分为 3种 模块 task ： 专门定义函数，用装饰器装饰作为一个任务（类似于flask的视图） broker： 中间人队列，存储任务 （例如 rabbitmq, redis等） worker： 从broker队列拿出任务来 正式运行 （可多个） 相关配置参数参考地址 https://www.cnblogs.com/cwp-bg/p/8759638.html ","link":"https://cythonlin.github.io/post/py-greater-celery-shi-yong-ji-lu/"},{"title":"PY => 自编魔法鬼句","content":"鬼句1： 我 = type('A',(), {'__eq__': lambda _,__: 'True' if _ is not __ else 'False', '__ne__': lambda _,__: 'True' if _ is __ else 'False'})() 你 = type('你') print(我 == 我) print(我 != 我) print(我 == 你) print(我 != 你) 鬼句2：数列 import math 数列 = type('数列', (), {'__getitem__': lambda self,_: list(range(_[0], _[4]+1, _[1]-_[0])) if _[1]-_[0]==_[2]-_[1] else (lambda a1,e: [a1 * e**(_) for _ in range(0,int(math.log(_[4]/a1, e))+1)])(_[0],_[1]/_[0])})() print(数列[1,3,9,...,729]) print(数列[1,3,5,...,21]) ","link":"https://cythonlin.github.io/post/py-greater-zi-bian-mo-fa-gui-ju/"},{"title":"PY => CentOS偷懒安装Python3.7.4","content":"依赖： yum install gcc make zlib zlib-devel openssl openssl-devel libffi-devel bzip2-devel ncurses-devel gdbm-devel readline-devel xz-devel sqlite-devel tk-devel -y 下载包（太慢，一般我选择手动，然后xftp上传） wget https://www.python.org/ftp/python/3.7.4/Python-3.7.4.tgz 解压 tar -zxvf Python-3.7.4.tgz &amp;&amp; cd Python-3.7.4/ 指定安装环境： ./configure prefix=/usr/local/python3 编译+安装： make &amp;&amp; make install 删除原有软连接（python原来链的是python2的，我基本不用python2，所以删了） rm -rf /usr/bin/python &amp;&amp; rm -rf /usr/bin/pip 添加软连接（这就是我们python3的软连接） ln -s /usr/local/python3/bin/python3 /usr/bin/python &amp;&amp; ln -s /usr/local/python3/bin/pip3 /usr/bin/pip (可选)若软连接不好使。可以使用 alias: vi /etc/profile: alias pip=/usr/local/python3/bin/pip3 alias python=/usr/local/python3/bin/python3 source /etc/profile ","link":"https://cythonlin.github.io/post/py-greater-centos-tou-lan-an-zhuang-python374/"},{"title":"PY => Modin(Ray)加速Pandas大熊猫","content":"前言 用过Pandas的都知道，数据大了后，处理很慢。so, modin在Pandas基础上封装一层透明壳。 并且依赖ray能实现多进程pandas操作。 特色：一行代码实现Pandas加速。。。 安装依赖环境 原生windows(不支持) WSL(支持) WSL就是 (Windows Sub Linux） windows内嵌的linux子系统，自行百度 sudo pip3 install psutil sudo pip3 install setproctitle pip install modin[ray] WSL 不建议接触，我是感觉没啥用。（小心你的C盘~~~） 除非你Aliyun, TencentCloud, VirtualBox, VMWare，都懒得用。。。 Linux/kaggle(支持) modin 依赖 ray （所以用了pip的 [] 语法）: 一条命令即可 pip install modin[ray] 不了解pip新鲜语法的，可以见官档： https://pip.pypa.io/en/stable/reference/pip_install/ MacOS(支持，买不起) 导入 import ray import modin.pandas as pd # 这个导入很眼熟不， 没错,就是 多了个 &quot;modin.&quot;， 用法和原生pandas没什么区别 使用 ray.init(num_cpus=4, ignore_reinit_error=True) # 第一个参数充分利用4核CPU。 # 第二个参数 ignore_reinit_error=True， 忽略重复初始化的 而产生的报错。 # 如果不忽略，那么你用Jupyter运行时： # 第一次会调用正常运行。 # 而之后每次重复调用就会报错。 所以这个参数最好设为True pd.DataFrame() ... ... 按原生pandas的语法正常使用即可。。。 Modin官档 https://modin.readthedocs.io/en/latest/using_modin.html ","link":"https://cythonlin.github.io/post/py-greater-modinrayjia-su-pandas-da-xiong-mao/"},{"title":"PY => docker-compose部署以及scrapyd+scrapyd-deploy上传代码踩坑","content":"前言 踩了 5-6个小时的坑，各种测试。人要没了。不过结果我已经满意了。 这篇文章是对 https://segmentfault.com/a/1190000020388794 的完善与排初BUG! 大坑1（scrapyd服务bind_address）： 我用的Docker (我记得之前不用docker的时候scrapyd的配置文件好像是默认的 0.0.0.0) 但是我发现 Docker容器里面 默认是 bind 127.0.0.1 （真是 f--k了.. 弄了好几个小时没发现） 如果你用的docker， 先在Docker 同级目录下新建一个 default_scrapyd.conf 然后写入如下配置（不用犹豫，全部复制进去即可）： [scrapyd] eggs_dir = eggs logs_dir = logs items_dir = jobs_to_keep = 5 dbs_dir = dbs max_proc = 0 max_proc_per_cpu = 4 finished_to_keep = 100 poll_interval = 5.0 bind_address = 0.0.0.0 http_port = 6800 debug = off runner = scrapyd.runner application = scrapyd.app.application launcher = scrapyd.launcher.Launcher webroot = scrapyd.website.Root [services] schedule.json = scrapyd.webservice.Schedule cancel.json = scrapyd.webservice.Cancel addversion.json = scrapyd.webservice.AddVersion listprojects.json = scrapyd.webservice.ListProjects listversions.json = scrapyd.webservice.ListVersions listspiders.json = scrapyd.webservice.ListSpiders delproject.json = scrapyd.webservice.DeleteProject delversion.json = scrapyd.webservice.DeleteVersion listjobs.json = scrapyd.webservice.ListJobs daemonstatus.json = scrapyd.webservice.DaemonStatus 最后在你的Dockerfile中末尾写入（视情况而定。 每个人的目录结构都不同） COPY default_scrapyd.conf /usr/local/lib/python3.6/site-packages/scrapyd/default_scrapyd.conf 大坑2 (docker-compose: 的command多命令问题) scrapyd部署需要两步： 先开启 scrapyd 服务 (scrapyd 命令即可) 再把爬虫程序通过 scrapyd-deploy 推送到 scrapyd服务上 很明显：2 是依赖 1 的。 错误解决方式： 但是，docker-compose command: 只能运行一条命令。 我按照常理使用N种思想想让他支持多条命令： 起初 用 &amp;&amp; 用 sh脚本 这个命令是最荒唐的。。。 网上千篇一律的博客。 百分之80都是互相抄袭的。（然后我还真用了。。） 最荒唐的命令如下：（我一直窃喜，以为找到解决办法了，就一直以这个命令为轴做调试，测试。。。。） command: - /bin/bash - -c - | scrapyd scrapyd-deploy Cython_lin -p Baidu 这个命令有两种结果（这两种结果还tm是随机的。 生无可恋）： scrapyd先执行，直接堵塞。后面命令失效（即使你用&amp; 也解决不了随机启动顺序问题） scrapyd-deploy Cython_lin -p Baidu 先执行 （直接报错，以为scrapyd还没起来呢） 到此为止：：：：：：： 我上面说的都是错误的方式！！！！！！！！！！！！！！ （差点一棵树上勒到死， 既然docker-compose文件内部只能运行一条命令。那么我们拿外面去运行啊！！） （思想已经绝望僵化的我， 这次是真的看见了希望，曙光） 正确解决方式: docker-compose.yml文件里面只写： command: scrapyd 然后保存退出， 然后执行 ： docker-compose up -d # 我们需要让 scrapyd启动一会，才可以启动 scrapy-deploy的（等一小会就行了） 然后继续执行： docker-compose exec crawl scrapyd-deploy Cython_lin -p Baidu 特别注意！说明以下！（docker-compose命令有两种方式）： docker-compose exec 正确 docker-compose run 错误 (在外面千万不要用这种 （当然我是说我这个业务）) 结束语 也许有些错误很小， 但是当你怀疑变多的时候，脑海会出现海量解决问题的分支。 然后机械排BUG， 最终生无可恋。。。。。 其实有时不妨先冷静下来， 然后专注，一锤定音。找出BUG！！！ ","link":"https://cythonlin.github.io/post/py-greater-docker-compose-bu-shu-yi-ji-scrapydscrapyd-deploy-shang-chuan-dai-ma-cai-keng/"},{"title":"PY => Scrapyd部署爬虫+封装Django-View接口调用","content":"前言 我之前做的项目：一直用的 Linux的Screen会话工具+ Scrapy的JOBDIR来控制爬虫开关。 但是有后来想到用 Web 来控制爬虫的开关。所以我想到了用Scrapyd服务实现。 部署爬虫项目 安装scrapyd服务 pip install scrapyd 启动scrapyd服务 scrapyd # 就这一条shell命令即可启动服务 如果你相对爬虫服务做一些配置，（比如 log目录，绑定地址，绑定端口。。等，可修改如下配置文件）： vi /usr/lib/python3.6/site-packages/scrapyd/default_scrapyd.conf 将爬虫程序推送到服务中 首先确保你的爬虫程序单测可以无误运行！ 情况1：（爬虫程序在linux， 上面讲的scrapyd服务也在linux） 首先安装个模块： pip install scrapyd-client 修改scrapy的scrapy.cfg文件： [deploy:Cython_lin] # 这个 Cython_lin 是服务名，可自定义，但后面还会用到 url = http://23.23.xx.xx:6800/ # 这是你上面启动的Scrapyd服务的 URL+PORT project = Baidu # 这个文件是你运行 scrapy startproject 时 自动创建的文件。 在项目根目录下， 就叫 &quot;scrapy.cfg&quot;： 正式推送本机爬虫程序到Scrapyd: 如下有3个说明： 1. 你需要在项目根目录下，执行这个命令 2. Cython_lin 就是我们上面 scrapy.cfg文件 配置的服务名 3. Baidu 就是 scrapy.cfg文件 里面的那个 project属性的值，也是 项目名 scrapyd-deploy Cython_lin -p Baidu 情况2，番外篇（Windows开发，Linux部署） 同样安装个客户端模块（windows）： pip install scrapyd-client 修改scrapy的scrapy.cfg文件(windows)： [deploy:Cython_lin] # 这个 Cython_lin 是服务名，可自定义，但后面还会用到 url = http://23.23.xx.xx:6800/ # 这是你上面启动的Scrapyd服务的 URL+PORT project = Baidu 建立一个文件名为 scrapyd-deploy.bat， 内容作下（windows）： 注意：这个文件是创建在python的安装路径下（如果你是虚拟环境，那么就去虚拟环境的python路径中） @echo off &quot;D:\\Virtualenv_All\\scrapy\\Scripts\\python.exe&quot; &quot;D:\\Virtualenv_All\\scrapy\\Scripts\\scrapyd-deploy&quot; %1 %2 %3 %4 %5 %6 %7 %8 %9 随后将刚才的路径，配置到环境变量中（如果不想设置环境变量，那么每次需要用绝对路径用scrapyd-deploy） scrapyd-deploy Cython_lin -p Baidu 这条命令和上面（linux版本）讲的是一模一样的， 同样要在scrapy根路径下执行 调用爬虫程序 前面我们已经完全将爬虫Scrapyd服务部署完成。接下来就是开启调用的环节： scrapyd采用 &quot;请求接口&quot; 的方式取开启或终止爬虫： 查看爬虫状况： curl http://23.23.xx.xx:6800/daemonstatus.json 正式开启爬虫程序： curl http://39.107.86.223:6800/schedule.json -d project=Baidu -d spider=zhidao # 注意: &quot;Baidu&quot;是项目名， 而 &quot;zhidao&quot; 是每个 spider的名字 （就是主程序的爬虫名name=...） # 注意: 开启的时候会给你一个ID， 你要记住它 ，方便我们停止 停止爬虫程序： curl http://23.23.xx.xx:6800/cancel.json -d project=Baidu -d job=运行ID Django视图内嵌控制爬虫程序 上面我们说了用 curl 发送请求去操控爬虫的开启与暂停。但是这并不是习惯的做法。 更好的是，通过Qt 或者 通过Web, 来操控爬虫。 pip install python-scrapyd-api # 预先安装此模块 from scrapyd_api import ScrapydAPI scrapyd = ScrapydAPI('39.107.xx.xx:6800') # 先获取 scrapyd远程服务的客户端连接 class SpiderView(View): # 我使用的是 Django的 CBV def get(self, request): state_dict = scrapyd.list_jobs('Baidu') # 列出项目所有爬虫任务，返回字典 if request.GET.get('tag') == 'start': # 检测爬虫是否为运行状态 scrapyd.schedule('Baidu', 'zhidao') # 'project_name', 'spider_name' return HttpResponse('0') # 如果正在运行，给前端一个值，按钮 if request.GET.get('tag') == 'stop': # 前端点下按钮如果get传值为stop try: state = state_dict['running'][0]['id'] # 若取不到，就抛异常 except: return HttpResponse('-1') # 随便返回一个值，不用处理 scrapyd.cancel('Baidu', state) # 根据 id取消 爬虫入伍 return HttpResponse('0') # 并返回0（这个0是我自定义的，前端也用的0） return HttpResponse('') 前端交接 其实后端接口做好了，前端就随意了。我用的 Nuxt+Vue，主要贴一下核心method吧： methods: { start() { axios.get('http://39.xx.xx:8000/spider', { # 注意这时请求的Django params: { 'tag': 'start' } }).then( (response) =&gt; { this.tag = Number.parseInt(response['data']) if (this.tag === 0) { # 如果django返回值为0 this.start_msg = '开启成功，切记不要重复开启' # 用于vue模板提示 this.start_unable = true # 把按钮禁用，防止重复请求 this.stop_unable = false # 同时把停止按钮由禁用设置为激活 this.start_layer() # 这个函数内部实现了消息弹框 // this.stop_unable = false } } ), stop() { axios.get('http://39.107.xx.xx:8000/spider', { # 注意这是请求的Django params: { 'tag': 'stop' } # 发一个 stop参数的get请求 }).then( (response) =&gt; { this.tag = Number.parseInt(response['data']) if (this.tag === 0) { # 如果返回0, 这个0是我在Django自己指定的。 this.stop_msg = '关闭成功，切记不要重复关闭' this.start_unable= false # 负负得正，开始按钮激活 this.stop_unable = true # 停止按钮禁用 this.stop_layer() # 无关禁用的弹窗信息 } } ) }, }, Docker+docker-compose 如果你也想过使用docker+docker-compose （开启服务，推送代码一体化） 请看下篇： https://segmentfault.com/a/1190000020424059 结束语 我是设置2个按钮（开启、关闭互斥，点哪个哪个就被禁用，与此同时另一个按钮就会被激活）。 当然，你也可以用 单机，双击。等用一个按钮来控制开启和关闭。 当然这些只是标志性的功能。 核心功能还是 Django视图中的 ScrapydAPI 这个API的使用 python-scrapyd-api官档：https://pypi.org/project/python-scrapyd-api/ ","link":"https://cythonlin.github.io/post/py-greater-scrapyd-bu-shu-pa-chong-feng-zhuang-django-view-jie-kou-diao-yong/"},{"title":"PY => Python-logging模块日志原理解析及使用","content":"logging模块 logging 模块是一个较庞大的模块。具有较完备的日志体系。 主要分为：主体 Logger - 处理器 - 格式器 logging 为 python 内置模块，无需安装。 导入方式： import logging 即可 日志等级排序 (弱 -&gt; 强) DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; FATAL DEBUG : 开发调试的一些信息(print调试。。。) INFO: 程序运行过程的重要信息（不宜过多） WARNING: 不影响程序运行的小问题，警告一下。记录下来以备以后解决。 ERROR: 影响程序, 有点严重。需要处理， 不然程序 （可能，可能）就挂了。 FATAL: 严重影响程序，立刻重新排查，修改代码吧。 logging体系组件 常用分为： （由外到里的包含关系） Logger （主体日志） 最大的容器，里面装 Handler Handler （处理器类） 里面装 Formatter Formatter (格式器类) 里面写一些打印信息的格式语法 为了方便说明，接下来，我会把上面的组件“由里到外”讲解。 Formatter(格式器类) 格式器：用来定义一些打印信息的字符串格式化的语法。 初始化一个 格式器： fmt = '[%(asctime)s] [%(filename)s: %(lineno)d] [%(levelname)s] =&gt; %(message)s' # 格式 console_formatter = logging.Formatter(fmt=fmt) # 实例化格式器，并把格式传进来 理解方式： 我相信你只有 “格式”这行 看不明白，这格式你可以随便搭配的， 见下方官方文档有参数大全： 官档格式大全：https://docs.python.org/3.7/library/logging.html#logrecord-attributes 打开官档，你会看见表格的第二列 Format。 里面的格式直接原封不动复制过来即可 eg: %(asctime)s 然后，你把这些格式用字符串拼接成自己喜欢的符号格式即可， eg: &quot;日期为 =&gt; %(asctime)s&quot; 也许你会疑惑，为什么这种 %(..)s 的格式能被识别。 而不是被当作原始字符串？？？ 注意第二行代码， fmt格式字符串只是 Formatter() 的参数，它里面会自动被解析的。这你就别操心了。 我把上例结果贴一下，你可能会看明白些： &gt;&gt;&gt; [2019-09-10 18:23:19,347] [logging11.py: 15] [WARNING] =&gt; 哈哈哈 asctime 是日期 filename 是文件名 lineno 是代码行 levelname 是日志等级名 （就是上面说的 INFO WARNING ERROR之类的） message 是 你要打印的日志信息 （下面会讲到，这里先小小埋一个点） Handler(处理器类) 处理器：用来装载上面说的 “格式器”，并处理日志 （处理器有很多种，按需选1个即可，下面说2种常用的）： 初始化一个 “流处理器” （比较常用）： handler = logging.StreamHandler() 或初始化一个 “文件处理器” （源码明确写了， 它继承的是 上面的 “流处理器”。通常用来日志持久化）： handler = logging.FileHandler('mylog.log', mode='a', encoding='utf-8') # 不必解释了吧。 这API语法很熟悉了吧。 这不就是我们常用的文件 open 语法么。。。。 装载“格式器” （差点忘了吧。实例化的格式器，还没用呢， 就是在这里装载） handler.setFormatter(fmt=file_formatter) 注意: 虽然 handler对象就可以用 setLevel()设置日志等级，但我不推荐在这里设置。继续往下看 Logger(主体日志类) Logger： 用来装载 “处理器的”。 实例化Logger有两种方法： 方法1：（非共享式创建, 不推荐） log= logging.Logger(name='my_log', level='INFO') # name 是给 Logger 起的名 # level是 日志等级（注意要大写）， 开篇我们讲到过， WARNING, INFO, ERROR 这些。 方法2：（Log池共享式创建， 推荐） log = logging.getLogger(name='console') # 有则取出，无则创建 # name如果不传，则取出root Logger （root Logger是logging默认给我们提供的，我一般不用） 说一下这两种方法的区别： 非共享式： 即为每次都需要重新创建。从0开始配置 Log池共享式： 从Log池取出索引来操作（就相当于函数传索引操作） 你每对取出的Log做出配置时，都会映射保存更新到 Log池 中。 当下次（或其他文件， 当然是一个完整的程序）调用 getLogger() 取出的 log，就是之前我们配好的。 装载 &quot;处理器&quot; ：（差点忘了吧， 上面定义的 处理器，还没用呢， 就是在这里用的）： log.addHandler(handler) 设置日志等级 （这步可忽略） log.setLevel('ERROR') 其实上面我们实例化Logger的时候，我们就已经传了一个 level参数，设置好了 日志等级。 所以 log.setLevel() 这个可以不设置 （包括前面提到，handler也有 setLevel） handler.setLevel() 开始输出日志信息，有以下日志等级相对应的API： log.debug (&quot;这是一条 调试 日志&quot;) log.info (&quot;这是一条 显示主要信息 日志&quot;) log.warning('这是一条 警告 日志') log.error (&quot;这是一条 错误 日志&quot;) log.fatal (&quot;这是一条 致命错误 日志&quot;) ### 回顾我们前面讲的 Formatter 格式器 我们第一个讲的就是格式器， 并说了一下常用格式。 其中有个 %(message)s, 它就是占位上面这些API里面的参数 eg: log.info('哈哈哈') %(message)s格式 占位输出的就是 哈哈哈 还有个 %(levelname)s，它就是占位上面这些API的方法名 eg: log.info('xxx') %(levelname)s 格式占位输出的就是 info 如果你对日志等级与日志的作用感到模糊，你一定要看我接下来的例子！！！！！！！！ 开篇时我就提过： 日志等级排序(弱=&gt;强) =&gt; （DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; FATAL） 你设置了一个日志等级 。那么你所用上面API对应的等级若“强于或等于” 此设定的等级，日志才会被处理 emmmmm, 如果没听懂，就当我放P了。。。 说的越正式，越不容易理解。 我们还是看下面的例子吧~~ 日志等级理解的小例子： log.setLevel('WARNING') 你看我们设置的日志等级是 WARNING log.debug(&quot;这是一条 调试 日志&quot;) # 这个 debug(), 你可以去开篇列的&quot;日志等级排序&quot;那里瞅一眼。 # debug 比 warning 弱， 所以 这条日志是 不会 被处理的。 # （白话理解：&quot;我给的界限是warning, 你一个 debug等级太低了，问题不严重。不配被记录。&quot;） log.info(&quot;这是一条 显示主要信息 日志&quot;) # 同理， info 也比 warning 弱， 此条日志也 不会 被处理 log.warning('这是一条 警告 日志') # warning == warning （我前面说了，强于 或 等于） 所以此条日志会被处理 log.error(&quot;这是一条 错误 日志&quot;) # error 比 warning 强， 所以此条日志 会 被处理 log.fatal(&quot;这是一条 致命错误 日志&quot;) # fatal 比 warning 强， 所以此条日志 会 被处理 # 再白话一下：&quot;你给我的容忍程度是 warning, 而你的这条日志都致命错误了，我肯定处理你啊&quot; 思考！上例我一直说一句话 “xxxxx, 此条日志才会被处理”。 那么这个“ 处理” ，到底是处理什么呢？？？ 这时不妨回头看看，上面讲的 “处理器”， 嗯， 没错。 这些日志就是 “处理器” 处理的。 你要是定义一个流处理器（logging.StreamHandler）, 它就会把日志输出到终端。 你要是定义一个文件处理器（logging.FileHandler）， 它就会自动把日志保存到文件中，做持久化 综合案例： 业务需求如下（随便举个案例，不一定有用）： 比 DEBUG(强)， 但又比 WARNING(弱) ，(不包括 WARNING) 的这类日志，只输出到终端。 这类日志输出格式无要求 比 WARNING（强） (包括 WARNING )的这类日志， 输出到 终端和文件 各一份。 这类日志输出格式有要求，格式为： [日期] [所在文件名: 代码所在行] [日志级别] =&gt; 日志内容 代码如下（自己使用的话，封装一下比较好）： import logging # 日志等级排序（弱-&gt; 强）： DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; FATAL fmt = '[%(asctime)s] [%(filename)s: %(lineno)d] [%(levelname)s] =&gt; %(message)s' # 格式 file_formatter = logging.Formatter(fmt=fmt) # 定义格式器， 把格式塞进来 file_handler = logging.FileHandler('mylog.log', mode='a', encoding='utf-8') # 定义文件处理器 file_handler.setFormatter(fmt=file_formatter) # 给文件处理器设置 一个 格式器 file_handler.setLevel('WARNING') # 给此处理器设置 日志等级 console_handler = logging.StreamHandler() # 定义流处理器，用于输出到终端 # StreamHandler未设置格式器，它会默认给你设置一个 %(message)s，即只有日志内容，没有日期文件名等 console_handler.setLevel('DEBUG') # 给流处理器设置 日志等级 log = logging.getLogger(name='file_log') # log池中取出一个log(若没有则新建) log.addHandler(file_handler) # 添加一个文件处理器（格式化 输出到 文件） log.addHandler(console_handler) # 再添加一个流处理器（无格式 输出到 终端） log.info('我只会输出到终端') # 因为 info只比 console_handler 设置的 DEBUG强 log.error('我既会输出到终端， 又会输出到文件') # 因为error 比 console_handler 设置的 DEBUG 强， 同时 error 也比 file_handler 设置的 WARNING 强 运行结果： 终端输出： &gt;&gt; 我只会输出到终端 我既会输出到终端， 又会输出到文件 mylog.log 文件中： [2019-09-10 23:54:59,055] [logging11.py: 20] [ERROR] =&gt; 我既会输出到终端， 又会输出到文件 ----------------------华丽分割线---------------- 投机取巧方式 （不推荐，这里开始往后，可不看） 这种方式只方便了一点点，但不灵活。 前面我们花了好大力气， 先是定义了一个 格式器 又是定义了一个 流处理器 和 文件处理器 Logger池中，实例化一个 Logger 并且把他们各种拼装， 设置日志等级。等操作 （虽然看起来很多。其实你捋通了。真的不复杂） 其实 logging体系中， 有一个默认初始的 Logger， 叫做 root Logger. 我们不需要实例化它，也不需要实例化&quot;格式器&quot;， 也不需要实例化控制器。 一行API就可以使用它 (默认是输出到终端的)： import logging fmt = '[%(asctime)s] [%(filename)s: %(lineno)d] [%(levelname)s] =&gt; %(message)s' # 格式 logging.basicConfig( # 默认使用的就是 root Logger level='DEBUG', # 设置日志等级为DEBUG format=fmt, # 设置格式 ) logging.info('我只会输出到终端') 运行结果： &gt;&gt; [2019-09-11 00:15:55,219] [logging11.py: 30] [INFO] =&gt; 我只会输出到终端 如果想输出到文件，那么只需加 filename 和 filemode 两个参数即可： import logging fmt = '[%(asctime)s] [%(filename)s: %(lineno)d] [%(levelname)s] =&gt; %(message)s' # 格式 logging.basicConfig( level='DEBUG', format=fmt, filename='mylog.log', # 文件名 filemode='a' # 文件操作符 ) 运行结果： mylog.log文件: [2019-09-11 00:20:22,396] [logging11.py: 32] [INFO] =&gt; ��ֻ��������ն� 但你发现没，往文件里面输出乱码了， 用耳朵都能想出来，我们没有配置 encoding。。。 但是，我告诉你， basicConfig() 是没有 encoding参数的。 那咋整 ？？ 但它有个参数叫做 handlers，handlers熟悉吧，没错就是我们上面讲的 &quot;处理器&quot;, 复数说明可以传多个 logging.basicConfig( level='DEBUG', format=fmt, handlers=[ logging.FileHandler(filename='mylog.log',mode='a',encoding='utf-8') ] # 看这里这个处理器的定义方法，和之前讲过的一模一样。在这里我们可以配 encoding ) # 这样就不会乱码了 Note: 以上就是用 logging.baseConfig() 简单日志实现 说了它是投机取巧，因为除了文件乱码有问题之外， 它还欠缺灵活性。 比如你想想对不同级别的日志，用不同格式输出出来。这时你单用basicConfig一行是搞不定的。 所以还是推荐用 getLogger()那套组合。 结束语 logging模块其实还有很多很多功能： 过滤器：（其实还有个这个组件，但我没用过，就没说） 格式器：（前面给了官档大全，里面还有根据进程、线程的(PID，TID, tName,pName)等格式来输出日志。） 控制器：（我只说了 stream 和 file）,其实还有很多，它们都在logging.handlers模块下： from logging.handlers import ( RotatingFileHandler, # 通过设置文件大小阈值，超出这个阈值，就会将日志转存新文件 TimedRotatingFileHandler, # 设置时间间隔，每过这个间隔，就会将日志转存新文件 HTTPHandler, # 通过HTTP协议将日志输出到远程服务器，（只支持 GET 和 POST） SMTPHandler, # 通过SMTP协议，将日志输出到远程邮箱了 SocketHandler, # 通过TCP协议发送到远程服务器。。。 DatagramHandler, # 通过UDP协议发送到远程服务器。。。 QueueHandler, # 发到队列中（如果想发RabbitMQ之类的，可以去github找别人写的成品） ) # 这些用法也很简单，看官档，或者用Pycharm ctrl+左键点进源码，看一下__init__()参数实例化即可 # 实例化后，用 xxx.addHandler() 添加到 logger即可使用 （和前面讲的 file和stream用法一样） 还可做成各类型配置文件使用：https://docs.python.org/3/library/logging.config.html#logging.config.fileConfig 官档案例大全：https://docs.python.org/3/howto/logging-cookbook.html#logging-cookbook ","link":"https://cythonlin.github.io/post/py-greater-python-logging-mo-kuai-ri-zhi-yuan-li-jie-xi-ji-shi-yong/"},{"title":"PY => Python-ORM之peewee：插件拓展（三）","content":"声明 本篇主要讲，关于peewee的一些拓展： 包括新式CRUD-数据导入导出-信号-数据库反向生成模型。 扩展官档：http://docs.peewee-orm.com/en/latest/peewee/playhouse.html 作者友好 与 peewee提问方式 当我用到拓展模块的 新CRUD时，文档给的内容少之又少。 因为拓展的新CRUD是真的方便好用，和（PyMongo的用法差不多） 但是功能却不全。并且与我们第二篇，讲的CRUD又不兼容。 所以在难以取舍之际， 我选择了提问。 peewee作者在官档中详细说到。 如果你有问题或疑惑可以通过以下两种方式： 去stack overflow 提问题，标签打上python 和 peewee。 peewee作者会不定期浏览并回答你给你帮助。 去https://groups.google.com/group/peewee-orm，这个google群组提问。（需要kexue 上 网） 所以我选择了去 stack overflow提问。 我问题发出去，应该是不到一小时，作者就给回复了，我惊了。。（发完我就睡觉了，第二天起来才看到） 提问内容传送门如下：https://stackoverflow.com/questions/57774747/is-there-liking-query-operator-join-query-in-playhouse-extensions-to-peewee/57776856#57776856 作者回复的意思是： 拓展的playhouse.dataset里面的DataSet 的 新式CRUD API 的涉及初衷就是为了简单使用。 但它并不会代替 核心CRUD (就是我们第二篇讲的CRUD) 并且，它设计的初衷就是让我们可以方便 （json/csv格式的数据 与 数据库的数据 相互导入或导出） 我说的这些操作，下面都会写到。 所以说，该提问就提问，你收获了peewee知识的同时，又能增加peewee的社区活跃度。 playhouse扩展模块的DataSet 我们前2篇文章就用了这一行代码就可以导入所有，因为所有基本功能都集成在 peewee下 from peewee import * 但接下来讲的是扩展，而扩展就是新分支了，与peewee没关系了 from playhouse.dataset import DataSet 数据库连接 进入正题，连接MySQL,你可以有两种连接方式： # 强调一下，官档中给出 DataSet 是在 playhouse.dataset 下 # 我再强调一下， 是 DataSet ， 而不是 DataBase !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! from playhouse.dataset import DataSet db = DataSet('mysql://root:Jilin963389970@39.107.86.223:3306/test') 官档-所有数据库的连接示例： http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#db-url 数据库与表的基本操作 创建表 owner = db['owner'] # 后面这个&quot;owner&quot; 是你自己指定的key, 也就是你指定的 &quot;表名&quot; # 前面这个 owner 变量用来接受返回结果，它就是一个实例化的表对象。 一会我们用它做一系列操作 我讲完了，表已经创建好了, (你脸上有没有出现问号。。) 惊不惊喜，意不意外？？ 题外话： 如果你用过，pymongo来使用MongoDB, 那这种原理应该不陌生的。 那 owner = db['owner'] 这一行代码究竟做了什么？ 1. 它会去数据库，创建一个 名为 owner的表 （有则返回，无则创建） 2. 不但创建了表， 而且它还在表中 自动创建了一个 id (int型，主键，自增) 列出表 &amp; 列出字段 列出表（等价于MySQL中的命令： show tables） print(db.tables) &gt;&gt; ['new_owner', 'owner'] 列出表中的字段（等价于MySQL中的命令： show columns）： table = db['owner'] # 先选择一个表。 print(table.columns) &gt;&gt; ['id', 'name', 'age', 'gender', 'hobby', 'nickname'] 统计表中记录数（就是行数） print(len(table)) &gt;&gt; 10 事务(transaction) 其实我在第一篇，已经讲过事务了 事务-传送门： https://segmentfault.com/a/1190000020265149#articleHeader11 当时我是用的 MySQLDatabase 连接工具。 我只讲了一种使用方法： db.atomic() 其实还有另外一种使用方法，就是 db.transaction() 这两种方法差不多， 你可以这么认为，就是把 atomic 和 transaction 单词换一下，用法一模一样 而现在我们是使用的 DataSet 连接工具 （开篇我强调过）： db = DataSet(....) DataSet 只提供给我们 transaction()这种用法， 而没有提供 atomic() 但我说了，这两种用法你可以认为只是单词换了一下。用法一样的。 因此你完全可以直接看我前面给的 &quot;事务-传送门&quot; 这篇之前写的文章。 CRUD (这种CRUD方式我不太推荐) 我们第二章详细讲过全套的CRUD，其实那就够了。 而本章这个拓展的CRUD，完全是另一种模式。另一个模块的东西。 但我不推荐用，因为感觉还没成熟，不完善（很多CRUD细节功能没有）， 官档给的也粗略。 此外，下面涉及到与 本套CRUD有关联的操作， 我下面统称为 &quot;拓展的CRUD&quot; 增加数据 owner.insert(name='Alice', age=20) owner.insert(name='Zhang', age=18) 这看起来没什么问题，我们之前讲的差不多。但你有没有意识到几个问题： 1. owner表 是自动创建出来的，它只有一个主键。 2. 我们没有创建 name 和 age 字段 3. 既然没有创建字段，为什么可以插入数据？？？ 解惑： 1. 我们调用第一个 insert()的时候, 它就会自动帮我们 去数据库创建对应参数的字段（固定了）: 数据类型关系对照如下： python数据类型 MySQL数据类型 int int(11) 允许为空 默认值未空 str text 允许为空 默认值未空 那peewee为我们自动创建的字段如下： +-------+---------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-------+---------+------+-----+---------+----------------+ | id | int(11) | NO | PRI | NULL | auto_increment | | name | text | YES | | NULL | | | age | int(11) | YES | | NULL | | +-------+---------+------+-----+---------+----------------+ 2. 创建后，它还会自动为我们根据我们传的参数， 对应的插入 值 3. 特别注意，特别注意： 假如我们插入这样一条错误&quot;类型&quot;的数据: owner.insert(name=18, age=18) # 按理说name应该为 text型，age应该为整形 虽然我们name给的整形，但，peewee内部会自动为我们转为将 name转为字符串类型 同理 字符串也会自动帮我们转为整形（对应数据库中的类型）。 但'abc'这样的转不了哦 修改数据 owner.update(name='Alice', age='50') 这行代码为我们做了如下事情： 将&quot;所有&quot;行数据， name改为 Alice, age改为 50 owner.update(name='Alice', age='50',columns=['name']) 多加了一个 columns参数，为我们做了如下事情： 先看最后的 columns=['name'] ： 其实他就等价于 where name = xxx 其实意思就是以 &quot;name字段为条件&quot; 修改数据：(而name我们已经给了 = 'Alice') 具体过程如下： 1. 找到 name = Alice， 的所有记录 2. 将 name= Alice 的记录 name改为Alice（相当于没变）, age 改为 50 owner.update(gender='man') 你仔细看看，我们之前是没有 gender这个字段的。 如果更新未创建字段，它会为我们做如下事情： 1. 它会自动为我们在这个表中创建一个 这个 &quot;gender新字段&quot; 2. 并且，&quot;所有行记录&quot; 都会被赋予成 &quot;man&quot; 值。 注意是 所有行，所有行，所有行 3. 如果你只想&quot;给指定记录部分&quot;赋予 &quot;man&quot;值， 那你可以加一个我们之前说的 &quot;columns参数&quot; 自然而然地，其他 &quot;未通过 columns指定的记录&quot; 就会被赋予 NULL 值。 查询数据 首先说一下它这个查询的特色。 可以对查询结果进行 切片，索引操作（和python的切片和索引是一模一样的） 索引之后自动转为 列表嵌字典 [{}, {}] 下面3种方式，也都支持 切片和索引（我就不举例了，很简单） 方式1：all() 获取全部数据 owner = db1['owner'] query = owner.all() for obj in query: print(obj) # 遍历每条记录， 结果是 字典 类型 方式2：find() 查询符合条件的所有数据 query = owner.find(name='Tom') # 查询名为 Tom的数据 for obj in query: print(obj) # 遍历每条记录， 结果是 字典 类型 方式3：find_one() 查询符合条件的 第一条 数据 print(owner.find_one(name='Tom')) 注意一下，find_one 查出来结果就是一条字典。 不要再遍历了，遍历就出事了。。。 删除数据 result = owner.delete(name='Tom') 说明： 1. 若 delete() 不指定参数，那么即为全部删除。。慎用 2. 返回值result的值， 代表删除数据的条数 Json/CSV数据 导入到数据库（只能联用拓展的CRUD） Json实例 我在当前目录下创建一个 new_owner.json， 内容如下： [ {&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 18}, {&quot;name&quot;: &quot;Zhang&quot;, &quot;age&quot;: 30} ] 主文件代码如下： new_owner = db['new_owner'] # 新建一张表，名为 new_owner new_owner.thaw( filename='new_owner.json', # 就是上面的那个 json文件 format='json' # 指定格式为json, 默认值是 csv ) 这样就导入好了，数据库内容如下： +----+------+-------+ | id | age | name | +----+------+-------+ | 1 | 18 | Alice | | 2 | 30 | Zhang | +----+------+-------+ 特殊情况分析： 假如基于上面创建好的数据库与数据， 将刚刚的json文件稍加改动，加个 &quot;gender&quot; [ {&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 18, &quot;gender&quot;: &quot;man&quot;}, {&quot;name&quot;: &quot;Zhang&quot;, &quot;age&quot;: 30} ] 假如我们还是用上面的一模一样的主文件代码（我这里就不重复写了），导入刚修改的 新json数据。 我们分析一下： 我说了，是在原有表 和 字段的 基础上去导入新数据。 （原有数据库字段为 name 和 age） 但是今时不同往日。。。 我们刚刚的json新添加了一个， gender。 而原有的数据库中，并没有这个字段。 如果我们还是用之前的这个代码： new_owner.thaw( filename='new_owner.json', # 就是上面的那个 json文件 format='json' # 指定格式为json, 默认值是 csv ) 那么，peewee会自动把这个 新 gender 键，同步到数据库，并生成 新字段 gender +----+------+-------+--------+ | id | age | name | gender | +----+------+-------+--------+ | 1 | 18 | Alice | NULL | | 2 | 30 | Zhang | NULL | | 3 | 18 | Alice | man | | 4 | 30 | Zhang | NULL | +----+------+-------+--------+ 看见了把，gender字段，会自动生成。 但是在某种情况下， 你并不想让数据库 创建这个 新字段 假如json中 &quot;gender&quot; ， 它属于一个脏数据，我们不需要他，而是单纯的想插入 name和age数据。 那么你可以，添加一个 strict=True 参数： new_owner.thaw( filename='new_owner.json', format='json', strict=True # 看这里，添加一个这个 strict=True就好了 ) strict=True意味着，它会对照json的key 与 数据库的字段， 并以数据库的字段为主，严格匹配插入。 CSV示例 其实和json的几乎差不多，注意csv格式，逗号分隔，设置表头。 new_owner.csv 内容如下: name,age Alice,18 Zhang,30 主文件代码如下 new_owner.thaw( filename='owner.csv', # format='csv' # 我说过，format默认参数就是csv，所以给不给format参数都行。 # strict=True # 严格匹配字段插入， 和上面讲的json是一模一样的作用。 ) 最后说个小细节，不知道你有没有注意，我们json和csv文件， 都没有指定id。 虽然没有指定，但是 peewee 同样会为我们自动创建 id (同样也是 int(11), 主键， 自增) 数据库数据导入到Json/CSV（只能联用拓展的CRUD） 导入都说完了，导出就更简单了。。就API变个名的事： 导出单个表的全部数据： owner = db['owner'] owner.freeze( filename='new_owner.json', format='json', # 指定格式为json, 你要是不指定，默认值是 csv ) 当然，这里有个特色，导出方式还可以 导出某个查询结果！！ query = owner.find(name='Tom') db.freeze( query, # 查询结果，注意这个查询结果，必须是个查询集类型。 filename='new_owner.json', format='json', # 指定格式为json, 你要是不指定，默认值是 csv ) 中场暂停。。。 至此，playhouse.dataset的DataSet里面的 新版CRUD，及其附属功能（json导入导出等）讲完。 也许你会很不适应。 （这新版的CRUD，如果实在不能掌握，就当了解即可） 最重要的还是第二篇文章的CRUD：https://segmentfault.com/a/1190000020265522 以下要讲的，就是 第二篇文章讲的，正常的 （from peewee import * ）里面的 CRUD 相关的操作了。 信号(Signal) 官方只设定了如下 4 种信号： 一、pre_save： 保存之前调用 二、post_save：保存之后调用 pre_save 和 post_save只支持下面两种API： 1. create() # 创建数据 触发： Owner.create(name='Tom') # create包含了 save() ，所以会自动触发 2. save() # 更新数据 触发： obj = Owner.get(name='Butch') obj.name = 'Alice' obj.save() 注意： 你想用保存信号，就必须用这两种API， 用 update()是不好使的哦！！！！！！！！！！ 三、pre_delete： 删除数据之前调用 四、post_delete：删除数据之后调用 pre_delete 和 post_delete 只支持一种 API ，那就是 delete_instance() 触发： objs = Owner.select().where(Owner.name=='lin') for obj in objs: obj.delete_instance() 接下来我们开始看代码如何写： 先把表和数据构造出来，还是老方式： from peewee import * # 注意是 playhouse里的Model， 以及前面提到的 4 种 信号 from playhouse.signals import Model, post_save, pre_save, pre_delete, post_delete db = MySQLDatabase('test', user='root', password='123', host='IP', port=3306, charset='utf8mb4') class Owner(Model): &quot;&quot;&quot; 特别注意，这个Model, 使用是playhouse.signals下的 Model 而不是 peewee 下的 Model, 这需要特别注意 两个模块都有Model, 所以把用的Model放在 相对偏下面导入 &quot;&quot;&quot; data = IntegerField() # 而字段依然是 from peewee import * 导入的 class Meta: database=db db.create_tables([Owner]) 进入正题：信号使用有两种方式： 方式1（装饰器方式）： # 前面的4种信号的用法就是用来，装饰一个自定义函数。 # 这个自定义函数就是信号出发之后，为我们做事的。 # 接下来我以 post_save 为例 （当然我这个例子只是强调一下语法，并没有实用价值） @pre_save(sender=Owner) # sender指定 我们的模型类 def aaa(model_class, instance, created=True): # 这个名字随便起 &quot;&quot;&quot; model_class: 就是 Owner这个类，默认传进来方便你使用 instance 和 create=True 照着写上就行不用管它 &quot;&quot;&quot; print(f&quot;数据入库之前我们捕获了此表名=&gt;{model_class}&quot;) for obj in model_class.select().dicts(): # model_class就是Owner类，它可以任意CRUD print(f&quot;再次查看此表信息{obj}&quot;) 触发信号： obj = Owner.get(name='Tom') obj.name = 'Rose' obj.save() 方式2 (函数连接)： 信号-官档：http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#connecting-handlers 其实你用 方式1 就行了。 数据库反向生成 Python模型类 参数官档：http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#command-line-options 看好参数大小写就行（test是我的数据库名）： python -m pwiz -e mysql -H 192.6.6.6 -p 3306 -u root -P test &gt; mymodel.py # 指定了 -P ，密码是后续命令行 追入的。 模型迁移 migration 我没怎么看，需要的自己瞅瞅。。 模型迁移-官档：http://docs.peewee-orm.com/en/latest/peewee/playhouse.html#schema-migrations 结束语 其实ORM都差不多。 Django：的ORM还算可以。但是不太好脱离框架单独使用 (相当于与Django平级)。 sqlalchemy：没怎么用过。之前看过几眼。感觉极度不适。感觉学习成本有点高（相当于与Python平级） peewee：是一个可单独使用的简便的ORM框架（写web爬虫之类的都能用得上，相当于与Python平级） 我感觉：如果ORM的学习成本大于SQL的学习成本， 那倒不如精修一下SQL，即使换了环境也能用得上。 有时候高阶ORM用多了，可能连SQL都不会写了。。。 （It's just for me......） 第一篇传送门：https://segmentfault.com/a/1190000020265149 第二篇传送门：https://segmentfault.com/a/1190000020265522 ","link":"https://cythonlin.github.io/post/py-greater-python-orm-zhi-peeweecha-jian-tuo-zhan-san/"},{"title":"PY => Python-ORM之peewee：CRUD完整解析（二）","content":"声明 上篇地址：https://segmentfault.com/a/1190000020265149 虽然上一篇，已经说明，但还是强调一下，peewee是 python-ORM（只支持 MySQL,Sqlite,postgresql ） 虽然ORM可以与多种数据库无缝相接，并且兼容性好， 但是某些细微的语法并不是数据库共有的。 我用MySQL, 所以下面说的都是基于MySQL（其他2种数据库也差不了多少， 99%是一样的） 总官档地址：http://docs.peewee-orm.com/en/latest/peewee/quickstart.html 官方Github地址：https://github.com/coleifer/peewee 增加数据 方式1：（推荐） zhang = Owner.create(name='Zhang', age=20) 方式2： zhang = Owner(name='Zhang1', age=18) zhang.save() # 你可以看见，它需要用save()，所以推荐用上一种方式。 方式3：（推荐） cython = Owner.insert(name='Cython', age=15).execute() # 方式1 和 方式2, 返回结果都是模型实例&quot;（也就意味着创建了一个实例）&quot; # 而本方式，返回结果是 最新插入的主键值&quot;（也就意味着不会创建实例）&quot; 如果存在外键关联，假如存在 Pet类 引用的 Owner的主键，插入数据方式有2种： 方式1： 用新建对象变量传值： lin = Owner.create(name='lin', age=20) tom1 = Pet.create(name='Tom', age=1, owner=lin) # 注意 owner = lin 方式2： 手动维护主键 id，通过主键传值（或者通过查询id）： lin = Owner.create(id=100, name='lin', age=20) # id自己给的值为 100 tom1 = Pet.create(name='Tom', age=1, owner=100) # 注意 owner=100 插入多条数据：（官档有好几种方法，我只说最提倡，最快速的方法（好处就是一次性提交，不用循环）） 方式1： &quot;&quot;&quot; 注意格式 [ {},{},{} ] 每个字典，对应一条记录。 &quot;&quot;&quot; data = [ {'name': 'Alice', 'age': 18}, {'name': 'Jack', 'age': 17}, ] Owner.insert_many(data).execute() 方式2： (就是不用在数据中都指定键了，方便一点) &quot;&quot;&quot; 注意格式 [ (),(),() ] 每个元组，对应一条记录。 &quot;&quot;&quot; data = [ ('Alice', 18), ('Jack', 17), ] User.insert_many(data, fields=[Owner.name, Owner.age]).execute() 注意一下：尾部都必须要带一个execute() 如果数据量过大，可能会出现OOM等问题。你可以手动分批，但是 peewee 给我们提供了成品的 api from peewee import chunked with mysql_db.atomic(): # 官档建议用事务包裹 for batch in chunked(data, 100): # 一次100条， chunked() 返回的是可迭代对象 Owner.insert_many(batch).execute() 防止数据重复插入的2种办法（或者防止设置了主键，重复插入抛出异常，导致程序无法运行）： 方法1： INGORE关键字 （这种方式是如果冲突了，就自动忽略） SQL: insert ignore into owner (name,age) values ('lin',30); peewee: Owner.insert(name='lin', age=30).on_conflict_ignore() 方法2：用 ON DUPLICATE KEY UPDATE （这种方式，是如果冲突了，你还可以做一些操作） SQL： insert into owner (name,age) values ('lin',30) ON DUPLICATE KEY UPDATE name='lin', age=30; # 如果冲突了，可以重新设置值 peewee: Owner.insert(name='lin', age=30).on_conflict( preserve=[Owner.name, Owner.age], # 若冲突，你想保留不变的字段 update={Owner.name: 'lin', Owner.age: 30} # 若冲突，你想更新什么 ).execute() # 注： preserve 和 update 按情况用，一般设置一个用就行了。 删除数据 方法1： php = Owner.get(name='PHP') # 获取单条对象 php.delete_instance() # 注意： delete_instance() 只能删除单条对象， 如果用select()查出来的，需要遍历才能删 方法2： Owner.delete().where(Owner.name == 'lin').execute() # 注意这种方法和添加类似， 最后也必须有个 execute() 修改数据 方式1：(不推荐) owner= 查询单个对象结果 owner.name = 'Pack' owner.name = 50 owner.save() # 你可以看见，我们还需要手动调用一下save() 方式2：（推荐） query = Owner.update(name='Pack', age=50).where(Owner.name == 'Zhang') query.execute() 查询数据 查询单条数据 (特别注意，如果你有多条，它只会给你返回第一条) &quot;&quot;&quot;存在则返回原有对象， 不存在则抛error&quot;&quot;&quot; one_owner = Owner.get(name='Zhang2') print(one_woner.age) 扩展版1： get_or_create &quot;&quot;&quot;存在则返回原有对象。不存在则插入数据，并返回新对象&quot;&quot;&quot; obj, status = Owner.get_or_create(name='Zhang23213',age=3123) print(obj.name, status) # obj就是返回的新对象 # status表示插入是否成功 True 或者 False 扩展版2： get_or_none &quot;&quot;&quot;存在则返回原有对象， 不存在则返回 None (不会抛error)&quot;&quot;&quot; Owner.get_or_none(name='abc') 查询多条数据 正常查询所有数据 owners = Owner.select() # 返回结果 owners 是对象集合，需要遍历 for owner in owners: # owner 是每个对象（对应每条记录） print(woner.name) 当然你可以在查询后转为 python 类dict格式: owners = Owner.select().dicts() # 返回结果 owners 是 &quot;类字典对象集合&quot; for owner in owners: # owner是每个字典对象， （它 对应每条记录） print(owner['name']) # 字典语法取值，懂了吧，不多说了。 上面的查询如果在数据大量的情况下可能会导致OOM，因此可转为迭代： &quot;&quot;&quot;再每个查询的最后加上 .iterator() 即可&quot;&quot;&quot; eg: owners = Owner.select().iterator() owners = Owner.select().dicts().iterator() 条件查询： 首先我先强调个，&quot;MySQL是否区分大小写&quot; 的事： MySQL5.7+，是区分大小写的; (MySQL8，和 MariaDB 我没试， 应该和 5.7是一样的) 但这个区分大小写 仅仅仅仅仅仅 是 针对于 SQL语句的表名 &quot;&quot; 引号外面的（就是非字符串语法） 举个例子： 现有一表，名叫 owner desc owner # 正确 desc OWNER # 错误，表不存在 这种情况下，因为不涉及字符串的 &quot;&quot; 引号操作，所以是严格区分大小写的。 &quot;而引号里面&quot; （其实就是涉及字符串）的数据语法，是 不区分 大小写的。 举个例子（因为下面例子都有 &quot;&quot; 字符串操作，所以都 不区分 大小写）： SQL: 查询例子： select * from owner where name='zHang' select * from owner where name='ZHANG' 他们俩查询的是同一个数据。 插入例子： insert into owner values(&quot;zhaNg&quot;) insert into owner values(&quot;zhang&quot;) 他们俩 插入的 也是同一个数据 peewee: 查询例子： ...where(name=&quot;zhang&quot;) ...where(name=&quot;ZHaNg&quot;) 他们俩查询的是 同一个数据。 插入例子： ...insert({'name':'Zhang') ...insert({'name': 'zhANG') 他们俩 插入的 也是同一个数据 官档-条件操作符：http://docs.peewee-orm.com/en/latest/peewee/query_operators.html#query-operators 上边的连接是官档操作符大全，下面我把部分常用摘出来说一下。 常用操作符 与或非： 与：&amp; 模型类.where( (User.is_active == True) &amp; (User.is_admin == True) ) 或：| 模型类.where( (User.is_admin) | (User.is_superuser) ) 非：~ 模型类.where( ~(User.username.contains('admin')) ) 我说两句，方便记忆： 1. SQL语句中&quot;与或非&quot; 是 &quot;and or not&quot; 语法， 为啥peewee不遵循？ 答： 因为，&quot;python原语法&quot;也是这三个。。。冲突, 所以 peewee改了。 2. 看上面的例子， 每个条件操作符 &quot;两边&quot;的代码 都用 &quot;()&quot; 括起来了 范围： # 查询年龄18到20的数据 (前闭后闭) for owner in Owner.select().where(Owner.age.between(18,20)): print(owner.age) 包含&amp;不包含： 不包含：not_in （同下） 不包含：in_ # 将姓名包含 Alice和Tom的记录找出来 for owner in Owner.select().where(Owner.name.in_(['Alice', 'Tom'])): print(owner.name) 是否为null: # True 就代表把所有 name 为 null 的 记录都查出来 # False 就代表把所有 name 为 非null 的 记录都查出来 for owner in Owner.select().where( Owner.name.is_null(True) ): print(owner.name) 以..开头 &amp; 以..结尾 以..开头： startswith 以..结尾： endswith # 把以 ali 开头的 都查询出来 for owner in Owner.select().where(Owner.name.startswith('ali')): print(owner.name) 模糊查询： # 将包含 li 字符串的数据查询出来 for owner in Owner.select().where(Owner.name.contains('li')): print(owner.name) 正则查询： 这个就有意思了。前面我们强调过，MySQL带引号字符串是不区分大小写的。 而正则功能提供给我们区分大小写的API。（这是个特例，只有正则区分大小写的功能。记住） 例子条件： 假如我们有一个数据 name为 Alice regexp： 严格区分大小写的正则 # 用的是 regexp,区分大小写, 条件给的是 al小写， 所以当然 查不出来，返回空 for owner in Owner.select().where(Owner.name.regexp('al*')): print(owner.name) iregexp：不区分大小写的正则 # 用的是 iregexp, 不区分大小写。 因此即使 你给 al小写， 也能够将 Alice查出来。 for owner in Owner.select().where(Owner.name.iregexp('al*')): print(owner.name) 统计记录数 count print(MyModel.select().count()) offset &amp; limit &quot;&quot;&quot;跳过前2行，从第2+1行开始，取1条， 其实取出的就是第3行&quot;&quot;&quot; for x in Owner.select().offset(2).limit(1).dicts(): print(x) 分页 paginate &quot;&quot;&quot; 1. paginate 第1个参数为 第几页 2. paginate 第2个参数为 一页几个数据 3. paginate会自动根据查询的所有记录总数 和 你传的 两个 参数来为你自动分页 &quot;&quot;&quot; for obj in MyModel.select().paginate(1,3).dicts(): # 第一页，每页三个数据 print(obj) # peewee提供给我们分页就这么多，想要更多需求，需要我们自己发散思维。 # 下面是我自己粗略写的一个笨拙的分页。。可以参考下。。 def page(document_count=None, per_page_size=None, start_page=1): page_count = (document_count // per_page_size) # 整除的页数（可能有残页） is_rest = (document_count % per_page_size) # 总数/每页数：是否能除尽 # 除尽代表整页直接返回，除不尽有残页 ，页码+1 返回 page_count = page_count if not is_rest else page_count + 1 for page in range(start_page, page_count + 1): for obj in MyModel.select().paginate(page, per_page_size).dicts().iterator(): yield obj document_count = MyModel.select().count() # 先获取记录总数 for obj in page(document_count=document_count, per_page_size=3, start_page=1): print(obj) # 如果你有需求分页切片或索引， 那么你可以封装成类，然后实现 __getitem__ 方法 document_count = MyModel.select().count() for obj in page(document_count=document_count, per_page_size=3, start_page=1): print(obj) 排序 order_by # 默认升序 asc() for owner in Owner.select().order_by(Owner.age): print(owner.age) # 降序 desc() for owner in Owner.select().order_by(Owner.age.desc()): print(owner.age) 分组 group_by # 用姓名分组，统计人头数大于1的所有记录，降序查询 query = Owner.select(Owner.name, fn.count(Owner.name).alias('total_num')) \\ .group_by(Owner.name) \\ .having(fn.count(Owner.name) &gt; 1) \\ .order_by(SQL('total_num').desc()) for owner in query: print(f'名字为{owner.name}的 人数为{owner.total_num}个') 分组注意事项，说几点： 1. 分组操作，和SQL的group by一样， group by后面写了什么字段， 前面select同时也必须包含 2. .alias('统计结果字段名')，是给统计后的结果起一个新字段名。 3. SQL('total_num') 的作用是给临时命名的查询字符串，当作临时字段使用，支持，desc()等API 4. peewee的API是高仿SQL写的，方便使用者。因此我们最好同步SQL的语法规范，按如下顺序： where &gt; group_by &gt; having &gt; order_by 聚合原理 一会讲peewee的fn聚合原理会涉及到 getattr()，如果你不了解，可以看下我之前写过的文章。 https://segmentfault.com/a/1190000019247293 聚合原理如下： （以上面分组的 fn.count() 为例） fn是我事先导入进来的（开篇我就说过 from peewee import * ）就导入了一切（建议练习使用） fn可以使用聚合操作，我看了一下源码：讲解下思路（不一定特别正确）： fn是 Function类实例的出的对象 Function() 定义了 __getattr__方法,（__getattr__开头我已经给链接了，不懂的可以传送） 当你使用 fn.xx() : xx 就会被当作字符串传到 __getattr__ ， __getattr__里面用装饰器模式，将你 xx 这个字符串。 经过一系列操作，映射为同名的SQL语句 （这系列操作包括大小写转换等） 所以你用 fn.count 和 fn.CoUNt 是一样的 说到底 fn.xx() , 的意思就是 fn 把 xx 当作字符串映射到SQL语句，能映射到就能执行 常用fn聚合函数 fn.count() 统计总人头数： for owner in Owner.select(fn.count(Owner.name).alias('total_num')): print(owner.total_num) fn.lower() / fn.upper() 名字转小写/大写（注意是临时转，并没有真的转），并查询出来： for owner in Owner.select(fn.Upper(Owner.name).alias('lower_name')): print(owner.lower_name) fn.sum() 年龄求和： for owner in Owner.select(fn.sum(Owner.age).alias('sum_age')): print(owner.sum_age) fn.avg() 求平均年龄： for owner in Owner.select(fn.avg(Owner.age).alias('avg_age')): print(owner.avg_age) fn.min() / fn.max() 找出最小/最大年龄: for owner in Owner.select(fn.max(Owner.age).alias('max_age')): print(owner.max_age) fn.rand() 通常用于乱序查询 (默认是升序的哦)： for owner in Owner.select().order_by() print(owner.name) 关联查询前提数据准备 from peewee import * mysql_db = MySQLDatabase('你的数据库名', user='你的用户名', password='你的密码', host='你的IP', port=3306, charset='utf8mb4') class BaseModel(Model): class Meta: database = mysql_db class Teacher(BaseModel): teacher_name = CharField() class Student(BaseModel): student_name = CharField() teacher = ForeignKeyField(Teacher, backref='student') class Course(BaseModel): course_name = CharField() teacher = ForeignKeyField(Teacher, backref='course') student = ForeignKeyField(Student, backref='course') mysql_db.create_tables([Teacher, Student, Course]) data = ( ('Tom', ('stu1', 'stu2'), ('Chinese',)), ('Jerry', ('stu3', 'stu4'), ('English',)), ) for teacher_name, stu_obj, course_obj in data: teacher = Teacher.create(teacher_name=teacher_name) for student_name in stu_obj: student = Student.create(student_name=student_name, teacher=teacher) for course_name in course_obj: Course.create(teacher=teacher, student=student, course_name=course_name) 关联查询 方式1：join (连接顺序 Teacer -&gt; Student ， Student -&gt; Course) # 注意： 你不用写 on ，因为peewee会自动帮你配对 query = Teacher.select(Teacher.teacher_name, Student.student_name, Course.course_name) .join(Student, JOIN.LEFT_OUTER). \\ # Teacer -&gt; Student join(Course, JOIN.LEFT_OUTER) \\ # Student -&gt; Course .dicts() for obj in query: print(f&quot;教师:{obj['teacher_name']}，学生:{obj['student_name']},课程:{obj['course_name']}&quot;) 方式2：switch （连接顺序 Teacer -&gt; Student ， Teacher -&gt; Course） # 说明，我给的数据例子，可能并不适用这种方式的语义，只是单纯抛出语法。 query = Teacher.select(Teacher.teacher_name, Student.student_name, Course.course_name) .join(Student) \\ # Teacher -&gt; Student .switch(Student) \\ # 注意这里，把join上下文权力还给了 Teacher .join(Course, JOIN.LEFT_OUTER) \\ # Teacher -&gt; Course .dicts() for obj in query: print(f&quot;教师:{obj['teacher_name']}，学生:{obj['student_name']},课程:{obj['course_name']}&quot;) 方式3：join_from（和方式2是一样的效果，只不过语法书写有些变化） query = Teacher.select(Teacher.teacher_name, Student.student_name, Course.course_name) .join_from(Teacher, Student) \\ # 注意这里，直接指明连接首尾对象 .join_from(Teacher, Course, JOIN.LEFT_OUTER) \\ # 注意这里，直接指明连接首尾对象 .dicts() for obj in query: print(f&quot;教师:{obj['teacher_name']}，学生:{obj['student_name']},课程:{obj['course_name']}&quot;) 方式4：关联子查询 （说明：关联子查询的意思就是:之前我们join的是个表，而现在join后面不是表，而是子查询。） SQL版本如下： SELECT t1.id, t1.student_name, t1.teacher_id, t2.stu_count FROM student AS t1 INNER JOIN ( SELECT t1.teacher_id AS new_teacher, count(t1.student_name) AS stu_count FROM student AS t1 GROUP BY t1.teacher_id ) AS t2 ON (t2.new_teacher = t1.teacher_id peewee版本如下： # 子查询（以学生的老师外键分组，统计每个老师的学生个数） temp_query = Student.select( Student.teacher.alias('new_teacher'), # 记住这个改名 fn.count(Student.student_name).alias('stu_count') # 统计学生，记住别名，照应下面.c语法 ).group_by(Student.teacher) # 以学生表中的老师外键分组 # 主查询 query = Student.select( Student, # select 传整个类代表，查询 temp_query.c.stu_count # 指定查询字段为 子查询的字段， 所以需要用 .c 语法来指定 ).join( temp_query, # 关联 子查询 on=(temp_query.c.new_teacher == Student.teacher) # 关联条件 ).dicts() for obj in query: print(obj) 方式5： 无外键关联查询 (无外键也可以join哦，自己指定on就行了) 重新建立一个无外键的表，并插入数据 class Teacher1(BaseModel): teacher_name = CharField() class Student1(BaseModel): student_name = CharField() teacher_id = IntegerField() mysql_db.create_tables([Teacher1, Student1]) data = ( ('Tom', ('zhang1', 1)), ('Jerry', ('zhang2', 2)), ) for teacher_name, student_obj in data: Teacher1.create(teacher_name=teacher_name) student_name, teacher_id = student_obj Student1.create(student_name=student_name, teacher_id=teacher_id) 现在我们实现无外键关联查询： &quot;&quot;&quot;查询学生 对应老师 的姓名&quot;&quot;&quot; query = Student1.select( Student1, # 上面其实已经讲过了,select里面传某字段就查某字段，传类就查所有字段 Teacher1 # 因为后面是join了,但peewee默认是不列出 Teacher1这张外表的。 # 所以需要手动指定Teacher1 （如果我们想查Teacher1表信息,这个必须指定） ).join( Teacher1, # 虽然无外键关联，但是依旧是可以join的（原生SQL也如此的） on=(Student1.teacher_id==Teacher1.id) # 这个 on必须手动指定了 # 强调一下，有外键的时候，peewee会自动为我们做on操作，所以我们不需要指定 # 但是，这个是无外键关联的情况，所以必须手动指定on, 不然找不着 ).dicts() for obj in query: print(obj) 方式6： 自关联查询 # 新定义个表 class Category(Model): name = CharField() parent = ForeignKeyField('self', backref='children') # 注意一下，外键引用这里写的是 &quot;self&quot; ，这是是固定字符串哦 ；backref是反向引用，说过了。 # 创建表 mysql_db.create_tables([Category]) # 插入数据 data = (&quot;son&quot;, (&quot;father&quot;, (&quot;grandfather&quot;, None))) def insert_self(data): if data[1]: parent = insert_self(data[1]) return Category.create(name=data[0], parent=parent) return Category.create(name=data[0]) insert_self(data) # 这是我自己定义的一个递归插入的方式。。可能有点low # 可能有点绕，我把插入结果直接贴出来吧 mysql&gt; select * from category; +----+-------------+-----------+ | id | name | parent_id | +----+-------------+-----------+ | 1 | grandfather | NULL | | 2 | father | 1 | | 3 | son | 2 | +----+-------------+-----------+ # 开始查询 Parent = Category.alias() # 这是表的（临时查询）改名操作。 接受参数 Parent 即为表名 # 因为自关联嘛，自己和自己，复制一份（改名就相当于临时自我拷贝） query = Category.select( Category, Parent ).join( Parent, join_type=JOIN.LEFT_OUTER, # 因为顶部类为空，并且默认连接方式为 inner # 所以最顶端的数据（grandfather）是查不到的 # 所以查所有数据需要用 ==&gt; 左连接 # on=(Parent.id == Category.parent) # 官档说 on 需要指定，但我试了,不写也能关联上 ).dicts() 至此，关联查询操作介绍结束！ 接下来对以上六种全部方式的做一些强调和说明： 你可以看见我之前六种方式都是用的dicts()，返回的是类字典格式。（此方式的字段名符合SQL规范） 当然你也可以以类对象的格式返回，（这种方式麻烦一点，我推荐还是用 dicts() ） 如果想返回类对象，见如下代码（下面这种方式多了点东西）： query = Teacher.select(Teacher.teacher_name, Student.student_name, Course.course_name) \\ .join_from(Teacher, Student) \\ .join_from(Teacher, Course, JOIN.LEFT_OUTER) # 注意，我没有用dicts() for obj in query: print(obj.teacher_name) # 这行应该没问题吧。本身Teacher就有teacher_name字段 # 注意了，按SQL原理来说，既然已经做了join查询，那么查询结果就应该直接具有所有表的字段的 # 按理说 的确是这样，但是peewee，需要我们先指定多表的表名，在跟写多表的字段,正确写法如下 print(obj.student.student_name) # 而不是 obj.student_name直接调用 print(obj.course.course_name) # 而不是 obj.course_name直接调用 # 先埋个点， 如果你看到下面的 N+1查询问题的实例代码和这个有点像。 # 但我直接说了， 这个是用了预先join()的， 所以涉及到外表查询后，不会触发额外的外表查询 # 自然也不会出现N+1的情况。 # 但如果你没有用join，但查询中涉及了外表，那么就会触发额外的外表查询，就会出现N+1的情况。 关联N+1查询问题： 什么是N+1 query? 看下面例子： # 数据没有什么特殊的，假设， 老师 和 学生的关系是一对多（注意，我们用了外键）。 class Teacher(BaseModel): teacher_name = CharField() class Student(BaseModel): student_name = CharField() teacher_id = ForeignKeyField(Teacher, backref='student') # 查询 teachers = Teacher.select() # 这是 1 次， 查出N个数据 for teacher_obj in teachers: for student in teacher_obj.student: # 这是 N 次循环（N代表查询的数据） print(student.student_name) # 每涉及一个外表属性，都需要对外表进行额外的查询, 额外N次 # 所以你可以看到， 我们总共查询 1+N次， 这就是 N+1 查询。 # （其实我们先做个 表连接，查询一次就可解决问题了。。 这 N+1这种方式 属实弟弟） # 下面我们介绍2种避免 N+1 的方式 peewee解决N+1问题有两种方式： 方式1：（join） 用 join 先连接好，再查询（前面说了6种方式的join，总有一种符合你需求的） 因为 peewee是支持用户显示调用join语法的， 所以 join是个 特别好的解决 N+1 的问题 方式2： （peewee的prefetch） # 当然，除了 join，你也可以使用peewee提供的下面这种方式 # 乍眼一看，你会发现和我们上面写的 n+1 查询方式的例子差不多，不一样，你仔细看看 teacher = Teacher.select() # 先预先把 主表 查出来 student = Student.select() # 先预先把 从表 查出来 teacher_and_student = prefetch(teacher, student) # 使用 prefetch方法 （关键） for teacher in teacher_and_student: # 下面就和N+1一样了 print(teacher.teacher_name) for student in teacher.student: print(student.student_name) 说明： 0. prefetch， 原理是，将有外键关系的主从表，隐式&quot;一次性&quot;取出来。&quot;需要时&quot;按需分配即可。 1. 使用prefetch先要把，有外键关联的主从表查出来（注意，&quot;必须必须要有外键，不然不好使&quot;） 2. prefetch(主表,从表) # 传进去就行，peewee会自动帮我们根据外键找关系 3. 然后正常 以外键字段 为桥梁 查其他表的信息即可 4. （ 题外话，djnago也有类似的prefetch功能,（反正都是避免n+1，优化ORM查询） 貌似给外键字段 设置select_related() 和 prefetch_related() 属性 ） 未结束语 本篇主要讲了，CRUD, 特别是针对查询做了大篇幅说明。 我还会有下一篇来介绍peewee的扩展功能。 上一篇传送门：https://segmentfault.com/a/1190000020265149 下一篇传送门：https://segmentfault.com/a/1190000020287565 ","link":"https://cythonlin.github.io/post/py-greater-python-orm-zhi-peeweecrud-wan-zheng-jie-xi-er/"},{"title":"PY => Python-ORM之peewee：模型-字段-索引-约束-事务（一）","content":"前言 去github搜 &quot;python orm&quot;，最高star居然不是sqlalchemy，而是peewee 后来得知peewee，比sqlalchemy简单好用。值得一学哦！！ 我总体感觉（peewee像 Django-ORM的分离版，，但比Django-ORM和SqlAlchemy 小巧，简单，文档也友好） 还有一个更重要的感觉就是， peewee 的 API方法名 和 SQL语句 的 单词 基本相似。 例如对比一下(关键词语法都是 update 和 where)： SQL语句：update Lang set name='Python' where name='Java'; Peewee：Lang.update(name='Python').where(Lang.name == 'Java') 这种良心的API，可以大大降低我们的学习成本，还可以巩固我们对SQL的记忆！！！！！！ 总官档地址：http://docs.peewee-orm.com/en/latest/peewee/quickstart.html 官方Github地址：https://github.com/coleifer/peewee 安装和导入 pip install peewee from peewee import * # peewee的模块很结构化，都在peewee中，如果懒就都导入进来。 当然你也可以熟了，按需导入 # 后面无特殊情况，就都是这样导入的。我就不提了。 数据库 postgresql 和 sqlite peewee 只支持 sqlite, mysql 和 postgresql 数据库， 如果你有需求用oracle等，请绕行。。。 如需sqlite 和 postgresql，配置请参考 http://docs.peewee-orm.com/en/latest/peewee/database.html#database mysql 当然我经常用MySQL，以后的所有都围绕mysql来讲，如下是基本配置 mysql_db = MySQLDatabase( 'lin', # 数据库 user='root', # 用户名 password='123', # 密码 host='IP', # IP port=3306, # 端口 charset='utf8mb4' # 字符集类型， utf8mb4 是 utf8的大哥 ) peewee的mysql引擎默认优先使用pymysql。 如果你没安装pymysql， 他就会去寻找 MySQLdb。 都没有就会报错。 嗯，都啥年代了，python3的时代，所以我们用 pymysql模块即可，若没安装，跳出来安装下即可 pip install pymysql 既然用的pymysql驱动，MySQLDatabase() 里面的写法 和 pymysql对象实例化的参数配置是一样的。 如果我给的例子的参数不够用，你可以来下面的链接自己选吧：https://github.com/PyMySQL/PyMySQL/blob/f08f01fe8a59e8acfb5f5add4a8fe874bec2a196/pymysql/connections.py#L494-L513 建立数据库连接 print(mysql_db.connect()) 关闭数据库连接 print(mysql_db.close()) 测试数据库连接是否关闭 mysql_db.is_closed() 列出数据库的所有表： mysql_db.get_tables() 列出所有字段的详细信息： print(db.get_columns('owner')) # 假设 owner是表名，下面同理 列出所有主键的字段： print(db.get_primary_keys('owner')) 列出所有索引字段的详细信息： print(db.get_indexes('owner')) 列出所有外键的字段： print(db.get_foreign_keys('owner')) Python 各种 web框架嵌入使用 peewee 案例传送门： 官档-Web案例：http://docs.peewee-orm.com/en/latest/peewee/database.html#framework-integration 表-记录-字段 ORM语法 和 数据库的 （表-记录-字段）对应关系如下： ORM结构数据库 类表 实例（对象）记录 类属性列 ### 默认自增主键ID 定义一个类，继承了peewee模块的Model类，这个类就可以当作Model来用了 首先建立一张\"空表\" mysql_db = MySQLDatabase('lin_test', user='root', password='123', host='ip', port=3306, charset='utf8mb4') class Owner(Model): class Meta: database=mysql_db # 这里是\"必须\" 要指定的， 指定哪一数据库 mysql_db.create_tables([Owner]) # 注意，源码是取出参数遍历，所以这里参数用列表 上述代码就可以建立一张\"空表\"。 为什么\"空表\" 用引号括起来呢？？ 这是关于peewee orm的机制，\"你若不指定（primary key）\"，它就会\"自动\"为你创建一个 \"名为 id\", \"类型为 int\", 并设置为 \"primary\" 的 \"自增(auto_increment)\" 的字段 但 一旦你把一个自定义的字段，设为主键，默认的id字段就会被覆盖： name = CharField(primary_key=True) # name设为了主键， 原有的默认id就没了 官档也说明：如果你想自己建立一个自增主键，并覆盖默认id。你可以用AutoField字段： new_id = AutoField() # 这句话直接就为你 设置为 int型 和 主键 和自增。 \"这是官档最推荐覆盖id的方法， 而不是自己弄一个 Integer，再设主键\" 自增id就讲完了， 不过你是否发现每个 类下都有 class Meta: database= xxx # 这是为每张表指定数据库，必须要指定的。不然它不知道你这个表在哪个数据库 既然这样，若我们要在一个数据库中创建很多很多表，那岂不是每次都需要给每张表指定一个数据库？？ 就像这样： class User(Model): class Meta: database = mysql_db class Owner(Model): class Meta: database = mysql_db 这样有点烦，但我们可以定义一个基类指定好数据库， 然后其他子类模型继承它就好了。 class BaseModel(Model): name = CharField(max_length=10) # 定义一个 name 字段 class Meta: database = mysql_db class User(BaseModel): # 继承基类 pass class Owner(BaseModel): # 继承基类 pass mysql_db.create_tables([User, Owner]) # 正式创建表， 基类不需要，可以不放进来 像上述代码CharField, 更多类型字段定义，官档给的很详细了，我不再赘述了。 官档-字段-参数：http://docs.peewee-orm.com/en/latest/peewee/models.html#field-types-table 但下面我还会挑一些主要常用（有一点点点难特别）的说一下。。。 外键字段（ForeignKeyField） 普通外键 class BaseModel(Model): # 基类 name = CharField(max_length=10) class Meta: database = mysql_db class Owner(BaseModel): # 主人类 pass class Pet(BaseModel): # 宠物类 owner = ForeignKeyField( Owner, backref='owner_conn', # 通过引用名获取对象。&quot;主人，你可以通过这个名字调用我&quot; on_delete='Cascade', # 级联删除 # 默认为None， 这时，你想删主人是删不掉的。会报错。 必须先删宠物再删主人。 # 设为 Cascade后， 你可以直接删主人。 他的宠物也会随之自动删除。 这就是级联删除 on_update=Cascade, # 级联更新，原理同 on_delete ) 层级外键（通常用于层级分类,自关联查询）： class Category(BaseModel): name = CharField() parent = ForeignKeyField('self', null=True, backref='children') 注： &quot;self&quot; 字符串是固定语法， 下一篇还会将，自关联查询 日期字段（DateTimeField） import datetime ...... date_time= DateTimeField(default=datetime.datetime.now) 表属性（Meta） 表属性就是可以 改表名，设置主键，联合主键，设置索引，联合索引等操作。不再赘述，见官档。 官档 Meta: http://docs.peewee-orm.com/en/latest/peewee/models.html#model-options-and-table-metadata 索引 和 约束 设置索引有3种方法： 通过定义字段的参数： 普通索引 name = CharField(index=True) 唯一索引 name = CharField(unique=True) 通过定义表属性Meta： 联合唯一索引 class Meta: indexes = ( (('字段1', '字段2'), True), # 字段1与字段2整体作为索引，True 代表唯一索引 (('字段1', '字段2'), False), # 字段1与字段2整体作为索引，False 代表普通索引 ) 需要注意的是，上面语法，三层元组嵌套， 元组你懂得， 一个元素时需要加个 , 逗号。 别忘了。 索引API： 官档：http://docs.peewee-orm.com/en/latest/peewee/models.html#advanced-index-creation 设置约束有2种方法： 通过定义字段的参数： -------通常用来单一字段主键： name = CharField(primary_key=True) 通过定义表属性Meta -------通常用作联合主键： class Meta: primary_key = CompositeKey('字段1', '字段2') # primary_key = False # 也可以不使用主键（不覆盖，也 取消 创建默认id字段） 事务 支持with上下文语法，支持事务嵌套，注意嵌套事务 只会回滚 离它最近 的一层之间的代码。 包裹在with语句中的代码，只要存在异常，就会回滚。嵌套的事务，也是有一处异常，所有层事务都会回滚。 当然你也可以手动 rollback()来回滚。 嵌套事务示例如下： with mysql_db.atomic() as transaction1: # 第一层事务。 atomic(), 固定语法就不说了。 User.create(username='Tom') with mysql_db.atomic() as transaction2: # 第二层事务 User.create(username='Jerry') User.create(username='Spike') transaction2.rollback() # 就近原则， 第二层的rollback()回滚 User.create(username='Butch') # 如果真的出现回滚，那么 从 第二层的 with() 开始算 事务内容， 到 rollback() 结束 # 形象例子： 顶部 面包片从 第二层的with()开始夹， 底部 面包片 夹到 rollback() # 注意一点，虽然是嵌套事务，但是每层with事务都有对应的名字（就是with as 之后变量）。 # 所以回滚写在哪层事务里面， 就要用哪层事务的名字（就近原则）。 不然会报错的。 # 错误实例： 倒数第二行的： transaction2.rollback() 写成 transaction1.rollback()。 错误！ 带有commit()的嵌套事务示例如下：（缩小事务的代码范围， 就像 &quot;面包里夹的东西变少了&quot; 的意思） with mysql_db.atomic() as transaction1: # 第一层事务 User.create(username='Tom') with mysql_db.atomic() as transaction2: # 第二层事务 User.create(username='Jerry') transaction2.commit() # 就这里变了， 插入了一行 commit User.create(username='Spike') transaction2.rollback() # rollback()回滚 User.create(username='Butch') # commit()，加入了这一行，就意味着 从 这行开始算 回滚内容，到 rollback() 结束 # 形象例子： （顶部 面包片 从commit() 这里开始夹, 底部 面包片 夹到 rollback() ） 上面无论哪个事务例子， 都必须注意： 每层事务，只管自己层内的 rollback()，才有效， 不能管其他层的。 就算你用 commit() 夹， 如果自己层内没有 rollback()， 那么你的 commit()是无效的（夹不住） 事务就差不多这些，官档还有一些用法和语法，但最终功能结果都是一样的。选一种（我的例子）就行。 官档-事务: http://docs.peewee-orm.com/en/latest/peewee/database.html#managing-transactions 闲杂用法 查看ORM对应的原生SQL语句： .....ORM语句.sql() # 后缀 .sql() 打印对应原生sql 执行原生SQL： # 注意，传数据用参数，不要用字符串拼接（防SQL注入） for owner in Owner.raw('select * from owner where name=%s', 'Alice'): print(owner.name) 更原生的执行原生SQL: print(mysql_db.execute_sql('select * from user').fetchall()) # sql，可以传位置参数（防注入），就像使用 pymysql一样。 表改名： 注：我说的改名只是查询时的临时名 下一篇文章查询，会提到 字段改名， 格式： 字段.alias('新字段名') 那表改名也差不多，有2种方式： 方式1： 格式： 表类.alias('新表名') 方式2： 格式： 新表名 = 表类.alias() 未结束语 本篇写了一些入门性的模型的建立，数据库，事务，索引，算是比较基本的。 当然还有更常用，更重要的CRUD等，会在下一篇介绍。 下一篇传送门：https://segmentfault.com/a/1190000020265522 ","link":"https://cythonlin.github.io/post/py-greater-python-orm-zhi-peeweemo-xing-zi-duan-suo-yin-yue-shu-shi-wu-yi/"},{"title":"PY => Python版-Docker使用selenium简单示例","content":"Dockerfile内容如下 FROM python RUN pip install -i http://pypi.douban.com/simple \\ requests selenium retrying --trusted-host pypi.douban.com docker-compose.yaml内容如下 version: &quot;3.7&quot; services: myspider: build: . volumes: # 数据卷映射 - /root/mycode:/root/mycode command: python /root/mycode/1.py # 依赖下方 selenium服务，注意此依赖仅仅能做到 # selenium服务先启动， myspider服务后启动（有的服务内部程序启动的快，有的慢） # 根本程度上还是解决不了 完全依赖 的 问题， 因此可以用延时处理等方法 depends_on: - selenium selenium: image: selenium/standalone-chrome # 拉取镜像完成自动化全套配置 ports: - &quot;4444:4444&quot; shm_size: 2g # 设置主机共享内存2g hostname: selenium # 其他容器可以用此名来访问 eg: http://selenium:4444/ 爬虫脚本代码1.py如下 import requests from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities from selenium.webdriver.chrome.options import Options from retrying import retry # 注：docker-compose.yaml中的depends_on那里提到过： # 服务可以起到先后依赖的效果。 # 而服务中的启动程序，并不能达到完全的依赖（启动速度相当的情况下，谁快谁慢运气成分） # 可以通过加延时来控制先后顺序 # import time # time.sleep(3) # 这种睡眠延时方式，或多或少不太精确，可以用下面的 retrying代替之 # 也可以通过 retrying模块装饰实现 # retrying用法，可参考 https://segmentfault.com/a/1190000019301761#articleHeader17 @retry( stop_max_attempt_number = 10000, stop_max_delay = 10*1000, ) def verify_request(): response = requests.get(&quot;http://selenium:4444&quot;, timeout=0.5) print(response) verify_request() # 下面基本上是连接Docker Selenium服务的固定写法，可当作模板套 options = Options() options.add_argument('--headless') with webdriver.Remote( command_executor='http://selenium:4444/wd/hub', # selenium为docker-compose的host名 desired_capabilities=DesiredCapabilities.CHROME, options=options ) as driver: driver.get('http://www.baidu.com') # 这里使用绝对路径， 否则数据卷映射失败 # 映射部分在上面 docker-compose.yaml 的 volumes部分 with open('/root/mycode/test.html', 'w') as f: f.write(driver.page_source) print('写入成功') 踩坑 selenium 因为有服务端程序，所以我们可以在远程&quot;云服务器用Docker容器部署&quot; 容器部署后。。。。 &quot;只能云服务器中访问， 不能在远程服务器访问。 （ 其实根本不需要在远程服务器访问的，某种邪恶的念头，让我走了弯路。。。一根筋想要远程访问 其实代码同是部署在容器中的，容器互通是完全OK的。 但如果你也想试试远程访问， 它却真的无法访问。。。。。。。。。。。 ）&quot; （ 我的认知里，既然云服务器宿主机可以访问容器内部 启动的服务端程序 而远程服务器不能访问容器内部的 启动的服务端程序。。。。那一定是容器和宿主机连通配置问题阿。 带着这个思路，找了很久很久， 然并卵。。。 ） 实在没办法了。。kexue上 网，搜一下这个问题的解决方案吧。 后来无意中发现，客户端现在居然可以连上了。。 后来测试，哇， 果然是 需要kexue 上 网才能 远程访问到，此服务端。。。 但我现在还没有明白，为什么我云服务器宿主机， 不需要kexue 上 网，也能成功访问内部容器的服务端？？ （虽然这个疑惑没有必要） ","link":"https://cythonlin.github.io/post/py-greater-python-ban-docker-shi-yong-selenium-jian-dan-shi-li/"},{"title":"PY => Flask框架与Sanic框架基本使用对比介绍","content":"Sanic 和 Flask 简要概述 &quot;&quot;&quot; Flask常用 Sanic和Flask很像，于是按着Sanic官方文档学了一下，对比Flask学习并做下笔记,回顾一下 &quot;&quot;&quot; Flask：轻量级Web框架，三方组件齐全，用时安装，扩展灵活度高。 Sanic: 和Flask特别像，基础语法，基础命名特别相似（使用几乎差不多）。 Sanic是基于Uvloop(没用过，了解即可，windows不支持)实现， 具有 异步-非阻塞的特性 （网上也有说Sanic可以通过一些操作后，可以在Windows环境下使用，我试了貌似不行） （Linux下运行才会具有最好的性能表现） python3.5+ 原生支持原生协程 async+await， 这也可以在sanic的视图中用到，下面会介绍 安装 pip install flask pip install sanic 入门程序 （Flask vs Sanic） Flask: from flask import Flask app = Flask(__name__) @app.route('/') def index(): # 注意这行 return 'hello' # 注意这行 if __name__=='__main__': app.run('0.0.0.0', 1119, debug=True) Sanic： from sanic import Sanic,response app = Sanic(__name__) @app.route('/') def index(request): # 注意这行 return response.text('hello') # 注意这行 if __name__=='__main__': app.run('0.0.0.0', 1119, debug=True) 使用Sanic的正确姿势 之 视图异步非阻塞 上面入门程序可以看到 flask 和 sanic 的路由对应的视图函数，是一样的 特殊的是：sanic支持异步高性能，每个 def 前面 需要加上 async 即可 eg: @app.route('/') async def index(): # 以后写每个视图函数前面都要加上 async return 'hello' 模板引擎(Flask VS Sanic) Flask: from flask import render_template app = Flask(__name__) @app.route('/') def home(): return render_template('index.html', data=[dict(name='Tom', age=18), dict(name='Jerry', age=20)] ) # 这个模板渲染器是flask库中自带的，不需额外安装 Sanic： pip install sanic-jinja2 # 还需安装这个三方插件 from sanic import Sanic,response from sanic_jinja2 import SanicJinja2 as sj # 导入就不说了，sj只是命名来方便调用 app = Sanic(__name__) tp = sj(app) # 注意这里，这个和 flask三方应用注册是一个道理 使用方式1：（Sanic特有的装饰器方式） @app.route('/') @tp.template('index.html') # 注意这行，以装饰器的方式渲染模板 async def index(request): return { # 注意这里，返回值就是向模板填充的数据 'data': [ dict(name='Tom', age=18), dict(name='Jerry', age=8) ] } 使用方式2： （和Flask模板使用方法很像，很像） @app.route('/') async def index(request): return tp.render( # 这个方法代替了 上一种方法的装饰器 'index.html', request, # 这个request参数，必须有 data = [ # 模板渲染数据作为 redner()的 **kwargs参数来传递 dict(name='Tom', age=18), dict(name='Jerry', age=8) ] ) if __name__=='__main__': app.run('0.0.0.0', 1119, debug=True) 小结： Flask的模板渲染机制是集成在 flask库中，用 render_template方法来直接渲染模板 并且，以方法参数的形式向模板传递数据 Sanic的模板渲染机制是以第三方插件 sanic-jinja2 中的 SanicJinja2组件来实现。 使用时，需要先注册到app中， 所接受的返回值，以装饰器的方式来渲染模板 个人看法： 某种程度上来说， Sanic 更加细粒度的将 功能 以第三方应用的方式划分出来。 即便如此，但我还是喜欢 flask 中 render_template机制。 response的各种返回方式对比分析(Flask VS Sanic) Flask: from flask import Markup, jsonify, send_file @app.route('/') def index(): # return 'hello' # Content-Type='text/plain' # return Markup('&lt;h1&gt;hello&lt;/h1&gt;') # 反转义 # return jsonify(dict(name='Tom')) # Content-Type-'application/json' # return send_file('static/1.png') # 返回各种类型文件 # return 'Tom'.encode('utf-8') # 返回字节形式 下面是修改 状态码 和 headers 的方式: # return render_template('home.html'), 220, {'a': 1} 格式： retrun 请求体,状态码,响应头 Sanic: @app.route('/') async def index(request): # return response.text('hello') # Content-Type='text/plain' # return response.html('&lt;h1&gt;hello&lt;h1/&gt;') # 代替反转义 # return response.json(dict(name='Tom')) # return response.redirect('/xxx') # return await response.file('static/1.png') # 返回各种类型文件，注意有个 await # return response.raw(b'Tom') # 返回原生字节类型数据 下面是修改 状态码 和 headers 的方式: 1. 如果返回的响应体 为 模板，就用下面的方式 @tp.template('index.html',status=220,headers={'name':'Tom'}) # 在装饰器参数里 2. 如果返回的响应体 为 非模板内容，就用如下方式 return response.text('hello',300,{'name':'Tom'}) 格式: response.text(请求体，状态码，响应头) 小结： 上面是针对response返回时，对各种数据类型的返回时可能用到的方式进行对比介绍。 同时还对比讲述了 如何 修改 响应头 和 状态码 个人看法： Flask： 1. response各种变形返回方式 都封装了 flask这个模块之中 2. response的响应信息（状态码，响应头）等， 通过 return 以 逗号或元组 方式构造返回. eg: return 响应体，状态码，响应头 Sanic: 1. response的各种变形返回方式 都封装了 sanic 模块的 response 中 （分类更加明确） 2. response的响应信息（状态码，响应头）等， 都放在函数中作为参数. eg: response.xx(响应体，状态码，响应头) request的各种请求方式对比分析 （Flask vs Sanic） Flask: from flask import request request.method # 获取用户的请求方式: GET 或 POST request.args # 接受get的url参数 request.form # 接受post的form表单 Content-Type='x-www-form-urlencoded' request.json # 必须接受 Content-Type='application/json' 格式请求的数据 request.data # 请求数据对应的Content-Type除了表单（xxx-form）格式外，都可用此接受 request.values # 如有form 和 url 联合参数，用这个接受 注：以上获取的对象都是 类字典对象， 可以使用字典的 get('前端name') 获取 value 此外： 你还可以使用 to_dict()方法，就变成了纯种的字典 {k: v} img = request.files.get() # 接受文件类型 img.save(img.filename) # filename获取文件原始名， save直接保存， 默认当前路径。 request.url # http://localhost:5000/login request.path # /login request.host # localhost:5000 request.host_url # http://localhost:5000/ request.remote_addr # 单纯获取IP地址 Sanic: flask中的request是导入进来的 而sanic中的request是在视图参数之中（参考django） eg: def f(request) 就是这个意思 request.method # 同Flask rqeust.args # 同Flask request.form # 同Flask request.json # 请求若为表单(xxx-form)格式会报错， 除了表单都可接受 request.body # 亲求若为表单(xxx-form)格式，则会出现一大堆 二进制信息，非表单都可接受 request.url # 同Flask request.path # 同Flask request.host # 同Flask request.ip # 同样单纯获取IP， 属性名和上面 flask稍微有点不同 路由讲解 （Flask vs Sanic） Flask: @app.route( 'login/&lt;int:id&gt;', # 路由参数解析并 自动转换int类型， 冒号后为接受参数 methods=['GET,'POST'], # 建议全部大写 endpoint='sign', # 默认为下面的视图函数名，即login，用于url_for反向解析 redirect_to='/xxx' # 重定向跳转，注意请求过来直接跳转，不进入下面视图函数 ) def login(id): # request是默认必须传递的参数， id是上面路由解析接收的 return f'{id+1}' # 路由参数 + python3.6新增语法实现 f-string 接受参数自增 Sanic: @app.route( 'login/&lt;int:id&gt;', # 路由参数解析并 自动转换int类型， 冒号后为接受参数 methods=['GET,'POST'], # 建议全部大写 name='sign' # 同 flask 的 endpoint， 用于 url_for反向解析 ) def login(request, id): # request是默认必须传递的参数， id是上面路由解析接收的 return f'{id+1}' # 路由参数 + python3.6新增语法实现 f-string 接受参数自增 Flask 模板 相关操作 (Flask) 注：由于 sanic 的 template还不成熟， 花式操作我也就没找，下面就只讲一下 flask的常用模板操作 模板宏(macro)： 主要目的是为了前端代码的复用 定义 模板宏 就和 定义 python的函数类似， 或者你可以把 macro 看作 python的 def eg: 定义部分：可理解为函数定义 {% macro user(type, value) %} &lt;input type=&quot;{{ type }}&quot; value=&quot;{{ value }}&quot;&gt; {% endmacro %} 调用部分：可理解为函数调用 {{ user('text', '用户名') }} {{ user('password','密码') }} 全局模板自定义函数： 视图文件.py中定义: @app.template_global() def f(x): return x**2 模板中调用: {{ f(5) }} 模板过滤器自定义函数： 视图文件.py中定义: @app.template_filter() def add(a, b): return a+b 模板中调用： {{ 1 | add(2) }} 模板继承： （可理解为挖坑 与 填坑） 父级模板: header.html xxxxxxx前面若干内容 {% block header %} # 这个 header，就是挖坑起的名，记住了 这里面就是你挖的坑，啥也不用写 {% endblock %} xxxxxxx后面若干内容 子级模板：content.html {% extends 'header.html' %} # 必须写上这句，这是填坑的暗号。。 {% block header %} # 这个名header是和上面挖坑的名一样 这里面就是你要填坑的数据 {% endblock %} 填完坑之后，这个 content.html子级模板的内容 = 父级模板内容 + 填坑内容 模板代码块的导入（插入）： 作用还是 html 代码的复用 写好一个 html 代码块，放在 header.html中 eg: &lt;h1&gt; Tom &lt;h1/&gt; # 注意，一个html文件中就写这一句就行 另一个文件： index.html 写入 如下代码: xxxx前面若干内容 {% include 'header.html' %} # 这一句就可把 那一个html内容全部插入进来 xxxx后面若干内容 （中间件）钩子函数 （Flask vs Sanic） Flask: @app.errorhandler(404) # 错误处理的钩子函数 def f(err): return '出错了' 注： 出现了异常， before_request装饰的函数会立刻断掉，而 after_request的会依然倒序执行 @app.before_request # 视图函数执行之前 def f(): return None代表正常按顺序执行， xxx retrun其他值，就不会进入视图函数，直接response返回 注： 如果有多个before_request，那么就 正序 装饰执行 @app.after_request # 视图执行之后，返回给 客户端之前 def f(res): 必须有个参数接受 response对象，并且return 回去 xxx return res 注： 如果有多个before_request，那么就 倒序 装饰执行 如果仍然不明白执行顺序，看西面例子： eg: @app.before_request def req1(): print('请求来了-1') @app.before_request def req2(): print('请求来了-2') @app.after_request def res1(response): print('请求走了-1') @app.after_request def res1(response): print('请求走了-2') @app.route('/lin') def lin(): print('进入视图') return '11' 结果&gt;&gt;&gt; 请求来了-1 请求来了-2 进入视图 请求走了-2 请求走了-1 # 这两个response是逆序的 Sanic: from sanic.exceptions import NotFound @app.exception(NotFound) # 错误处理的钩子函数 def f(request, err): return response.text('出错了') 请求，返回中间件和 flask大同小异，顺序有些区别，我直接上例子了 eg: @app.middleware('request') async def req(request): print('请求来了-1') @app.middleware('request') async def req(request): print('请求来了-2') @app.middleware('response') async def res(request, response1): print('请求走了-1') return response1 @app.middleware('response') async def res(request, response1): print('请求走了-2') return response1 @app.route(xxx) async def f(request): print('进入视图') return response.text('xx') 结果&gt;&gt;&gt; 请求来了-1 请求来了-2 进入视图 请求走了-2 # 注意这里即可，只返回一个。下面不同点会详讲 总结：Flask 与 Sanic 中间件返回顺序对比 相同点： request处理部分 : 装饰器代码vs执行流程 =&gt; 正序 response处理部分: 装饰器代码vs执行流程 =&gt; 逆序 不同点： Sanic 只执行 最后一个 用装饰器注册的 response Flask 执行顺序是 全部 用装饰器注册的 逆序返回的 response 蓝图 （Flask vs Sanic） 蓝图使用三部曲： 1. 新建目录和文件，创建蓝图对象 2. 在主app文件中, 导入蓝图对象 3. 注册蓝图对象 Flask: 1. 新建 /Admin/user.py，写入如下代码 from flask import Blueprint user_bp = Blueprint('user_bp', __name__, url_prefix='/admin') # 增加url前缀 @user_bp.route('/user') def f(): return 'admin-user' 2. 在app中导入 蓝图对象 from Admin.user import user_bp 3. 注册蓝图 app.register_blueprint(user_bp) # 这里也可以写url前缀, 如果写了就会覆盖上面写的 Sanic: 1. 新建 /Admin/user.py，写入如下代码 from sanic import Blueprint, response user_bp = Blueprint( __name__, url_prefix='/admin') @user_bp.route('/user') async def f(request): return response.text('admin-user') 2. 在app中导入 蓝图对象（同Flask） from Admin.user import user_bp 3. 注册蓝图 (本来是和flask一样用register_blueprint，后来版本更新改用 blueprint注册) app.blueprint(user_bp) # 这里也可以写url前缀, 如果写了就会覆盖上面写的 注：Flask的蓝图对象，同 Flask类似，都具有模板路径、静态文件路由 与 静态文件本地路径的配置 因此，蓝图实例化的时候，配置响应参数即可： template_folder = 'xxx' # 对应本地模板路径 ，默认 templates static_folder = 'xxx' # 对应本地文件路径 ，默认 static static_url_path = '/xxx' # 对应url路径 ，默认 /static 注2： 如果蓝图 和 app 的 模板或静态文件命名重复，那么会优先选择 app下的模板或静态文件 CBV （Flask vs Sanic） CBV(Class-Based-View)： 就是拿类当作视图 （我们之前使用的函数作为视图） Flask 的 CBV感觉没 FVB好用， CBV是Django的重点 Flask: from flask import views class UserView(views.MethodView): methods = ['GET'] # 这里写 的是 允许的请求方式 decorators = [装饰器名,] # 全局装饰器顺序装饰， 单独给函数加@装饰器也可以 def get(self,*args, **kwargs): return xx def post(self, *args, **kwargs): return xx app.add_url_rule( '/user',None,UserView.as_view('endpoint名') ) Sanic: from sanic.views import HTTPMethodView class UserView(HTTPMethodView): async def get(self, request): return text('get') async def post(self, request): return text('post') app.add_route(UserView.as_view(), '/') # Sanic 视图在前，路由在后。 总结： 讲道理，CBV在这两个轻型框架感觉用的很笨拙。。 还是很用CBV较好 Flask的flash （Flask） flash原理： 服务器给flash设置了值，那么用户每请求一次，就会把session放到用户cookie中 (后面会提到session插件方法) 与此同时也把 flash值记录在里面。 flash就相当于一跟管道 flash(): # 把值塞进管道 get_flashed_messages(): # 把值从管道取出来 from flask import flash, get_flashed_messages # flash是基于 session来实现的，所以需要写一句： app.secret_key = '111' @app.route('/lin') def lin(): flash('666') return redirect('/user') @app.route('/user') def user(): msg = get_flashed_messages() print(msg) # 注意从 flash取出来的是列表，因为你可以把不同数据多次 填入 flash return '' &gt;&gt;&gt; [666] ","link":"https://cythonlin.github.io/post/py-greater-flask-kuang-jia-yu-sanic-kuang-jia-ji-ben-shi-yong-dui-bi-jie-shao/"},{"title":"PY => Python版-Redis分布式锁简单实现","content":"定义代码如下 import redis import contextlib import pickle import os, socket, threading class RedisLock: def __init__(self, lock_name, host='', port=6379, db=0): self.lock_name = lock_name self.redis = redis.Redis(connection_pool=redis.ConnectionPool(host=host, port=port, db=db)) def acquire_lock(self, lock_id, expire=None): lock_id = lock_id if lock_id else self.get_lock_id() return True if self.redis.set(self.lock_name, pickle.dumps(lock_id), nx=True, ex=expire) else False # Above 1 line code can replace with follow codes to debug # if self.redis.set(self.lock_name, pickle.dumps(lock_id), nx=True, ex=expire): # print('Lock Succeed') # return True # else: # print('Lock Failed') # return False def release_lock(self, lock_id=None): lock_id = lock_id if lock_id else self.get_lock_id() if lock_id == pickle.loads(self.redis.get(self.lock_name)): self.redis.delete(self.lock_name) # print('Unlock Succeed') return True else: # print('Unlock Failed') return False @contextlib.contextmanager def lock(self, lock_id=None, expire=None): if not self.acquire_lock(lock_id, expire): exit(0) yield self self.release_lock(lock_id) def get_lock_id(self): &quot;&quot;&quot; hostname+processID+threadName&quot;&quot;&quot; return f'{socket.gethostname()}{os.getpid()}{threading.current_thread().name}' 调用代码如下 redis_lock = RedisLock('lockname', host='Your IP') # 第一个匿名参数必传，作为 redis的key with redis_lock.lock() as lock: print('You Can Do Something Here') 注意说明 1. 注释部分是我写的时候，调试用的代码，最后写完的时候都替换为简洁的语法. 2. 因为锁具有互斥特性， 所以选择 set() 的 nx 参数来实现， nx参数：我个人一直这样记（读作 not exist） ====&gt; 不存在 理解： 不存在则添加，存在就不添加了。 举一反三，没有锁就加个锁。有锁就不加锁了。 3. set方法 的 ex 参数， 可代替expire方法的来设置过期时间 4. redis 有 很多指令变形。 比如 set(nx=, ex=) 可拆分为 setnx setex 但 &quot;set() 这种指令更优&quot;， 能用就尽量用， 理由如下： &quot;set指令 好处是 set具有原子性&quot;, 避免了解决资源竞争的同时引发自身可能出现的资源竞争 5. 我使用装饰器版 的 上下文管理器，对代码做了封装， 所以调用时，用&quot;with语句&quot;即可 6. with redis_lock.lock() as lock ，&quot;lock() 这里可以自己指定2个参数&quot;： lock_id=None # 这是区分不同线程的唯一标识符，默认为（主机名+进程ID+线程名），可自传 expire=None # 过期时间，秒为单位 7. 需要注意一个点，与redis通信是以二进制形式。 所以我在代码内部对 lock_id 做了&quot;pickle序列化&quot; 当然如果是字符串用 encode() 与 decode() 来实现也是可以的。 ","link":"https://cythonlin.github.io/post/py-greater-python-ban-redis-fen-bu-shi-suo-jian-dan-shi-xian/"},{"title":"PY => Python版-Docker使用Appium简单实验","content":"Docker-Appium安装 Github docker-appium地址 ： https://github.com/appium/appium-docker-android 创建临时容器（用于简单命令测试） docker run --privileged -d -p 4723:4723 -v ~/.android:/root/.android -v /dev/bus/usb:/dev/bus/usb --name container-appium appium/appium 若使用模拟器等（非USB连接） 模拟器的adb将USB模式转为TCP连接方式： adb -s 127.0.0.1:62001 tcpip 1119 远程Docker连接此模拟器： docker exec -it container-appium adb connect 192.168.0.103:1119 查看虚拟设备是否连接成功 docker exec -it container-appium adb devices 若使用真机（USB连接）可直接使用如下配置 Dockerfile内容如下 FROM python RUN pip install -i http://pypi.douban.com/simple \\ requests retrying appium-python-client --trusted-host pypi.douban.com docker-compose.yaml内容如下 version: &quot;3.7&quot; services: myspider: build: . volumes: - /root/mycode:/root/mycode command: python /root/mycode/1.py depends_on: - appium appium: image: appium/appium # 拉取镜像完成自动化全套配置 ports: - &quot;4723:4723&quot; privileged: true hostname: appium # command: adb connect 192.168.0.103:1119 # command: # - /bin/sh # - -c # - | # adb connect 192.168.0.103:1119 # adb devices # entrypoint: adb connect 192.168.0.103:1119 volumes: - ~/.android:/root/.android - /dev/bus/usb:/dev/bus/usb 爬虫脚本代码1.py如下 from appium import webdriver from retrying import retry import requests import time config = {} config['platformName'] ='Android' config['platformVersion'] = '7.1.1' config['deviceName'] = '坚果 Pro 2' config['noReset'] = True config['appPackage'] = 'org.mozilla.firefox' config['appActivity'] = 'org.mozilla.gecko.BrowserApp' ################### 查看 appPackage 和 appActivity ################# ## 注意： ## 这两个值是针对某一软件的配置，你需要在手机上打开你这个软件，然后再执行此命令： ## 我此例就是用的 手机里面的火狐浏览器。 ## 那么我首先需要，将火狐打开。 ## 然后再执行如下命令才能查到想对应的配置。 ## 否则，查的是你手机运行状态的主界面应用程序的配置信息 ## 命令如下： ## docker exec -it container-appium adb shell # 进入 adb shell ## dumpsys activity | grep mFocusedActivity ## 返回结果 / 前面的是 appPackage 的值 ## 返回结果 / 后面的是 appActivity 的值 @retry( stop_max_attempt_number = 1000000, stop_max_delay = 10*1000, ) def verify_request(): response = requests.get(&quot;http://appium:4723/wd/hub&quot;,timeout=0.5) print(response) verify_request() with webdriver.Remote( command_executor='http://appium:4723/wd/hub', desired_capabilities=config ) as driver: driver.get('https://tieba.baidu.com/index.html') time.sleep(5) with open('/root/mycode/test.html', 'w') as f: f.write(driver.page_source) print('写入成功') time.sleep(3) 前情链接 Pycharm 与 Docker 相关操作：https://segmentfault.com/a/1190000020050218 Docker 中 Selenium的使用： https://segmentfault.com/a/1190000020067591 ","link":"https://cythonlin.github.io/post/py-greater-python-ban-docker-shi-yong-appium-jian-dan-shi-yan/"},{"title":"PY => Pycharm连接Docker及代码自动上传与运行","content":"Docker镜像报错 我尝试创建 /etc/docker/daemon.json文件， 并写如下内容： { &quot;registry-mirrors&quot;: [&quot;https://rs3ab060.mirror.aliyuncs.com&quot;], &quot;hosts&quot;: [&quot;unix:///var/run/docker.sock&quot;, &quot;tcp://0.0.0.0:6006&quot;] } 然后，重启docker服务，一直失败。 根本无法启动。 经排查，是hosts这个配置项有问题。 终极解决办法： systemctl edit dockerd.service # 复制这条命令执行，并写粘贴如下内容 [Service] ExecStart= ExecStart=/usr/bin/dockerd ctrl + s 保存 ctrl + x 退出 一、Pycharm创建Docker客户端 二、修改Docker客户端配置 在pycharm底部点这个打开配置 填写Docker服务端，的IP地址及端口（tcp:// 不能少） 上面这个配置完后，下面会出现 connected successfully，说明配置成功。 如果配置失败，有几种情况： 一、 你当前主机环境 无法 ping 通 Docker所在服务器（云服务器、防火墙、的端口没放通） 二、 Docker未做 IP配置 如何配置参考下面链接的解决方案即可配置成功。 https://segmentfault.com/q/1010000020042977 配置完成后，记得重启 Docker服务 三、Pycharm sftp向Docker所在主机上传代码 连接测试成功后， 不要点OK， 选最上面的 Mappings项,继续配置 上面配置完后，点OK，然后开始上传 四、创建Docker里的Python解释器 ctrl+alt+s打开配置， 然后按下图操作添加解释器 五、给脚本指定解释器 及 相关配置 下面其他都是默认配好的， 主要配置下面红框部分的 2 处内容 切记： 上面的第一个红框： Path mappings项 必须配，不然就会出现下图的错误 这个错误，stack, github都没找到解决办法。 （明明已经是绝对路径了，还是让你提供绝对路径） 后来无奈之下，随手把 Path mappings 这项映射配了。 居然就成功了。。。。。！！ 六、最后直接执行本地python文件即可 就是我们平时在pycharm正常写代码的执行操作。。。 总结 + 解惑 总结 我们创建了Docker客户端,并修改了相关配置 sftp上传代码（我们映射部分，填的是路径， 它会自动把该路径下的文件上传到云服务器） 创建python解释器，并做出详细配置（各种映射） 疑惑解析： 为啥修改代码保存一次，远程服务器也会同步自动修改？？？？？？？？？ 因为我们在sftp的时候，mapppings选项中 配置了客户端和云服务器的代码目录 映射。 并且，我们在第一次同步代码的时候勾选了，自动同步选项 （Automatic upload） 为啥可以操作远程的Docker里面的python解释器？？？？？？ 首先我们创建了 docker 的客户端。并做出了 连接远程服务器的配置。 基于上述条件，我们在建解释器时做了如下操作: 选定云服务器的Docker, 选定Docker中的镜像 选定Docker中的镜像中的python解释器 python镜像是Docker（内部），为什么我们可以直接访问云服务器（外部）来达到需求 ？？？？？？ 因为我们在配置解释器的时候， 配置了数据卷映射（第 五 章截图内Docker Container settings项） 当然这属于Docker数据卷（volume）知识点范畴。 Docker数据卷教程参考链接： END ","link":"https://cythonlin.github.io/post/py-greater-pycharm-lian-jie-docker-ji-dai-ma-zi-dong-shang-chuan-yu-yun-xing/"},{"title":"PY => Docker语法全面回忆","content":"Hello Docker 官方安装教程：https://docs.docker.com/install/linux/docker-ce/ubuntu/ 进去选好对应系统/发行版， 照着命令复制-粘贴-运行。 就可以安装成功（根本不需要多余操作） Image（镜像） docker search docker search python # 列出dockerhub 提供的 image docker pull（下载） docker pull python:3.7 # 从 dockerhub下载 image 冒号:数字 用来指定版本（不指定就是最新版本） docker images（列出） docker images # 列出本地镜像 （或 docker image ls） docker images py* # 也可以通过名称来筛选查看 image， 也可使用通配符 docker rmi（删除） docker rmi &quot;image名&quot; 或 &quot;imageID&quot; # 删除 image docker rmi python -f # 强制删除（当image内有容器运行无法删除时，可通过-f强制删除） 如果两个image有相同 &quot;imageID&quot;，会删除失败， 这时可以考虑用 &quot;image名&quot; 来删除 如果两个image有相同的 &quot;image名&quot;， 那么可以考虑用 &quot;image名:Tag&quot; 来删除 docker save（保存备份） 方式1：docker save python &gt; python.tar # 可追加多个image来 把多个image打包保存 方式2: docker save python -o python.tar python.tar文件 可分享传输，给别人还原加载使用 注： 上令为例，如果有多个python版本， 那么会将所有python images 都会打包在一起保存 如果你有多个镜像， 为了避免混淆，一定要指定一下版本号 docker save python:latest docker load（还原） 方式1： docker load -i python.tar 方式2： docker load &lt; python.tar docker tag（改名，改版本号） docker tag python:latest py:3.7 # 把 &quot;python:latest&quot; 改为 &quot;py:3.7 &quot; 注1： 若image名不为&lt;none&gt;, 那么首先会将 image 复制创建一份，然后改名 注2： 若原image名为 &lt;none&gt; ，那么 改名后，会直接在原有image上直接改名 docker inspect（查看详细信息） docker inspect python docker history（查看分层历史信息） docker history mypython:3.7 Container（容器） docker create（创建） docker create --name py-con python:latest # --name后自定义名字， 最后指定哪一个镜像 docker create -it python:latest python # 创建带有标准输入环境的容器，并执行python命令 -t 为了给容器创建一个 terminal -i 为了给容器提供一个 标准输入流 （否则，容器终端里无法输入） 注：创建默认是 created状态， 需要下面 docker start 命令来启动 docker start（开启） docker start -ai 容器ID # 以标标准环境开启容器 -a 代表提供标准输出 -i 同create -i ，提供标准输入 docker run（创建+启动, 推荐） docker run -it python:latest python # 一套搞定 create+start 的繁杂过程， -it同上不解释 # 命令防混淆解释： 根据python:latest镜像 ，创建并执行容器，同时执行 python命令 docker run -d -it python:latest python # 其他不变，多加一个 -d， 可以创建并放入 &quot;后台&quot; 执行 （不加-d , 默认&quot;前台&quot;） docker run -dit --rm python:latest python # --rm 参数代表 容器停止，即（exited状态）， 就会自动删除。 docker run --network bridge -itd mypython:latest python # --network 代表指定网络（若不指定，默认也是bridge，见下面 网络章节） docker run -dit -p 6006:6379 redis # 端口映射， 宿主机（6006）：容器(6379) (-P代表容器内部所有端口 与宿主机随即映射) docker run --restart always # --restart always 代表 docker服务重启时， 里面的容器也会跟着重启 docker run --name mysql -e MYSQL_ROOT_PASSWORD=123 -p 3306:3306 -itd mysql # mysql示例，前提是已经 docker pull mysql docker stop（终止，结束） docker ps # 查看一下 &quot;运行中容器ID&quot; docker stop 容器ID # 停止 &quot;运行中&quot; 的容器 ( 默认 10秒钟后 才停止) docker stop -t 0 374 # -t指定时间 0秒后， 即瞬间就可以停止 扩展（停止所有正在运行的容器）： docker stop $(docker ps -q) # -q参数代表只显示容器ID docker restart（重启） docker restart -t 0 281 # 0秒重启 docker pause（暂停） docker pause 281 # 暂停容器内的所有进程， 注意是暂停， 不是终止 docker unpause（继续） docker unpause 281 # 把暂停的容器，继续开启 docker ps（查看） docker ps # 列出所有 &quot;运行中&quot; 的容器 docker ps -a # 列出所有 容器 docker logs（查看输出日志） docker logs 281 # 查看容器内部输出日志 docker logs -f 281 # -f 代表阻塞监控， (和 tail -f 一个道理) docker rename（重命名） docker rename 281 python # 把 281的容器 改名为 python docker inspect（查看容器详细信息） docker inspect 374 # 查看容器所有信息 docker rm（删除） docker rm 容器ID # 删除已停止的容器 docker rm 容器ID -f # 强制删除（运行中）等特殊情况的容器 docker attach（进入到容器命令执行处） docker run -itd python:latest python # 新创建容器，名执行 python命令 docker attach 281 # 直接进入281这个容器，并直接跳到 python控制台内部 &quot;注： 进入python控制台后，再退出去，就意味着， 容器的退出。&quot; docker exec（执行命令） docker exec -it 281 python # 在&quot;容器外部&quot;, 执行&quot;内部容器&quot;的 python命令 &quot;注： 与上一条attach不同的是，退出python控制台后，容器依旧运行！（因为是在容器外面执行的python命令）&quot; Container and Images（容器与镜像关联） docker commit (把容器&quot;封装&quot;成一个新镜像) docker commit 4d mypython:3.7 # 把4d这个容器所有内容,封装为一个&quot;名字:版本&quot;叫&quot;mypython:3.7&quot;的镜像 docker export (容器导出为一个文件) docker export fc8 -o mypython.tar # 把此容器导出为一个.tar文件 ，和前面说过的 image的 save类似 docker import (把export导出的文件导出，并&quot;生成&quot;一个镜像） docker import mypython.tar mypython:latest 注： 把export导出的 mypython.tar文件导入 并 直接创建一个 mypython:latest 的 镜像 docker commit &amp; docker import区别 前面说过： docker commit 是 直接把一个container 封装为一个image docker import 是 把export导出的container.tar文件 再 导入进来，并重新生成一个 新 image docker commit 是继承封装的，并创建具有分层历史记录 （docker history imageID 即可查看） docker import 是直接生成的，不具有分层记录 （docker history imageID 即可查看） 网络 docker network ls （查看） docker network ls bridge（网桥）：容器默认网络模式 容器-容器网络连接： container1(etho0)--veth1--Docker(bridge)--veth2--container2(etho0) 容器-宿主机网络连接： container1(etho0)--veth1--Docker(bridge)--宿主机(etho0) 注： veth是创建网络时，自动创建的，不需要手动管理 host（主机）： 容器网络和主机使用同一个网络 容器-容器网络连接： container1(etho0)--宿主机--container2(etho0) 容器-宿主机网络连接： container1(etho0)--宿主机 容器网络（特殊host）： container1--container2 # 就是 ‘每个容器互相把对方认作为 宿主机’ 这个意思 使用方法： docker run -it --network container:24f1 mypython:latest ls # container:24f1 的container是语法关键词 24f1是连接的对方容器（） null（无网络）：所有容器无网络 docker network create （创建） docker network create -d bridge mybridge # 可创建多个bridge docker network create -d host myhost # 只可创建一个host（默认就有一个，故无法创建） docker network create -d null mynull # 只可创建一个null（默认就有一个，故无法创建） docker network rm （删除） docker network rm ab5 注：默认自带的网络不可以删除（null host 和 自带的一个 bridge） docker network connect (给容器绑定网络) docker network connect mybridge 4c4 # 给4c4这个容器绑定一个 mybridge网络（自定义的bridge） docker inspect 4c4 # 查看一下容器信息，最下面就是网络 注：一个container 可以绑定 多个bridge 网络， docker network disconnect (给容器 解除绑定的网络) docker network disconnect mybridge 4c4 # 给容器解除绑定网络mybridge 注： 一个container 中 bridge 和 none 网络不可以共存， （若冲突，则先disconnect再connect） 注2：host 网络不能 connect 和 disconnect 数据卷 (volume) docker volume create（创建数据卷） docker volume create myvolume 注： myvolume为数据卷名 docker volume ls（列出数据卷） docker volume ls 注： 若数据卷未指定名字，当 使用docker run -v 方式时，则会新建数据卷ID，并以此ID命名。 docker volume prune（删除未被容器使用的 所有 数据卷） docker volume prune 注：容器占用的数据卷，删不了 docker volume rm （删除 一个 或 多个 指定数据卷） docker volume rm myvolume 注： 删除 myvolume这个数据卷，当然也可以连续参数，追加删除多个数据卷 挂载数据卷 &quot;&quot;&quot;意义： 可以让 宿主机 与 容器 数据联通共享&quot;&quot;&quot; 方式1 （-v参数） -v使用方式1：（指定路径映射挂载） docker run -itd -v /root:/root mypython:latest python # -v 宿主机路径:容器路径 测试： cd /root touch aaa.txt # 宿主机创建文件 aaa.txt docker exec -it cfb ls /root # 结果可看见容器里面也有 aaa.txt 文件 -v使用方式2：（指定数据卷对象 映射挂载） docker run -itd -v myvolume:/root mypython:latest python # 冒号前面 变成了myvolume 注1： 这个myvolume就是一个数据卷对象， 执行上面这条命令，就会为我们自动创建这个数据卷对象 注2： 由于没有宿主映射路径，那么映射的宿主路径 是什么呢？？ docker volume inspect myvolume # 结果Mountpoint后面的就是，宿主机映射的 默认钩子路径 cd /var/lib/docker/volumes/myvolume11/_data # 此路径和volume名有关 touch bbb.txt # 宿主机创建文件 bbb.txt docker exec -it 916 ls /root # 打印结果可见，容器内部也有bbb.txt，说明成功共享。 方式2：（--mount参数，同样包括 -v的两种使用方式， 另外还新增另一种 文件&quot;缓存&quot;挂载方式） docker run -itd --mount type=volume,src=myvolume11,dst=/root mypython:latest python 注： type: 指定类型（路径映射: bind）或 (数据卷对象映射: volume) 或（内存映射：tmpfs） src: 对应上面方式1（宿主机路径） 或 对应上面方式2（数据卷名） 或 省略此项（对应新增） dst: 容器路径 逗号分隔，其他没变 docker run -itd --mount type=tmpfs,dst=/root mypython:latest python (tmpfs&quot;缓存&quot;挂载) &quot;综上,可总结为3种挂载选择用途&quot;： 一. &quot;宿主路径 与 容器路径&quot; 映射挂载 二. &quot;数据卷 与 容器路径&quot; 映射挂载 三. &quot;宿主内存 与 容器路径&quot; 映射挂载 &quot;综上,可总结为2种挂载参数使用&quot;： 一、 &quot;-v 参数&quot; 2种用途 (路径映射 和 数据卷对象映射) 二、 &quot;--mount 参数&quot; 3种用途 (路径映射 和 数据卷对象映射 和 内存映射) 容器之间共享数据 &quot;&quot;&quot;借助已经拥有数据卷的容器 来 创建一个新容器&quot;&quot;&quot; docker run -itd --volumes-from 6252 python:latest # 借助6252容器创建新容器，来共享数据卷 验证： docker exec -it 97db touch /root/abc # 新容器 创建一个文件abc docker exec -it 6252 ls /root # 旧容器查看 ，也有新文件abc，共享成功 细节注意事项 一、若将 &quot;空数据卷&quot; 挂载到 容器非空目录中，则&quot;此容器目录下的内容 会copy一份到 数据卷中&quot; 二、若将 &quot;非空数据卷&quot; 挂载到 容器任意目录中，则&quot;数据卷的数据 会copy到这个目录中，并将此目录原数据隐藏&quot; 更通俗一点理解就是： 数据卷大哥说：&quot;如果我这里有数据， 你的容器来挂载，你的数据就会被我这里面的数据覆盖。。&quot; 数据卷大哥又说：&quot;如果我这里是空的（没有数据），那么 你的容器来挂载， 你的数据就要提供一份给我&quot; DockerHub（仓库） 无认证 私有仓库 搭建仓库 docker pull registry # 拉取 registry镜像 docker run -itd \\ --restart always \\ # docker重启时，此容器也跟着重启 --name myregistry \\ # 指定容器名 -p 6006:5000 \\ # 端口映射 （registry服务默认为5000端口，映射为6006） -v /root:/var/lib/registry \\ # 绑定数据卷 （持久化存储）， 冒号后面的容器路径时默认的 registry # 拉取的 registry镜像 验证：（一种web服务，所以通过固定Url访问即可） 外部浏览器验证： 浏览器输入 服务器外网IP:6006/v2/_catalog 即可 服务器内部验证： curl 127.0.0.1:6006/v2/_catalog 上传镜像 一、先把要上传的镜像改名 docker tag mypython:latest 127.0.0.1:6006/mython_hub 注： 目标名固定格式（需注意，必须此格式）： IP:Port/新镜像名 二、开始上传 docker push 127.0.0.1:6006/mython_hub # docker push 镜像名，注意这里用ID不好使，必须用这名 三、验证 同上面搭建仓库时的验证方法， 可看见结果 repositories列表中多了一个 刚刚上传的镜像 curl 127.0.0.1:6006/v2/_catalog 下载镜像 docker pull 127.0.0.1:6006/mython_hub 注： 这个名就是上传时候的 那个名， 一样的 Dockerfile（配置文件式） Dockerfile认知 Docker 与 docker命令的关系就相当于 shell编程 与 单条命令 主要就是把上面讲的所有命令连起来，脚本式执行， 当然dockerfile也有自己的语法关键词。 Dockerfile是基于缓存，所以里面的文件内容(某条命令) &quot;如果未发生改变，则不会重新执行（用的是缓存）&quot; Dockerfile机制： 一、若在结尾每&quot;追加&quot;一条新命令,重新构建Dockerfile时，&quot;只会执行这个新命令，其他旧命令都会使用缓存&quot; 二、若新命令 是在&quot;中间插入编写的&quot;，则此条新命令&quot;之前的命令用缓存&quot;, &quot;之后&quot;的命令都会重新执行一遍， 三、FROM 关键字是 Dockerfile的入口。 新命令只要不是 写在 &quot;FROM的下一条&quot;, 那么所有新命令及其之后的命令都会在 构建Dockerfile时--&gt; 触发&quot;层层封装&quot;机制 ，即每条&quot;非缓存命令&quot;运行一遍，都会commit封装一层镜像 Dockerfile构建 docker build /Dockerfile所在路径 -t mypython:v2 注1： 指定Dockerfile所在路径即可，build会自动帮我们找到dockerfile文件 注2： 如果Dockerfile就在当前路径下，那么可以用 . 来替代绝对路径 注3： -t 给镜像指定名字 Dockerfile语法 FROM &quot;下载镜像,类似 docker pull&quot; FROM python:latest # 同样可以指定版本号 RUN | CMD | ENTRYPOINT 这三个 命令 都有共同的 2种书写方式： 一、（exec）格式--当前进程执行 eg: python -V # 就是玩linux的命令正常写 二、（shell） 格式--子进程执行 eg: [&quot;python&quot;, &quot;-V&quot;] # 命令与作为字符串列表来书写， 和py的scrapy的shell类似 RUN: &quot;构建镜像过程中&quot;执行的命令， 比如安装东西之类的。。（可写多个） CMD: 启动容器时 执行的命令， 就和 之前说过的 docker run 跟的命令是一样的 &quot;但是 docker run 要是指定了一个命令，那么 这个CMD配置就会失效&quot; ENRTYPOINT: 和CMD类似， 不过 在docker run 指定新命令是， ENTRYPOINT的命令是不会被覆盖的。都会执行 ADD | COPY &quot;&quot;&quot;将宿主机文件 拷贝 到镜像的某个目录中&quot;&quot;&quot; COPY aaa.txt /root # 将aaa.txt 拷贝到 镜像的/root目录中 ADD aaa.txt /root # 和COPY一样，不过 ADD可以将压缩文件拷贝进去后，&quot;自动解压&quot; ENV &quot;&quot;&quot;就相当于编程语言的 变量赋值&quot;&quot;&quot; ENV name=python ENV nickname=$name # $name 意为取出 name变量的值 WORKDIR &quot;&quot;&quot;切换目录 类似cd命令&quot;&quot;&quot; WORKDIR /root VOLUME &quot;&quot;&quot;添加数据卷&quot;&quot;&quot; VOLUME /root # 就相当于前面说过的docker run -v /root, 即自动创建一个数据卷映射到 容器的/root EXPOSE &quot;&quot;&quot;暴露端口&quot;&quot;&quot; EXPOSE 6379 EXPOSE 3306 # 可以用多个 EXPOSE 暴露多个端口 注1： 暴露端口后，可以通过 前面说的 docker run -P 来做自动端口映射 注2： 或者不暴露端口，直接使用手动映射-p，都是可以的。 官方模板参考网址 官方文档：https://docs.docker.com/engine/reference/builder/ 各种开源Dockerfile模板：https://github.com/docker-library/docs/tree/master/ Docker Compose Docker-Compose认知 一、Dockerfile 可以看作是 Docker命令的组合 二、Docker-Compose 可以看作是 Dockerfile的组合（也称作 容器编排工具） 三、Docker-Compose 文件默认名为 docoker-compose.yaml 四、docoker-compose.yaml 文件指令中间都有空格 eg: version: 3.7（3.7之前是有空格的） 五、docoker-compose.yaml 采用缩进对格式语法进行区分 Docker-Compose安装 官方安装教程：https://docs.docker.com/compose/install/ 从上往下，命令复制-粘贴-运行。。。Easy略 Docker-Compose文件指令 version: &quot;3.7&quot; # 必有 # 此版本号与docker版本对应地址： https://docs.docker.com/compose/compose-file/ services: # services关键字，写上就行， 必有 mypython: # mypython是我随便起个名 build: . # Dockerfile的路径位置， build是构建Dockerfile文件的 ports: -&quot;6006:3003&quot; # 注意-后面是有空格的，markdown语法充冲突，我就没写空格 command: xxxx # 覆盖Dockerfile中的 CMD depends_on: # 依赖的服务， （被依赖的先执行，也就是myredis先执行） -myredis # -后有空格 myredis: # 同理 myredis 也是我随便起的名 image: redis # 指定一个成品镜像 类似DockerfilE的 FROM指令 container_name: myredis # 指定容器名 networks: # 使用下面创建的mynet网络 -mynet # (同-后有空格，避免markdown语法冲突) volumes: # 使用下面创建的myvolume数据卷,并映射到容器的/root目录 -myvolume:/root # -后有空格，（特别注意 :后面不允许有空格） hostname: myredis # 因为容器创建时IP可能动态改变，指定名称，则可通过名称来代替IP # 若不指定 hostname， 则默认为服务名， 即 myredis networks: # 创建网络 mynet: # 给网络起名为 mynet driver: &quot;bridge&quot; # 指定为桥接模式 volumes: # 创建数据卷 myvolume: # 给数据卷起名为 myvolume driver: &quot;local&quot; # 默认就是local，即数据卷存储在宿主机的目录下 预检查docker-compose.yml文件语法格式是否有误 docker-compose config 注：需要在 docker-compose.yml 所在目录下执行 启动/停止 docker comopse docker-compose up # 前台终端阻塞执行（就是执行之后，你不能在终端输入东西了） docker-compose up -d # 后台终端非阻塞执行 （作为服务一样后台执行） docker-compose stop # 停止编排（即停止 所有 编排运行的容器） END ","link":"https://cythonlin.github.io/post/py-greater-docker-yu-fa-quan-mian-hui-yi/"},{"title":"PY => Redis与Python操作Redis语法对比解析","content":"前言 R: 代表 redis-cli P: 代表 python的redis 准备 pip install redis pool = redis.ConnectionPool(host='39.107.86.223', port=6379, db=1) redis = redis.Redis(connection_pool=pool) redis.所有命令 下面命令所有命令我都省略了， 有和Python内置函数冲突的我会加上 redis. 全局命令 dbsize（返回key的数量） R: dbsize P: print(redis.dbsize()) exists（是否存在某key） R: exists name P: exists('name') keys（列出所有key,可使用通配符） R: keys na* P: keys('na*') 注：时间复杂度为 O(n) scan (对应keys，迭代取出所有keys) R: scan 0 match '*' count 4 P: keys_iter = redis.scan_iter(match='*', count=4) 注：这种scan，API后面也会有， 所以我全部放在最后的结束语中讲 info (查看资源信息) R: info # 也可以填参数 info memory info cpu 等 P: redis.info() # redis.info('CPU') redis.info('MEMORY') type (列出类型) R: type name P: redis.type('name') # type和python的冲突了，所以这里我写全了 redis中类型有： none string list set zset hash 过期时间 expire(设置) R: expire name 秒数 P: expire('name', 秒数) ttl(查询) R: ttl name P: ttl('name') # 返回剩余过期时间值 # 返回值为 -2 则代表 无此 key # 返回值为 -1 则代表 有此 key ， 但未设置过期时间 persist(删除) R: persist name P: persist('name') 自增，自减 incr incrby 加上一个整数 R: incr age 或 incrby age 1 P: incr age 1 或 incrby age 1 # python实现 的 incr 被 重定向为 incrby，所以用哪个都行 decr decrby 减去一个整数 同上 incrbyfloat 加减一个浮点数 同上 字符串相关操作 set 设置值 R: set name lin P: redis.set('name', 'lin') set选项（原子操作） nx（设置默认值） R: set name lin nx P: redis.set('name', 'lin', nx=True) 注： nx 代表key不存在才会将值设置成功， 类似python dict的 setdefault，即给key设置默认值 xx（更新值） R: set name Tom xx P: redis.set('name', 'lin', xx=True) 注： xx 代表key存在才会将值更新成功。 如果key不存在， 则更新失败。 get 获取值 R: get name P: redis.get('name') 注：通过py redis客户端的 get取出的都是 字节二进制类型， 所以需要手动转为对应类型 前面提到的 incr decr 等， 操作返回结果直接就是 int， 而非 字节类型 mset 批量设置 R: mset name lin age 18 p: redis.mset( {'name': 'lin', 'age': 18} ) mget 批量获取 R: mget name age p: redis.mget('name', 'age') # 返回值为 字节类型的 列表 getset 设置新值并返回旧值 R: getset name zhang P: print( redis.getset('name', 'zhang') ) append 字符串追加拼接 R: append name abc P: redis.append('name', 'abc') strlen 获取字符串长度 R: strlen name P: print( redis.strlen('name') ) 注： 与编程语言的普遍API不同的是， strlen返回的字符串 长度是 字符对应编码的长度。。。。 中文 utf-8 占 3个字节 getrange 字符串切片 （从0开始，前闭后闭） R: getrange name 1 2 P: redis.getrange('name', 1, 2) setrange 字符串按索引赋值（覆盖） R: setrange name 0 abc # 把第0个位置开始， 逐个覆盖赋值为 abc， 多余的不变 P: redis.setrange('name', 0, 'abc') del 删除键值 R: del k1 k2 P: redis.delete(k1, k2) Hash相关操作(可对应为 文档-属性-值) hset 设置 1条文档，1个属性-值 R: hset user name lin P: redis.hset('user', 'name', 'lin') hget 获取 1条文档，1个属性 R: hget user name P: print(redis.hget('user', 'name')) hmset 设置 1条文档， 多个属性-值 R: hmset user name lin age 18 P: redis.hmset('user', {'user': 'lin', 'age': 18}) hmget 获取 1条文档， 多个属性-值 R: hmget user name age P: print(redis.hmget('user', 'name', 'age')) hkeys 获取所有 key R: hkeys user P: print(redis.hkeys('user')) hvals 获取所有 values R: hvals user P: print(redis.hvals('user')) hgetall 获取 一条文档，所有属性值（慎用，见下一条API） R: hgetall user # 返回为列表， 偶数索引为key，奇数索引为vaLue(从0开始) P: print(redis.hgetall('user')) # 返回为 dict格式 注： hgetall 会将所有key-value取出来，所以数据量庞大可能会造成性能影响。 大批量数据在python是怎么处理来着？？？？？？？ 没错，就是迭代器，当然python的redis模块已为我们封装好一个API，hscan_iter, 见一条API hscan （hash迭代，大体上可代替 hgetall使用） R: hscan user 0 match * count 200 # 按游标，按数量取 # 0代表游标从头开始 # match是关键字 # * 是key的通配符 # count 是一次接待的条数 P: result_iter = redis.hscan_iter('user', match= 'na*', count=2) # python的 cursor参数没有，是因为源码中被固定设置为 0了， 其他参数解释同上 # 返回结果为可迭代对象，可遍历取出。 hexists 检测是否存在某key R: hexists user name1 # 存在返回 1，不存在返回 0 P: print(redis.hexists('user', 'name')) # 存在返回True hlen 统计获取一个文档，所有属性的 总数 R: hlen user P: print(redis.hlen('user')) hdel 删除指定字段 R: hdel key field P: redis.hdel('key', 'field') List相关操作 lpush (左压栈) R: lpush list1 1 2 3 P: redis.lpush('list1', 1,2,3) rpush (右压栈，同左压栈，略) lpop (左弹栈) R: lpop list2 P: print(redis.lpop('list2')) rpop (右弹栈，同左弹栈，略) blpop (左阻塞弹栈，列表为空时，就阻塞了) R: blpop list2 1000 # 1000为过期时间为1000秒，1000秒后自动解除阻塞，有值加入也会解除阻塞 P: redis.blpop('list2', timeout=1000) brpop (右阻塞弹栈，同左阻塞弹栈，略) linsert ( 在指定 值 的 前后 插入值) R: linsert list2 before Tom jerry # 在Tom前插入 jerry, before代表之前 P: redis.linsert('list2', 'after', 'b', 'Tom') # 在b的后面插入Tom, after代表之后 lset (按索引赋值, 注意索引不要越界) R：lset list2 4 zhang P: redis.lset('list2', 4, 'zhang') lindex (按索引取值, 索引可正可负) R: lindex list2 -3 P: print(redis.lindex('list2', 3)) llen （获取列表元素个数） R: llen list2 P: print(redis.llen('list2')) ltrim (注意：在原数据上切片，不返回值。) R: ltrim list2 3 10 # 保留 索引 3-10 的列表数据，其他都删除 P: print(redis.ltrim('list2', 2, -1)) # 索引前闭后闭，可正可负 lrem (删除指定值) R: lrem list2 0 Tom # 0 这个位置的参数代表删除值的个数 # 0 代表全部删除， 删除全部Tom值 # 正数代表 从左到右 删除n个。 eg: lrem list2 5 Tom 即为 从左到右 删除5个Tom值 # 负数代表 从右到左 删除n个。 eg: lrem list2 -5 Tom 即为 从右到左 删除5个Tom值 P: print(redis.lrem('list2', -5, 1)) # 解释同上 lrange(遍历，正负索引都可，前闭后闭) R: lrange list1 0 -1 P: print(redis.lrange('list2', 0, -1)) Set相关操作 sadd （插入元素） R: sadd set1 1 2 3 P: redis.sadd('set1', *[1,2,3]) srem （删除指定值的元素） R: srem set1 Tom P: redis.srem('set1', 'Tom') scard （获取集合中元素个数） R: scard set1 P: redis.scard('set1') sismember (判断某元素是否在集合) R: sismember set1 Tom P: redis.sismember('set1', 'Tom') srandmember (随机取出集合指定个数元素) “”“类似py的 random.choices，注意有s”“ R: srandmember set1 2 # 从集合随机中取出2个元素 P: redis.srandmember('set1', 2) smembers (取出集合中所有元素) R: smembers set1 P: redis.smembers('set1') 注： 同 hgetall， 如果一次性取出，可能会出问题，所以需要迭代获取，见下 sscan sscan (游标/迭代取出集合所有元素) R: sscan set1 0 match * count 200 P: result_iter = redis.sscan_iter('set1', match='*', count=200) # 遍历迭代 sdiff (差集) R: sdiff sset1 sset2 P: print(redis.sdiff('sset1', 'sset2')) sinter（交集） R: sinter sset1 sset2 P: print(redis.sinter('sset1', 'sset2')) sunion (并集) R: sunion sset1 sset2 P: print(redis.sunion('sset1', 'sset2')) zset 有序集合相关操作 zadd (有序插入) R: zadd zset 100 Tom 90 Jerry # 100是权重，Tom是数据值， 注意redis-cli 权重在前，值在后 P: redis.zadd('zset', {'Tom': 100, 'Jerry': 90}) # 注意，py语法，权重作为字典的value 注特别注意： zadd的默认机制是同值，不同权重时，会更新值的权重 eg: 上面再插入一条 Tom, 不过这次的权重是 50 （ zadd zset 50 Tom），则Tom的权重会更新为50 这时就延申出2个参数，（应该还记得前面讲的 set的 nx 和 xx参数吧，没错 zadd也有） nx: （不存在才更新（添加）， 存在则更新（添加） 失败） R: zadd zset nx 1000 Tom P: redis.zadd('zset',{'Tom': 1000}, nx=True) 注： 如果Tom这个值之前存在，则这个1000就不会被更新了 若不存在，则就会新创建，并把这个1000设置成功 nx：（存在才更新（添加）， 不存在则更新（添加） 失败） R: zadd zset xx 1000 Tom P：redis.zadd('zset',{'Tom': 1000}, xx=True) 注： 如果Tom这个值之前存在，则1000才会更新成功 如果不存在，比如 {'张三':500}, 张三本来就不存在，用了xx, 他不会被添加进来，更何谈更新 zrange (遍历) R: zrange zset 0 -1 P: print(redis.zrange('zset', 0, -1)) # 返回值为列表 withscores 参数（把权重也带出来返回）： R: zrange zset 0 -1 withscores # 注意， 返回时 奇数位 是值， 偶数位是权重 P: print(redis.zrange('zset', 0, -1, withscores=True)) # 返回列表嵌套元组，[(值，权重)] zrevrange (逆序-降序，遍历) 这条API就是多了 &quot;rev&quot; 三个字母, reversed单词 熟悉把， python内置逆序高阶函数。。就是那个意思 操作同zrange，略 zrangebyscore (根据权重来遍历) R: zrangebyscore zset 40 99 limit 1 3 # 查出权重在40-99之内的数据，并从第1条开始，返回3条 # 40-99都是闭区间， 要想变成开区间这样写即可 (40 (99 P: print(redis.zrangebyscore('zset', 40, 99, start=1, num=3)) zrevrangebyscore (根据权重来 逆序遍历) 操作同 zrangebyscore， 略 这API设计的，还不如，直接弄成一条命令，然后加一个逆序参数，吐槽！！！！ zrem (删除某值) R: zrem zset Tom # 删除Tom这个值 P: print(redis.zrem('zset','Tom')) zremrangebyscore (删除 权重 范围内的值) R: zremrangebyscore zset 70 90 # 把权重在70-90分的所有数据删除 P: redis.zremrangebyscore('zset', 70, 90) zremrangebyrank (删除 索引 范围内的值) R: zremrangebyrank zset 0 -1 # 删除所有值 （ 0到-1的索引就代表所有值啦！） P: redis.zremrangebyrank('zset', 0, -1) # redis的API风格真的。。。没办法python也无奈同名 zcard (获取有序集合的 所有 元素个数) R: zcard zset P: print(redis.zcard('zset')) zcount (统计有序集合的 某权重范围的 元素个数) R: zcount zset 10 69 # 同样默认闭区间， ( 可改为开区间 P: print(redis.zcount('zset',50, 69)) zrank (获取某元素的索引) R: zrank zset Jerry # 不用猜，索引肯定从0开始 P: print(redis.zrank('zset', 'Jerry')) zrevrank (逆序 获取某元素的索引) 逆序获取索引，比如最后一个，索引就是0 具体操作，同 zrank， 略 zscore (获取某元素对应的权重) R: zscore zset Jerry P: print(redis.zscore('zset', 'Jerry')) zscan (迭代方式和返回 所有元素及其权重) &quot;&quot;&quot; 嗯?似曾相识燕归来？ 前面说过的 scan hsacn sscan 还有接下来要说的 zscan 都是一个样子的，都是为了应对大数据来迭代处理 python版的redis给了我们一个简化函数,那就是 _iter结尾的， eg: hscan_iter() 这种 _iter结尾的函数，不用我们来传游标cursor参数， 为啥呢?? 一. 因为python有生成器-迭代器机制阿！（当然 _iter等函数的源码就是用yield为我们实现的） 二. cursor游标不易于管理 &quot;&quot;&quot; R: zscan zset 0 match * count 5 P: zset_iter = redis.zscan_iter('zset', match='*', count=5) # 同理返回可迭代对象 注：还要说明一下： match参数: 过滤查询数据（其实过滤完了，数据量小了也没必要用scan了，此参数主要用在&quot;hscan&quot;之类的） &quot;因此match参数可不写&quot;, &quot;match='*' 和 不传是一个效果的。&quot; count参数： Py源码解释 ``count`` allows for hint the minimum number of returns 意思就是： 这个参数是一次迭代&quot;最少&quot;取5个&quot;，但不管怎么说，最终还是会取出全部数据！！ zpopmax (弹出最大优先级数据对，redis5.+新增) R: zpopmax zset1 2 # 2代表弹出最大的2对key:score,不写，默认只弹一对key:score P: data = redis.zpopmax(zset1, count=None) # 原理同上 zpopmax可等价于下面两条命令的加起来的效果： data = redis.zrange(zset1, -1, -1) zrem(zset1, data) 注：无论count指定几个或不指定，py返回值为 [(key, score)] 列表嵌元组这种格式。 zpopmin (弹出最小优先级数据对，redis5.+新增) 用法同zpopmax zpopmax可等价于下面两条命令的加起来的效果： data = redis.zrange(zset1, 0, 0) # 就这里变了，默认升序，故最小值需要从第0条开始弹 zrem(zset1, data) 注： zpopmax 和 zpopmin 这两个方法是 redis5.+才有的。 前面也说了这种方法 = zrange + zrem 很明显，由原来的多行操作。变成了原子操作。 我想，redis新增这两条命令，应该正是解决资源竞争的这一问题！！！！！！ Redis两种持久化的方式 生成RDB文件 （三种方法） &quot;&quot;&quot;RDB机制就是 触发生成RDB文件，将Redis数据以二进制形式写入其中， 触发方式有如下三种&quot;&quot;&quot; RDB基本配置： vi /etc/redis/redis.conf dbfilename dump.rdb # 配置RDB文件名 dir /var/lib/redis # 配置RDB文件存放目录 （ll 命令查看 dump.rdb是否为最新时间） appendonly no # 若为yes, 会优先按照aof文件来恢复，或不恢复 上述配置，可在下面三种方法实现的时候，自动触发生成RDB文件。并在redis启动时恢复RDB文件 触发方式1：save （阻塞） R: save P: redis.save() 触发方式2：bgsave （开fork进程，异步,非阻塞） R: bgsave P: redis.bgsave() 触发方式3：自动动态生成RDB文件（配置文件） 在上面RDB基本配置基础上，追加如下配置 vi /etc/redis/redis.conf save 100 10 # 100秒钟改变10条数据就会，自动生成RDB文件 RDB缺点 大数据耗时，RDB文件写入影响IO性能。宕机数据不可控 生成AOF文件（三种方法） &quot;&quot;&quot;AOF机制就是 每执行一条命令，都会记录到缓冲区，在根据某种策略刷新到AOF文件中，策略有如下三种&quot;&quot;&quot; AOF基本配置： vi /etc/redis/redis.conf appendonly yes # 开关，先要打开 appendfilename &quot;appendonly.aof&quot; # AOF文件名 dir /var/lib/redis # AOF文件目录（和RDB是一样的） 刷新策略1：always always 即缓冲区有一条命令，就会刷新追加到AOF文件中 （安全可靠，耗IO） 刷新策略2：everysec （默认） everysec 即每过1秒 就会把缓冲区的命令 刷新追加到AOF文件中 如果就在这一秒钟宕机，那么数据就丢失了。。。（1秒不可控） 刷新策略3：no no 即 什么时候刷新，全听操作系统自己的 （完全不可控） AOF重写机制 （两种方法,异步） 重写清洁过程： 如上可知，越来越多的命令会追加到AOF中，其中可能会有一些类似 一、键值覆盖： set name tom set name jerry 二、超时时间过期 三、多条插入（可用一条命令代替） 如上无用命令，会让AOF文件变得繁杂。 可通过 AOF重写策略优化来达到化简，提高恢复速度等。 重写原理（查找资料 + 个人理解）： 一、 开fork子进程 新弄一份AOF文件，它的任务就是把当前redis中的数据重新按照上面的 ”重写清洁过程“ 捋一遍，并记录到这个新AOF文件中 二、 此时主进程可以正常接受用户的请求及修改，（这时可能子进程AOF，和数据库内容不一致,往下看） 三、 其实---第一条开fork的时候，顺便也开了一份内存空间A（名为重写缓冲区） 用来平行记录 用户新请求的命令 四、 当子进程AOF重写完事后， 会把上面 空间A中 中的数据命令追加到 AOF中（类似断点复制） 五、 新AOF替代 旧的AOF 打个比方（针对于 二、三、四）： 就是，你给我一个任务，我正做着，你又给我很多任务，我当然忙不过来 那这样，你先拿个清单记录下来，一会等我忙完了，咱们对接一下就好了） 重写方式1：bgrewriteaof R: bgrewriteaof P: redis.bgrewriteaof() 重写方式2：配置文件实现自动重写 在上面AOF基本配置的基础上，追加如下配置 vi /etc/redis/redis.conf appendfsync everysec # 就是上面说的三种策略，选一种 always no auto-aof-rewrite-min-size 64mb # 当AOF文件超过64mb就会自动重写 auto-aof-rewrite-percentage 100 # 100为增长率， 每一次的限制大小是之前的100%,也就是二倍 no-appendfsync-on-rewrite yes # yes 就是不把 “重写缓冲区” 的内容 刷新到 磁盘 注意这个参数: 这就是针对上面 ’重写原理‘ 中的第三条 中的 内存空间A（重写缓冲区） 如果这个 重写缓冲区 不刷新持久化到磁盘中， 要是宕机了，那么这个缓冲区的数据就会丢失。 丢失多少呢？ 据悉（linux中 最多最多 会丢失 30秒的数据） 如果你将其 设置为 no，那么 重写缓冲区 就会像 前面讲的 原始AOF一样地 刷新持久化到硬盘中。 但是你想想， 如果 重写缓冲区 和 原始AOF 都做持久化刷新 那么 它们就会 竞争 IO，性能必定大打折扣，特殊情况下，还可能 堵塞。 so, 要性能（设为yes）， 要数据完整安全(设为no)， 自己选.... 结束语 本文主要写了关于 redis 以及 python操作redis的语法对比详细解释！！ python的redis API 也是非常够意思了，函数名几乎完全还原 原生Redis！！ 语法部分印象比较深刻的就是 &quot;redis的 scan家族函数&quot; 以及 &quot;python的 scan_iter&quot;家族函数： 上面陆陆续续讲了那么多数据结构，都有它们各自的&quot;遍历所有数据的操作&quot; 但对于大量数据的情况下， 这些遍历函数就都变成渣渣了， 可能会造成&quot;OOM（内存溢出）等情况&quot; 这时 redis 机智的为我们 提供了一些列 &quot;scan家族函数&quot; , 当然这些函数是都需要游标控制的。 &quot;游标cursor&quot;是比较头疼的东西， 因此 python本着 人性化的思想： 将 &quot;scan家族函数&quot; 封装为 &quot;scan_iter家族函数&quot;， 让我们省去了游标的操作，可以愉快编程！ 那我就列出全部大家族 以及 对应 原始遍历函数： 原始遍历 redis python keys scan scan_iter hgetall hscan hscan_iter smembers sscan sscan_iter zrange zscan zscan_iter 沿着这个对应规律，之前我发现一件事情： 为什么 &quot;list 的 lrange 没有对应的 lscan？&quot; 我像zz一样还去ov查了一遍， 居然还看到一位外国朋友和我有一样的疑问。。。 解答者的一句话，我直接就清醒了， &quot;Instead, you should use LRANGE to iterate the list&quot; 由于顺着规律，思维定势，却忘记了 &quot;lrange本身就可以带索引来迭代 &quot; lrange list1 0 n 这时我突然又想起 zrange不也是和 lrange语法一样么？？？ 为何 zrange单独设立了一个 zscan， 而 list却没？？？ (查了一下好像是list底层性能之类的原因，我也没愿意继续看了。。。) scan 与 iter家族函数，各自的数据结构章节都有写， 并且在&quot;zset&quot;那节的 &quot;zscan&quot;那里做出了详细的分析 END ","link":"https://cythonlin.github.io/post/py-greater-redis-yu-python-cao-zuo-redis-yu-fa-dui-bi-jie-xi/"},{"title":"PY => MongoDB与PyMongo语法对比解析","content":"阅读须知 由于是对比书写: M: 代表 Mongo原生语法 P: 代表 PyMongo书写方法 后面提到：”同上“ 字眼： 意思就是 Mongo 和 PyMongo 语句是一模一样的， 一个字都不差，复制上去，可以直接运行 （也许你很好奇，为什么 一个是Python语言里的PyMongo，一个是Mongo） 他们的语句为什么可以做到一模一样 ？？ 答：因为 Mongo和Python都可以 给变量赋值， PyMongo的语法设计也是模仿Mongo的。 所以：我巧妙的 把二者的变量设为同一个，函数90%都一致， 所以整条语句就一模一样了！ 主要语法区别： 1. 函数命名 Mongo 方法函数大都以 驼峰命名 PyMongo方法函数大都以 _ 下划线分割命名 2. 函数参数 Mongo : 基本都是 {} + [] 各组组合格式 PyMongo：同上， 但{}的 key需要使用字符串格式， 有些情况，还需要使用命名参数代替 {} 3. 空值 与 Bool Mongo: null true false PyMongo: None True False 前置安装配置环境 客户端连接： pip install pymongo import pymongo M: Mongo P: cursor = pymongo.MongoClient('ip',port=27017) 选择数据库： M: use test P: db = cursor['test'] # 记住这个db， 下面复用这个参数 选择集合： (记住table变量名，下面就直接用他们了) 注意，注意，注意 M: table = db.zhang P: table = db['zhang'] 注：选择库，选择集合的时候 注意事项： Mongo中： xx.xx 用 . 的语法 PyMongo中：也可以 用 xx.xx 这样, 但是这样用在PyCharm中没有语法提示 所以提倡 xx['xx'] 用索引的方式使用 Mongo 与 PyMongo 返回结果的游标比较 Mongo中： 大多数查询等结果返回都是游标对象 如果不对游标遍历，那么Mongo的游标会默认为你取出 前 20 个 值 当然，你也可以索引取值 关闭操作： .close() PyMongo中： 同样，大多数查询等结果返回都是游标对象（如果你学过ORM，可以理解游标就像 ORM的查询集） 所以必须通过 list() 或 遍历 或 索引 等操作才能真正取出值 关闭操作： .close() 或者 用 Python 的 with 上下文协议 增 save() M: table.save({}) # 估计要废弃了 P: 将要被废弃 用insert_one代替它 insert() M: table.insert() # 包括上面两种，可以一个 {}，可以多个 [{},{}] P: PyMongo源码明确说明，insert()语法将被废弃，请用 insert_one({}) 和 insert_many([])代替 insert_one() 和 insert_many() M: table.insertOne( {} ) # 驼峰 table.insertMany([ {},{} ]) # 驼峰 P: table.insert_one( {} ) # 下划线 table.insert_many([ {},{} ]) # 下划线 删 remove() 参数1：删除查询条件 参数2：删除选项 M: table.remove({'name':'zhangsan'}, {'justOne': true}) # 我更喜欢用delete的 P: PyMongo中,此方法将被废弃。 将会被 delete_one() 和 delete_many() 代替 deleteOne() # 只删除一条 M: table.deleteOne({'name': 'lin3'}) P: table.delete_one({'name': 'lin3'}) # deleteMany() # 删除多条 M: table.deleteMany({'name': 'lin3'}) P: table.delete_many({'name': 'lin3'}) 注意： 不知道这两个函数是否让你想起了前面讲的 insertOne 和 insertMany，他们看起来很像，语法不同： insertMany([]) # 参数需要用 [] 包起来 deleteMany({}) # 参数不需要 注意2： table.deleteMany({}) # 空 {}， 代表删除所有文档 （慎行，慎行，慎行） 删除整个集合： table.drop() # 删除集合（连同 所有文档， 连同 索引，全部删除） 改 &quot;&quot;&quot; 文档修改, 注意： _id 不可修改 &quot;&quot;&quot; 三种更新方法： update(将要废弃，可跳过，直接看2，3点的方法) update({查询条件}, {更新操作符} , {更新选项}) M: table.update({'name': {'regex&#039;:&#039;li&#039;}},{&#039;set':{'name':'lin2'}}, {multi: true}) P: table.update({'name': {'regex&#039;: &#039;li&#039;}}, {&#039;set': {'name': 'lin3'}},multi=True) 注意1: 第三个参数 multi如果不设置，默认只更新一条文档，设置为 true ，就会更新多条文档 注意2： Mongo写法： {multi: true} # Mongo 和往常一样，采用json格式， true小写 Python写法： multi = True # python是采用命名参数来传递， True大写 updateOne(更新一条) M: updateOne( {查询条件}, {更新操作符} ) P: update_one updateMany(更新多条) M: updateMany( {查询条件}, {更新操作符} ) 其实参数是一模一样的，只不过方法名区分 P: update_many 注： 这三个方法的参数 是基本一模一样的 所以下面讲具体 {查询条件}, {更新操作符} 时 就统一用 update()来写了 普通更新操作符： $set(更新) 注：规则就是：&quot;有则改之， 无则添加&quot; M: table.update({'5':5},{'$set': {'lin': [5,6,7,8]} }) P: 同上 微扩展(关于内嵌数组)： table.update({'5':5},{'$set': {'lin.0': '呵呵' }) # lin.0代表数组的第一个元素 当数组的索引越界，这个时候就视为数组的添加操作。 eg: 假定我们给 lin.10 一个值，那么 中间空出的那么多索引，会自动填充 null $unset(删除) 注：删除的键对应的value可以随便写，写啥都会删除， 写 '' 只是为了语义明确（规范） M: table.update({'6':6}, {'$unset': {'6':''}}) # 把此条记录的 '6' 字段删除 P: 同上 微扩展(关于嵌套数组)： table.update({'5':5}, {'$unset': {'lin.0':''}}) # lin.0同样代表数组第一个元素 注：数组的删除 并不是真正的删除， 而是把值 用 null 替换 $rename(改名，替换) M: table.update({'name':'lin'}, {'rename&#039;:{&#039;name&#039;:&#039;nick&#039;}}) # name变成了nick P: 同上 微扩展（文档嵌套）： 如果文档是嵌套的 eg: { a: {b:c} } M: table.update({&#039;lin&#039;:&#039;lin&#039;}, {&#039;rename': {'a.b':'d'}}) P: 同上 结果 =&gt; {&quot;a&quot; : { }, &quot;d&quot; : &quot;c&quot; } 解析： b 属于 子文档 a.b 表示 通过父文档的a 来取出 子文档的b 如果整体a.b被 rename为 d，那么 d会被安排到父文档的层级里，而a设为空。 举个栗子： 你有一个箱子，里面 有一个 儿子级别 和 孙子级别 的箱子 （共3层） 现在你把 孙子级别的箱子 单独拿出来， 把整个箱子替换掉 就是这种思想。。。自己体会吧 （这种语法，好像Python列表的切片赋值。。形容可能不太恰当） $inc： {$inc: { 'age': -2}} # 减少两岁，正数表示加法，负数表示减法，简单，不举例了 特例：如果字段不存在，那么，此字段会被添加， 并且值就是你设定的值(0+n=n) $mul: {$mul: { 'age': 0.5}} # 年龄除以2，整数表示乘法，小数表示除法，简单，不举例了 特例：如果字段不存在，那么，此字段会被添加， 并且值为0 (0*n=0) $min {$min: { 'age': 30}} # 30比原有值小：就替换， 30比原有值大，则不做任何操作 $max {$max: { 'age': 30}} # 30比原有值大：就替换， 30比原有值小，则不做任何操作 特例：min和max特例相同，即如果字段不存在，那么，此字段会被添加， 并且值就是你设定的值 数组更新操作符： &quot;&quot;&quot; 单数组: xx 内嵌数组: xx.索引 &quot;&quot;&quot; $addToSet（有序，无重复，尾部添加） 原始数据： {'1':1} M: table.update({'1':1}, {'$addToSet':{'lin':[7,8]}}) P: 同上 结果 =&gt; {&quot;1&quot;: 1,&quot;lin&quot;: [ [7, 8 ] ]} # [7,8] 整体插入进来， 特别注意这是二级列表 $each ( 给[7,8]加个 $each，注意看结果变化 ) M: table.update({'1': 1}, {'addToSet&#039;: {&#039;lin&#039;: {&#039;each':[7, 8]} }}) P: 同上 结果 =&gt; {&quot;1&quot;: 1, &quot;lin&quot;: [7,8]} # 7,8单独插入进来，参考python的 * 解构 push(数据添加，比push(数据添加， 比push(数据添加，比addToSet强大，可任意位置,可重复) &quot;&quot;&quot; 补充说明: $addToSet:添加数据有重复，会自动去重 $push :添加数据有重复，不会去重，而是直接追加 &quot;&quot;&quot; 原始数据: {'1':1} M: table.update( { '1': 1 }, { 'push&#039;: { &#039;lin&#039;: { &#039;each': [ {'a': 5, 'b': 8 }, { 'a': 6, 'b': 7 }, {'a': 7, 'b': 6 } ], 'sort′:′a′:−1,′sort&#x27;: { &#x27;a&#x27;: -1 }, &#x27;sort′:′a′:−1,′position': 0, '$slice': 2 }}}) # 这里为了清晰点，我就把所有括号折叠起来了 P: 同上 结果 =&gt; {&quot;1&quot; : 1, &quot;lin&quot; : [ { &quot;a&quot; : 7, &quot;b&quot; : 6 }, { &quot;a&quot; : 6, &quot;b&quot; : 7 } ] } 终极解析： 1. 添加数组： 先走 $sort =&gt; 根据a 逆序排列 2. 再走 $position, 0表示：索引定位从0开始 3. 再走 $slice, 2表示： 取2个 4. 最后走 $each,把数组元素逐个放进另一个数组，说过的，相当于python的 * 解构操作， $pop（只能 删除 头或尾 元素） M: table.update({'a': a}, {'$pop': {'lin': 1}}) # 删除最后一个 P: 同上 注1：$pop参数， 1代表最后一个， -1代表第一个。 这个是值得注意一下的，容易记反 注2：如果全部删没了，那么会剩下空[]， 而不是彻底删除字段 $pull (删除 任何位置 的 指定的元素) M: table.update({'1': 1},{'$pull':{ 'lin':[7,8]}}) # 删除数组中[7,8]这个内嵌数组 P: 同上 $pullAll(基本和 $pull 一致) M: table.update({'1': 1},{'pullAll&#039;:{ &#039;lin&#039;:[ [7,8] ]}}) # 同pull，但多了个 [] P: 同上 注： $pull 和 $pullAll 针对于 内嵌文档 和 内嵌数组 有细小差别， 差别如下： 内嵌数组： $pull 和 $pullAll 都严格要求内嵌数组的 排列顺序，顺序不一致，则不返回 内嵌文档: $pullAll : 严格要求内嵌文档的顺序， 顺序不一致，则 不返回 $pull : 不要求内嵌文档的循序， 顺序不一致，一样可以返回 查 &quot;&quot;&quot; 第一个参数的条件是 筛选出 数据的记录（文档） 第二个参数的条件是 筛选出 数据的记录中的 属性（字段），不配置 就是 默认 取出所有字段 find({查询条件}, {投影设置}) &quot;&quot;&quot; 投影解释 哪个字段 设置为 0， 此字段就不会被投影， 而其他字段全部被投影 哪个字段 设置为 1， 此字段就会被单独投影， 其他字段不投影 {'name': 0, 'age': 0} # 除了 name 和 age ，其他字段 都 投影 {'name': 1, 'age': 1} # 只投影 name 和 age, 其他字段 不 投影，（_id除外） 注意:所有字段必须满足如下要求： 一： 你可以不设置，默认都会被投影 二： 如果你设置了，就必须同为0，或者同为1,不允许0，1 混合设置(_id除外) 三： _id虽然可以参与混合设置，但是它只可以设为0， 不可以设为1，因为1是它默认的 通俗理解（0和1的设定）：另一种理解思想 ====&gt; 设置为1： 就是 加入 白名单 机制 设置为0， 就是 加入 黑名单 机制 注： _id字段是 MongoDB的默认字段，它是会一直被投影的(默认白名单) 但是，当你强制指定 {'_id': 0} ，强制把 _id指定为0，他就不会被投影了（变为黑名单） 语法： M: queryset = table.find({}, {'name': 0}) P: 同上 投影-数组切片($slice) &quot;&quot;&quot;针对投影时的value为数组的情况下，对此数组切片，然后再投影&quot;&quot;&quot; 数据条件： {'arr1': [5,6,7,8,9] } 整形参数： M: queryset = table.find({},{'arr1':{'$slice': 2}}) # 2表示前2个, -2表示后两个 P: 同上，一模一样，一字不差 结果: { 'arr1': [5,6] } 数组参数： [skip, limit] M: queryset = table.find({},{'arr1':{'$slice': [2,3]}}) # 跳过前2个，取3个 P: 同上，一模一样，一字不差 输出结果 =&gt; { 'arr1': {7,8,9] } 注： 这种数组参数，你可以用 skip+limit 方式理解 也可以用, python的索引+切片方式理解 （skip开始查索引（0开始数）, 然后取limit个） 投影-数组过滤($elemMatch) &quot;&quot;&quot; 针对投影时 的value为数组的情况下，根据指定条件 对 数组 过滤，然后再投影 注意这个过滤机制： 从前向后找，遇到一个符合条件的就立刻投影(类似 python正则的 search) &quot;&quot;&quot; 数据条件: {'arr1': [6,7,8,9]} M: queryset = table.find({}, {'arr1': {'$elemMatch': {'$gt':5}} }) P: 同上 输出结果 =&gt; &quot;arr1&quot; : [ 6 ] 解析：（我自己总结的伪流程，可参考理解） 1. 准备投影 2. 发现数组，先处理数组，可看到数组中有 elemMatch条件 elemMatch在投影中定义为： ”你给我一个条件，我把符合条件的 数组每个元素从前向后筛选 遇到第一个符合条件的就返回, 剩下的都扔掉 （这里的返回你可以理解为 return） “ 3. 把 2 步骤 返回的数据 投影 limit() limit: （只取前n条） M: queryset = table.find({'name':'lin'}).limit(n) # n就是取的条数 P: 同上 skip() skip: （跳过n条，从第n+1条开始取） M: queryset = table.find({'name':'lin'}).skip(n) # 从0开始数 P: 同上 解释一下skip这个参数n： 假如n等于2 ，就是从第三个（真实个数）开始取 =&gt; 你可以借鉴数组索引的思想 a[2] count() count: （统计记录数） M: count_num = table.find({'name':'lin'}).skip(1).limit(1).count() P: count_num = table.count_documents(filter={'name':'lin'}, skip=1, limit=1) 分析： find() -&gt; 查出 3 条数据 skip(1) -&gt; 跳过一条，就是从第二条开始取 limit(1) -&gt; 接着上面的来，从第二条开始取（算本身哦），取一个，实际上取的就是第二条 count() -&gt; 3 # 也许你很惊讶，按常理来说，结果应该为 1(看下面) count(applySkipLimit=false) # 这是 API原型，这个参数默认为False applySkipLimit: 看名字你就知道这函数作用了吧 默认不写为 False: 不应用(忽略) skip(), limit() 来统计结果 ==&gt; 上例结果为 3 设为 True： 结合 skip(), limit() 来统计最终结果 ==&gt; 上例结果为 1 注： 对于 count() ，Mongo 和 PyMongo都有此方法，且用法是一模一样的。 那为什么上面PyMongo中我却用了 count_documents() 而不是 count() ????? 答： 因为 运行 或者后 戳进PyMongo源码可清晰看见，未来版本 count() API将要废除。 官方建议我们用 count_documents() 它的好处是把 skip() 和 limit() 由两个函数调用 变为 2个参数传进去了。 sort() sort: 排序 M: queryset = table.find({'name':'lin'}).sort({'_id': -1}) # 注意，参数是{} 对象 P: queryset = table.find({'name':'lin'}).sort( '_id', -1 ) # 注意，这是2个参数 第一个参数，代表 排序依据的字段属性 第二个参数，代表 升/降 1 : 升序 eg: 456 -1: 降序 eg: 654 特别注意： 3连招顺序（优先级要牢记） () sort -&gt; skip -&gt; limit （排序 - 定位 - 挑选） 无论你代码什么顺序，它都会这个顺序执行 eg: queryset = table.find({'name': 'lin'}).sort('_id', -1).skip(1).limit(1) 也许你会有这样一个疑惑： 为什么 count_documents 没有放进连招里面？ 答： 你仔细想想， 统计个数，和你排不排序有关系吗？ 没错，一点关系都没有。。。 sort() 和 count() 没有联系 数组操作符 已有数据条件： { name: ['张','李','王'] } $all: M: queryset = table.find({'name': {'$all': ['张','李']}}) # 数组值里必须包含 张和李 P：同上，一模一样，一字不差 $elemMatch： M: queryset = table.find({'name': {'$elemMatch': {'$eq':'张'} }}) # 数组值有张 就行 P: 同上，一模一样，一字不差 正则 M: db.xx.find( {name: { $regex: /^a/, $options:'i' }} ) P: queryset = db.xx.find({'name': {'$regex': 'LIN', '$options': 'i'}}) PyMongo版的或者这样写-&gt; import re e1 = re.compile(r'LIN', re.I) # 把Python的正则对象 代替 Mongo语句 queryset = db.xx.find({'name': {'$regex': re1 }}) 聚合 聚合表达式 字段路径表达式： $name # 具体字段 系统变量表达式： $$CURRENT # 表示管道中，当前操作的文档 反转义表达式： $literal: '$name' # 此处 $name 原语法被破坏，现在它只是单纯的字符串 聚合管道 &quot;&quot;&quot; 单个管道，就像 Python中的 map等高阶函数原理， 分而治之。 只不过，MongoDB善于将管道串联而已。 .aggregate([ 里面写管道各种操作 ]) &quot;&quot;&quot; $match（管道查询） M: queryset = table.aggregate([{'$match': {'name': 'zhangsan'}}]) P: 同上 $project(管道投影) 数据条件 =&gt; [ {&quot;id&quot;:'xxx', &quot;name&quot; : &quot;zhangsan&quot;, &quot;age&quot; : 15 }, {&quot;id&quot;:'xxx', &quot;name&quot; : &quot;lisi&quot;, &quot;age&quot; : 18 }, {&quot;id&quot;:'xxx', &quot;name&quot; : &quot;wangwu&quot;, &quot;age&quot; : 16 } ] M: queryset = table.aggregate([{'$project': {'_id': 0,'new':'5'}}]) P: 同上 结果 =&gt; [{'new': '5'}, {'new': '5'}, {'new': '5'}] 注：'new'是在投影的时候新加的，会被投影。但是加了此新值，除了_id，其他属性默认都不会被投影了 $skip (管道跳过，原理同前面讲过skip() 略) $limit（管道截取，原理同前面讲过的limit() ） M: queryset = table.aggregate([{'$skip': 1},{'$limit':1}]) P: 同上 解释： 一共三条文档， skip跳过了第一条，从第二条开始取，limit取一条，所以最终取的是第二条 $sort (管道排序，同上,不解释) M: queryset = table.aggregate([{'$sort':{'age':1}}]) P: 同上 $unwind（管道展开数组， 相当于 数学的 分配律） 数据条件 =&gt; {&quot;name&quot; : &quot;Tom&quot;, &quot;hobby&quot; : [ &quot;sing&quot;, &quot;dance&quot; ]} path小参数: M: table.aggregate([{'$unwind':{'path': '$hobby'}}]) # 注意 path是语法关键词 P: 同上 结果 =&gt; { &quot;_id&quot; : xx, &quot;name&quot; : &quot;Tom&quot;, &quot;hobby&quot; : &quot;sing&quot; } { &quot;_id&quot; : xx, &quot;name&quot; : &quot;Tom&quot;, &quot;hobby&quot; : &quot;dance&quot; } 形象例子： a * [b+c] =&gt; a*b + a*c includeArrayIndex小参数： M: queryset = table.aggregate([{'$unwind': { 'path':'$hobby', 'includeArrayIndex':'index' # 展开的同时会新增index字段记录原索引 }}]) P: 同上 结果 =&gt; {&quot;name&quot; : &quot;Tom&quot;, &quot;hobby&quot; : &quot;sing&quot;, &quot;index&quot; : NumberLong(0) } {&quot;name&quot; : &quot;Tom&quot;, &quot;hobby&quot; : &quot;dance&quot;, &quot;index&quot; : NumberLong(1) } 注意： $unwind 上面有两种特殊情况： 情况一： 文档中无 hobby字段 或 hobby字段为 空数组[] 那么该文档不参与unwind展开操作， 自然就不会显示结果。 若想让这种文档也参与 unwind展开操作,那么需要追加小参数 'preserveNullAndEmptyArrays':true # 与 path同级书写 最终结果，这种字段的文档也会被展示出来，并且 index会被赋予一个 null值 情况二： 文档中有 hobby字段，但是该字段的值并不是数组 那么该文档 会 参与 unwind展开操作，并且会显示出来， 同样 index 会被赋予一个 null值 $lookup（使用方式一） 使用方式（一）：集合关联 ===&gt; 我的理解是，相当于关系型数据库的 多表查询机制 集合 &lt;=&gt; 表 ， 多表查询 &lt;=&gt; 多集合查询 自身集合 与 外集合 根据我们指定的 关联字段 关联后， 如有关联， 则新字段的值为 [外集合的关联文档， 。。。]， 有几条文档关联，这个数组就会有几条 废话不多说，先重新创建两个集合： db.user.insertOne({'name':'猫', 'country': ['China','USA']}) # 一条 db.country.insertMany([{'name':'China'}, {'name':'USA'}]) # 两条 table = db.user # 看好，我赋值了一下，下面直接写table就行了 M: queryset = table.aggregate([{ '$lookup': { 'from': 'country', # 需要连接的另外一个集合的名称（外集合） 'localField': 'country', # （主集合）连接的 依据 字段 'foreignField': 'name', # （外集合）连接的 依据 字段 'as': 'new_field' # 最终关联后查询出来的数据，生成新字段，as用来起名 } }]) P: 同上 结果 =&gt; { &quot;_id&quot; : ObjectId(&quot;5d2a6f4dee909cc7dc316bf1&quot;), &quot;name&quot; : &quot;猫&quot;, &quot;country&quot; : [ &quot;China&quot;, &quot;USA&quot; ], # 这行之前应该不用解释，这就是 user集合本身的数据，没变 &quot;new_field&quot; : [ # 这行是新加的字段，后面解释 { &quot;_id&quot; : ObjectId(&quot;5d2a6fcbee909cc7dc316bf2&quot;), &quot;name&quot; : &quot;China&quot; }, { &quot;_id&quot; : ObjectId(&quot;5d2a6fcbee909cc7dc316bf3&quot;), &quot;name&quot; : &quot;USA&quot; } ] } 解释： 1. new_field是我们新添加的字段 2. 因为user集合和country集合 我们给出了2个依据关联字段 并且这两个关联字段 'China' 和 'USA' 的值都相等 所以最终 user集合的new_field字段中 会添加 两条 country集合的文档 到 [] 中 3. 如果无关联, 那么 new_field字段中的值 为 空[] $lookup（使用方式二）: 使用方式二：不做集合的关联，而是直接把（外集合）经过条件筛选，作为新字段放到（主集合）中。 M: queryset = table.aggregate([{ '$lookup': { 'from': 'country', # 外集合 'let': {'coun': '$country'}, # 使（主集合）的变量 可以放在（外集合）使用 'pipeline': [{ # 外集合的专属管道，里面只可以用外集合的属性 '$match': { # 因为设置了 let，所以这里面可以用主集合变量 '$expr': { # $expr使得$match里面可以使用 聚合操作 '$and': [ {'$eq': ['$name', 'China']}, # 注意，这是聚合的 $eq用法 {'$eq': ['$$coun',['China', 'USA']]} ] } } }], 'as': 'new_field' } }]) P: 同上 解释： 把（外集合） pipeline里面按各种条件 查到的文档， 作为（主集合）new_field 的值。 当然，如果不需要主集合中的属性，可以舍弃 let 字段 $group （分组--统计种类） 用法1（分组--统计字段种类） M: queryset = table.aggregate([{'$group': {'_id': '$name'}}]) # _id是固定写法 P: 同上 结果 =&gt; [{'_id': '老鼠'}, {'_id': '狗'}, {'_id': '猫'}] 用法2（分组--聚合） 数据条件： { &quot;name&quot; : &quot;猫&quot;, &quot;country&quot; : [ &quot;China&quot;, &quot;USA&quot; ], &quot;age&quot; : 18 } { &quot;name&quot; : &quot;狗&quot;, &quot;country&quot; : &quot;Japna&quot; } { &quot;name&quot; : &quot;老鼠&quot;, &quot;country&quot; : &quot;Korea&quot;, &quot;age&quot; : 12 } { &quot;name&quot; : &quot;猫&quot;, &quot;country&quot; : &quot;Japna&quot; } M: queryset = table.aggregate([{ '$group': { '_id': '$name', # 根据name字段分组 'type_count': {'$sum': 1}, # 统计每个分类的 个数 'ageCount': {'$sum': '$age'}, # 统计age字段的 数字和 'ageAvg': {'$avg': '$age'}, # 统计age字段的 平均值 'ageMin': {'$min': '$age'}, # 统计age字段的 最小值 'ageMax': {'$max': '$age'}, # 统计age字段的 最大值 } }]) p: 同上 结果: { &quot;_id&quot; : &quot;老鼠&quot;, &quot;type_count&quot; : 1, &quot;ageCount&quot; : 12, &quot;ageAvg&quot; : 12, &quot;ageMin&quot; : 12, &quot;ageMax&quot; : 12 } { &quot;_id&quot; : &quot;狗&quot;, &quot;type_count&quot; : 1, &quot;ageCount&quot; : 0, &quot;ageAvg&quot; : null, &quot;ageMin&quot; : null, &quot;ageMax&quot; : null } { &quot;_id&quot; : &quot;猫&quot;, &quot;type_count&quot; : 2, &quot;ageCount&quot; : 18, &quot;ageAvg&quot; : 18, &quot;ageMin&quot; : 18, &quot;ageMax&quot; : 18 } 注意： 若想直接对整个集合的 做统计，而不是分组再统计 把 _id改为 null即可 { _id: 'null' } # (或者随便写一个匹配不到的 字符串或数字都行，分不了组，就自动给你统计整个集合了) $out (聚合操作后，将结果写入新集合) &quot;&quot;&quot; 我的理解是重定向 操作， 或者理解为 视图 操作 写入的集合如果存在，那么会全部覆盖（但保留索引） 聚合过程遇到错误，那么会自动执行 ’回滚’操作 &quot;&quot;&quot; M: table.aggregate([ { '$group': {'_id': '$name'} }, { '$out': 'newCollection' } ]) P: 同上 最后验证： db.newCollection.find() ，你就会看到新集合 及其 里面的内容 聚合管道 ==&gt; 第二个参数 table.aggregate([之前说的都是这里面的参数], 下面说这个参数) allowDiskUse: true 每个聚合管道占用内存需 &lt; 16M， 过大就会出问题 allowDiskUse设置为true， 会将内存的 写入到临时文件中，减缓内存压力。 官方文档：write data to the _tmp subdirectory in the dbPath directory Default: /data/db on Linux and macOS, \\data\\db on Windows 它说： 默认在 dbPath配置变量下的 子目录_tmp下， dbPath默认为 : /data/db M: queryset = table.aggregate([{ '$group': {'_id': '$name'}}], {'allowDiskUse': true} ) P: queryset = table.aggregate([{ '$group': {'_id': '$name'}}], allowDiskUse=True, # 注意，这里语法稍有不一样 ) 索引 创建索引： 单键索引 M: table.createIndex({'name':1}) P: table.create_index([('name',-1)]) # -1代表逆序索引，注意是元组 联合索引 索引命中：最左匹配原则 eg 1,2，3 这三个创建联合索引， 可命中索引为：【1,12,123】 M: table.createIndex( {'name':1}, {}, {} ) # 多个{} P: table.create_index([ ('name',-1), (), () ]) # 多个元组 多键索引 多键是针对于数组来讲的，创建单键的字段 指定为 数组字段， 默认就会设置为多键索引 唯一索引 （unique） '''注意： 如果集合中，不同文档的字段有重复，创建唯一索引的时候会报错''' M: table.createIndex({'name':1}, {'unique':true}) P: table.create_index([('name', 1),('counrty',1)], unique=True) 稀疏索引 (sparse) eg: 一个集合中： 给 name创建 唯一索引 插入文档1: 有 name字段 插入文档2: 无 name字段 （MongoDB会在索引库中，把没有的字段的 索引设为 {字段:null} ） 再插入文档3, 无name字段 --&gt; 同样也会把索引库中 name设为 null 但是就在这个时候，刚要把索引库中的 name字段设为 null的时候。。。 唯一索引告诉你：” 我这里已经有了一个，{ name:null }，请你滚 ” 然后就无情的给你报错了（重复索引字段） 那咋整啊， 别急，稀疏索引就是给你办这事的 设置稀疏索引。 MongoDB就不会把 没有的字段 加入到索引库了 所以，索引库里面就不会自动添加 {字段: null} 重新再次插入文档3， 无name字段， 可成功插入，不存在null的重复问题了 M: table.createIndex({'name':1}, {'unique':true, 'sparse':true}) P: table.create_index([('name', 1),('counrty',1)], unique=True, sparse=True) 查询索引 M：queryset = table.getIndexes() P: queryset = table.list_indexes() 删除索引 方式1： M: table.dropIndex('索引名') # 索引名可通过 上面查询索引的指令查 P: table.drop_index('索引名') 方式2： M: table.dropIndexes() # 删除全部，_id除外， 想指定删除多个，可用列表列出 P: table.drop_indexes() 查看索引性能(是否有效) table.上面说过的任一函数().explain() # 链式调用 explain，表示列出此操作的性能 eg: M: queryset = table.explain().find({'name':'猫'}) P: 同上 结果中找到： queryPlanner -&gt; winningPlan -&gt; inputStage -&gt; stage # stage结果对应说明如下 COLLSCAN # 未优化，还是搜的整个集合 IXSCAN # 索引起到作用 索引对投影的优化： queryPlanner -&gt; winningPlan -&gt; stage # stage结果对应说明如下 FETCH # 索引 对投影 未优化 PROJECTION # 索引 对投影 起到优化作用 索引对排序的优化： 同上 stage 最好 不是 sort 按索引 正序（逆序） 取数据， 这样就有效避免了机械排序的过程 ","link":"https://cythonlin.github.io/post/py-greater-mongodb-yu-pymongo-yu-fa-dui-bi-jie-xi/"},{"title":"PY => Linux实用工具命令","content":"Ubuntu16升级到18.04 “”“ 有特殊新颖强迫症癖好可以升下，如果你觉得16.04好用，就根本没必要升了 ”“” 我当时租的 云服务器通常都是16.04，现在估计也是16.04较多 我是个Python3的玩家， 我只想用Python的最新版，或最新标准版3.6-3.7 道理很简单，我追求新颖，虽然新版不稳定，但是你可以尝鲜并领先于别人。。。 废话不多说： Ubuntu16.04: 默认装的是Python2.7 + Python3.4 或3.5 Ubuntu18.04: 默认装的是Python2.7 + Python3.6.+ 我喜欢升级系统版本来直接让对应软件升级： lsb_release -a # 查看系统版本 apt update # 一路选Y apt dist-upgrade apt autoremove # 一路选Y apt install update-manager-core # 比较重要 do-release-upgrade -d # 完事 注： 如果其中某个命令过程弹出一个黑窗口让你选择， 我记得有个 core-new-update 字眼的，选这个即可 # 选择 新 核心 vim多行注释（Python为例） Ubuntu是肯定可以用， CentOS我记得好像用不了 直接一套连招： 注释： 1. ctrl + v 2. 按 下 箭头方向键，选中多行 3. shift + i 4. 输入 # 5. 按两下 ESC键 取消注释： 1. ctrl + v 2. 按 下 箭头方向键，选中多行 3. 按 x 键 注： 此套连招，需要熟练度， 如果不行就多练练。 手不好使，就不要怪我了~~~~ vimrc简单配置： “”“ tab = 4缩进, 设置行号 基于文件名的简单语法高亮 不要想着删一行，直接打开vimrc 全部复制上去，保存退出就可以用了 ”“” vi ~/.vimrc syntax on set nu set expandtab set tabstop=4 set shiftwidth=4 set softtabstop=4 locate查询库 locate xxx # 虽然很简单的命令，但是如果，一直为下线，并且还想查找新东西，那么需要更新库 updatedb # 更新搜索库， 完事 如果 updatedb命令出错，那就安装一下 mlocate就好了： yum/apt-get install mlocate htop查看服务器负载 ubuntu/centos中有 top ： 此命令可以详细查看服务器各种负载，资源状况，但是看着不方便，非常费劲 htop ： top的升级版， 容易看出资源消耗情况 如果未安装，可通过 apt-get/yum install htop 安装 htop -d 0.1 # 0.1秒动态更新一次资源数据。 看起来有一点炫。 pkill ps -el 查看 PId + kill -9 PID # 这套连招可强制杀死一个进程 但假如你有一连串的python进程 想要 全部KILL pkill python # 完事，和python有关的全KILL掉了，也可以通过通配符* 的方式来简写 patree列出进程 tree: 此命令用来列出目录层级结构 pstree: 用来列出 正在运行的 程序（所有进程的层级结构，进程名） pstree -p: 以层级的方式，不仅列出进程名， 还列出所有进程的(PID) alias改名 vi /etc/profile alias python='python3' # 这里是举个例子，配完，python命令就等价于 python3命令 alias pip='pip3' source /etc/profile # 不要忘记，这条命令 更新一下配置文件 重定向管道 ( &gt; 与 tee ) date &gt; date.txt # 截流， 屏幕上不显示 date | tee date.txt # 不截流， 屏幕上显示， 并且还能输入到文件 ! 叹号命令 ”“ !命令前缀 : 运行最后一条以这个前缀开头的命令 ”“ 假如下面是你最后按顺序再command终端 输入的四条命令： python aaa.py python bbb.py npm run dev pip install ccc 需求：你想迅速运行 python bbb.py这条命令 你只需 !py # 即可， 愣着干啥呢， 已经完事了，这条命令就等于你上面那一大串 解惑： 1. !py -&gt; py开头的有两条，为啥不执行第一条 python aaa.py 因为 ! 叹号 的语法意思就是选择 最后出现以py开头的执行。 2. 为啥直接 !p 不行呢？ 因为以p开头的最后一条命令是 pip install ccc。 这也不是你想要的啊。 ctrl+z 和 &amp; 和 nohup 和 screen ctrl+c： 强制终止（最常用的，先提出来。。。。 应该都知道的。。） ctrl+z： 可以把正在运行中的程序挂起到后台（注意这时候程序就暂停了） jobs: 粗略列出 后台挂起的程序 jobs -l: 详细列出 后台挂起的程序 （其实就是多列出了个PID） kill -9 PID # 顺水推舟地使用此命令来通过 PID 干掉后台挂起的程序 fg: 把挂起的后台的程序 拿回来继续执行 fg 程序编号: 如果有多个后台挂起的程序，选择一个继续执行，编号可通过 jobs 查看。 使用场景： 加入正在 vim 编辑东西，想跳出编辑器，写点别的，那么可以 进行下面操作： 不要保存， 直接 ctrl+z 然后去 做别的事 做完了想回到 vim继续编辑剩下的内容, 直接 fg &amp;： 运行 的 时刻 就直接 放到后台 (不实用) eg: (通常是费时的操作，或者长时间占用 command命令行） sleep 100 &amp; # 这样费时100秒的操作，直接让它滚去后台执行，别占我地方 局限： 假如你用 python xxxx.py &amp; 运行一个web服务器，虽然它会滚去后台一直运行。 前台command界面看起来很干净了。 但是，如果有用户请求过来， 你服务器收到打印在终端的信息 可不会乖乖打印在后台。 打印信息会钻出来到前台command界面来恶心你。。。。。 为了解决这一问题， 请移步 下面 ... nohup: (通常和 上面的 &amp; 联合使用) （实用性一般） eg: 同样运行一个服务器 nohup python xxx.py &amp; # 它会乖乖的滚去后台执行，并且将要打印的消息 也不会打印到前台，也不会打印到后台。 那打印的内容去哪了呢？？ nohup会自动给你 把打印的东西重定向 到一个 nohup.out文件，通常在当前目录下 # 这时你的前台command 就不会受到 骚扰了。。。。 也许你以为它很好用了，可以完美的决绝运行一个费时的进程。但是 它依然是个 loser。。. 局限： 当我一直用 nohup.out 部署一些服务程序的时候，一部署就是很多天。 当时学校断电（我租的阿里云ECS服务器）. 虽然服务器可以不间断的执行。 但是 我需要 用 xshell 一直去远程 查看我的 程序运行的怎么样了啊， 是否崩了？ 可惜的是，每次xshell关闭了，你再上去， 想找 通过 nohup &amp; 连招 放到后台程序的时候， 你会痛苦无比。。。非常惨 后来。。。。。。。。。 screen: （会话机制） 没错，后来我就发现了 这个 screen工具 如果你那里没有，就安装一下喽： apt-get / yum install screen 然后需要配置一下： vi ~/.screenrc # 创建文件 shell -$SHELL # 输入此行内容，保存退出 至于为何配置这个： 说实话我有点忘了 貌似不同screen会话对于不同的shell 权限有限制，导致很多东西用不了，so do it 直接说一套我常用且超级实用连招把： eg: screen -S 会话名 # 创建 并 进入这个会话（就像进入另一个世界，但资源共享） python xxx.py # 举例子，这是我的爬虫程序，需要运行 三天 ctrl + a + d # 跳出会话，回到正常命令行，但是这个会话以及你的程序依然在运行， ....... 这段时间（除了关闭服务器）你做什么都行，关闭 shell也可以的。 screen -r 上面的会话名 # 恢复到会话中， 你会发现，爬虫程序还在那里好好的运行呢。 如果你忘记会话名了，不要紧： screen -ls # 这条命令会列出所有 你创建过的会话，你也可以看到会话名 总结： 所以，我不知道阁下能不能感受到 screen的魅力。 我们可以 通过 screen -r 这一条命令来 迅速回到程序运行的环境。 nohup &amp;： 你用这个执行的程序，关了 xshell, 你再连，你就不好找这个运行的程序了 screen 有一套完好的系统（就像有开关一样）来控制会话 你进入screen会话中写程序，screen 自然就能有条不紊的管理好 你程序的运行状态 举个白话例子： 你正遛二哈呢，突然有急事： nohup &amp; : 你说：‘二哈，等我回来，自己玩把’ 你回来时， 二哈跑丢了。 （不受控制，程序找不到了，虽然可以花力气找到） screen : 你说：‘二哈，等我回来，自己玩吧，但我要把你拴起来’ 你回来时， 二哈还在那里乖乖的玩。（程序有效的受到控制，你可以轻松定位） sudo su等操作 不好意思，用了好几年linux： 什么 su- su 之类的， 我至今都没搞太明白。。。。。。。。。。 但是，我只用下面这一条命令，这些年就没遇到过问题： sudo -s # 变为root, 如果你之前输入过密码， 这条命令还会免去让你输入密码的环节 环境变量 把某路径加入到环境变量中： vi /etc/profile export PATH=$PATH:/XXX/XX 做一些配置： vi /etc/profile vi ~/.bashrc 等修改完后 source /etc/profile # 用一个 source 命令执行一下使配置更新生效 注： Linux-shell我研究的不深,所以这里可能有些说的不是特别明朗。 其实 这些配置文件 以及 source . sh bash 这些命令 和 用户 以及权限等都是有很大关系的。 有兴趣可自行深入了解 激活Ubuntu的root用户 也许你ubuntu只能用root权限 但你不能使用root用户登录，或不能使用root用户进行ssh连接 ubuntu其实是存在 root 用户的, 不用创建，给个密码就能直接使用 sudo passwd root root用户远程SSH连接 假如你的 xshell 连接不上远程服务器， 可能未开启 或 未安装 sshd服务 如下命令即可： sudo apt-get install ssh systemctl start ssh 如果你想要用root用户直接ssh连接, 可作如下配置： sudo vim /etc/ssh/sshd_config PermitRootLogin yes systemctl restart ssh MySQL5.7+/MariaDB修改密码小坑 当阁下还在停留在Mysql5.7版本之前，修改密码可以用下面这两种方式： mysqladmin -uroot password -p 或内部设置： update user set password=password('root') where user='root' 但是 MySQL5.7 之后 或者 MariaDB ,上面这俩方法 都不能 修改密码了。 正确修改方式： 先进入MySQL交互式： MySQL -uroot -p update mysql.user set authentication_string=password(&quot;密码&quot;) where user=&quot;用户名&quot; update user set plugin=&quot;mysql_native_password&quot;; flush privileges 最后重启服务， 完事 另外，值得提醒的一点容易出现的问题： 无论你连接的是什么数据库，如果你连不上 首先你应该想到的是，数据库配置文件： bind 0.0.0.0 # 每种数据库几乎都需要先改成这样，才能被外界访问 其次，你用的是云服务器的话 看看是否放通了 对应数据库的宽口 最后，是否开了防火墙： 没记错的话，Centos 和 Ubuntu 防火墙不是同一种，真正需要关闭可自行百度 其他命令 &quot;&quot;&quot; 一时间也记不起来还用过哪些了 &quot;&quot;&quot; wc : 统计字,词，行（自己选参数） curl : 可以请求 url，并返回数据（相当于一个小小爬虫），也可以向某个接口发送请求 ssh-keygen ：生成公私密钥 cd - : 跳回到上一次的路径 lsb_release -a : 查看Linux发行版信息 grep : 过滤字符 tar : 压缩/解压 netstat : 查端口 等 ","link":"https://cythonlin.github.io/post/py-greater-linux-shi-yong-gong-ju-ming-ling/"},{"title":"PY => Python正则全解详解","content":"预编译 import re re1 = re.compile(r'元字符 组成的正则规则') # 元字符下面会说 re1.方法() # 方法下边也会说 元字符： 表示普通字符： . # 除了\\n外 都可以匹配的到 \\d # 只匹配 纯数字 0-9 \\D # 和 \\d相反， 除了数字全都匹配 \\s # 只匹配空格 \\S # 和 \\s相反，除了空格，全都匹配 # 我喜欢用 [\\s\\S]*? 匹配所有 \\w # 只匹配 纯数字 或 大小写字母 或 下划线 \\W # 与 \\w 恰好相反， 除了 纯数字、大小写字母、下划线 全都匹配 [] # [abcde] 只要包含这个列表的字符，都可以匹配的到。但默认只取一个， 简写 [a-e] eg: re.compile(r'[e-h]').match('hello python ').group(0) &gt;&gt;&gt; h 此外: [^abcde] 或 [^a-e] 表示 '排除'，意思就是 除了abcde全匹配 匹配表示边界的： ^ # 匹配 起始 位置，受 re.M 影响 #注意：不要和 [^123] 除123之外搞混 eg: import re r1 = re.compile(r'^\\d+') print(r1.search('456hello123').group()) &gt;&gt;&gt; 456 $ # 匹配 结尾 位置，受 re.M 影响 eg: import re s = &quot;&quot;&quot; 123abc456 678abc789 &quot;&quot;&quot; r1 = re.compile(r'\\d+$',re.M) # 注意这里加入了re.M print(r1.findall(s)) &gt;&gt;&gt; ['456', '789'] # 这是写了re.M，就意味着 每一行都给你单独按照规则处理 &gt;&gt;&gt; ['789'] # 如果没写re.M, 那么就按照整体，去最后一行的尾部 注： 其实re.M的本质是 是根据\\n，进行 断行，断行后对每一行按照规则单独处理 \\b： # 匹配 单词的 边界（除了 数字、中英字母、下划线 的 所有符号） eg: import re s = '你好啊----好个P' r1 = re.compile(r'\\b好') print(r1.findall(s)) &gt;&gt;&gt; 好 # 解释：这个‘好’是，后面 的那个。因为后面的 ’好‘ 字 左边是符号，而非单词字符 \\B： # 匹配 单词 非 边界（包括 数字、中英字母、下划线） eg: import re s = '你好啊----好个P' r1 = re.compile(r'\\b好') print(r1.findall(s)) &gt;&gt;&gt; 好 # 解释：这个‘好’是，前面 的那个。因为前面的 ’好‘ 字 左边是中文字符。属于非边界 # 所以就匹配上了 再次总结： \\b 与 \\B： \\b： 匹配边界字符。边界字符：（除了 数字、字母、汉字、下划线的所有符号） \\B： 匹配非边界字符。非边界字符：（数字、字母、汉字、下划线） 匹配表示数量的： * ： 0次 或 多次 eg： 你* + ： 1次 或 多次 eg: 你+ ? ： 0次 或 一次 eg: 你? {m} : 出现m次 eg: 你{3} {m,} : 至少 出现m次 eg: 你{3,} # 涉及到贪婪模式，不深的不要用 {m,n}: m次 到 n次 之间任意一次就行 eg: 你{3,6} 表示分组： | ： 相当于或运算符， 两边写的是 正则表达式， 优先选择左边的 () : 括起来里面的内容，就变成了分组。 可以用 .group(1)提取，如果有更多那就 group(2).. (?P&lt;name&gt;) ： 在上面分组的基础上 起别名 (?P=name) : 根据分组的别名来使用分组 eg: s = '&lt;h1&gt;你好&lt;/h1&gt;' r1 = re.compile(r'&lt;(?P&lt;name1&gt;\\w+)&gt;(\\w+)&lt;/(?P=name1)&gt;').match(s).group(2) print(r1) &gt;&gt;&gt; 你好 \\数字 ：提取的分组可以在 同一个正则中 复用 eg: s = '&lt;h1&gt;你好&lt;/h1&gt;' r1 = re.compile(r'&lt;(\\w+)&gt;(\\w+)&lt;/\\1&gt;') # \\1 代表复用第一个分组 print(r1.match(s).group(2)) # 2代表提取第二个分组 &gt;&gt;&gt; 你好 匹配模式 re.M # 多行匹配， 影响 ^ 和 $，上面讲 ^ 与 $已经详解了。 re.I # 忽略大小写 eg: s = 'aAbB' r1 = re.compile(r'aabb', re.I).match(s).group() print(r1) &gt;&gt;&gt; aAbB re.S # 提升 . 的权限， 让 . 可以 匹配到换行符 s = &quot;&quot;&quot; hello python &quot;&quot;&quot; r1 = re.compile(r'.*', re.S).match(s).group() # 注意这里 re.S print(r1) &gt;&gt;&gt; hello python 注意：如果不写 re.S 那么 .* 只能匹配到第一行的空字符串，因为遇到第一个空行的\\n就停止了 re.X # 可以给正则分行写，并可以加注释， eg: import re title = '1好2你3' r1 = re.compile(r&quot;&quot;&quot; 1 # 注释1 看这两行 好 # 注释2 看这两行，1 和 好 没有加逗号。但是他们属于整体的规则，你可以加注释 &quot;&quot;&quot;, re.X) # 把正则可以分行写, 用了re.X后，分行的正则会被看作为一行 result = r1.match(title).group() print(result) # 输出结果： 1好 贪婪模式 与 非贪婪模式 个人理解： 贪婪模式：(Python默认使用的就是 贪婪模式) 你想匹配 一个句子中的 一个单词， 但是你写的规则恰好可以 满足 匹配所有单词。 那么它就会 贪婪的 把所有单词 全部 都给你匹配出来。 (贪) 使用方法： * 或 + 非贪婪模式： 即使你把规则写的很好，并且能把所有字符串都匹配到， 但是如果你加上了 非贪婪模式。 在满足规则条件的前提下，只匹配一个. 使用方法： *? 或 +? eg1：基于search的贪婪模式（match同此） 我们先回忆一下：search()方法的 最核心思想就是：从前往后搜，搜到一个满足的就直接返回。 OK，继续。 贪婪：（默认）： import re r1 = re.compile(r'\\d+') print(r1.search('你好333你好333你好').group()) &gt;&gt;&gt; 333 # 满足规则后 尽可能贪， 所以第一串连着的 '333' 搜到了就直接返回了 非贪婪（就多了个问号 ? ）： import re r1 = re.compile(r'\\d+?') print(r1.search('你好333你好333你好').group()) &gt;&gt;&gt; 3 # 嗯，你的规则就是 至少一个数字，搜到了一个就可以返回了，干得漂亮。 eg2: 基于findall的贪婪模式（如果你findall与规则，理解的不透彻，这个会有点绕的，前方高能） 先回忆一下：findall()方法的 最核心思想就是：拿着 定死的 规则，把所有满足规则的都提出来 OK，继续。 贪婪（默认）： import re r1 = re.compile(r'\\d+') print(r1.findall('你好333你好333你好')) &gt;&gt;&gt; ['333', '333'] 解释： 规则是匹配至少一位数字。 但是 贪婪模式 提醒了 规则：“你的任务是给我尽可能的 多匹配数字” findall 拿着 被贪婪化的 规则 去匹配原始字符串 被贪婪模式 提醒过的规则果然不负众望， 一次提一串连着的 ‘333‘ findall 拿着它 提取了 两次 ,就把所有数字提取出来了 结果就是 ['333', '333'] 非贪婪： import re r1 = re.compile(r'\\d+?') print(r1.findall('你好333你好333你好')) &gt;&gt;&gt; ['3', '3', '3', '3', '3', '3'] 解释： 规则 同样是 匹配至少一位数字。 但是 非 贪婪模式 提醒了 规则：“你的任务是给我尽可能的 少 匹配数字” findall 拿着 被贪婪化的 规则 去匹配原始字符串 被贪婪模式 提醒过的规则果然不负众望， 一次只提取一个 ‘3‘ findall 拿着它 提取了 六次 ,才把所有数字提取出来了 结果就是 ['3', '3', '3', '3', '3', '3'] 匹配方法 match(): ''' match()方法是 根据规则从第一个开始，向后逐个匹配，如果有一个字符匹配不上，就返回None ''' s = 'hello python' re1 = re.compile(r'he') re1.match('') result = re1.match(s).group() if re1.match(s) else None # 注意：非None才有group方法 print(result) # 通过 group()方法获得的才是最终 正则匹配的字符串 &gt;&gt;&gt; he 简单分组提取： s = 'hello python' re1 = re.compile(r'h(e)llo') # 给e加个一个(),就代表添加了分组，一会要把他提出来 result = re1.match(s).group(1) if re1.match(s) else None # 注意上方的 group(1) 这个参数是1，代表 只 提取 分组 里面的内容 &gt;&gt;&gt; e # 如果是 group() 或 group(0) 代表提取 整个正则规则 的内容 &gt;&gt;&gt; hello print(result) &gt;&gt;&gt; e 嵌套-平行（深度-广度）分组提取： 原理：分组提取先提取嵌套的，后提取平行的 (专业点就是先深度，后广度) eg： a = '123-%%%-dd' result = re.compile(r'123(-(%%%)-)d(d)').match(a).groups() # 或者用 group(1), group(2), group(3) 代替groups() 单个看也行 print(result) &gt;&gt;&gt; ('-%%%-', '%%%', 'd') search(): &quot;&quot;&quot; search() 方法是： 从前向后按规则‘搜索’, 直到搜到位置，搜不到就返回None &quot;&quot;&quot; s = &quot;aaa123aaa&quot; r1 = re.compile(r'\\d+').search(s).group() print(r1) &gt;&gt;&gt; 123 findall(): &quot;&quot;&quot; findall() 方法是： 按照正则规则，搜索所有符合规则的字符串，以列表的形式作为结果返回 &quot;&quot;&quot; s = &quot;aaa---123---bbb&quot; r1 = re.compile(r'\\w+').findall(s) print(r1) &gt;&gt;&gt; ['aaa', '123', 'bbb'] 微不足道的扩展： a = '123-%%%-dd' result = re.compile(r'-(.*?)-').findall(a) print(result) &gt;&gt;&gt; %%% # 解释： findall() 方法中 如果规则中含有分组，那么就会只返回分组中提取的的内容 finditer(): &quot;&quot;&quot; finditer() 和 findall() 使用方式一样，只不过返回结果是 可迭代对象，easy,此处不在多说 &quot;&quot;&quot; split(): &quot;&quot;&quot; split()方法是：按照规则去切割，切割结果以列表的方式返回 &quot;&quot;&quot; 语法关联： 我们知道字符串 有 split() 方法，可以按照一个参数损耗来切割，但是这个参数只能指定一个 如果让你在多种规则的前提下切割，需要怎么办。 巧了，正则切割split() 方法就是解决这个问题的， 实例如下： s = &quot;aaa%%123@@bbb&quot; # 可以看见，%和@符号把字符分开了，现在我们只想要字符 r1 = re.compile(r'\\W+').split(s) # \\W 大写： 以非单词性字符作为损耗规则，来切割 print(r1) &gt;&gt;&gt; ['aaa', '123', 'bbb'] sub(): &quot;&quot;&quot; sub()方法是： 按照规则匹配选出代替换的字符，然后自己 给定字符去替换 &quot;&quot;&quot; 场景1：常用方式，自己给定目标字符串，按规则匹配并直接替换原始字符串 eg: s = &quot;aaa%%123@@bbb&quot; r1 = re.compile(r'\\W+').sub('你好',s) print(r1) &gt;&gt;&gt; aaa你好123你好bbb 场景2：正则匹配后的结果 经过函数操作，函数的返回值作为 替换的最终结果 eg: s = &quot;aaa%%123@@bbb&quot; r1 = re.compile(r'\\W+').sub(lambda a:a.group()*2, s) print(r1) &gt;&gt;&gt; aaa%%%%123@@@@bbb 解释： 按照规则匹配到的字符是 %%和@@，经过函数 乘以2后， 就替换成了 %%%%和@@@@ subn(): &quot;&quot;&quot; subn() 和 sub()语法几乎一样，唯一的扩展功能就是 返回结果是元组，(字符串, 次数) &quot;&quot;&quot; s = &quot;aaa%%123@@bbb&quot; r1 = re.compile(r'\\W+').subn('你好',s) print(r1) &gt;&gt;&gt; ('aaa你好123你好bbb', 2) ","link":"https://cythonlin.github.io/post/py-greater-python-zheng-ze-quan-jie-xiang-jie/"},{"title":"PY => Python三程","content":"GIL的理解 GIL这个话题至今也是个争议较多的，对于不用应用场景对线程的需求也就不同，说下我听过的优点： 1. 我没有用过其他语言的多线程，所以无法比较什么，但是对于I/O而言，Python的线程还是比较高效的。 2. 有些第三方基于Python的框架和库，比如Tensorflow等基于C/C plus plus重写的Python线程机制。 3. 至于换成Cython编译器解决GIL，这个只是听过，没用过。 4. Python多线程对于web、爬虫方面也可以表现出较好的性能。 5. Python多进程是完好的，可以把资源消耗较少的非必要线程工作转为多进程来工作。 6. 计算密集型就别想多线程了，一律多进程。 7. Python还有细粒度且高效的协程。 8. 如果有N核CPU，那么同时并行的进程数就是N，每个进程里面只有一个线程能抢到工作权限。 所以同一时刻最大的并行线程数=进程数=CPU的核数（这条我的个人理解很模糊，参考吧） 多线程 多线程有2种通过start的那种方式，非常普遍，此处就不写了。 新版线程池 future库 是python3.2新出的功能（记住这个future） 方式1：（sublime运行后直接贴上来了） from time import sleep from concurrent.futures import ThreadPoolExecutor,as_completed,ALL_COMPLETED,wait executor = ThreadPoolExecutor(max_workers=10) # 初始化线程池10个坑 def f(): sleep(15) return 100 all_tasks = [executor.submit(f) for _ in range(10)] # 提交10个线程，全放池里执行 # for per_thread in as_completed(all_tasks): # print(per_thread.result()) ---------# 注意上面，as_completed(all_tasks) 是等待future对象完成后才执行主线程 ---------# 注意下面，wait和 as_completed() 的 作用一样，就和普通版的join() 相似 for per_thread in all_tasks: print(per_thread.result()) wait(all_tasks, return_when=ALL_COMPLETED) # 还可以选FIRST_COMPLETED，待第一个完成后 print('主线程') 方式2： map多线程版 value_list = executor.map(func, list(range(10))) # 返回的直接是map后的序列 for value in value_list: print(value) 注意： 这个map方式，如果要传多个参数就涉及到高阶函数那节讲的偏函数了。 多进程 多进程有2种通过start的那种方式+普通进程池，同样非常普遍，此处就不写了，自己百度一下。 新版进程池 同样是和上面用一样的future库，惊不惊喜。（可以看出好的程序要向着统一封装的方向优化） 也许你会惊讶，因为只把thread单词改为processing就是进程池版本了，就是这么简单！！！！！ from time import sleep import multiprocessing from concurrent.futures import ProcessPoolExecutor,as_completed,ALL_COMPLETED,wait executor = ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) def f(): sleep(15) return 100 if __name__ == '__main__': # 这句要加 all_tasks = [executor.submit(f) for _ in range(multiprocessing.cpu_count())] for per_thread in as_completed(all_tasks): print(per_thread.result()) # for per_thread in all_tasks: # print(per_thread.result()) # wait(all_tasks, return_when=ALL_COMPLETED) print('主进程') # 这就是 futures 模块 设计思想的魅力 多协程 前言： 也许你记得，函数用到 yield 来代替 return 就变成了 生成器。其特点是代码片段断点式执行。 如果有多个yield， 就可以自己用程序来切换执行程序。（这就是协程的特点） 推荐：（学习中。。。） 此笔者写的很好： https://juejin.im/post/5ccf0d18e51d453b557dc340 ","link":"https://cythonlin.github.io/post/py-greater-python-san-cheng/"}]}