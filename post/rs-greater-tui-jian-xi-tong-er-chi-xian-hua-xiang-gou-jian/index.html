<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>RS =&gt; 推荐系统（二）离线画像构建  | Cython_lin</title>
<meta name="description" content="" />
<link rel="shortcut icon" href="https://cythonlin.github.io/favicon.ico">
<link rel="stylesheet" href="https://cythonlin.github.io/styles/main.css">

<script src="https://cythonlin.github.io/media/js/jquery.min.js"></script>
<script src="https://cythonlin.github.io/media/js/masonry.pkgd.min.js"></script>
<script src="https://cythonlin.github.io/media/js/aos.js"></script>
<script src="https://cythonlin.github.io/media/js/pace.min.js"></script>
<script src="https://cythonlin.github.io/media/js/view-image.min.js"></script>
<script src="https://cythonlin.github.io/media/js/jquery.magnific-popup.min.js"></script>
<script src="https://cythonlin.github.io/media/js/functions.js"></script>
    <meta name="referrer" content="never">
    <meta name="description" content="文章离线画像构建
Spark配置基类抽取
from pyspark import SparkConf
from pyspark.sql import SparkSession
import os

class SparkSessionBas..." />
    <meta name="keywords" content="RS" />
    <script src="https://cythonlin.github.io/media/js/waterfall.min.js"></script>
    <script src="https://cythonlin.github.io/media/js/prism.min.js"></script>
  </head>
  <body>
            <header id="header" class="grid-container">
        <!-- start: .menu-wrapper -->
        <div class="menu-mobile"> 
          <i class="fa fa-reorder"></i>
        </div>
        <div class="menu-wrapper">
          <div class="">
            <div class="logo">
              <a href="https://cythonlin.github.io"><img src="\media\images\custom-headerLogo.png" alt=""></a>
            </div>
            <!-- start: .main-nav -->

            <nav class="main-nav grid-container grid-parent">
              <ul id="menu-header" class="menu gradient-effect">
                <li class=""><a href="https://cythonlin.github.io" class="menu">首页</a></li>
                
                  <li class="" >
                    <a href="/archives" class="menu">
                      归档
                    </a>
                  </li>
                
                  <li class="" >
                    <a href="/tags" class="menu">
                      标签
                    </a>
                  </li>
                
                  <li class="" >
                    <a href="https://cythonlin.github.io/post/the-future-is-promising" class="menu">
                      关于
                    </a>
                  </li>
                
                <li class="search-menu-item hide-on-mobile hide-on-tablet"><a href="#search-lightbox" class="lightbox mfp-inline"><i class="fa fa-search-line"></i></a></li>
              </ul>
            </nav>
            <a href="#search-lightbox" class="lightbox epcl-search-button mfp-inline hide-on-tablet hide-on-desktop"><i class="fa fa-search-line"></i></a>
            <!-- end: .main-nav -->
            <div class="clear"></div>
            <div class="border hide-on-tablet hide-on-mobile"></div>
          </div>    
          <div class="clear"></div>
        </div>
        <!-- end: .menu-wrapper -->
        <div class="clear"></div>
      </header>
      <div class="hide-on-mobile hide-on-tablet hide-on-desktop">
        <div id="search-lightbox" class="grid-container grid-small grid-parent mfp-hide">
          <div class="search-wrapper section">
            <form id="gridea-search-form" data-update="1613707599272" action="/search/index.html" class="search-form" _lpchecked="1">
              <input type="text" name="q" id="s" value="" class="search-field" placeholder="搜点啥..." aria-label="搜点啥..." required="">
              <button type="submit" class="submit" aria-label="Submit">
                <i class="fa fa-search-line"></i>
              </button>
            </form>
          </div>
        </div>
      </div>

      <main id="single" class="main grid-container fullcover no-sidebar aos-init aos-animate" data-aos="fade">

        <div class="center content">
          <div class="featured-image cover" style="background-image: url('/media/images/gridea.jpg');">
            <div class="meta top"> 
              <time class="meta-info" style="float:left;" datetime="2020-09-29"><i class="fa fa-calendar"></i><span class="lately">5 个月前</span></time>
              
              <a href="https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-er-chi-xian-hua-xiang-gou-jian/#comments" class="comments meta-info" title="">
                <i class="fa fa-comment remixicon"></i><span class="comment-count valine-comment-count" data-xid="/rs-greater-tui-jian-xi-tong-er-chi-xian-hua-xiang-gou-jian/"> </span>
              </a>
              <span id="/rs-greater-tui-jian-xi-tong-er-chi-xian-hua-xiang-gou-jian/" class="leancloud_visitors views-counter meta-info" title=""><i class="fa fa-leancloud remixicon"></i><span class="leancloud-visitors-count"></span></span>
              
            </div>
            <div class="info">
              <div class="tags ">
                
                      <a href="https://cythonlin.github.io/tag/EjFvvnhFs/" class="ctag ctag-0 ctag-EjFvvnhFs" aria-label="">RS</a>
                    
              </div>
              <h1 class="title ularge white bold">RS =&gt; 推荐系统（二）离线画像构建 </h1>
            </div>
          </div>
        </div>  

        <div class="epcl-page-wrapper">
          <div class="left-content grid-70 np-mobile">
            <article class="main-article post">
              <section class="post-content">
                <div class="text">
                  <h1 id="文章离线画像构建">文章离线画像构建</h1>
<h3 id="spark配置基类抽取">Spark配置基类抽取</h3>
<pre><code>from pyspark import SparkConf
from pyspark.sql import SparkSession
import os

class SparkSessionBase(object):

	SPARK_APP_NAME = None
	SPARK_URL = &quot;yarn&quot;

	SPARK_EXECUTOR_MEMORY = &quot;2g&quot;
	SPARK_EXECUTOR_CORES = 2
	SPARK_EXECUTOR_INSTANCES = 2

	ENABLE_HIVE_SUPPORT = False

	def _create_spark_session(self):
		conf = SparkConf()  # 创建spark config对象
		config = (
			(&quot;spark.app.name&quot;, self.SPARK_APP_NAME),  # 设置启动的spark的app名称，没有提供，将随机产生一个名称
			(&quot;spark.executor.memory&quot;, self.SPARK_EXECUTOR_MEMORY),  # 设置该app启动时占用的内存用量，默认2g
	 		(&quot;spark.master&quot;, self.SPARK_URL),  # spark master的地址
			(&quot;spark.executor.cores&quot;, self.SPARK_EXECUTOR_CORES),  # 设置spark executor使用的CPU核心数，默认是1核心
			(&quot;spark.executor.instances&quot;, self.SPARK_EXECUTOR_INSTANCES),
			(&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;),
		)

	conf.setAll(config)

	# 利用config对象，创建spark session
	if self.ENABLE_HIVE_SUPPORT:
		return SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()
	else:
		return SparkSession.builder.config(conf=conf).getOrCreate()
</code></pre>
<h3 id="主应用导入基类">主应用导入基类</h3>
<pre><code># pip install pyspark
# pip install findspark

import findspark
findspark.init()

import os
import sys
# 如果当前代码文件运行测试需要加入修改路径，避免出现后导包问题
BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))
sys.path.insert(0, os.path.join(BASE_DIR))
print(BASE_DIR)
PYSPARK_PYTHON = &quot;/miniconda2/envs/reco_sys/bin/python&quot;
# 当存在多个版本时，不指定很可能会导致出错
os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHON
os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHON
from offline import SparkSessionBase

class OriginArticleData(SparkSessionBase):

	SPARK_APP_NAME = &quot;mergeArticle&quot;
	SPARK_URL = &quot;yarn&quot;

	ENABLE_HIVE_SUPPORT = True

	def __init__(self):
		self.spark = self._create_spark_session()

oa = OriginArticleData()   # oa就是带有配置的 sparkSession的实例化对象
</code></pre>
<h3 id="文章-表-合并">文章 表 合并</h3>
<p>文章基本信息表+文章内容表+频道表：<br>
titlce_content = oa.spark.sql(&quot;select a.article_id, a.channel_id, a.title, b.content from news_article_basic a inner join news_article_content b on a.article_id=b.article_id)<br>
因为得到的是 DF类型，想要用SQL，可以把DF注册为临时表<br>
titlce_content.registerTempTable('temptable')<br>
再把 频道表 的 频道名 合并进来<br>
channel_title_content = oa.spark.sql(&quot;select t.*, n.channel_name from temptable t left join news_channel n on t.channel_id=n.channel_id&quot;)</p>
<h3 id="文章-字段-合并">文章 字段 合并</h3>
<p>将 文章标题+文章内容+文章频道 的列，拼接成一个大字符串<br>
import pyspark.sql.functions as F<br>
import gc</p>
<pre><code># 增加channel的名字，后面会使用
basic_content.registerTempTable(&quot;temparticle&quot;)
channel_basic_content = oa.spark.sql(
	&quot;select t.*, n.channel_name from temparticle t left join news_channel n on t.channel_id=n.channel_id&quot;
)

# 利用concat_ws方法，将多列数据合并为一个长文本内容（频道，标题以及内容合并）
oa.spark.sql(&quot;use article&quot;)
sentence_df = channel_basic_content.select(&quot;article_id&quot;, &quot;channel_id&quot;, &quot;channel_name&quot;, &quot;title&quot;, &quot;content&quot;, \
	F.concat_ws(
		&quot;,&quot;,											# 指定大字符串分隔符
		channel_basic_content.channel_name,         
		channel_basic_content.title,
		channel_basic_content.content					
		).alias(&quot;sentence&quot;)                    # 新列 大字符串 取名
	)
del basic_content
del channel_basic_content
gc.collect()

# sentence_df.write.insertInto(&quot;article_data&quot;)       # 写入提前创建好的Hive表中
</code></pre>
<h3 id="分词">分词</h3>
<pre><code>def segmentation(partition):          # 就这一行的缩进需要调整下
import os
import re

import jieba
import jieba.analyse
import jieba.posseg as pseg
import codecs

abspath = &quot;/root/words&quot;

# 结巴加载用户词典
userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;)
jieba.load_userdict(userDict_path)

# 停用词文本
stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;)

def get_stopwords_list():
    &quot;&quot;&quot;返回stopwords列表&quot;&quot;&quot;
    stopwords_list = [i.strip()
                      for i in codecs.open(stopwords_path).readlines()]
    return stopwords_list

# 所有的停用词列表
stopwords_list = get_stopwords_list()

# 分词
def cut_sentence(sentence):
    &quot;&quot;&quot;对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词&quot;&quot;&quot;
    # print(sentence,&quot;*&quot;*100)
    # eg:[pair('今天', 't'), pair('有', 'd'), pair('雾', 'n'), pair('霾', 'g')]
    seg_list = pseg.lcut(sentence)
    seg_list = [i for i in seg_list if i.flag not in stopwords_list]
    filtered_words_list = []
    for seg in seg_list:
        # print(seg)
        if len(seg.word) &lt;= 1:
            continue
        elif seg.flag == &quot;eng&quot;:
            if len(seg.word) &lt;= 2:
                continue
            else:
                filtered_words_list.append(seg.word)
        elif seg.flag.startswith(&quot;n&quot;):
            filtered_words_list.append(seg.word)
        elif seg.flag in [&quot;x&quot;, &quot;eng&quot;]:  # 是自定一个词语或者是英文单词
            filtered_words_list.append(seg.word)
    return filtered_words_list

for row in partition:
    sentence = re.sub(&quot;&lt;.*?&gt;&quot;, &quot;&quot;, row.sentence)    # 替换掉标签数据
    words = cut_sentence(sentence)
    yield row.article_id, row.channel_id, words
</code></pre>
<h3 id="计算-tf-idf">计算 TF-IDF</h3>
<p>TF:<br>
ktt.spark.sql(&quot;use article&quot;)<br>
article_dataframe = ktt.spark.sql(&quot;select * from article_data limit 20&quot;)<br>
words_df = article_dataframe.rdd.mapPartitions(segmentation).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;words&quot;])</p>
<pre><code>from pyspark.ml.feature import CountVectorizer
# 总词汇的大小，文本中必须出现的次数
cv = CountVectorizer(inputCol=&quot;words&quot;, outputCol=&quot;countFeatures&quot;, vocabSize=200*10000, minDF=1.0)
# 训练词频统计模型
cv_model = cv.fit(words_df)
cv_model.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;)

# cv_model.vocabulary 查看统计词表（相当于groupby结果的 key,  但不包括value）
</code></pre>
<p>训练TF-IDF:<br>
# 词语与词频统计<br>
from pyspark.ml.feature import CountVectorizerModel<br>
cv_model = CountVectorizerModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/CV.model&quot;)<br>
# 得出词频向量结果<br>
cv_result = cv_model.transform(words_df)<br>
# 训练IDF模型 (把 tf结果传进去，其实说是 IDF模型，计算结果得出的就是 TF-IDF)<br>
from pyspark.ml.feature import IDF<br>
idf = IDF(inputCol=&quot;countFeatures&quot;, outputCol=&quot;idfFeatures&quot;)<br>
idfModel = idf.fit(cv_result)<br>
idfModel.write().overwrite().save(&quot;hdfs://hadoop-master:9000/headlines/models/IDF.model&quot;)</p>
<pre><code># idfModel.idf.toArray()[:20]    查看逆文档频率矩阵
</code></pre>
<p>TF-IDF结果数据格式：<br>
列1， 列...， 列 TF-IDF<br>
(1000,[804,1032],[6.349777077,7.0761797]) 。。。<br>
使用TF-IDF模型，取Top-K个词：<br>
from pyspark.ml.feature import CountVectorizerModel<br>
cv_model = CountVectorizerModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model&quot;)<br>
from pyspark.ml.feature import IDFModel<br>
idf_model = IDFModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model&quot;)<br>
cv_result = cv_model.transform(words_df)<br>
tfidf_result = idf_model.transform(cv_result)</p>
<pre><code>def func(partition):            
	TOPK = 20
	for row in partition:
		# 找到索引与IDF值并进行排序
		_ = list(zip(row.idfFeatures.indices, row.idfFeatures.values))   # [ (indexes,values)，  (indexes,values)]
		_ = sorted(_, key=lambda x: x[1], reverse=True)
		result = _[:TOPK]
		for word_index, tfidf in result:
			yield row.article_id, row.channel_id, int(word_index), round(float(tfidf), 4)
			# yield这句注定了返回结果格式 (多层for循环 yield， 原本一行数据按每个单词爆炸展开)
			# article_id,   channel_id,   word_index,   tfidf
			# 1				 100         40         15.5
			# 1				 100         14         10.3         
			# 1				 100         23         13.2
            
            
_keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;index&quot;, &quot;tfidf&quot;])
</code></pre>
<p>我们的目标是： 构成  词+TFIDF值, 而不是索引+TFIDF<br>
cv_model.vocabulary 结果是所有单词的列表。 上面的 index就是对应这个列表的索引<br>
最终构建一个词典+索引表：<br>
index	word<br>
..		..	<br>
然后将 主表（文章id,频道id,索引，tfidf）与 词典表（index+word） 合并<br>
得到  （文章id,频道id, 词， tfidf）</p>
<h3 id="计算-textrank">计算 TextRank</h3>
<p>TextRank和核心就是设定一个固定窗口来滑动<br>
把每个窗口内的每个词， 设为字典的Key, value就是他附近的n个词的列表<br>
然后每个词都这样做， 遇到相同的词就追加到字典的value 列表中<br>
# 分词<br>
def textrank(partition):<br>
import os</p>
<pre><code>import jieba
import jieba.analyse
import jieba.posseg as pseg
import codecs

abspath = &quot;/root/words&quot;

# 结巴加载用户词典
userDict_path = os.path.join(abspath, &quot;ITKeywords.txt&quot;)
jieba.load_userdict(userDict_path)

# 停用词文本
stopwords_path = os.path.join(abspath, &quot;stopwords.txt&quot;)

def get_stopwords_list():
    &quot;&quot;&quot;返回stopwords列表&quot;&quot;&quot;
    stopwords_list = [i.strip()
                      for i in codecs.open(stopwords_path).readlines()]
    return stopwords_list

# 所有的停用词列表
stopwords_list = get_stopwords_list()

class TextRank(jieba.analyse.TextRank):
    def __init__(self, window=20, word_min_len=2):
        super(TextRank, self).__init__()
        self.span = window  # 窗口大小
        self.word_min_len = word_min_len  # 单词的最小长度
        # 要保留的词性，根据jieba github ，具体参见https://github.com/baidu/lac
        self.pos_filt = frozenset(
            ('n', 'x', 'eng', 'f', 's', 't', 'nr', 'ns', 'nt', &quot;nw&quot;, &quot;nz&quot;, &quot;PER&quot;, &quot;LOC&quot;, &quot;ORG&quot;))

    def pairfilter(self, wp):
        &quot;&quot;&quot;过滤条件，返回True或者False&quot;&quot;&quot;

        if wp.flag == &quot;eng&quot;:
            if len(wp.word) &lt;= 2:
                return False

        if wp.flag in self.pos_filt and len(wp.word.strip()) &gt;= self.word_min_len \
                and wp.word.lower() not in stopwords_list:
            return True
# TextRank过滤窗口大小为5，单词最小为2
textrank_model = TextRank(window=5, word_min_len=2)
allowPOS = ('n', &quot;x&quot;, 'eng', 'nr', 'ns', 'nt', &quot;nw&quot;, &quot;nz&quot;, &quot;c&quot;)

for row in partition:
    tags = textrank_model.textrank(row.sentence, topK=20, withWeight=True, allowPOS=allowPOS, withFlag=False)
    for tag in tags:
        yield row.article_id, row.channel_id, tag[0], tag[1]

# 计算textrank
textrank_keywords_df = article_dataframe.rdd.mapPartitions(textrank).toDF(
	[&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;textrank&quot;]
)

# textrank_keywords_df.write.insertInto(&quot;textrank_keywords_values&quot;)
</code></pre>
<p>textrank运行结果如下：<br>
hive&gt; select * from textrank_keywords_values limit 20;<br>
OK<br>
文章ID  channel   word    textrank<br>
98319   17      var     20.6079<br>
98323   17      var     7.4938<br>
98326   17      var     104.9128<br>
然后和 tfidf一样 根据 textrank值，  取TOP-K个词</p>
<h3 id="计算-主题词-和-关键词">计算 主题词 和 关键词</h3>
<p>关键词：TEXTRANK计算出的结果TOPK个词以及权重<br>
主题词：TEXTRANK的TOPK词 与 ITFDF计算的TOPK个词的交集<br>
格式如下：<br>
hive&gt; desc article_profile;<br>
OK<br>
article_id              int                     article_id<br>
channel_id              int                     channel_id<br>
keywords               map								 keywords<br>
topics						  array								topics</p>
<pre><code>hive&gt; select * from article_profile limit 1;    
# 这里把结果按行排列开方便观看
article_id			26
channel_id		   17            
关键词字典		  {&quot;策略&quot;:0.3973770571351729,&quot;jpg&quot;:0.9806348975390871,&quot;用户&quot;:1.2794959063944176,&quot;strong&quot;:1.6488457985625076,&quot;文件&quot;:0.28144603583387057,&quot;逻辑&quot;:0.45256526469610714,&quot;形式&quot;:0.4123994242601279,&quot;全自&quot;:0.9594604850547191,&quot;h2&quot;:0.6244481634710125,&quot;版本&quot;:0.44280276959510817,&quot;Adobe&quot;:0.8553618185108718,&quot;安装&quot;:0.8305037437573172,&quot;检查更新&quot;:1.8088946300014435,&quot;产品&quot;:0.774842382276899,&quot;下载页&quot;:1.4256311032544344,&quot;过程&quot;:0.19827163395829256,&quot;json&quot;:0.6423301791599972,&quot;方式&quot;:0.582762869780791,&quot;退出应用&quot;:1.2338671268242603,&quot;Setup&quot;:1.004399549339134} 

主题词列表				[&quot;Electron&quot;,&quot;全自动&quot;,&quot;产品&quot;,&quot;版本号&quot;,&quot;安装包&quot;,&quot;检查更新&quot;,&quot;方案&quot;,&quot;版本&quot;,&quot;退出应用&quot;,&quot;逻辑&quot;,&quot;安装过程&quot;,&quot;方式&quot;,&quot;定性&quot;,&quot;新版本&quot;,&quot;Setup&quot;,&quot;静默&quot;,&quot;用户&quot;]
</code></pre>
<h3 id="增量更新-离线文章画像">增量更新 离线文章画像</h3>
<p>更新流程：<br>
1、toutiao 数据库中，news_article_content 与news_article_basic—&gt;更新到article数据库中article_data表，方便操作<br>
2. 第一次：所有更新，后面增量每天的数据更新26日：1：00<sub>2：00，2：00</sub>3：00，左闭右开,一个小时更新一次<br>
3、刚才新更新的文章，通过已有的idf计算出tfidf值以及hive 的textrank_keywords_values<br>
4、更新hive的article_profile</p>
<p>离线更新文章画像 代码组装：Pycharm<br>
注意在Pycharm中运行要设置环境：</p>
<pre><code>PYTHONUNBUFFERED=1
JAVA_HOME=/root/bigdata/jdk
SPARK_HOME=/root/bigdata/spark
HADOOP_HOME=/root/bigdata/hadoop
PYSPARK_PYTHON=/root/anaconda3/envs/reco_sys/bin/python
PYSPARK_DRIVER_PYTHON=/root/anaconda3/envs/reco_sys/bin/python
</code></pre>
<p>具体代码如下：<br>
import os<br>
import sys<br>
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(<strong>file</strong>)))<br>
sys.path.insert(0, os.path.join(BASE_DIR))<br>
from offline import SparkSessionBase<br>
from datetime import datetime<br>
from datetime import timedelta<br>
import pyspark.sql.functions as F<br>
import pyspark<br>
import gc</p>
<pre><code>class UpdateArticle(SparkSessionBase):
&quot;&quot;&quot;
更新文章画像
&quot;&quot;&quot;
SPARK_APP_NAME = &quot;updateArticle&quot;
ENABLE_HIVE_SUPPORT = True

SPARK_EXECUTOR_MEMORY = &quot;7g&quot;

def __init__(self):
    self.spark = self._create_spark_session()

    self.cv_path = &quot;hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model&quot;
    self.idf_path = &quot;hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model&quot;

def get_cv_model(self):
    # 词语与词频统计
    from pyspark.ml.feature import CountVectorizerModel
    cv_model = CountVectorizerModel.load(self.cv_path)
    return cv_model

def get_idf_model(self):
    from pyspark.ml.feature import IDFModel
    idf_model = IDFModel.load(self.idf_path)
    return idf_model

@staticmethod
def compute_keywords_tfidf_topk(words_df, cv_model, idf_model):
    &quot;&quot;&quot;保存tfidf值高的20个关键词
    :param spark:
    :param words_df:
    :return:
    &quot;&quot;&quot;
    cv_result = cv_model.transform(words_df)
    tfidf_result = idf_model.transform(cv_result)
    # print(&quot;transform compelete&quot;)

    # 取TOP-N的TFIDF值高的结果
    def func(partition):
        TOPK = 20
        for row in partition:
            _ = list(zip(row.idfFeatures.indices, row.idfFeatures.values))
            _ = sorted(_, key=lambda x: x[1], reverse=True)
            result = _[:TOPK]
            #         words_index = [int(i[0]) for i in result]
            #         yield row.article_id, row.channel_id, words_index

            for word_index, tfidf in result:
                yield row.article_id, row.channel_id, int(word_index), round(float(tfidf), 4)

    _keywordsByTFIDF = tfidf_result.rdd.mapPartitions(func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;index&quot;, &quot;tfidf&quot;])

    return _keywordsByTFIDF

def merge_article_data(self):
    &quot;&quot;&quot;
    合并业务中增量更新的文章数据
    :return:
    &quot;&quot;&quot;
    # 获取文章相关数据, 指定过去一个小时整点到整点的更新数据
    # 如：26日：1：00~2：00，2：00~3：00，左闭右开
    self.spark.sql(&quot;use toutiao&quot;)
    _yester = datetime.today().replace(minute=0, second=0, microsecond=0)
    start = datetime.strftime(_yester + timedelta(days=0, hours=-1, minutes=0), &quot;%Y-%m-%d %H:%M:%S&quot;)
    end = datetime.strftime(_yester, &quot;%Y-%m-%d %H:%M:%S&quot;)

    # 合并后保留：article_id、channel_id、channel_name、title、content
    # +----------+----------+--------------------+--------------------+
    # | article_id | channel_id | title | content |
    # +----------+----------+--------------------+--------------------+
    # | 141462 | 3 | test - 20190316 - 115123 | 今天天气不错，心情很美丽！！！ |
    basic_content = self.spark.sql(
        &quot;select a.article_id, a.channel_id, a.title, b.content from news_article_basic a &quot;
        &quot;inner join news_article_content b on a.article_id=b.article_id where a.review_time &gt;= '{}' &quot;
        &quot;and a.review_time &lt; '{}' and a.status = 2&quot;.format(start, end))
    # 增加channel的名字，后面会使用
    basic_content.registerTempTable(&quot;temparticle&quot;)
    channel_basic_content = self.spark.sql(
        &quot;select t.*, n.channel_name from temparticle t left join news_channel n on t.channel_id=n.channel_id&quot;)

    # 利用concat_ws方法，将多列数据合并为一个长文本内容（频道，标题以及内容合并）
    self.spark.sql(&quot;use article&quot;)
    sentence_df = channel_basic_content.select(&quot;article_id&quot;, &quot;channel_id&quot;, &quot;channel_name&quot;, &quot;title&quot;, &quot;content&quot;, \
                                               F.concat_ws(
                                                   &quot;,&quot;,
                                                   channel_basic_content.channel_name,
                                                   channel_basic_content.title,
                                                   channel_basic_content.content
                                               ).alias(&quot;sentence&quot;)
                                               )
    del basic_content
    del channel_basic_content
    gc.collect()

    sentence_df.write.insertInto(&quot;article_data&quot;)
    return sentence_df

def generate_article_label(self, sentence_df):
    &quot;&quot;&quot;
    生成文章标签  tfidf, textrank
    :param sentence_df: 增量的文章内容
    :return:
    &quot;&quot;&quot;
    # 进行分词
    words_df = sentence_df.rdd.mapPartitions(segmentation).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;words&quot;])
    cv_model = self.get_cv_model()
    idf_model = self.get_idf_model()

    # 1、保存所有的词的idf的值，利用idf中的词的标签索引
    # 工具与业务隔离
    _keywordsByTFIDF = UpdateArticle.compute_keywords_tfidf_topk(words_df, cv_model, idf_model)

    keywordsIndex = self.spark.sql(&quot;select keyword, index idx from idf_keywords_values&quot;)

    keywordsByTFIDF = _keywordsByTFIDF.join(keywordsIndex, keywordsIndex.idx == _keywordsByTFIDF.index).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;tfidf&quot;])

    keywordsByTFIDF.write.insertInto(&quot;tfidf_keywords_values&quot;)

    del cv_model
    del idf_model
    del words_df
    del _keywordsByTFIDF
    gc.collect()

    # 计算textrank
    textrank_keywords_df = sentence_df.rdd.mapPartitions(textrank).toDF(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;textrank&quot;])
    textrank_keywords_df.write.insertInto(&quot;textrank_keywords_values&quot;)

    return textrank_keywords_df, keywordsIndex

def get_article_profile(self, textrank, keywordsIndex):
    &quot;&quot;&quot;
    文章画像主题词建立
    :param idf: 所有词的idf值
    :param textrank: 每个文章的textrank值
    :return: 返回建立号增量文章画像
    &quot;&quot;&quot;
    keywordsIndex = keywordsIndex.withColumnRenamed(&quot;keyword&quot;, &quot;keyword1&quot;)
    result = textrank.join(keywordsIndex, textrank.keyword == keywordsIndex.keyword1)

    # 1、关键词（词，权重）
    # 计算关键词权重
    _articleKeywordsWeights = result.withColumn(&quot;weights&quot;, result.textrank * result.idf).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;weights&quot;])

    # 合并关键词权重到字典
    _articleKeywordsWeights.registerTempTable(&quot;temptable&quot;)
    articleKeywordsWeights = self.spark.sql(
        &quot;select article_id, min(channel_id) channel_id, collect_list(keyword) keyword_list, collect_list(weights) weights_list from temptable group by article_id&quot;)
    def _func(row):
        return row.article_id, row.channel_id, dict(zip(row.keyword_list, row.weights_list))
    articleKeywords = articleKeywordsWeights.rdd.map(_func).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keywords&quot;])

    # 2、主题词
    # 将tfidf和textrank共现的词作为主题词
    topic_sql = &quot;&quot;&quot;
            select t.article_id article_id2, collect_set(t.keyword) topics from tfidf_keywords_values t
            inner join 
            textrank_keywords_values r
            where t.keyword=r.keyword
            group by article_id2
            &quot;&quot;&quot;
    articleTopics = self.spark.sql(topic_sql)

    # 3、将主题词表和关键词表进行合并，插入表
    articleProfile = articleKeywords.join(articleTopics,
                                          articleKeywords.article_id == articleTopics.article_id2).select(
        [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keywords&quot;, &quot;topics&quot;])
    articleProfile.write.insertInto(&quot;article_profile&quot;)

    del keywordsIndex
    del _articleKeywordsWeights
    del articleKeywords
    del articleTopics
    gc.collect()

    return articleProfile


if __name__ == '__main__':
ua = UpdateArticle()
sentence_df = ua.merge_article_data()
if sentence_df.rdd.collect():
    rank, idf = ua.generate_article_label(sentence_df)
    articleProfile = ua.get_article_profile(rank, idf)
</code></pre>
<p>使用工具：Supervisor+Apscheduler<br>
# pip install APScheduler<br>
from apscheduler.schedulers.blocking import BlockingScheduler<br>
from apscheduler.executors.pool import ProcessPoolExecutor</p>
<pre><code>from scheduler.update import update_article_profile

# 创建scheduler，多进程执行
executors = {
	'default': ProcessPoolExecutor(3)
}
scheduler = BlockingScheduler(executors=executors)
# 添加定时更新任务更新文章画像,每隔一小时更新， trigger还有其他定时方式
scheduler.add_job(update_article_profile, trigger='interval', hours=1)
scheduler.start()
</code></pre>
<p>自定义Logger:<br>
import logging<br>
import logging.handlers<br>
import os</p>
<pre><code>logging_file_dir = '/root/logs/'
def create_logger():
	# 离线处理更新打印日志
	 log_trace = logging.getLogger('offline')
     
	trace_file_handler = logging.FileHandler(
        os.path.join(logging_file_dir, 'offline.log')
    )
	 trace_file_handler.setFormatter(logging.Formatter('%(message)s'))
    
    log_trace.addHandler(trace_file_handler)
    log_trace.setLevel(logging.INFO)
</code></pre>
<p>supervisor管理apscheduler:<br>
[program:offline]<br>
environment=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python<br>
command=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/scheduler/main.py<br>
directory=/root/toutiao_project/scheduler<br>
user=root<br>
autorestart=true<br>
redirect_stderr=true<br>
stdout_logfile=/root/logs/offlinesuper.log<br>
loglevel=info<br>
stopsignal=KILL<br>
stopasgroup=true<br>
killasgroup=true</p>
<h3 id="word2vec与文章相似度">Word2Vec与文章相似度</h3>
<pre><code>w2v.spark.sql(&quot;use article&quot;)
article = w2v.spark.sql(&quot;select * from article_data where channel_id=18 limit 2&quot;)
words_df = article.rdd.mapPartitions(segmentation).toDF(['article_id', 'channel_id', 'words'])
</code></pre>
<p>Spark Word2Vec API介绍：<br>
模块：from pyspark.ml.feature import Word2Vec</p>
<pre><code>API：class pyspark.ml.feature.Word2Vec(vectorSize=100, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, seed=None, inputCol=None, outputCol=None, windowSize=5, maxSentenceLength=1000)
参数说明：
	vectorSize=100: 词向量长度
	minCount：过滤次数小于默认5次的词
	windowSize=5：训练时候的窗口大小
	inputCol=None：输入列名
	outputCol=None：输出列名
</code></pre>
<p>Spark Word2Vec训练保存模型：<br>
new_word2Vec = Word2Vec(vectorSize=100, inputCol=&quot;words&quot;, outputCol=&quot;model&quot;, minCount=3)<br>
new_model = new_word2Vec.fit(words_df)<br>
new_model.save(&quot;hdfs://hadoop-master:9000/headlines/models/test.word2vec&quot;)<br>
上传历史数据训练的模型：<br>
hadoop dfs -put ./word2vec_model /headlines/models/</p>
<h3 id="增量更新-文章向量计算">增量更新-文章向量计算</h3>
<p>有了词向量之后，我们就可以得到一篇文章的向量了，为了后面快速使用文章的向量，我们会将每个频道所有的文章向量保存起来。</p>
<p>目的：保存所有历史训练的文章向量<br>
步骤：<br>
1、加载某个频道模型，得到每个词的向量<br>
2、获取频道的文章画像，得到文章画像的关键词(接着之前增量更新的文章article_profile)<br>
3、计算得到文章每个词的向量<br>
4、计算得到文章的平均词向量即文章的向量<br>
加载某个频道模型，得到每个词的向量<br>
from pyspark.ml.feature import Word2VecModel<br>
channel_id = 18<br>
channel = &quot;python&quot;<br>
wv_model = Word2VecModel.load(<br>
&quot;hdfs://hadoop-master:9000/headlines/models/word2vec_model/channel_%d_%s.word2vec&quot; % (channel_id, channel))<br>
vectors = wv_model.getVectors()<br>
获取新增的文章画像，得到文章画像的关键词：<br>
# 选出新增的文章的画像做测试，上节计算的画像中有不同频道的，我们选取Python频道的进行计算测试<br>
# 新增的文章画像获取部分<br>
profile = w2v.spark.sql(&quot;select * from article_profile where channel_id=18 limit 10&quot;)<br>
# profile = articleProfile.filter('channel_id = {}'.format(channel_id))</p>
<pre><code>profile.registerTempTable(&quot;incremental&quot;)
articleKeywordsWeights = w2v.spark.sql(
                &quot;select article_id, channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight&quot;)
_article_profile = articleKeywordsWeights.join(vectors, vectors.word==articleKeywordsWeights.keyword, &quot;inner&quot;)
</code></pre>
<p>计算得到文章的平均词向量即文章的向量<br>
def avg(row):<br>
x = 0<br>
for v in row.vectors:<br>
x += v<br>
#  将平均向量作为article的向量<br>
return row.article_id, row.channel_id, x / len(row.vectors)</p>
<pre><code>articleKeywordVectors.registerTempTable(&quot;tempTable&quot;)
articleVector = w2v.spark.sql(
	&quot;select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from tempTable group by article_id&quot;).rdd \
    # 分组之后， 求map的平均之前， 结果是  artile_id, channel_id, vector_list # vector_list 是二维数组。
    .map(avg).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;articleVector&quot;])
    # 求map的平均之后结果是 article_id, channel_id, article_vector # article_vector 代表文章向量
</code></pre>
<h3 id="文章相似度计算">文章相似度计算</h3>
<h4 id="存在的问题以进行某频道全量所有的两两相似度计算-但是事实当文章量达到千万级别或者上亿级别特征也会上亿级别计算量就会很大-以下有两种类型解决方案">存在的问题：以进行某频道全量所有的两两相似度计算。但是事实当文章量达到千万级别或者上亿级别，特征也会上亿级别，计算量就会很大。以下有两种类型解决方案：</h4>
<ol>
<li>每个频道的文章先进行聚类（缺点，（分成几个簇）也是个超参数）</li>
<li>局部敏感哈希LSH(Locality Sensitive Hashing)<br>
基本思想1：LSH算法基于一个假设，如果两个文本在原有的数据空间是相似的，那么分别经过哈希函数转换以后的它们也具有很高的相似度<br>
基本思想2: 经常使用的哈希函数，冲突总是难以避免。LSH却依赖于冲突，在解决NNS(Nearest neighbor search )时，我们期望：<br>
离得越近的对象，发生冲突的概率越高<br>
离得越远的对象，发生冲突的概率越低</li>
</ol>
<h4 id="局部敏感哈希lshlocality-sensitive-hashing-lsh过程">局部敏感哈希LSH(Locality Sensitive Hashing) LSH过程：</h4>
<p>mini hashing(略)	<br>
Random Projection（特征压缩）：<br>
Random Projection是一种随机算法.随机投影的算法有很多，如PCA、Gaussian random projection - 高斯随机投影。<br>
随机桶投影是用于欧几里德距离的 LSH family。其LSH family将x特征向量映射到随机单位矢量v，并将映射结果分为哈希桶中。哈希表中的每个位置表示一个哈希桶。	<br>
使得：<br>
离得越近的对象，发生冲突的概率越高<br>
离得越远的对象，发生冲突的概率越低</p>
<h4 id="代码实现">代码实现：</h4>
<p>读取数据，进行类型处理(数组转换类型为Vector)：<br>
from pyspark.ml.linalg import Vectors<br>
# 选取部分数据做测试<br>
article_vector = w2v.spark.sql(&quot;select article_id, articlevector from article_vector where channel_id=18 limit 10&quot;)<br>
train = articlevector.select(['article_id', 'articleVector'])</p>
<pre><code>def _array_to_vector(row):
	return row.article_id, Vectors.dense(row.articleVector)

train = train.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])
</code></pre>
<p>相似度计算（BRP进行FIT）：<br>
函数参数说明：<br>
class pyspark.ml.feature.BucketedRandomProjectionLSH(inputCol=None, outputCol=None, seed=None, numHashTables=1, bucketLength=None)<br>
inputCol=None：输入特征列<br>
outputCol=None：输出特征列<br>
numHashTables=1：哈希表数量，几个hash function对数据进行hash操作<br>
bucketLength=None：桶的数量，值越大相同数据进入到同一个桶的概率越高<br>
method:<br>
# 计算df1每个文章相似的df2数据集的数据<br>
approxSimilarityJoin(df1, df2, 2.0, distCol='EuclideanDistance')  # 转为向量</p>
<pre><code># 代码调用：
from pyspark.ml.feature import BucketedRandomProjectionLSH

brp = BucketedRandomProjectionLSH(inputCol='articleVector', outputCol='hashes', numHashTables=4.0, bucketLength=10.0)
model = brp.fit(旧文章向量)
</code></pre>
<p>计算相似的文章以及相似度<br>
# 计算文章和文章之间的相似度<br>
similar = model.approxSimilarityJoin(新增文章向量, 新增文章向量, 2.0, distCol='Similarity')  # 输出列名<br>
similar.sort(['EuclideanDistance']).show()<br>
计算结果：<br>
datasetA(新增),    datasetB（旧的）,     Similarity<br>
[2,[文章向量]]		[5,[文章向量]]			0.0051<br>
[1,[文章向量]]		[3,[文章向量]]			0.0054<br>
[2,[文章向量]]		[8,[文章向量]]			0.0053<br>
[1,[文章向量]]		[4,[文章向量]]			0.0052<br>
[2,[文章向量]]		[7,[文章向量]]			0.0055<br>
...</p>
<h3 id="文章相似度存储-hbase">文章相似度存储 HBase</h3>
<h4 id="存储目标存储-文章相似文章-相似度">存储目标：存储 文章，相似文章， 相似度</h4>
<p>调用foreachPartition：<br>
foreachPartition不同于map和mapPartition。<br>
无返回结果，主要用于离线分析之后的数据（数据库存储等）落地<br>
如果想要返回新的一个数据DF，就使用map后者。<br>
我们需要建立一个HBase存储文章相似度的表：<br>
create 'article_similar', 'similar'</p>
<pre><code># 存储格式如下：
		 表           row_key  column_family   value
	put 'article_similar', '1', 	'similar:1',    0.2
	put 'article_similar', '1', 	'similar:2',    0.34
</code></pre>
<p>HBase 开启失败可能的原因的：</p>
<ol>
<li>
<p>时间未同步的解决办法：<br>
ntpdate 0.cn.pool.ntp.org<br>
或<br>
ntpdate ntp1.aliyun.com</p>
</li>
<li>
<p>thrift服务未开启的解决办法：<br>
hbase-daemon.sh start thrift<br>
happybase代码实现：<br>
def save_hbase(partition):<br>
import happybase<br>
pool = happybase.ConnectionPool(size=3, host='hadoop-master')</p>
<p>with pool.connection() as conn:<br>
# 建议表的连接<br>
table = conn.table('article_similar')<br>
for row in partition:<br>
if row.datasetA.article_id == row.datasetB.article_id:<br>
pass<br>
else:<br>
table.put(str(row.datasetA.article_id).encode(),<br>
{&quot;similar:{}&quot;.format(row.datasetB.article_id).encode(): b'%0.4f' % (row.EuclideanDistance)})<br>
# 手动关闭所有的连接<br>
conn.close()</p>
<p>similar.foreachPartition(save_hbase)</p>
</li>
</ol>
<h3 id="文章相似度增量更新代码整理">文章相似度增量更新代码整理</h3>
<pre><code>def compute_article_similar(self, articleProfile):
    &quot;&quot;&quot;
    计算增量文章与历史文章的相似度 word2vec
    :return:
    &quot;&quot;&quot;
    # 得到要更新的新文章通道类别(不采用)
    # all_channel = set(articleProfile.rdd.map(lambda x: x.channel_id).collect())
    def avg(row):
        x = 0
        for v in row.vectors:
            x += v
        #  将平均向量作为article的向量
        return row.article_id, row.channel_id, x / len(row.vectors)

    for channel_id, channel_name in CHANNEL_INFO.items():

        profile = articleProfile.filter('channel_id = {}'.format(channel_id))
        wv_model = Word2VecModel.load(
            &quot;hdfs://hadoop-master:9000/headlines/models/channel_%d_%s.word2vec&quot; % (channel_id, channel_name))
        vectors = wv_model.getVectors()

        # 计算向量
        profile.registerTempTable(&quot;incremental&quot;)
        articleKeywordsWeights = ua.spark.sql(
            &quot;select article_id, channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight where channel_id=%d&quot; % channel_id)

        articleKeywordsWeightsAndVectors = articleKeywordsWeights.join(vectors,
                                                        vectors.word == articleKeywordsWeights.keyword, &quot;inner&quot;)
        articleKeywordVectors = articleKeywordsWeightsAndVectors.rdd.map(
            lambda r: (r.article_id, r.channel_id, r.keyword, r.weight * r.vector)).toDF(
            [&quot;article_id&quot;, &quot;channel_id&quot;, &quot;keyword&quot;, &quot;weightingVector&quot;])

        articleKeywordVectors.registerTempTable(&quot;tempTable&quot;)
        articleVector = self.spark.sql(
            &quot;select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from tempTable group by article_id&quot;).rdd.map(
            avg).toDF([&quot;article_id&quot;, &quot;channel_id&quot;, &quot;articleVector&quot;])

        # 写入数据库
        def toArray(row):
            return row.article_id, row.channel_id, [float(i) for i in row.articleVector.toArray()]
        articleVector = articleVector.rdd.map(toArray).toDF(['article_id', 'channel_id', 'articleVector'])
        articleVector.write.insertInto(&quot;article_vector&quot;)

        import gc
        del wv_model
        del vectors
        del articleKeywordsWeights
        del articleKeywordsWeightsAndVectors
        del articleKeywordVectors
        gc.collect()

        # 得到历史数据, 转换成固定格式使用LSH进行求相似
        train = self.spark.sql(&quot;select * from article_vector where channel_id=%d&quot; % channel_id)

        def _array_to_vector(row):
            return row.article_id, Vectors.dense(row.articleVector)
        train = train.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])
        test = articleVector.rdd.map(_array_to_vector).toDF(['article_id', 'articleVector'])

        brp = BucketedRandomProjectionLSH(inputCol='articleVector', outputCol='hashes', seed=12345,
                                          bucketLength=1.0)
        model = brp.fit(train)
        similar = model.approxSimilarityJoin(test, train, 2.0, distCol='EuclideanDistance')

        def save_hbase(partition):
            import happybase
            for row in partition:
                pool = happybase.ConnectionPool(size=3, host='hadoop-master')
                # article_similar article_id similar:article_id sim
                with pool.connection() as conn:
                    table = connection.table(&quot;article_similar&quot;)
                    for row in partition:
                        if row.datasetA.article_id == row.datasetB.article_id:
                            pass
                        else:
                            table.put(str(row.datasetA.article_id).encode(),
                                      {b&quot;similar:%d&quot; % row.datasetB.article_id: b&quot;%0.4f&quot; % row.EuclideanDistance})
                    conn.close()
        similar.foreachPartition(save_hbase)
</code></pre>
<p>添加函数到主函数中文件中，修改update更新代码：<br>
ua = UpdateArticle()<br>
sentence_df = ua.merge_article_data()<br>
if sentence_df.rdd.collect():<br>
rank, idf = ua.generate_article_label(sentence_df)<br>
articleProfile = ua.get_article_profile(rank, idf)<br>
ua.compute_article_similar(articleProfile)</p>
<h1 id="用户画像构建与更新">用户画像构建与更新</h1>
<h3 id="组成成分">组成成分</h3>
<p>用户基本信息+用户行为(历史+新增)<br>
用户行为包括：<br>
hive&gt; select * from user_action limit 1;<br>
OK<br>
2019-03-05 10:19:40		0		{&quot;action&quot;:&quot;exposure&quot;,&quot;userId&quot;:&quot;2&quot;,&quot;articleId&quot;:&quot;[16000, 44371, 16421, 16181, 17454]&quot;,&quot;algorithmCombine&quot;:&quot;C2&quot;} 2019-03-05<br>
我们需要对用户行为（字典）数据格式平铺处理<br>
user_id	action_time	article_id	share	click	collected	exposure	read_time</p>
<h3 id="步骤">步骤：</h3>
<p>1、创建HIVE基本数据表<br>
2、读取固定时间内的用户行为日志<br>
3、进行用户日志数据处理<br>
4、存储到user_article_basic表中</p>
<h4 id="创建hive基本数据表">创建HIVE基本数据表</h4>
<pre><code>create table user_article_basic(
user_id BIGINT comment &quot;userID&quot;,
action_time STRING comment &quot;user actions time&quot;,
article_id BIGINT comment &quot;articleid&quot;,
channel_id INT comment &quot;channel_id&quot;,
shared BOOLEAN comment &quot;is shared&quot;,
clicked BOOLEAN comment &quot;is clicked&quot;,
collected BOOLEAN comment &quot;is collected&quot;,
exposure BOOLEAN comment &quot;is exposured&quot;,
read_time STRING comment &quot;reading time&quot;)
COMMENT &quot;user_article_basic&quot;
CLUSTERED by (user_id) into 2 buckets
STORED as textfile
LOCATION '/user/hive/warehouse/profile.db/user_article_basic';
</code></pre>
<h4 id="读取增量用户行为数据-固定时间内的用户行为日志">读取增量用户行为数据-固定时间内的用户行为日志</h4>
<p>关联历史日期文件<br>
# 在进行日志信息的处理之前，先将我们之前建立的user_action表之间进行所有日期关联，spark hive不会自动关联<br>
import pandas as pd<br>
from datetime import datetime</p>
<pre><code>def datelist(beginDate, endDate):
	date_list=[datetime.strftime(x,'%Y-%m-%d') for x in list(pd.date_range(start=beginDate, end=endDate))]
	return date_list

dl = datelist(&quot;2019-03-05&quot;, time.strftime(&quot;%Y-%m-%d&quot;, time.localtime()))

fs = pyhdfs.HdfsClient(hosts='hadoop-master:50070')
for d in dl:
	try:
		_localions = '/user/hive/warehouse/profile.db/user_action/' + d
		if fs.exists(_localions):
			uup.spark.sql(&quot;alter table user_action add partition (dt='%s') location '%s'&quot; % (d, _localions))
	except Exception as e:
		# 已经关联过的异常忽略,partition与hdfs文件不直接关联
		pass
sqlDF = uup.spark.sql(
	&quot;select actionTime, readTime, channelId, param.articleId, param.algorithmCombine, param.action, param.userId from user_action where dt={}&quot;.format(time_str)
)
</code></pre>
<h4 id="原始数据格式与目标数据格式">原始数据格式与目标数据格式</h4>
<p>原始数据格式：（行为参数都算在 action列内）<br>
actionTime	readTime	channelID	articleId	算法名称	action		userId<br>
123						1						exposure	1<br>
321						1						click		1<br>
目标数据格式（行为参数1拆4，action列被爆炸为4列，均为bool类型）<br>
user_id	action_time	article_id	shared	clicked	collected	expore	read_time<br>
1					1			false	false	false		true<br>
1					2			false	true	false		true</p>
<h4 id="进行用户日志数据格式处理">进行用户日志数据格式处理</h4>
<pre><code>if sqlDF.collect():
def _compute(row):
    # 进行判断行为类型
    _list = []
    if row.action == &quot;exposure&quot;:
        for article_id in eval(row.articleId):
            _list.append(
                [row.userId, row.actionTime, article_id, row.channelId, False, False, False, True, row.readTime])
        return _list
    else:
        class Temp(object):
            shared = False
            clicked = False
            collected = False
            read_time = &quot;&quot;

        _tp = Temp()
        if row.action == &quot;share&quot;:
            _tp.shared = True
        elif row.action == &quot;click&quot;:
            _tp.clicked = True
        elif row.action == &quot;collect&quot;:
            _tp.collected = True
        elif row.action == &quot;read&quot;:
            _tp.clicked = True
        else:
            pass
        _list.append(
            [row.userId, row.actionTime, int(row.articleId), row.channelId, _tp.shared, _tp.clicked, _tp.collected,
             True,
             row.readTime])
        return _list
# 进行处理
# 查询内容，将原始日志表数据进行处理
_res = sqlDF.rdd.flatMap(_compute)
data = _res.toDF([&quot;user_id&quot;, &quot;action_time&quot;,&quot;article_id&quot;, &quot;channel_id&quot;, &quot;shared&quot;, &quot;clicked&quot;, &quot;collected&quot;, &quot;exposure&quot;, &quot;read_time&quot;])
</code></pre>
<h4 id="将上述目标格式的数据按照-userid-和-articleid-分组">将上述目标格式的数据按照 userid 和 articleid 分组</h4>
<p>先合并历史数据，存储到user-article-basic表中<br>
# 合并历史数据，插入表中<br>
old = uup.spark.sql(&quot;select * from user_article_basic&quot;)<br>
# 由于合并的结果中不是对于user_id和article_id唯一的，一个用户会对文章多种操作<br>
new_old = old.unionAll(data)<br>
HIVE目前支持hive终端操作ACID，不支持python的pyspark原子性操作，并且开启配置中开启原子性相关配置也不行。<br>
new_old.registerTempTable(&quot;temptable&quot;)<br>
# 按照用户，文章分组存放进去<br>
uup.spark.sql(<br>
&quot;insert overwrite table user_article_basic select user_id, max(action_time) as action_time, &quot;<br>
&quot;article_id, max(channel_id) as channel_id, max(shared) as shared, max(clicked) as clicked, &quot;<br>
&quot;max(collected) as collected, max(exposure) as exposure, max(read_time) as read_time from temptable &quot;<br>
&quot;group by user_id, article_id&quot;)</p>
<h3 id="用户画像标签权重计算">用户画像标签权重计算</h3>
<h4 id="如何存储">如何存储</h4>
<p>用户画像，作为特征提供给一些算法排序，方便与快速读取使用<br>
选择存储在Hbase当中。<br>
然后用 Hive 外表关联 hbase<br>
如果离线分析也想要使用我们可以建立HIVE到Hbase的外部表。</p>
<h4 id="hbase表设计">HBase表设计</h4>
<pre><code>		table_name		column1 column2  column3 
create 'user_profile', 'basic','partial','env'

					row_key   column_family					value
put 'user_profile', 'user:2', 'partial:{channel_id}:{topic}': weights
put 'user_profile', 'user:2', 'basic:{info}': value
put 'user_profile', 'user:2', 'env:{info}': value
</code></pre>
<h4 id="hive表设计">Hive表设计</h4>
<pre><code>create external table user_profile_hbase(
user_id STRING comment &quot;userID&quot;,
information map&lt;string, DOUBLE&gt; comment &quot;user basic information&quot;,
article_partial map&lt;string, DOUBLE&gt; comment &quot;article partial&quot;,
env map&lt;string, INT&gt; comment &quot;user env&quot;)
COMMENT &quot;user profile table&quot;
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,basic:,partial:,env:&quot;)
TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;user_profile&quot;);
</code></pre>
<h4 id="spark-sql关联表读取问题">Spark SQL关联表读取问题</h4>
<p>创建关联表之后，离线读取表内容需要一些依赖包。解决办法：<br>
拷贝/root/bigdata/hbase/lib/下面hbase-<em>.jar 到 /root/bigdata/spark/jars/目录下<br>
拷贝/root/bigdata/hive/lib/h</em>.jar 到 /root/bigdata/spark/jars/目录下<br>
上述操作三台虚拟机都执行一遍。</p>
<h4 id="用户画像频道关键词获取与权重计算">用户画像频道关键词获取与权重计算</h4>
<p>目标：获取用户1~25频道(不包括推荐频道)的关键词，并计算权重<br>
1、读取user-article-basic表，合并行为表与文章画像中的主题词<br>
2、进行用户权重计算公式、同时落地存储<br>
# 获取基本用户行为信息，然后进行文章画像的主题词合并<br>
uup.spark.sql(&quot;use profile&quot;)<br>
# 取出日志中的channel_id<br>
user_article_ = uup.spark.sql(&quot;select * from user_article_basic&quot;).drop('channel_id')<br>
uup.spark.sql('use article')<br>
article_label = uup.spark.sql(&quot;select article_id, channel_id, topics from article_profile&quot;)<br>
# 合并使用文章中正确的channel_id<br>
click_article_res = user_article_.join(article_label, how='left', on=['article_id'])<br>
将关键词字段的列表爆炸<br>
import pyspark.sql.functions as F<br>
click_article_res = click_article_res.withColumn('topic', F.explode('topics')).drop('topics')<br>
爆炸后格式如下：<br>
user_id article_id topic ...<br>
1     1			 python<br>
1     1			 golang<br>
1     1			 linux<br>
...    ...		  ...</p>
<h4 id="用户画像之标签权重算法">用户画像之标签权重算法</h4>
<p>用户标签权重 =( 行为类型权重之和) × 时间衰减<br>
行为类型权重 的 分值 的确定需要整体协商<br>
行为					分值<br>
阅读时间&lt;1000ms		  1<br>
阅读时间&gt;=1000ms	  2<br>
收藏					2<br>
分享					3<br>
点击					5<br>
时间衰减: 1/(log(t)+1) ,t为时间发生时间距离当前时间的大小。<br>
# 计算每个用户对每篇文章的标签的权重<br>
def compute_weights(rowpartition):<br>
&quot;&quot;&quot;处理每个用户对文章的点击数据<br>
&quot;&quot;&quot;<br>
weightsOfaction = {<br>
&quot;read_min&quot;: 1,<br>
&quot;read_middle&quot;: 2,<br>
&quot;collect&quot;: 2,<br>
&quot;share&quot;: 3,<br>
&quot;click&quot;: 5<br>
}</p>
<pre><code>import happybase
from datetime import datetime
import numpy as np
#  用于读取hbase缓存结果配置
pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090)

# 读取文章的标签数据
# 计算权重值
# 时间间隔
for row in rowpartition:
    t = datetime.now() - datetime.strptime(row.action_time, '%Y-%m-%d %H:%M:%S')
    # 时间衰减系数
    time_exp = 1 / (np.log(t.days + 1) + 1)

    if row.read_time == '':
        r_t = 0
    else:
        r_t = int(row.read_time)
    # 浏览时间分数
    is_read = weightsOfaction['read_middle'] if r_t &gt; 1000 else weightsOfaction['read_min']

    # 每个词的权重分数
    weigths = time_exp * (
                row.shared * weightsOfaction['share'] + row.collected * weightsOfaction['collect'] + row.
                clicked * weightsOfaction['click'] + is_read)

#        with pool.connection() as conn:
#            table = conn.table('user_profile')
#            table.put('user:{}'.format(row.user_id).encode(),
#                      {'partial:{}:{}'.format(row.channel_id, row.topic).encode(): json.dumps(
#                          weigths).encode()})
#            conn.close()

click_article_res.foreachPartition(compute_weights)
</code></pre>
<p>落地Hbase中之后，在HBASE中查询，happybase或者hbase终端<br>
import happybase<br>
# 用于读取hbase缓存结果配置<br>
pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090)</p>
<pre><code>with pool.connection() as conn:
	table = conn.table('user_profile')
	# 获取每个键 对应的所有列的结果
	data = table.row(b'user:2', columns=[b'partial'])
	conn.close()

# 等价于  hbase(main):015:0&gt; get 'user_profile', 'user:2'
</code></pre>
<h3 id="基础信息画像更新">基础信息画像更新</h3>
<pre><code>def update_user_info(self):
    &quot;&quot;&quot;
    更新用户的基础信息画像
    :return:
    &quot;&quot;&quot;
    self.spark.sql(&quot;use toutiao&quot;)

    user_basic = self.spark.sql(&quot;select user_id, gender, birthday from user_profile&quot;)

    # 更新用户基础信息
    def _udapte_user_basic(partition):
        &quot;&quot;&quot;更新用户基本信息
        &quot;&quot;&quot;
        import happybase
        #  用于读取hbase缓存结果配置
        pool = happybase.ConnectionPool(size=10, host='172.17.0.134', port=9090)
        for row in partition:

            from datetime import date
            age = 0
            if row.birthday != 'null':
                born = datetime.strptime(row.birthday, '%Y-%m-%d')
                today = date.today()
                age = today.year - born.year - ((today.month, today.day) &lt; (born.month, born.day))

            with pool.connection() as conn:
                table = conn.table('user_profile')
                table.put('user:{}'.format(row.user_id).encode(),
                          {'basic:gender'.encode(): json.dumps(row.gender).encode()})
                table.put('user:{}'.format(row.user_id).encode(),
                          {'basic:birthday'.encode(): json.dumps(age).encode()})
                conn.close()

    user_basic.foreachPartition(_udapte_user_basic)
    logger.info(
        &quot;{} INFO completely update infomation of basic&quot;.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
        
# hbase(main):016:0&gt; get 'user_profile', 'user:2'
</code></pre>
<h3 id="用户画像增量更新定时开启">用户画像增量更新定时开启:</h3>
<p>用户画像增量更新代码整理<br>
添加定时任务以及进程管理<br>
from offline.update_user import UpdateUserProfile</p>
<pre><code>def update_user_profile():
	&quot;&quot;&quot;
	更新用户画像
	&quot;&quot;&quot;
	uup = UpdateUserProfile()
	if uup.update_user_action_basic():
		uup.update_user_label()
		uup.update_user_info()
scheduler.add_job(update_user_profile, trigger='interval', hours=2)</code></pre>

                </div>
                <div class="clear"></div>
              </section>
            </article>
            <div class="clear"></div>

            <section class="related section">
              
              <article class="prev grid-50 tablet-grid-50 grid-parent">
                <div class="thumb cover lazy loaded" style="background-image: url('https://cythonlin.github.io/media/images/EjFvvnhFs.jpg');"></div>
                 <a href="https://cythonlin.github.io/post/py-greater-hbase/" class="full-link"></a>
                 <div class="info">
                  <time datetime="2020-09-29">2020-09-29</time>
                  <h4 class="title white no-margin">PY =&gt; HBase</h4>
                </div>
                 <span class="epcl-button red">
                  <img src="https://cythonlin.github.io/media/images/left-arrow.svg" width="15" alt="Left Arrow">
                </span>
                <div class="overlay"></div>
              </article>
              
              
              <article class="next grid-50 tablet-grid-50 grid-parent">
                <div class="thumb cover lazy loaded" style="background-image: url('https://cythonlin.github.io/media/images/EjFvvnhFs.jpg');"></div>
                 <a href="https://cythonlin.github.io/post/py-greater-spark-mappartition/" class="full-link"></a>
                 <div class="info">
                  <time datetime="2020-09-29">2020-09-29</time>
                  <h4 class="title white no-margin">PY =&gt; Spark mapPartition</h4>
                </div>
                 <span class="epcl-button red">
                  <img src="https://cythonlin.github.io/media/images/right-arrow.svg" width="15" alt="Left Arrow">
                </span>
                <div class="overlay"></div>
              </article>
              

                <div class="clear"></div>
            </section>

              <div class="clear"></div>
              
            
<!--               <div id="comments" class="bg-white hosted ">
                <p>请到客户端“主题--自定义配置--valine”中填入ID和KEY</p>
              </div> -->
              <div class="clear"></div>
            

            </div>
          </div>
      </main>

          <footer id="footer" class="grid-container">
        <div class="widgets row gradient-effect">
            <div class="default-sidebar border-effect">
              <div class="grid-33 tablet-grid-50 mobile-grid-100">
                <section id="tag_cloud-2" class="widget widget_epcl_posts_thumbs underline-effect">
                  <h4 class="widget-title title white bordered">最新文章</h4>
                  
                  
                  <article class="item post-0 post type-post status-publish format-standard has-post-thumbnail hentry">
                    <a href="https://cythonlin.github.io/post/pr-greater-powershell-7/" class="thumb hover-effect">
                      <span class="fullimage cover" style="display:block;border-radius:50%;background-image: url('/media/images/gridea.jpg');"></span>
                    </a>
                    <div class="info gradient-effect">
                      <time datetime="2021-02-18">2021-02-18</time>
                      <h4 class="title usmall">
                        <a href="https://cythonlin.github.io/post/pr-greater-powershell-7/">PR =&gt; Powershell 7</a>
                      </h4>
                    </div>
                    <div class="clear"></div>
                  </article>
                  
                  
                  
                  <article class="item post-1 post type-post status-publish format-standard has-post-thumbnail hentry">
                    <a href="https://cythonlin.github.io/post/pr-greater-oh-my-posh3/" class="thumb hover-effect">
                      <span class="fullimage cover" style="display:block;border-radius:50%;background-image: url('/media/images/gridea.jpg');"></span>
                    </a>
                    <div class="info gradient-effect">
                      <time datetime="2021-02-12">2021-02-12</time>
                      <h4 class="title usmall">
                        <a href="https://cythonlin.github.io/post/pr-greater-oh-my-posh3/">PR =&gt; Oh-my-Posh3+Banner</a>
                      </h4>
                    </div>
                    <div class="clear"></div>
                  </article>
                  
                  
                  
                  <article class="item post-2 post type-post status-publish format-standard has-post-thumbnail hentry">
                    <a href="https://cythonlin.github.io/post/pr-greater-terminal-ying-yong/" class="thumb hover-effect">
                      <span class="fullimage cover" style="display:block;border-radius:50%;background-image: url('/media/images/gridea.jpg');"></span>
                    </a>
                    <div class="info gradient-effect">
                      <time datetime="2021-02-06">2021-02-06</time>
                      <h4 class="title usmall">
                        <a href="https://cythonlin.github.io/post/pr-greater-terminal-ying-yong/">PR =&gt; Terminal 应用</a>
                      </h4>
                    </div>
                    <div class="clear"></div>
                  </article>
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  <div class="clear"></div>
                </section>
              </div>

              <div class="grid-33 tablet-grid-50 mobile-grid-100">
                <section id="tag_cloud-2" class="widget widget_tag_cloud underline-effect">
                  <h4 class="widget-title title white bordered">标签云</h4>
                  <div class="tagcloud">
                    
                      <a href="https://cythonlin.github.io/tag/sKPdrpV6Y/" class="ctag ctag-0 ctag-sKPdrpV6Y" aria-label="">PR</a>
                    
                      <a href="https://cythonlin.github.io/tag/wAXYBHxvH/" class="ctag ctag-1 ctag-wAXYBHxvH" aria-label="">Python</a>
                    
                      <a href="https://cythonlin.github.io/tag/c137hZpK4/" class="ctag ctag-2 ctag-c137hZpK4" aria-label="">IDE</a>
                    
                      <a href="https://cythonlin.github.io/tag/XH05Gb5J6/" class="ctag ctag-3 ctag-XH05Gb5J6" aria-label="">Golang</a>
                    
                      <a href="https://cythonlin.github.io/tag/EjFvvnhFs/" class="ctag ctag-4 ctag-EjFvvnhFs" aria-label="">RS</a>
                    
                      <a href="https://cythonlin.github.io/tag/n16me-6oV/" class="ctag ctag-5 ctag-n16me-6oV" aria-label="">AI</a>
                    
                      <a href="https://cythonlin.github.io/tag/FLS_eF6Eg/" class="ctag ctag-6 ctag-FLS_eF6Eg" aria-label="">KG</a>
                    
                      <a href="https://cythonlin.github.io/tag/1L4lr0i2f/" class="ctag ctag-7 ctag-1L4lr0i2f" aria-label="">Docker</a>
                    
                      <a href="https://cythonlin.github.io/tag/sJAb6NuUK/" class="ctag ctag-8 ctag-sJAb6NuUK" aria-label="">DB</a>
                    
                  </div>
                  <div class="clear"></div>
                </section>
              </div>

              <div class="grid-33 tablet-grid-50 mobile-grid-100">
                <section id="epcl_about-2" class="widget widget_epcl_about underline-effect">
                  <h4 class="widget-title title white bordered">关于我</h4>
                  <div class="avatar">
                    <a href="" class="translate-effect thumb"><span class="fullimage cover" style="background-image: url(https://cythonlin.github.io/images/avatar.png);"></span></a>
                  </div>
                  <div class="info">
                    <h4 class="title small author-name gradient-effect no-margin"><a href="">Cython_lin</a></h4>
                    <p class="founder"></p>
                    <div class="social">
                      
                        
                      
                        
                      
                        
                      
                        
                      
                        
                      
                    </div> 
                  </div>
                  <div class="clear"></div>
                  </section>
              </div>

            </div>
            <div class="clear"></div>
        </div>

        <div class="logo">
          <a href="https://cythonlin.github.io"><img src="\media\images\custom-footerLogo.png" alt=""></a>
        </div>
        <p class="published border-effect">
          ©2019 共 77 篇文章
          <br/>

        </p>
        
        <a href="javascript:void(0)" id="back-to-top" class="epcl-button dark" style="display:none">
          <i class="fa fa-arrow"></i>
        </a>
    </footer>
    
    <div class="clear"></div>

        

      
    <script src="https://cythonlin.github.io/media/js/functions-post.js"></script>

    </div>
    <!-- end: #wrapper -->
  </body>
</html>
