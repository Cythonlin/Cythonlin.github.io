<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>PY =&gt; PySpark-Spark SQL | Cython_lin</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://cythonlin.github.io/favicon.ico?v=1601353247269">
<link rel="stylesheet" href="https://cythonlin.github.io/styles/main.css">



<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="Spark SQL
Spark SQL 分为三类：

SQL
DataFrame (参考pandas，但略有不同)
Datasets (由于python是动态的，所以不支持python)

初始环境：
import findspark
fi..." />
    <meta name="keywords" content="RS" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://cythonlin.github.io">
        <img src="https://cythonlin.github.io/images/avatar.png?v=1601353247269" class="site-logo">
        <h1 class="site-title">Cython_lin</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://cythonlin.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">PY =&gt; PySpark-Spark SQL</h2>
            <div class="post-date">2020-09-29</div>
            
            <div class="post-content" v-pre>
              <h1 id="spark-sql">Spark SQL</h1>
<p>Spark SQL 分为三类：</p>
<ol>
<li>SQL</li>
<li>DataFrame (参考pandas，但略有不同)</li>
<li>Datasets (由于python是动态的，所以不支持python)</li>
</ol>
<h3 id="初始环境">初始环境：</h3>
<pre><code>import findspark
findspark.init()

from pyspark.sql import SparkSession        
spark = SparkSession.builder.appName('myspark').getOrCreate()    # 初始化session
# spark.sparkContext.parallelize([1,2,3,4]).collect()  # 里面包含之前说过的sparkContext
...
中间这部分留给下面写
...
spark.stop()         # 关闭 session
</code></pre>
<p>从json导入为df:<br>
df = spark.read.json(&quot;file:///home/lin/data/user.json&quot;,multiLine=True)<br>
打印DF字段信息：<br>
df.printSchema()</p>
<pre><code>root
 |-- age: long (nullable = true)
 |-- gender: string (nullable = true)
 |-- name: string (nullable = true)
</code></pre>
<h3 id="crud">CRUD</h3>
<p>增<br>
from pyspark.sql import functions as f</p>
<pre><code># schema就相当于 pandas 指定的 columns, 双层序列，  [2x4] 的样本
df1 = spark.createDataFrame([[1,2,3,4],[5,6,7,8]],schema=['1_c','2_c', '3_c', '4_c'])
+-----+-----+-----+-----+
|1\_col|2\_col|3\_col|4\_col|
+-----+-----+-----+-----+
|    1|    2|    3|    4|
|    5|    6|    7|    8|
+-----+-----+-----+-----+

# lit可以在指定空列的时候，指定 null值， 或者 int型（里面有很多类型，可以发现）
# df2 = df1.withColumn('null_col', f.lit(None)).withColumn('digit_col', f.lit(2))
df2 = df1.withColumn('5_col', df1['4_col']+1)   # 在原来列字段基础上。
df2.show()
</code></pre>
<p>删<br>
df2 = df1.drop('age')        # 删除 age列<br>
df2 = df1.dropna()           # 删除空行<br>
df2 = df1.drop_duplicates()  # 删除重复-行<br>
改<br>
和&quot;增&quot;，差不多，只不过字段，指定为原有字段字符串即可。</p>
<p>查<br>
下面的讲的（投影、过滤、排序、分组），几乎都是查。</p>
<h3 id="投影">投影</h3>
<p>投影所有：<br>
df.show(n=20)        # 默认就是 n=20 只返回 前20条记录</p>
<pre><code>+---+------+--------+
|age|gender|    name|
+---+------+--------+
| 18|   man|zhangsan|
+---+------+--------+
</code></pre>
<p>选中某列投影：<br>
df.select('name','age').show()        # 若直接写 '*', 和直接 df.show()是一个效果</p>
<pre><code>+--------+---+
|    name|age|
+--------+---+
|zhangsan| 18|
+--------+---+
</code></pre>
<p>或者用另两种方式投影（投影过程可计算）：<br>
df.select(df['name'],df['age']+20).show() # 同上，这是另一种写法，注意一下列名<br>
df.select(df.name, df.age+20).show()      # 同上，这是另二种写法，注意一下列名</p>
<pre><code>+--------+----------+
|    name|(age + 20)|
+--------+----------+
|zhangsan|        38|
+--------+----------+
</code></pre>
<p>取出前N条DF，并转化为 [ {} , {} ] 格式<br>
df_user.take(1)  # [Row(age=18, name='张三')]<br>
df_user.head(1)  # [Row(age=18, name='张三')]<br>
df_user.first()  # Row(age=18, name='张三')    # 注意,无列表</p>
<h3 id="排序">排序</h3>
<pre><code>df_user.head(1)
df_user.sort(df_user.name.desc()).show()

# 另外说明一点, df的每个熟悉都有,一些操作符函数, desc()就是一种操作符函数
</code></pre>
<h3 id="过滤">过滤：</h3>
<pre><code>df.filter( df['age'] &gt; 15).show()

+---+------+----+
|age|gender|name|
+---+------+----+
+---+------+----+
</code></pre>
<h3 id="分组">分组：</h3>
<pre><code>df.groupBy('name').count().show()

+--------+-----+
|    name|count|
+--------+-----+
|zhangsan|    1|
+--------+-----+
</code></pre>
<h3 id="join">Join</h3>
<pre><code>df_user.join(df_user, on=df_user.name==df_user.name, how='inner').show()

+----+---+----+---+
|name|age|name|age|
+----+---+----+---+
|李四| 20|李四| 20 |
|张三| 18|张三| 18 |
+----+---+----+---+
# 特别提醒， 此 Join， 只要都进来是 DF格式的任何数据库，都可 Join
# 比如： MySQL 和 Hive  ,  Json 也可。
</code></pre>
<h3 id="储存为临时视图表-并调用sql语句">储存为临时视图（表), 并调用sql语句：</h3>
<pre><code>df.createOrReplaceTempView('user')                  # 创建为 user临时视图
df_sql = spark.sql('select * from user').show()     # spark.sql返回的还是df, 所以要show()

+---+------+--------+
|age|gender|    name|
+---+------+--------+
| 18|   man|zhangsan|
+---+------+--------+
</code></pre>
<h3 id="rdd-与-df互转">RDD 与 DF互转</h3>
<p>RDD -&gt; DF<br>
### RDD -&gt; DF 需要把RDD做成两种格式(任选其一)<br>
### 第一种 Row 格式<br>
from pyspark import Row<br>
rdd_user = spark.sparkContext.parallelize( [('张三',18), ('李四',20)] )<br>
rdd_user_row = rdd_user.map(lambda x:Row(name=x[0], age=x[1]))<br>
print(rdd_user_row.collect()) # [Row(age=18, name='张三'), Row(age=20, name='李四')]<br>
df_user = spark.createDataFrame(rdd_user_row)</p>
<pre><code>### 第二种 [('张三', 18),('李四', 20)]
    rdd_user = spark.sparkContext.parallelize( [('张三',18), ('李四',20)] )
    df_user = rdd_user.toDF(['name', 'age'])        # 给定列名
df_user.show()
</code></pre>
<p>DF -&gt; RDD<br>
rdd_row = df_user.rdd.map(lambda x: x.asDict())  # 或者 x.name, x.age取值<br>
rdd_row.collect()  # [{'age': 18, 'name': '张三'}, {'age': 20, 'name': '李四'}]</p>
<h3 id="csv读写">CSV读写</h3>
<p>从HDFS中读取(我们先新建一个CSV并扔到HDFS中),<br>
vi mydata.csv:<br>
name,age<br>
zhangsan,18<br>
lisi, 20</p>
<pre><code>hadoop fs -mkdir /data                 # 在HDFS中新建一个目录 /data
hadoop fs -put mydata.csv /data        # 并把本地 mydata.csv扔进去 (-get可拿出来)
</code></pre>
<p>在代码中读取 HDFS数据:<br>
df = spark.read.csv(&quot;hdfs:///data/mydata.csv&quot;, header=True)<br>
df.show()<br>
# header=True 代表, csv文件的第一行作为csv的抬头(列名)<br>
# df.write.csv(&quot;hdfs:///data/mydata.csv&quot;, header=True)   # read改为write就变成了写</p>
<h3 id="hive读写">Hive读写</h3>
<p>Hive的配置与依赖之前讲过了（最值得注意的是需要先启动一个 metadata的服务）<br>
先验传送门：<a href="https://segmentfault.com/a/1190000020841646">https://segmentfault.com/a/1190000020841646</a><br>
import findspark<br>
findspark.init()</p>
<pre><code>from pyspark.sql import SparkSession   

spark = SparkSession.builder\
    .appName(&quot;Spark Hive Example&quot;)\
    .master(&quot;local[*]&quot;)\
    .config(&quot;hive.metastore.uris&quot;, &quot;thrift://localhost:9083&quot;)\
    .enableHiveSupport()\
    .getOrCreate()

spark.sql(&quot;use mydatabase&quot;)        # 执行Hive 的 SQL, 切换数据库（前提你得有）
</code></pre>
<p>读：<br>
df = spark.table('person').show()  # 直接对表操作 (注意，sql语句也可)<br>
写：<br>
df = spark.table('person')<br>
df2 = df.withColumn('nickname', df.name)  # 稍微变动一下，添一个字段<br>
df2.write.saveAsTable(&quot;new_person&quot;)       # 写入新表</p>
<h3 id="mysql读写">MySQL读写</h3>
<p>读：<br>
# 注意0：有好几种方式，我只列举一个 成对的读写配置。<br>
# 注意1: url中 &quot;hive&quot;是数据库名. 你也可以起为别的名<br>
# 注意2：table的值--&quot;TBLS&quot;,  它是 MySQL中&quot;hive库&quot;中的一个表。<br>
# 注意3：特别注意！ TBLS不是我们想要的表。他只是一个大表，管理了我们hive的信息<br>
#        TBLS中的 一个列属性 &quot;TBL_NAME&quot; 才是真正我们需要的表！！</p>
<pre><code>df = spark.read.jdbc(
    url='jdbc:mysql://localhost:3306/hive?user=root&amp;password=123',
    table=&quot;TBLS&quot;,
    properties={&quot;driver&quot;:&quot;com.mysql.jdbc.Driver&quot;},
)

df.show()
df.select(&quot;TBL_NAME&quot;).show()
</code></pre>
<p>写：<br>
df.write.jdbc(<br>
url='jdbc:mysql://localhost:3306/hive?user=root&amp;password=123',<br>
table=&quot;new_table&quot;,<br>
mode='append',<br>
properties={&quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;}<br>
)</p>
<pre><code># 同是特别注意： 和读一样， 它写入的新表，也是一个整体的表结构。
#     此表的一个列&quot;TBL_NAME&quot;，它才对应了我们真正要操作的表</code></pre>

            </div>
            
              <div class="tag-container">
                
                  <a href="https://cythonlin.github.io/tag/EjFvvnhFs/" class="tag">
                    RS
                  </a>
                
              </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://cythonlin.github.io/post/py-greater-pyspark-spark-corerdd/">
                  <h3 class="post-title">
                    PY =&gt; PySpark-Spark Core（RDD）
                  </h3>
                </a>
              </div>
            

            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>






  </body>
</html>
