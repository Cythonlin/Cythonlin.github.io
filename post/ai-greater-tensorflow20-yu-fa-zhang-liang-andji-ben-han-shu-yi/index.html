<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>AI =&gt; Tensorflow2.0语法 - 张量&amp;基本函数(一) | Cython_lin</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://cythonlin.github.io/favicon.ico?v=1601375607321">
<link rel="stylesheet" href="https://cythonlin.github.io/styles/main.css">



<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="前言
TF2.0 是之前学习的内容，当时是写在了私有的YNote中，重写于SF。
TF2.0-GPU 安装教程传送门：https://segmentfault.com/a/1190000020304638
之前接触过 TF1, 手动sess..." />
    <meta name="keywords" content="AI" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://cythonlin.github.io">
        <img src="https://cythonlin.github.io/images/avatar.png?v=1601375607321" class="site-logo">
        <h1 class="site-title">Cython_lin</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://cythonlin.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">AI =&gt; Tensorflow2.0语法 - 张量&amp;基本函数(一)</h2>
            <div class="post-date">2020-09-29</div>
            
            <div class="post-content" v-pre>
              <h1 id="前言">前言</h1>
<p>TF2.0 是之前学习的内容，当时是写在了私有的YNote中，重写于SF。<br>
TF2.0-GPU 安装教程传送门：https://segmentfault.com/a/1190000020304638<br>
之前接触过 TF1, 手动session机制，看着很是头疼。 TF2.0不需要做这些<br>
TF2.0 理解起来更容易（逐渐 Pythonic and Numpic）<br>
TF2.0 后端采用keras接口 (构建网络层)，更方便。<br>
TF2.0 的keras接口定义的模型层，都实现了 call方法。意味大多数实例对象可以当作函数来直接调用</p>
<h1 id="行列轴">行列轴</h1>
<p>以列表为例（抽象举例，摞起来的面包片。。。。）<br>
[               # 最外层，无意义不用记<br>
[1,2,3],    # 面包片1  (第一个样本)<br>
[4,5,6],    # 面包片2  (第二个样本)<br>
]</p>
<ol>
<li>每个 次内层列表 代表一个样本， 比如 [1,2,3] 整体代表 第一个样本</li>
<li>最内层元素代表属性值。  eg:  1,2,3 单个拿出来都是属性值。</li>
<li>例子： 元素5 单独拿出来，它就被看做  &quot;第二个样本的，属性值5&quot;     （当然横纵索引依然都是从0取的）</li>
</ol>
<p>以刚才的数据为例：<br>
t = tf.constant(<br>
[<br>
[1., 2., 3.],<br>
[4., 5., 6.]<br>
]<br>
)</p>
<pre><code>print(tf.reduce_sum(t, axis=0))  # 求和操作，上下压扁， 聚合样本
&gt;&gt; tf.Tensor([5. 7. 9.], shape=(3,), dtype=float32) 

print(tf.reduce_sum(t, axis=1))  # 求和操作，左右压扁， 聚合属性
&gt;&gt; tf.Tensor([ 6. 15.], shape=(2,), dtype=float32)   
</code></pre>
<p>注：Numpy轴也是这样的，我最初用x，y轴方式 抽象 去记忆， 基本上是记不住的。。太多概念混淆。<br>
但如果你记不住，你每次使用各种操作和聚合API时，都会自己在心理重新花大量时间理一遍。浪费时间。</p>
<blockquote>
<p>所以你一定要练习理解，要做到：“瞄一眼，就能知道这种维度的数据的意义，以及轴操作的意义”</p>
</blockquote>
<p>我自己的记忆方式（axis=0, axis=1）：</p>
<ol>
<li>
<p>0轴通常代表，样本（上下压扁）</p>
</li>
<li>
<p>1轴通常代表，属性（左右压扁）<br>
常需要用 axis参数 的相关聚合函数：<br>
tf.reduce_sum()  # 求和<br>
tf.reduce_mean() # 平均值<br>
tf.reduce_max()  # 最大值<br>
tf.reduce_min()  # 最小值<br>
tf.square()      # 平方<br>
tf.concat()      # 拼接</p>
<p>注： 如果 axis参数 &quot;不传&quot;， 那么&quot;所有维度&quot;都会被操作。</p>
</li>
</ol>
<h1 id="常用导入">常用导入</h1>
<pre><code># 基本会用到的
import numpy as np
import tensorflow as tf
from tensorflow import keras

# 可选导入
import os, sys, pickle

import scipy
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler      # 标准化
from sklearn.model_selection import train_test_split  # 训测分离
</code></pre>
<h1 id="张量操作符">张量&amp;操作符</h1>
<h2 id="常量普通的张量">常量（普通的张量）</h2>
<p>###定义：<br>
c = tf.constant( [[1., 2., 3.], [4., 5., 6.]] )    # 数字后面加个点代表 转float32 类型<br>
print(c)<br>
&gt;&gt; tf.Tensor([[1. 2. 3.] [4. 5. 6.]], shape=(2, 3), dtype=float32)<br>
###六则运算（加减乘除，矩阵乘, 矩阵转置）<br>
先说矩阵乘法（大学都学过的，运算过程不说了）：<br>
语法格式：  a @ b<br>
条件要求：  a的列数 === b的行数     （必须相等）<br>
eg:       （5行2列 @ 2行10列 = 5行10列）</p>
<pre><code>特例： (第0维度，必须相等)
    t1 = tf.ones([2, 20, 30])
    t2 = tf.ones([2, 30, 50])
    print( (t1@t2).shape )
    &gt;&gt; (2, 20, 50)   # 第0维没变， 后2维照常按照矩阵乘法运算
</code></pre>
<p>矩阵转置：<br>
tf.transpose(t)<br>
# 不仅可以普通转置，还可以交换维度<br>
t2 = tf.transpose(t,[1,0])    # 行变列，列变行。 和基本的转置差不多（逆序索引，轴变逆序）</p>
<pre><code># 或假如以 （2,100，200，3）形状 为例
t = tf.ones([2, 100, 200, 3])                 
print(tf.transpose(t, [1, 3, 0, 2]).shape) # 轴交换位置
&gt;&gt; (100, 3, 2, 200)
# 原1轴 -&gt; 放在现在0轴
# 原3轴 -&gt; 放在现在1轴
# 原0轴 -&gt; 放在现在2轴
# 原2轴 -&gt; 放在现在3轴
</code></pre>
<p>加减乘除都具有&quot;广播机制&quot; ：<br>
形象（广播机制解释）解释：<br>
我尝试用白话解释下：<br>
1. 我形状和你不一样， 但我和你运算的时候，我会尽力扩张成 你的形状  来和你运算。<br>
2. 扩张后如果出现空缺， 那么把自己复制一份，填补上 （如果补不全，就说明不能运算）<br>
3. 小形状 服从 大形状 (我比你瘦，我动就行。 你不用动。。。)<br>
eg:<br>
t = tf.constant(<br>
[<br>
[1, 2, 3],<br>
[4, 5, 6],<br>
]<br>
)<br>
t + [1,2,1]<br>
过程分析：<br>
[1,2,1] 显然是 小形状， 它会自动尝试变化成大形状 -&gt;<br>
第一步变形（最外层大框架满足， 里面还有空缺）：<br>
[<br>
[1,2,1],<br>
]<br>
第二步变形 (把自己复制，然后填补空缺)：<br>
[<br>
[1,2,1],<br>
[1,2,1],          # 这就是复制的自己<br>
]<br>
第三步运算（逐位相加）<br>
[             +     [              =  [<br>
[1,2,3],            [1,2,1],          [2,4,4],<br>
[4,5,6],            [1,2,1],          [5,7,7],<br>
]                   ]                 [<br>
抽象（广播机制）演示：<br>
假如 t1 的 shape为 [5,200,100,50]<br>
假如 t2 的 shape为 [5,200]</p>
<pre><code>注意：我以下的数据演示，全是表示 Tensor的形状，形状，形状！
    [5,200,1,50]     # 很明显，开始这2行数据 维度没匹配， 形状也没对齐
           [5,1]
------------------------            
    [5,200,1,50]    
          [5,50]     # 这行对齐补50
------------------------            
    [5,200,5,50]     # 这行对齐补5
          [5,50]
------------------------            
    [5,200,5,50]           
    [1, 1, 5,50]     # 这行扩张了2个， 默认填1
------------------------            
    [5,200,5,50]           
    [1,200, 5,50]    # 这行对齐补200
------------------------            
    [5,200,5,50]           
    [5,200,5,50]     # 这行对齐补5

注意：
    1. 每个维度形状：二者必须有一个是1， 才能对齐。 （不然ERROR，下例ERROR-&gt;）
        [5,200,1,50]
              [5,20]    # 同理开始向右对齐，但是  50与20都不是1，所以都不能对齐，所以ERROR
    2. 若维度缺失：
        依然是全部贴右对齐
        然后先从右面开始，补每个维度的形状 
        然后扩展维度，并默认设形状为1
        然后补扩展后维度的形状（因为默认设为1了，所以是一定可以补齐的）
</code></pre>
<p>当然上面说的都是运算时的自动广播机制<br>
你也可以手动广播：<br>
t1 = tf.ones([2, 20, 1])                # 原始形状  【2，20，1】<br>
print(tf.broadcast_to(t1, [5,2,20,30]).shape) # 目标形状【5，2，20，30】</p>
<pre><code>[5,2,20,30]
  [2,20, 1]
-----------
[5,2,20,30]
  [2,20,30]
-----------
[5,2,20,30] 
[1,2,20,30]
-----------
[5,2,20,30]
[5,2,20,30]

注：因为是手动广播，所以只能 原始形状 自己向 目标形状 ”补充维度，或者补充形状“
    而目标形状是一点也不能动的。
</code></pre>
<p>扩充维度（f.expand_dims）+ 复制（tile） 代替 =&gt; 广播（tf.broadcasting）<br>
同样是上面的例子，我想把形状  [2,20,1]  ，变成 [5，2，20，30]<br>
t1 = tf.ones([2, 20, 1])<br>
a = tf.expand_dims(t1,axis=0)  # 0轴索引处插入一个轴， 结果[1,2,20,1]<br>
print(tf.tile(a,[5,1,1,30]).shape)    # 结果 [5, 2, 20, 30]<br>
流程：<br>
[5,2,20,30]<br>
[2,20,1]<br>
-----------<br>
[5,2,20,30]    # tf.expand_dims(t1,axis=0)<br>
[1,2,20,1]     # 0号索引插入一个新轴（增维）<br>
-----------<br>
[5,2,20,30]    # tf.tile(5,1,1,30)   (形状对齐，tile每个参数代表对应轴的形状扩充几倍)<br>
[5,2,30,30]         1<em>5 2</em>1 20<em>1 1</em>30<br>
tile 与 broadcasting的区别：</p>
<ol>
<li>tile是物理复制，物理空间增加</li>
<li>而broadcasting是虚拟复制，（为了计算，隐式实现的复制，并没有物理空间增加）</li>
<li>tile可以对任意（整数倍复制n*m,  mn同为整数）</li>
<li>而broadcasting（原始数据形状只能存在1的情况下才能扩张。   1*n   , n为整数）<br>
压缩维度（tf.squeeze）：<br>
就是把每个维度为1的维度都删除掉   （就像数学  a * 1 = a）<br>
print(tf.squeeze(tf.ones([2,1,3,1])).shape)
<blockquote>
<blockquote>
<blockquote>
<p>(2, 3)</p>
</blockquote>
</blockquote>
</blockquote>
</li>
</ol>
<p>当然你也可以指定维度压缩（默认不指定，所有维度为1的全部压缩）：<br>
print(tf.squeeze(tf.ones([2,1,3,1]), axis=-1).shape)<br>
&gt;&gt;&gt; (2, 1, 3)</p>
<h3 id="索引切片">索引&amp;切片</h3>
<p>灵魂说明：无论索引还是切片， （行列 是使用  逗号  分隔的）， 并且无论行列，索引都是从0开始的。<br>
索引：取一个值<br>
print(t[1,2])  # 逗号前面代表行的索引， 逗号后面是列的索引<br>
&gt;&gt; tf.Tensor(6.0, shape=(), dtype=float32)<br>
切片：取子结构 （有两种方式）<br>
方式1（冒号切片）：<br>
print(t[:, 1:])  # 逗号前面是行。只写: 代表取所有行。逗号后面是列。 1: 代表第二列到最后<br>
&gt;&gt; tf.Tensor([[2. 3.] [5. 6.]], shape=(2, 2), dtype=float32)</p>
<p>方式2（省略号切片）: （我相信不了解Numpy的人都没听说过 python的 Ellipsis ， 就是省略号类）<br>
先自己去运行玩玩这行代码：<br>
print(... is Ellipsis)<br>
&gt;&gt;&gt; True</p>
<p>回到正题：（省略号 ... 切片，是针对多维度的， 如果是二维直接用:即可）<br>
(我们以三维为例，这个就不适合称作行列了)<br>
# shape 是 (2, 2, 2)<br>
t = tf.constant(<br>
[   # 一维<br>
[   # 二维<br>
[1, 2], # 三维<br>
[3, 4],<br>
],<br>
[<br>
[5, 6],<br>
[7, 8],<br>
],<br>
]<br>
)<br>
伪码：t[1维切片, 二维切片, 三维切片]<br>
代码：t[:, :, 0:1]     # 1维不动， 2维不动， 3维 取一条数据<br>
结果: shape为 (2,2,1)<br>
[   # 一维<br>
[   # 二维<br>
[1], # 三维<br>
[3],<br>
],<br>
[<br>
[5],<br>
[7],<br>
],<br>
]<br>
看不明白就多看几遍。<br>
发现没，即使我不对 1维，和 2维切片，我也被迫要写 2个: 来占位<br>
那假如有100个维度，我只想对最后一个维度切片。 前99个都不用动， 那难道我要写 99个 : 占位？？<br>
不，如下代码即可解决：<br>
print(t[..., 0:1])       # 这就是 ... 的作用 （注意，只在 numpy 和 tensorflow中有用）<br>
tensor 转 numpy 类型<br>
t.numpy()    # tensor 转为 numpy 类型</p>
<h2 id="变量">变量</h2>
<p>定义：<br>
v = tf.Variable(   # 注意： V是大写<br>
[<br>
[1, 2, 3],<br>
[4, 5, 6]<br>
]<br>
)<br>
变量赋值(具有自身赋值的性质)：<br>
注意： 变量一旦被定义，形状就定下来了。 赋值（只能赋给同形状的值）</p>
<pre><code>v.assign(
    [
        [1,1,1], 
        [1,1,1],
    ]
)
print(v)
&gt;&gt; &lt;tf.Variable 'Variable:0' shape=(2, 3) dtype=int32, numpy=array([[1, 1, 1],[1, 1, 1]])&gt;
</code></pre>
<p>变量取值（相当于转换为Tensor）：<br>
特别： 变量本身就是 Variable类型， 取值取出得是 Tensor （包括切片取值，索引取值等）<br>
print( v.value() )<br>
&gt;&gt; tf.Tensor([[1 2 3] [4 5 6]], shape=(2, 3), dtype=int32)<br>
变量 索引&amp;切片 赋值：<br>
常量:是不可变的。所以只有取值，没有赋值。<br>
变量:取值、赋值都可以<br>
v.assign(xx)  类似于 python的 v=xx</p>
<pre><code>v[0, 1].assign(100)          # 索引赋值,  v.assign 等价于
v[0, :].assign([10, 20, 30]) # 注意，切片赋值传递的需要是容器类型

特别注意： 前面说过，变量 结构形状 是 不可变的，赋值的赋给的是数据。
           但是你赋值的时候要时刻注意，不能改变变量原有形状
拿切片赋值为例：
    你切多少个，你就得赋多少个。 并且赋的值结构要一致。
    举个栗子： 你从正方体里面挖出来一个小正方体。那么你必须填补一块一模一样形状的小正方体）
    
还有两种扩展API：
    v.assign_add()  # 类似python 的 +=
    v.assign_sub()  # 类似python 的 -=
</code></pre>
<p>变量 索引&amp;切片 取值<br>
同 常量切片取值（略）<br>
Variable 转 Numpy<br>
print(v.numpy())</p>
<h2 id="不规则张量raggedtensor">不规则张量（RaggedTensor）</h2>
<p>定义：<br>
rag_tensor = tf.ragged.constant(<br>
[<br>
[1,2],<br>
[2,3,4,5],<br>
]<br>
)    # 允许每个维度的数据长度参差不齐<br>
拼接：假如需要&quot;拼接 不规则张量&quot; （可使用 tf.concat(axis=) ）<br>
0轴：竖着拼接（样本竖着摞起来）可随意拼接。 拼接后的依然是&quot;不规则张量&quot;<br>
1轴：横着拼接（属性水平拼起来）这时候需要你样本个数必须相等， 否则对不上，报错<br>
总结： 样本竖着随便拼， 属性横着（必须样本个数相等） 才能拼<br>
RaggedTensor 普通 Tensor：<br>
说明：普通Tensor是必须要求， 长度对齐的。入 对不齐的 末尾补0<br>
tensor = rag_tensor.to_tensor()</p>
<h2 id="稀疏张量-sparse-tensor">稀疏张量 （Sparse Tensor）</h2>
<p>特点（可理解为 记录索引）：</p>
<ol>
<li>
<p>只记录非0的坐标位置， indices参数：每个 子列表 表示 一个坐标</p>
</li>
<li>
<p>虽然只记录坐标，但是转为普通Tensor后，只有坐标位置 有值， 其他位置的值全是0</p>
</li>
<li>
<p>填充范围，取决于 dense_shape的设定<br>
定义：<br>
s = tf.SparseTensor(<br>
indices=[[0, 1], [1, 0], [2, 3]],  # 注意，这个索引设置需要是（从左到右，从上到下）的顺序设置<br>
values=[1, 2, 3],   # 将上面3个坐标值分别设值为 1，2，3<br>
dense_shape=[3, 4]  # Tensor总范围<br>
)<br>
print(s)</p>
<blockquote>
<blockquote>
<p>SparseTensor(indices=tf.Tensor([[0 1], [1 0],[2 3]], shape=(3, 2), dtype=int64)。。。<br>
转为普通 Tensor (转为普通Tensor后，看见的才是存储真正的值)<br>
tensor = tf.sparse.to_dense(s)<br>
print(tensor)</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>tf.Tensor([ [0 1 0 0],[2 0 0 0],[0 0 0 3] ], shape=(3, 4), dtype=int32)<br>
如果上面使用 to_dense() 可能会遇到错误：<br>
error: is out of range</p>
</blockquote>
</blockquote>
<p>这个错误的原因是创建 tf.SparseTensor(indices=) ，前面也说了indices，要按（从左到右，从上到下）顺序写<br>
当然你也可以用排序API，先排序，然后再转：<br>
eg:<br>
_ = tf.sparse.reorder(s)        # 先将索引排序<br>
tensor = tf.sparse.to_dense(_)  # 再转</p>
</li>
</ol>
<h1 id="tffunction">tf.function</h1>
<p>这个API作为一个装饰器使用， 用来将 Python 语法转换 尽可能有效的转换为 TF语法、图结构<br>
import tensorflow as tf<br>
import numpy as np</p>
<pre><code>@tf.function
def f():
    a = np.array([1, 2, 3])
    b = np.array([4, 5, 6])
    return a + b
print( f() )
&gt;&gt;&gt; tf.Tensor([5 7 9], shape=(3,), dtype=int32)
</code></pre>
<blockquote>
<p>你应该发现了一个特点，我们定义的 f()函数内部，一个tf语法都没写。 只装饰了一行 @tf.function<br>
而调用结果返回值居然是个  tensor 。<br>
这就是 @tf.function 装饰器的作用了！</p>
</blockquote>
<p>当然函数里面，也可以写 tf的操作，也是没问题的。<br>
但注意一点， 函数里面不允许定义 变量， 需要定义的变量 应 拿到函数外面定义<br>
a = tf.Variable([1,2,3])  # 如需tensor变量，应该放在外面</p>
<pre><code>@tf.function
def f():
    # a = tf.Variable([1,2,3])  # 这里面不允许定义变量!
    pass
</code></pre>
<h1 id="合并相加tfconcat">合并相加（tf.concat）</h1>
<p>我的理解就是（基本数学的 合并同类项）<br>
# 合并同类项的原则就是有1项不同，其他项完全相同。<br>
# 前提条件：（最多，有一个维度的形状不相等。   注意是最多）<br>
t1 = tf.ones([2,5,6])<br>
t2 = tf.ones([6,5,6])<br>
print( tf.concat([t1,t2],axis=0).shape )   # axis=0,传了0轴，那么其他轴捏死不变。只合并0轴<br>
&gt;&gt; (8,5,8)</p>
<h1 id="堆叠降维tfstack">堆叠降维（tf.stack）</h1>
<p>我的理解就是（小学算术的，进位，（进位就是扩充一个维度表示个数））<br>
# 前提条件：所有维度形状必须全部相等。<br>
tf1 = tf.ones([2,3,4])<br>
tf2 = tf.ones([2,3,4])<br>
tf3 = tf.ones([2,3,4])<br>
print(tf.stack([tf1,tf2,tf3], axis=0).shape)<br>
# 你可以想象有3组 [2,3,4]，然后3组作为一个新维度，插入到 axis对应的索引处。<br>
&gt;&gt; (3, 2, 3, 4)        # 对比理解，如果这是tf.concat(), 那么结果就是 (6,3,4)</p>
<h1 id="拆分降维tfunstack">拆分降维（tf.unstack）</h1>
<p>和tf.stack正好是互逆过程，指定axis维度是几，它就会拆分成几个数据，同时降维。<br>
a = tf.ones([3, 2, 3, 4])<br>
for x in tf.unstack(a, axis=0):<br>
print(x.shape)</p>
<pre><code>结果如下（分成了3个 [2,3,4]）
&gt;&gt;&gt; (2, 3, 4)
&gt;&gt;&gt; (2, 3, 4)
&gt;&gt;&gt; (2, 3, 4)
</code></pre>
<h1 id="拆分不降维tfsplit">拆分不降维（tf.split）</h1>
<p>###语法：<br>
和tf.unstack的区别就是，tf.unstack是均分降维，  tf.stack是怎么分都不会降维，且能指定分隔份数<br>
a = tf.ones([2,4,35,8])<br>
for x in tf.split(a, axis=3,num_or_size_splits=[2,2,4]):<br>
print(x.shape)</p>
<pre><code>结果： 
    &gt;&gt; (2, 4, 35, 2)  # 最后一维2
    &gt;&gt; (2, 4, 35, 2)  # 最后一维2
    &gt;&gt; (2, 4, 35, 4)  # 最后一维4 
</code></pre>
<p>###使用场景：<br>
假如我们想切分数据集为（train-test-valid） 3分比例为 6：2：2<br>
方法1：（scikit-learn 连续分切2次）<br>
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)<br>
x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train,test_size=0.2)<br>
# 源码中显示  test_size如果不传。默认为0.25。<br>
# 思路，因为 scikit-learn 只能切出2个结果： 所以我们需要切2次：<br>
# 第一次 从完整训练集 切出来 （剩余训练集， 测试集）<br>
# 第二次 从剩余数据集 切出来 （剩余训练集2， 验证集）<br>
方法2： （tf.split）<br>
x = tf.ones([1000, 5000])<br>
y = tf.ones([1000, 1])</p>
<pre><code>x_train, x_test, x_valid = tf.split(
    x, 
    num_or_size_splits=[600,200,200],        # 切3份
    axis=0
)
y_train, y_test, y_valid = tf.split(
    y,
    num_or_size_splits=[600,200,200],       #  同样切3份
    axis=0
)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)
print(x_valid.shape, y_valid.shape)

结果
&gt;&gt;&gt;  (600, 5000) (600, 1)
&gt;&gt;&gt;  (200, 5000) (200, 1)
&gt;&gt;&gt;  (200, 5000) (200, 1)
</code></pre>
<h1 id="高级索引tfgather">高级索引（tf.gather）</h1>
<p>numpy这种索引叫做 fancy indexing(如果我没记错的话)<br>
data = tf.constant([6,7,8])        # 当作真实数据<br>
index = tf.constant([2, 1, 0])     # 当作索引<br>
print(tf.gather(data, index))<br>
&gt;&gt; tf.Tensor([8 7 6], shape=(3,), dtype=int32)</p>
<h1 id="排序tfsort">排序（tf.sort）</h1>
<pre><code>data = tf.constant([6, 7, 8])
print(tf.sort(data, direction='DESCENDING')) # 'ASCENDING'
# 默认是ASCENDING升序

tf.argsort()    # 同上， 只不过返回的是排序后的，对应数据的index
</code></pre>
<h1 id="top-ktfmathtop_k">Top-K(tf.math.top_k)</h1>
<p>查找出最大的n个（比先排序然后切片的性能要好）<br>
a = tf.math.top_k([6,7,8],2)   # 找出最大的两个，返回是个对象<br>
print(a.indices)    # 取出最大的两个 索引 （）<br>
print(a.values)     # 取出最大的两个 值<br>
&gt;&gt; tf.Tensor([2 1], shape=(2,), dtype=int32)<br>
&gt;&gt; tf.Tensor([8 7], shape=(2,), dtype=int32)</p>
<h1 id="tfgradienttape-自定义求导">tf.GradientTape （自定义求导）</h1>
<h3 id="求偏导">求偏导</h3>
<pre><code>v1, v2 = tf.Variable(1.), tf.Variable(2.) # 变量 会 被自动侦测更新的
c1, c2 = tf.constant(1.), tf.constant(2.) # 常量 不会 自动侦测更新

y = lambda x1,x2: x1**2 + x2**2

with tf.GradientTape(persistent=True) as tape:
    &quot;&quot;&quot;默认这个 tape使用一次就会被删除， persistent=True 代表永久存在，但后续需要手动释放&quot;&quot;&quot;
    # 因为常量不会被自动侦测，所以我们需要手动调用 watch() 侦测
    tape.watch(c1) # 如果是变量，就不用watch这两步了
    tape.watch(c2)
    f = y(c1,c2)   # 调用函数，返回结果
c1_, c2_ = tape.gradient(f, [c1,c2])  
    # 参数2:传递几个自变量，就会返回几个 偏导结果
    # c1_ 为 c1的偏导
    # c2_ 为 c2的偏导
del tape  # 手动释放 tape
</code></pre>
<h3 id="求二阶偏导gradient嵌套">求二阶偏导（gradient嵌套）</h3>
<pre><code>v1, v2 = tf.Variable(1.), tf.Variable(2.) # 我们使用变量

y = lambda x1,x2: x1**2 + x2**2
with tf.GradientTape(persistent=True) as tape2:
    with tf.GradientTape(persistent=True) as tape1:
        f = y(v1,v2)
    once_grads = tape1.gradient(f, [v1, v2])       # 一阶偏导        
# 此列表推导式表示：拿着一阶偏导，来继续求二阶偏导（注意，用tape2）
twice_grads = [tape2.gradient(once_grad, [v1,v2]) for once_grad in once_grads] # 二阶偏导

print(twice_grads)
del tape1    # 释放
del tape2    # 释放
</code></pre>
<h3 id="说明">说明</h3>
<pre><code>求导数（一个自变量）：tape1.gradient(f, v1)       # gradient传 1个自变量
求偏导（多个自变量）：tape1.gradient(f, [v1,v2])  # gradient传 1个列表， 列表内填所有自变量
</code></pre>
<h3 id="sgd随机梯度下降">SGD（随机梯度下降）</h3>
<p>方式1：手撕（不使用优化器）<br>
v1, v2 = tf.Variable(1.), tf.Variable(2.)  # 我们使用变量<br>
y = lambda x1, x2: x1 ** 2 + x2 ** 2       # 二元二次方程</p>
<pre><code>learning_rate = 0.1                        # 学习率
for _ in range(30):                        # 迭代次数
    with tf.GradientTape() as tape:        # 求导作用域
        f = y(v1,v2)
    d1, d2 = tape.gradient(f, [v1,v2])     # 求导， d1为 v1的偏导，  d2为v2的偏导
    v1.assign_sub(learning_rate * d1)
    v2.assign_sub(learning_rate * d2)
print(v1)
print(v2)

实现流程总结：
    1. 偏导 自变量v1,v2求出来的。    （d1, d2 = tape.gradient(f, [v1,v2])）
    2. 自变量v1,v2的衰减 是关联 偏导的（ 衰减值 = 学习率*偏导）
    3. 我们把前2步套了一个大循环（并设定迭代次数），  1-2-1-2-1-2-1-2-1-2 步骤往复执行
</code></pre>
<p>方式2：借用 Tensorflow 优化器(optimizer) 实现梯度下降<br>
v1, v2 = tf.Variable(1.), tf.Variable(2.)  # 我们使用变量<br>
y = lambda x1, x2: x1 ** 2 + x2 ** 2       # 二元二次函数 ,  通常这个函数我们用作计算loss</p>
<pre><code>learning_rate = 0.1                        # 学习率
optimizer = keras.optimizers.SGD(learning_rate=learning_rate)    # 初始化优化器
for _ in range(30):                        # 迭代次数
    with tf.GradientTape() as tape:
        f = y(v1,v2)
    d1, d2 = tape.gradient(f, [v1,v2])     # d1为 v1的偏导，  d2为v2的偏导

    optimizer.apply_gradients(             # 注意这里不一样了，我们之前手动衰减
        [                                  # 而现在这些事情， optimizer.SGD帮我们做了
            (d1, v1),                      # 我们只需把偏导值，和自变量按这种格式传给它即可
            (d2, v2),
        ]
    )                                      
    # 通常这种格式，我们用 zip() 实现
    # eg:
        # model = keras.models.Sequential([......])
        # .......
        # grads = tape.gradient(f, [v1,v2])
        # optimizer.apply_gradients(
        #     zip(grads, model.trainable_variables)
        # )
print(v1)
print(v2)

实现流程总结：
1. 偏导 是自变量v1,v2求出来的    （d1, d2 = tape.gradient(f, [v1,v2])） # 此步骤不变
2. 把偏导 和 自变量 传给optimizer.apply_gradients()  optimizer.SGD() 自动帮我们衰减。
3. 我们还是把前2步套了一个大循环（并设定迭代次数），  1-2-1-2-1-2-1-2-1-2 步骤往复执行。

注： 假如你用adam等之类的其他优化器，那么可能有更复杂的公式，如果我们手撕，肯能有些费劲。
     这时候我们最好使用 optimizer.Adam ...等各种 成品，优化器。通用步骤如下
     1. 先实例化出一个优化器对象
     2. 实例化对象.apply_gradients([(偏导,自变量)])</code></pre>

            </div>
            
              <div class="tag-container">
                
                  <a href="https://cythonlin.github.io/tag/n16me-6oV/" class="tag">
                    AI
                  </a>
                
              </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://cythonlin.github.io/post/ai-greater-tensorflow20stablejiang-lin/">
                  <h3 class="post-title">
                    AI =&gt; Tensorflow2.0（stable）降临
                  </h3>
                </a>
              </div>
            

            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>






  </body>
</html>
