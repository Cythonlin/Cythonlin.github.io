<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>RS =&gt; 推荐系统（三）离线召回与排序  | Cython_lin</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://cythonlin.github.io/favicon.ico?v=1601375607321">
<link rel="stylesheet" href="https://cythonlin.github.io/styles/main.css">



<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="召回设计
召回排序流程
匿名用户：
通常使用用户冷启动方案，区别在于user_id为匿名用户手机识别号(黑马头条不允许匿名用户)
所有只正针对于登录用户：

用户冷启动（前期点击行为较少情况）
非个性化推荐
热门召回：自定义热门规则，根据当..." />
    <meta name="keywords" content="RS" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://cythonlin.github.io">
        <img src="https://cythonlin.github.io/images/avatar.png?v=1601375607321" class="site-logo">
        <h1 class="site-title">Cython_lin</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://cythonlin.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">RS =&gt; 推荐系统（三）离线召回与排序 </h2>
            <div class="post-date">2020-09-29</div>
            
            <div class="post-content" v-pre>
              <h1 id="召回设计">召回设计</h1>
<h3 id="召回排序流程">召回排序流程</h3>
<h4 id="匿名用户">匿名用户：</h4>
<pre><code>通常使用用户冷启动方案，区别在于user_id为匿名用户手机识别号(黑马头条不允许匿名用户)
所有只正针对于登录用户：
</code></pre>
<h4 id="用户冷启动前期点击行为较少情况">用户冷启动（前期点击行为较少情况）</h4>
<p>非个性化推荐<br>
热门召回：自定义热门规则，根据当前时间段热点定期更新维护人点文章库<br>
新文章召回：为了提高新文章的曝光率，建立新文章库，进行推荐<br>
个性化推荐：<br>
基于内容的协同过滤在线召回：基于用户实时兴趣画像相似的召回结果用于首页的个性化推荐</p>
<h4 id="后期离线部分用户点击行为较多用户画像完善">后期离线部分（用户点击行为较多，用户画像完善）</h4>
<p>建立用户长期兴趣画像（详细）：包括用户各个维度的兴趣特征<br>
训练排序模型<br>
LR模型、FTRL、Wide&amp;Deep<br>
离线部分的召回：<br>
基于模型协同过滤推荐离线召回：ALS<br>
基于内容的离线召回：或者称基于用户画像的召回</p>
<h3 id="召回表设计与模型召回">召回表设计与模型召回</h3>
<h4 id="召回表设计">召回表设计</h4>
<p>我们的召回方式有很多种。<br>
多路召回结果存储模型召回 与 内容召回的结果 需要进行相应频道推荐合并。<br>
方案：基于模型与基于内容的召回结果存入同一张表，避免多张表进行读取处理<br>
由于HBASE有多个版本数据功能存在的支持<br>
TTL=&gt;7776000, VERSIONS=&gt;999999<br>
如下：<br>
create 'cb_recall', {NAME=&gt;'als', TTL=&gt;7776000, VERSIONS=&gt;999999}<br>
alter 'cb_recall', {NAME=&gt;'content', TTL=&gt;7776000, VERSIONS=&gt;999999}<br>
alter 'cb_recall', {NAME=&gt;'online', TTL=&gt;7776000, VERSIONS=&gt;999999}</p>
<pre><code># 例子（多版本）：
put 'cb_recall', 'recall:user:5', 'als:1',[45,3,5,10]
put 'cb_recall', 'recall:user:5', 'als:1',[289,11,65,52,109,8]
put 'cb_recall', 'recall:user:5', 'als:2',[1,2,3,4,5,6,7,8,9,10]
put 'cb_recall', 'recall:user:2', 'content:1',[45,3,5,10,289,11,65,52,109,8]
put 'cb_recall', 'recall:user:2', 'content:2',[1,2,3,4,5,6,7,8,9,10]


hbase(main):084:0&gt; desc 'cb_recall'
Table cb_recall is ENABLED                                                                             
cb_recall                                                                                              
COLUMN FAMILIES DESCRIPTION                                                                            
{NAME =&gt; 'als', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false'
, KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 
'7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE
_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_
OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                    
{NAME =&gt; 'content', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'fa
lse', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL
          =&gt; '7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', C
ACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS
_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                
{NAME =&gt; 'online', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'fal
se', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL 
=&gt; '7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CA
CHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_
ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                 
3 row(s)	
</code></pre>
<p>（几乎不用）在HIVE用户数据数据库下建立HIVE外部表,若hbase表有修改，则进行HIVE 表删除更新<br>
create external table cb_recall_hbase(<br>
user_id STRING comment &quot;userID&quot;,<br>
als map&lt;string, ARRAY<BIGINT>&gt; comment &quot;als recall&quot;,<br>
content map&lt;string, ARRAY<BIGINT>&gt; comment &quot;content recall&quot;,<br>
online map&lt;string, ARRAY<BIGINT>&gt; comment &quot;online recall&quot;)<br>
COMMENT &quot;user recall table&quot;<br>
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'<br>
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,als:,content:,online:&quot;)<br>
TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;cb_recall&quot;);</p>
<h4 id="增加一个历史召回结果表">增加一个历史召回结果表</h4>
<pre><code>create 'history_recall', {NAME=&gt;'channel', TTL=&gt;7776000, VERSIONS=&gt;999999}

put 'history_recall', 'recall:user:5', 'als:1',[1,2,3]
put 'history_recall', 'recall:user:5', 'als:1',[4,5,6,7]
put 'history_recall', 'recall:user:5', 'als:1',[8,9,10]
</code></pre>
<p>为什么增加历史召回表？<br>
1、直接在存储召回结果部分进行过滤，比之后排序过滤，节省排序时间<br>
2、防止Redis缓存没有消耗完，造成重复推荐，从源头进行过滤</p>
<h3 id="基于模型召回集合计算">基于模型召回集合计算</h3>
<h4 id="als模型推荐实现">ALS模型推荐实现</h4>
<p>步骤：<br>
1、数据类型转换,clicked以及用户ID与文章ID处理<br>
2、ALS模型训练以及推荐<br>
3、推荐结果解析处理<br>
4、推荐结果存储<br>
数据类型转换,clicked( bool 转 int)<br>
ur.spark.sql(&quot;use profile&quot;)<br>
user_article_click = ur.spark.sql(&quot;select * from user_article_basic&quot;).<br>
select(['user_id', 'article_id', 'clicked'])<br>
# 更换类型<br>
def change_types(row):<br>
return row.user_id, row.article_id, int(row.clicked)</p>
<pre><code>user_article_click = user_article_click.rdd.map(change_types).toDF(['user_id', 'article_id', 'clicked'])
</code></pre>
<p>这步处理结果格式如下：<br>
user_id	article_id	clicked<br>
0<br>
1<br>
用户ID与文章ID处理，编程ID索引（原用户ID和文章ID是长字符串，ALS模型不能处理，要重新编排ID索引）<br>
from pyspark.ml.feature import StringIndexer<br>
from pyspark.ml import Pipeline<br>
# 用户和文章ID超过ALS最大整数值，需要使用StringIndexer进行转换<br>
user_id_indexer = StringIndexer(inputCol='user_id', outputCol='als_user_id')<br>
article_id_indexer = StringIndexer(inputCol='article_id', outputCol='als_article_id')<br>
pip = Pipeline(stages=[user_id_indexer, article_id_indexer])<br>
pip_fit = pip.fit(user_article_click)<br>
als_user_article_click = pip_fit.transform(user_article_click)<br>
ALS 模型训练与推荐（ALS模型需要输出用户ID列，文章ID列以及点击列）<br>
from pyspark.ml.recommendation import ALS<br>
# 模型训练和推荐默认每个用户固定文章个数<br>
als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1)<br>
model = als.fit(als_user_article_click)<br>
recall_res = model.recommendForAllUsers(100)<br>
结果：<br>
als_user_id	recommendations<br>
1			[[article_id, 分数]]</p>
<h4 id="推荐结果处理">推荐结果处理</h4>
<p>通过StringIndexer变换后的下标知道原来的和用户ID<br>
# recall_res得到需要使用StringIndexer变换后的下标<br>
# 保存原来的下表映射关系<br>
refection_user = als_user_article_click.groupBy(['user_id']).max('als_user_id').withColumnRenamed(<br>
'max(als_user_id)', 'als_user_id')<br>
refection_article = als_user_article_click.groupBy(['article_id']).max('als_article_id').withColumnRenamed(<br>
'max(als_article_id)', 'als_article_id')</p>
<pre><code># Join推荐结果与 refection_user映射关系表
# +-----------+--------------------+-------------------+
# | als_user_id | recommendations | user_id |
# +-----------+--------------------+-------------------+
# | 8 | [[163, 0.91328144]... | 2 |
 # | 0 | [[145, 0.653115], ... | 1106476833370537984 |
 
recall_res = recall_res.join(refection_user, on=['als_user_id'], how='left').select(
['als_user_id', 'recommendations', 'user_id'])
</code></pre>
<p>对推荐文章ID后处理：得到推荐列表,获取推荐列表中的ID索引<br>
# Join推荐结果与 refection_article映射关系表<br>
# +-----------+-------+----------------+<br>
# | als_user_id | user_id | als_article_id |<br>
# +-----------+-------+----------------+<br>
# | 8 | 2 | [163, 0.91328144] |<br>
# | 8 | 2 | [132, 0.91328144] |<br>
import pyspark.sql.functions as F<br>
recall_res = recall_res.withColumn('als_article_id', F.explode('recommendations')).drop('recommendations')</p>
<pre><code># +-----------+-------+--------------+
# | als_user_id | user_id | als_article_id |
# +-----------+-------+--------------+
# | 8 | 2 | 163 |
# | 8 | 2 | 132 |
def _article_id(row):
	return row.als_user_id, row.user_id, row.als_article_id[0]
</code></pre>
<p>进行索引对应文章ID获取<br>
als_recall = recall_res.rdd.map(_article_id).toDF(['als_user_id', 'user_id', 'als_article_id'])<br>
als_recall = als_recall.join(refection_article, on=['als_article_id'], how='left').select(<br>
['user_id', 'article_id'])<br>
# 得到每个用户ID 对应推荐文章<br>
# +-------------------+----------+<br>
# | user_id |				 article_id |<br>
# +-------------------+----------+<br>
# | 1106476833370537984 |   44075 |<br>
# | 1 | 					 44075 |<br>
获取每个文章对应的频道，推荐给用户时按照频道存储:<br>
ur.spark.sql(&quot;use toutiao&quot;)<br>
news_article_basic = ur.spark.sql(&quot;select article_id, channel_id from news_article_basic&quot;)</p>
<pre><code>als_recall = als_recall.join(news_article_basic, on=['article_id'], how='left')
als_recall = als_recall.groupBy(['user_id', 'channel_id']).agg(F.collect_list('article_id')).withColumnRenamed(
                    'collect_list(article_id)', 'article_list')

als_recall = als_recall.dropna()
</code></pre>
<h4 id="召回结果存储">召回结果存储</h4>
<p>HBASE表设计概览：<br>
put 'cb_recall', 'recall:user:5', 'als:1',[45,3,5,10,289,11,65,52,109,8]<br>
put 'cb_recall', 'recall:user:5', 'als:2',[1,2,3,4,5,6,7,8,9,10]<br>
存储代码如下：<br>
def save_offline_recall_hbase(partition):<br>
&quot;&quot;&quot;离线模型召回结果存储<br>
&quot;&quot;&quot;<br>
import happybase<br>
pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)<br>
for row in partition:<br>
with pool.connection() as conn:<br>
# 获取历史看过的该频道文章<br>
history_table = conn.table('history_recall')<br>
# 多个版本<br>
data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),<br>
'channel:{}'.format(row.channel_id).encode())</p>
<pre><code>            history = []
            if len(data) &gt;= 2:
                for l in data[:-1]:
                    history.extend(eval(l))
            else:
                history = []

            # 过滤reco_article与history
            reco_res = list(set(row.article_list) - set(history))

            if reco_res:

                table = conn.table('cb_recall')
                # 默认放在推荐频道
                table.put('recall:user:{}'.format(row.user_id).encode(),
                          {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                # 放入历史推荐过文章
                history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                  {'channel:{}'.format(row.channel_id): str(reco_res).encode()})
            conn.close()

als_recall.foreachPartition(save_offline_recall_hbase)
</code></pre>
<h3 id="离线用户基于内容召回集">离线用户基于内容召回集</h3>
<p>目标<br>
知道离线内容召回的概念<br>
知道如何进行内容召回计算存储规则<br>
应用<br>
应用spark完成离线用户基于内容的协同过滤推荐</p>
<h4 id="基于内容召回实现文章向量之前已经弄好了">基于内容召回实现（文章向量之前已经弄好了）</h4>
<p>过滤用户点击的文章<br>
# 基于内容相似召回（画像召回）<br>
ur.spark.sql(&quot;use profile&quot;)<br>
user_article_basic = self.spark.sql(&quot;select * from user_article_basic&quot;)<br>
user_article_basic = user_article_basic.filter(&quot;clicked=True&quot;)</p>
<pre><code>def save_content_filter_history_to__recall(partition):
    &quot;&quot;&quot;计算每个用户的每个操作文章的相似文章，过滤之后，写入content召回表当中（支持不同时间戳版本）
    &quot;&quot;&quot;
    import happybase
    pool = happybase.ConnectionPool(size=10, host='hadoop-master')

    # 进行为相似文章获取
    with pool.connection() as conn:

        # key:   article_id,    column:  similar:article_id
        similar_table = conn.table('article_similar')
        # 循环partition
        for row in partition:
            # 获取相似文章结果表
            similar_article = similar_table.row(str(row.article_id).encode(),
                                                columns=[b'similar'])
            # 相似文章相似度排序过滤，召回不需要太大的数据， 百个，千
            _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True)
            if _srt:
                # 每次行为推荐10篇文章
                reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10]

                # 获取历史看过的该频道文章
                history_table = conn.table('history_recall')
                # 多个版本
                data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                           'channel:{}'.format(row.channel_id).encode())

                history = []
                if len(data) &gt;= 2:
                    for l in data[:-1]:
                        history.extend(eval(l))
                else:
                    history = []

                # 过滤reco_article与history
                reco_res = list(set(reco_article) - set(history))

                # 进行推荐，放入基于内容的召回表当中以及历史看过的文章表当中
                if reco_res:
                    # content_table = conn.table('cb_content_recall')
                    content_table = conn.table('cb_recall')
                    content_table.put(&quot;recall:user:{}&quot;.format(row.user_id).encode(),
                                      {'content:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                    # 放入历史推荐过文章
                    history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                      {'channel:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

        conn.close()

user_article_basic.foreachPartition(save_content_filter_history_to__recall)
</code></pre>
<h3 id="离线用户召回定时更新">离线用户召回定时更新</h3>
<h4 id="定时更新代码">定时更新代码</h4>
<pre><code>import os
import sys
# 如果当前代码文件运行测试需要加入修改路径，否则后面的导包出现问题
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(BASE_DIR))
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline

from pyspark.ml.recommendation import ALS
from offline import SparkSessionBase
from datetime import datetime
import time
import numpy as np


class UpdateRecall(SparkSessionBase):

    SPARK_APP_NAME = &quot;updateRecall&quot;
    ENABLE_HIVE_SUPPORT = True

    def __init__(self, number):

        self.spark = self._create_spark_session()
        self.N = number

    def update_als_recall(self):
        &quot;&quot;&quot;
        更新基于模型（ALS）的协同过滤召回集
        :return:
        &quot;&quot;&quot;
        # 读取用户行为基本表
        self.spark.sql(&quot;use profile&quot;)
        user_article_click = self.spark.sql(&quot;select * from user_article_basic&quot;).\
            select(['user_id', 'article_id', 'clicked'])

        # 更换类型
        def change_types(row):
            return row.user_id, row.article_id, int(row.clicked)

        user_article_click = user_article_click.rdd.map(change_types).toDF(['user_id', 'article_id', 'clicked'])
        # 用户和文章ID超过ALS最大整数值，需要使用StringIndexer进行转换
        user_id_indexer = StringIndexer(inputCol='user_id', outputCol='als_user_id')
        article_id_indexer = StringIndexer(inputCol='article_id', outputCol='als_article_id')
        pip = Pipeline(stages=[user_id_indexer, article_id_indexer])
        pip_fit = pip.fit(user_article_click)
        als_user_article_click = pip_fit.transform(user_article_click)

        # 模型训练和推荐默认每个用户固定文章个数
        als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1)
        model = als.fit(als_user_article_click)
        recall_res = model.recommendForAllUsers(self.N)

        # recall_res得到需要使用StringIndexer变换后的下标
        # 保存原来的下表映射关系
        refection_user = als_user_article_click.groupBy(['user_id']).max('als_user_id').withColumnRenamed(
            'max(als_user_id)', 'als_user_id')
        refection_article = als_user_article_click.groupBy(['article_id']).max('als_article_id').withColumnRenamed(
            'max(als_article_id)', 'als_article_id')

        # Join推荐结果与 refection_user映射关系表
        # +-----------+--------------------+-------------------+
        # | als_user_id | recommendations | user_id |
        # +-----------+--------------------+-------------------+
        # | 8 | [[163, 0.91328144]... | 2 |
        #        | 0 | [[145, 0.653115], ... | 1106476833370537984 |
        recall_res = recall_res.join(refection_user, on=['als_user_id'], how='left').select(
            ['als_user_id', 'recommendations', 'user_id'])

        # Join推荐结果与 refection_article映射关系表
        # +-----------+-------+----------------+
        # | als_user_id | user_id | als_article_id |
        # +-----------+-------+----------------+
        # | 8 | 2 | [163, 0.91328144] |
        # | 8 | 2 | [132, 0.91328144] |
        import pyspark.sql.functions as F
        recall_res = recall_res.withColumn('als_article_id', F.explode('recommendations')).drop('recommendations')

        # +-----------+-------+--------------+
        # | als_user_id | user_id | als_article_id |
        # +-----------+-------+--------------+
        # | 8 | 2 | 163 |
        # | 8 | 2 | 132 |
        def _article_id(row):
            return row.als_user_id, row.user_id, row.als_article_id[0]

        als_recall = recall_res.rdd.map(_article_id).toDF(['als_user_id', 'user_id', 'als_article_id'])
        als_recall = als_recall.join(refection_article, on=['als_article_id'], how='left').select(
            ['user_id', 'article_id'])
        # 得到每个用户ID 对应推荐文章
        # +-------------------+----------+
        # | user_id | article_id |
        # +-------------------+----------+
        # | 1106476833370537984 | 44075 |
        # | 1 | 44075 |
        # 分组统计每个用户，推荐列表
        # als_recall = als_recall.groupby('user_id').agg(F.collect_list('article_id')).withColumnRenamed(
        #     'collect_list(article_id)', 'article_list')
        self.spark.sql(&quot;use toutiao&quot;)
        news_article_basic = self.spark.sql(&quot;select article_id, channel_id from news_article_basic&quot;)
        als_recall = als_recall.join(news_article_basic, on=['article_id'], how='left')
        als_recall = als_recall.groupBy(['user_id', 'channel_id']).agg(F.collect_list('article_id')).withColumnRenamed(
            'collect_list(article_id)', 'article_list')
        als_recall = als_recall.dropna()

        # 存储
        def save_offline_recall_hbase(partition):
            &quot;&quot;&quot;离线模型召回结果存储
            &quot;&quot;&quot;
            import happybase
            pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)
            for row in partition:
                with pool.connection() as conn:
                    # 获取历史看过的该频道文章
                    history_table = conn.table('history_recall')
                    # 多个版本
                    data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                               'channel:{}'.format(row.channel_id).encode())

                    history = []
                    if len(data) &gt;= 2:
                        for l in data[:-1]:
                            history.extend(eval(l))
                    else:
                        history = []

                    # 过滤reco_article与history
                    reco_res = list(set(row.article_list) - set(history))

                    if reco_res:

                        table = conn.table('cb_recall')
                        # 默认放在推荐频道
                        table.put('recall:user:{}'.format(row.user_id).encode(),
                                  {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()})
                        conn.close()

                        # 放入历史推荐过文章
                        history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                          {'channel:{}'.format(row.channel_id): str(reco_res).encode()})
                    conn.close()

        als_recall.foreachPartition(save_offline_recall_hbase)

    def update_content_recall(self):
        &quot;&quot;&quot;
        更新基于内容（画像）的推荐召回集, word2vec相似
        :return:
        &quot;&quot;&quot;
        # 基于内容相似召回（画像召回）
        ur.spark.sql(&quot;use profile&quot;)
        user_article_basic = self.spark.sql(&quot;select * from user_article_basic&quot;)
        user_article_basic = user_article_basic.filter(&quot;clicked=True&quot;)

        def save_content_filter_history_to__recall(partition):
            &quot;&quot;&quot;计算每个用户的每个操作文章的相似文章，过滤之后，写入content召回表当中（支持不同时间戳版本）
            &quot;&quot;&quot;
            import happybase
            pool = happybase.ConnectionPool(size=10, host='hadoop-master')

            # 进行为相似文章获取
            with pool.connection() as conn:

                # key:   article_id,    column:  similar:article_id
                similar_table = conn.table('article_similar')
                # 循环partition
                for row in partition:
                    # 获取相似文章结果表
                    similar_article = similar_table.row(str(row.article_id).encode(),
                                                        columns=[b'similar'])
                    # 相似文章相似度排序过滤，召回不需要太大的数据， 百个，千
                    _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True)
                    if _srt:
                        # 每次行为推荐10篇文章
                        reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10]

                        # 获取历史看过的该频道文章
                        history_table = conn.table('history_recall')
                        # 多个版本
                        data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                                   'channel:{}'.format(row.channel_id).encode())

                        history = []
                            if len(_history_data) &gt; 1:
                                for l in _history_data:
                                    history.extend(l)

                        # 过滤reco_article与history
                        reco_res = list(set(reco_article) - set(history))

                        # 进行推荐，放入基于内容的召回表当中以及历史看过的文章表当中
                        if reco_res:
                            # content_table = conn.table('cb_content_recall')
                            content_table = conn.table('cb_recall')
                            content_table.put(&quot;recall:user:{}&quot;.format(row.user_id).encode(),
                                              {'content:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                            # 放入历史推荐过文章
                            history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                              {'channel:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                conn.close()

        user_article_basic.foreachPartition(save_content_filter_history_to__recall)


if __name__ == '__main__':
    ur = UpdateRecall(500)
    ur.update_als_recall()
    ur.update_content_recall()
</code></pre>
<p>定时更新代码，在main.py和update.py中添加以下代码：<br>
from offline.update_recall import UpdateRecall<br>
from schedule.update_profile import update_user_profile, update_article_profile, update_recall</p>
<pre><code>def update_recall():
    &quot;&quot;&quot;
    更新用户的召回集
    :return:
    &quot;&quot;&quot;
    udp = UpdateRecall(200)
    udp.update_als_recall()
    udp.update_content_recall()
</code></pre>
<p>main中添加<br>
scheduler.add_job(update_recall, trigger='interval', hour=3)</p>
<h1 id="排序设计">排序设计</h1>
<h3 id="排序模型">排序模型</h3>
<p>宽模型 + 特征⼯程<br>
LR/MLR + 非ID类特征(⼈⼯离散/GBDT/FM)<br>
spark 中可以直接使用<br>
宽模型 + 深模型<br>
wide&amp;deep,DeepFM<br>
使用TensorFlow进行训练<br>
深模型：<br>
DNN + 特征embedding<br>
使用TensorFlow进行训练</p>
<h3 id="特征处理原则">特征处理原则</h3>
<p>离散数据<br>
one-hot编码<br>
连续数据<br>
归一化<br>
图片/文本<br>
文章标签/关键词提取<br>
embedding</p>
<h3 id="优化训练方式">优化训练方式</h3>
<p>使用Batch SGD优化<br>
加入正则化防止过拟合</p>
<h3 id="spark-lr-进行预估">spark LR 进行预估</h3>
<p>目的：通过LR模型进行CTR预估<br>
步骤：<br>
1、需要通过spark读取HIVE外部表，需要新的sparksession配置<br>
增加HBASE配置<br>
2、读取用户点击行为表，与用户画像和文章画像，构造训练样本<br>
3、LR模型进行训练<br>
4、LR模型预测、结果评估</p>

            </div>
            
              <div class="tag-container">
                
                  <a href="https://cythonlin.github.io/tag/EjFvvnhFs/" class="tag">
                    RS
                  </a>
                
              </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://cythonlin.github.io/post/py-greater-hbase/">
                  <h3 class="post-title">
                    PY =&gt; HBase
                  </h3>
                </a>
              </div>
            

            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>






  </body>
</html>
