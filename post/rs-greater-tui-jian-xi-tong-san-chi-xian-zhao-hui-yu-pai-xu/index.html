<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>RS =&gt; 推荐系统（三）离线召回  | Cython_lin</title>
<meta name="description" content="" />
<link rel="shortcut icon" href="https://cythonlin.github.io/favicon.ico">
<link rel="stylesheet" href="https://cythonlin.github.io/styles/main.css">

<script src="https://cythonlin.github.io/media/js/jquery.min.js"></script>
<script src="https://cythonlin.github.io/media/js/masonry.pkgd.min.js"></script>
<script src="https://cythonlin.github.io/media/js/aos.js"></script>
<script src="https://cythonlin.github.io/media/js/pace.min.js"></script>
<script src="https://cythonlin.github.io/media/js/view-image.min.js"></script>
<script src="https://cythonlin.github.io/media/js/jquery.magnific-popup.min.js"></script>
<script src="https://cythonlin.github.io/media/js/functions.js"></script>
    <meta name="referrer" content="never">
    <meta name="description" content="召回设计
召回排序流程
匿名用户：
通常使用用户冷启动方案，区别在于user_id为匿名用户手机识别号(不允许匿名用户)
所有只正针对于登录用户：

用户冷启动（前期点击行为较少情况）
非个性化推荐
热门召回：自定义热门规则，根据当前时间段..." />
    <meta name="keywords" content="RS" />
    <script src="https://cythonlin.github.io/media/js/waterfall.min.js"></script>
    <script src="https://cythonlin.github.io/media/js/prism.min.js"></script>
  </head>
  <body>
            <header id="header" class="grid-container">
        <!-- start: .menu-wrapper -->
        <div class="menu-mobile"> 
          <i class="fa fa-reorder"></i>
        </div>
        <div class="menu-wrapper">
          <div class="">
            <div class="logo">
              <a href="https://cythonlin.github.io"><img src="\media\images\custom-headerLogo.png" alt=""></a>
            </div>
            <!-- start: .main-nav -->

            <nav class="main-nav grid-container grid-parent">
              <ul id="menu-header" class="menu gradient-effect">
                <li class=""><a href="https://cythonlin.github.io" class="menu">首页</a></li>
                
                  <li class="" >
                    <a href="/archives" class="menu">
                      归档
                    </a>
                  </li>
                
                  <li class="" >
                    <a href="/tags" class="menu">
                      标签
                    </a>
                  </li>
                
                  <li class="" >
                    <a href="https://cythonlin.github.io/post/the-future-is-promising" class="menu">
                      关于
                    </a>
                  </li>
                
                <li class="search-menu-item hide-on-mobile hide-on-tablet"><a href="#search-lightbox" class="lightbox mfp-inline"><i class="fa fa-search-line"></i></a></li>
              </ul>
            </nav>
            <a href="#search-lightbox" class="lightbox epcl-search-button mfp-inline hide-on-tablet hide-on-desktop"><i class="fa fa-search-line"></i></a>
            <!-- end: .main-nav -->
            <div class="clear"></div>
            <div class="border hide-on-tablet hide-on-mobile"></div>
          </div>    
          <div class="clear"></div>
        </div>
        <!-- end: .menu-wrapper -->
        <div class="clear"></div>
      </header>
      <div class="hide-on-mobile hide-on-tablet hide-on-desktop">
        <div id="search-lightbox" class="grid-container grid-small grid-parent mfp-hide">
          <div class="search-wrapper section">
            <form id="gridea-search-form" data-update="1615464655548" action="/search/index.html" class="search-form" _lpchecked="1">
              <input type="text" name="q" id="s" value="" class="search-field" placeholder="搜点啥..." aria-label="搜点啥..." required="">
              <button type="submit" class="submit" aria-label="Submit">
                <i class="fa fa-search-line"></i>
              </button>
            </form>
          </div>
        </div>
      </div>

      <main id="single" class="main grid-container fullcover no-sidebar aos-init aos-animate" data-aos="fade">

        <div class="center content">
          <div class="featured-image cover" style="background-image: url('/media/images/gridea.jpg');">
            <div class="meta top"> 
              <time class="meta-info" style="float:left;" datetime="2020-09-29"><i class="fa fa-calendar"></i><span class="lately">5 个月前</span></time>
              
              <a href="https://cythonlin.github.io/post/rs-greater-tui-jian-xi-tong-san-chi-xian-zhao-hui-yu-pai-xu/#comments" class="comments meta-info" title="">
                <i class="fa fa-comment remixicon"></i><span class="comment-count valine-comment-count" data-xid="/rs-greater-tui-jian-xi-tong-san-chi-xian-zhao-hui-yu-pai-xu/"> </span>
              </a>
              <span id="/rs-greater-tui-jian-xi-tong-san-chi-xian-zhao-hui-yu-pai-xu/" class="leancloud_visitors views-counter meta-info" title=""><i class="fa fa-leancloud remixicon"></i><span class="leancloud-visitors-count"></span></span>
              
            </div>
            <div class="info">
              <div class="tags ">
                
                      <a href="https://cythonlin.github.io/tag/EjFvvnhFs/" class="ctag ctag-0 ctag-EjFvvnhFs" aria-label="">RS</a>
                    
              </div>
              <h1 class="title ularge white bold">RS =&gt; 推荐系统（三）离线召回 </h1>
            </div>
          </div>
        </div>  

        <div class="epcl-page-wrapper">
          <div class="left-content grid-70 np-mobile">
            <article class="main-article post">
              <section class="post-content">
                <div class="text">
                  <h1 id="召回设计">召回设计</h1>
<h3 id="召回排序流程">召回排序流程</h3>
<h4 id="匿名用户">匿名用户：</h4>
<pre><code>通常使用用户冷启动方案，区别在于user_id为匿名用户手机识别号(不允许匿名用户)
所有只正针对于登录用户：
</code></pre>
<h4 id="用户冷启动前期点击行为较少情况">用户冷启动（前期点击行为较少情况）</h4>
<p>非个性化推荐</p>
<pre><code>热门召回：自定义热门规则，根据当前时间段热点定期更新维护人点文章库
新文章召回：为了提高新文章的曝光率，建立新文章库，进行推荐
</code></pre>
<p>个性化推荐：</p>
<pre><code>基于内容的协同过滤在线召回：基于用户实时兴趣画像相似的召回结果用于首页的个性化推荐
</code></pre>
<h4 id="后期离线部分用户点击行为较多用户画像完善">后期离线部分（用户点击行为较多，用户画像完善）</h4>
<p>建立用户长期兴趣画像（详细）：包括用户各个维度的兴趣特征<br>
训练排序模型</p>
<pre><code>LR模型、FTRL、Wide&amp;Deep
</code></pre>
<p>离线部分的召回：</p>
<pre><code>基于模型协同过滤推荐离线召回：ALS
基于内容的离线召回：或者称基于用户画像的召回
</code></pre>
<h3 id="召回表设计与模型召回">召回表设计与模型召回</h3>
<h4 id="召回表设计">召回表设计</h4>
<p>我们的召回方式有很多种。<br>
多路召回结果存储模型召回 与 内容召回的结果 需要进行相应频道推荐合并。<br>
方案：基于模型与基于内容的召回结果存入同一张表，避免多张表进行读取处理</p>
<pre><code>由于HBASE有多个版本数据功能存在的支持
TTL=&gt;7776000, VERSIONS=&gt;999999
</code></pre>
<p>如下：</p>
<pre><code>create 'cb_recall', {NAME=&gt;'als', TTL=&gt;7776000, VERSIONS=&gt;999999}
alter 'cb_recall', {NAME=&gt;'content', TTL=&gt;7776000, VERSIONS=&gt;999999}
alter 'cb_recall', {NAME=&gt;'online', TTL=&gt;7776000, VERSIONS=&gt;999999}

# 例子（多版本）：
put 'cb_recall', 'recall:user:5', 'als:1',[45,3,5,10]
put 'cb_recall', 'recall:user:5', 'als:1',[289,11,65,52,109,8]
put 'cb_recall', 'recall:user:5', 'als:2',[1,2,3,4,5,6,7,8,9,10]
put 'cb_recall', 'recall:user:2', 'content:1',[45,3,5,10,289,11,65,52,109,8]
put 'cb_recall', 'recall:user:2', 'content:2',[1,2,3,4,5,6,7,8,9,10]


hbase(main):084:0&gt; desc 'cb_recall'
Table cb_recall is ENABLED                                                                             
cb_recall                                                                                              
COLUMN FAMILIES DESCRIPTION                                                                            
{NAME =&gt; 'als', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false'
, KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 
'7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE
_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_
OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                    
{NAME =&gt; 'content', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'fa
lse', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL
          =&gt; '7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', C
ACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS
_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                
{NAME =&gt; 'online', VERSIONS =&gt; '999999', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'fal
se', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL 
=&gt; '7776000 SECONDS (90 DAYS)', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CA
CHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_
ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}                 
3 row(s)	
</code></pre>
<p>（几乎不用）在HIVE用户数据数据库下建立HIVE外部表,若hbase表有修改，则进行HIVE 表删除更新</p>
<pre><code>create external table cb_recall_hbase(
user_id STRING comment &quot;userID&quot;,
als map&lt;string, ARRAY&lt;BIGINT&gt;&gt; comment &quot;als recall&quot;,
content map&lt;string, ARRAY&lt;BIGINT&gt;&gt; comment &quot;content recall&quot;,
online map&lt;string, ARRAY&lt;BIGINT&gt;&gt; comment &quot;online recall&quot;)
COMMENT &quot;user recall table&quot;
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,als:,content:,online:&quot;)
TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;cb_recall&quot;);
</code></pre>
<h4 id="增加一个历史召回结果表">增加一个历史召回结果表</h4>
<pre><code>create 'history_recall', {NAME=&gt;'channel', TTL=&gt;7776000, VERSIONS=&gt;999999}

put 'history_recall', 'recall:user:5', 'als:1',[1,2,3]
put 'history_recall', 'recall:user:5', 'als:1',[4,5,6,7]
put 'history_recall', 'recall:user:5', 'als:1',[8,9,10]
</code></pre>
<p>为什么增加历史召回表？</p>
<pre><code>1、直接在存储召回结果部分进行过滤，比之后排序过滤，节省排序时间
2、防止Redis缓存没有消耗完，造成重复推荐，从源头进行过滤
</code></pre>
<h3 id="基于模型召回集合计算">基于模型召回集合计算</h3>
<h4 id="als模型推荐实现">ALS模型推荐实现</h4>
<p>步骤：<br>
1、数据类型转换,clicked以及用户ID与文章ID处理<br>
2、ALS模型训练以及推荐<br>
3、推荐结果解析处理<br>
4、推荐结果存储<br>
数据类型转换,clicked( bool 转 int)</p>
<pre><code>ur.spark.sql(&quot;use profile&quot;)
user_article_click = ur.spark.sql(&quot;select * from user_article_basic&quot;).\
            select(['user_id', 'article_id', 'clicked'])
# 更换类型
def change_types(row):
    return row.user_id, row.article_id, int(row.clicked)

user_article_click = user_article_click.rdd.map(change_types).toDF(['user_id', 'article_id', 'clicked'])
</code></pre>
<p>这步处理结果格式如下：</p>
<pre><code>user_id	article_id	clicked
					0
					1
</code></pre>
<p>用户ID与文章ID处理，编程ID索引（原用户ID和文章ID是长字符串，ALS模型不能处理，要重新编排ID索引）</p>
<pre><code>from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline
# 用户和文章ID超过ALS最大整数值，需要使用StringIndexer进行转换
user_id_indexer = StringIndexer(inputCol='user_id', outputCol='als_user_id')
article_id_indexer = StringIndexer(inputCol='article_id', outputCol='als_article_id')
pip = Pipeline(stages=[user_id_indexer, article_id_indexer])
pip_fit = pip.fit(user_article_click)
als_user_article_click = pip_fit.transform(user_article_click)
</code></pre>
<p>ALS 模型训练与推荐（ALS模型需要输出用户ID列，文章ID列以及点击列）</p>
<pre><code>from pyspark.ml.recommendation import ALS
# 模型训练和推荐默认每个用户固定文章个数
als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1)
model = als.fit(als_user_article_click)
recall_res = model.recommendForAllUsers(100)
</code></pre>
<p>结果：</p>
<pre><code>als_user_id	recommendations
1			[[article_id, 分数]]
</code></pre>
<h4 id="推荐结果处理">推荐结果处理</h4>
<p>通过StringIndexer变换后的下标知道原来的和用户ID</p>
<pre><code># recall_res得到需要使用StringIndexer变换后的下标
# 保存原来的下表映射关系
refection_user = als_user_article_click.groupBy(['user_id']).max('als_user_id').withColumnRenamed(
'max(als_user_id)', 'als_user_id')
refection_article = als_user_article_click.groupBy(['article_id']).max('als_article_id').withColumnRenamed(
'max(als_article_id)', 'als_article_id')

# Join推荐结果与 refection_user映射关系表
# +-----------+--------------------+-------------------+
# | als_user_id | recommendations | user_id |
# +-----------+--------------------+-------------------+
# | 8 | [[163, 0.91328144]... | 2 |
 # | 0 | [[145, 0.653115], ... | 1106476833370537984 |
 
recall_res = recall_res.join(refection_user, on=['als_user_id'], how='left').select(
['als_user_id', 'recommendations', 'user_id'])
</code></pre>
<p>对推荐文章ID后处理：得到推荐列表,获取推荐列表中的ID索引</p>
<pre><code># Join推荐结果与 refection_article映射关系表
# +-----------+-------+----------------+
# | als_user_id | user_id | als_article_id |
# +-----------+-------+----------------+
# | 8 | 2 | [163, 0.91328144] |
# | 8 | 2 | [132, 0.91328144] |
import pyspark.sql.functions as F
recall_res = recall_res.withColumn('als_article_id', F.explode('recommendations')).drop('recommendations')

# +-----------+-------+--------------+
# | als_user_id | user_id | als_article_id |
# +-----------+-------+--------------+
# | 8 | 2 | 163 |
# | 8 | 2 | 132 |
def _article_id(row):
	return row.als_user_id, row.user_id, row.als_article_id[0]
</code></pre>
<p>进行索引对应文章ID获取</p>
<pre><code>als_recall = recall_res.rdd.map(_article_id).toDF(['als_user_id', 'user_id', 'als_article_id'])
als_recall = als_recall.join(refection_article, on=['als_article_id'], how='left').select(
                    ['user_id', 'article_id'])
# 得到每个用户ID 对应推荐文章
# +-------------------+----------+
# | user_id |				 article_id |
# +-------------------+----------+
# | 1106476833370537984 |   44075 |
# | 1 | 					 44075 |
</code></pre>
<p>获取每个文章对应的频道，推荐给用户时按照频道存储:</p>
<pre><code>ur.spark.sql(&quot;use toutiao&quot;)
news_article_basic = ur.spark.sql(&quot;select article_id, channel_id from news_article_basic&quot;)

als_recall = als_recall.join(news_article_basic, on=['article_id'], how='left')
als_recall = als_recall.groupBy(['user_id', 'channel_id']).agg(F.collect_list('article_id')).withColumnRenamed(
                    'collect_list(article_id)', 'article_list')

als_recall = als_recall.dropna()
</code></pre>
<h4 id="召回结果存储">召回结果存储</h4>
<p>HBASE表设计概览：</p>
<pre><code>put 'cb_recall', 'recall:user:5', 'als:1',[45,3,5,10,289,11,65,52,109,8]
put 'cb_recall', 'recall:user:5', 'als:2',[1,2,3,4,5,6,7,8,9,10]
</code></pre>
<p>存储代码如下：</p>
<pre><code>def save_offline_recall_hbase(partition):
    &quot;&quot;&quot;离线模型召回结果存储
    &quot;&quot;&quot;
    import happybase
    pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)
    for row in partition:
        with pool.connection() as conn:
            # 获取历史看过的该频道文章
            history_table = conn.table('history_recall')
            # 多个版本
            data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                       'channel:{}'.format(row.channel_id).encode())

            history = []
            if len(data) &gt;= 2:
                for l in data[:-1]:
                    history.extend(eval(l))
            else:
                history = []

            # 过滤reco_article与history
            reco_res = list(set(row.article_list) - set(history))

            if reco_res:

                table = conn.table('cb_recall')
                # 默认放在推荐频道
                table.put('recall:user:{}'.format(row.user_id).encode(),
                          {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                # 放入历史推荐过文章
                history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                  {'channel:{}'.format(row.channel_id): str(reco_res).encode()})
            conn.close()

als_recall.foreachPartition(save_offline_recall_hbase)
</code></pre>
<h3 id="离线用户基于内容召回集">离线用户基于内容召回集</h3>
<p>目标</p>
<pre><code>知道离线内容召回的概念
知道如何进行内容召回计算存储规则
</code></pre>
<p>应用</p>
<pre><code>应用spark完成离线用户基于内容的协同过滤推荐
</code></pre>
<h4 id="基于内容召回实现文章向量之前已经弄好了">基于内容召回实现（文章向量之前已经弄好了）</h4>
<p>过滤用户点击的文章</p>
<pre><code># 基于内容相似召回（画像召回）
ur.spark.sql(&quot;use profile&quot;)
user_article_basic = self.spark.sql(&quot;select * from user_article_basic&quot;)
user_article_basic = user_article_basic.filter(&quot;clicked=True&quot;)

def save_content_filter_history_to__recall(partition):
    &quot;&quot;&quot;计算每个用户的每个操作文章的相似文章，过滤之后，写入content召回表当中（支持不同时间戳版本）
    &quot;&quot;&quot;
    import happybase
    pool = happybase.ConnectionPool(size=10, host='hadoop-master')

    # 进行为相似文章获取
    with pool.connection() as conn:

        # key:   article_id,    column:  similar:article_id
        similar_table = conn.table('article_similar')
        # 循环partition
        for row in partition:
            # 获取相似文章结果表
            similar_article = similar_table.row(str(row.article_id).encode(),
                                                columns=[b'similar'])
            # 相似文章相似度排序过滤，召回不需要太大的数据， 百个，千
            _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True)
            if _srt:
                # 每次行为推荐10篇文章
                reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10]

                # 获取历史看过的该频道文章
                history_table = conn.table('history_recall')
                # 多个版本
                data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                           'channel:{}'.format(row.channel_id).encode())

                history = []
                if len(data) &gt;= 2:
                    for l in data[:-1]:
                        history.extend(eval(l))
                else:
                    history = []

                # 过滤reco_article与history
                reco_res = list(set(reco_article) - set(history))

                # 进行推荐，放入基于内容的召回表当中以及历史看过的文章表当中
                if reco_res:
                    # content_table = conn.table('cb_content_recall')
                    content_table = conn.table('cb_recall')
                    content_table.put(&quot;recall:user:{}&quot;.format(row.user_id).encode(),
                                      {'content:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                    # 放入历史推荐过文章
                    history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                      {'channel:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

        conn.close()

user_article_basic.foreachPartition(save_content_filter_history_to__recall)
</code></pre>
<h3 id="离线用户召回定时更新">离线用户召回定时更新</h3>
<h4 id="定时更新代码">定时更新代码</h4>
<pre><code>import os
import sys
# 如果当前代码文件运行测试需要加入修改路径，否则后面的导包出现问题
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(BASE_DIR))
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline

from pyspark.ml.recommendation import ALS
from offline import SparkSessionBase
from datetime import datetime
import time
import numpy as np


class UpdateRecall(SparkSessionBase):

    SPARK_APP_NAME = &quot;updateRecall&quot;
    ENABLE_HIVE_SUPPORT = True

    def __init__(self, number):

        self.spark = self._create_spark_session()
        self.N = number

    def update_als_recall(self):
        &quot;&quot;&quot;
        更新基于模型（ALS）的协同过滤召回集
        :return:
        &quot;&quot;&quot;
        # 读取用户行为基本表
        self.spark.sql(&quot;use profile&quot;)
        user_article_click = self.spark.sql(&quot;select * from user_article_basic&quot;).\
            select(['user_id', 'article_id', 'clicked'])

        # 更换类型
        def change_types(row):
            return row.user_id, row.article_id, int(row.clicked)

        user_article_click = user_article_click.rdd.map(change_types).toDF(['user_id', 'article_id', 'clicked'])
        # 用户和文章ID超过ALS最大整数值，需要使用StringIndexer进行转换
        user_id_indexer = StringIndexer(inputCol='user_id', outputCol='als_user_id')
        article_id_indexer = StringIndexer(inputCol='article_id', outputCol='als_article_id')
        pip = Pipeline(stages=[user_id_indexer, article_id_indexer])
        pip_fit = pip.fit(user_article_click)
        als_user_article_click = pip_fit.transform(user_article_click)

        # 模型训练和推荐默认每个用户固定文章个数
        als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1)
        model = als.fit(als_user_article_click)
        recall_res = model.recommendForAllUsers(self.N)

        # recall_res得到需要使用StringIndexer变换后的下标
        # 保存原来的下表映射关系
        refection_user = als_user_article_click.groupBy(['user_id']).max('als_user_id').withColumnRenamed(
            'max(als_user_id)', 'als_user_id')
        refection_article = als_user_article_click.groupBy(['article_id']).max('als_article_id').withColumnRenamed(
            'max(als_article_id)', 'als_article_id')

        # Join推荐结果与 refection_user映射关系表
        # +-----------+--------------------+-------------------+
        # | als_user_id | recommendations | user_id |
        # +-----------+--------------------+-------------------+
        # | 8 | [[163, 0.91328144]... | 2 |
        #        | 0 | [[145, 0.653115], ... | 1106476833370537984 |
        recall_res = recall_res.join(refection_user, on=['als_user_id'], how='left').select(
            ['als_user_id', 'recommendations', 'user_id'])

        # Join推荐结果与 refection_article映射关系表
        # +-----------+-------+----------------+
        # | als_user_id | user_id | als_article_id |
        # +-----------+-------+----------------+
        # | 8 | 2 | [163, 0.91328144] |
        # | 8 | 2 | [132, 0.91328144] |
        import pyspark.sql.functions as F
        recall_res = recall_res.withColumn('als_article_id', F.explode('recommendations')).drop('recommendations')

        # +-----------+-------+--------------+
        # | als_user_id | user_id | als_article_id |
        # +-----------+-------+--------------+
        # | 8 | 2 | 163 |
        # | 8 | 2 | 132 |
        def _article_id(row):
            return row.als_user_id, row.user_id, row.als_article_id[0]

        als_recall = recall_res.rdd.map(_article_id).toDF(['als_user_id', 'user_id', 'als_article_id'])
        als_recall = als_recall.join(refection_article, on=['als_article_id'], how='left').select(
            ['user_id', 'article_id'])
        # 得到每个用户ID 对应推荐文章
        # +-------------------+----------+
        # | user_id | article_id |
        # +-------------------+----------+
        # | 1106476833370537984 | 44075 |
        # | 1 | 44075 |
        # 分组统计每个用户，推荐列表
        # als_recall = als_recall.groupby('user_id').agg(F.collect_list('article_id')).withColumnRenamed(
        #     'collect_list(article_id)', 'article_list')
        self.spark.sql(&quot;use toutiao&quot;)
        news_article_basic = self.spark.sql(&quot;select article_id, channel_id from news_article_basic&quot;)
        als_recall = als_recall.join(news_article_basic, on=['article_id'], how='left')
        als_recall = als_recall.groupBy(['user_id', 'channel_id']).agg(F.collect_list('article_id')).withColumnRenamed(
            'collect_list(article_id)', 'article_list')
        als_recall = als_recall.dropna()

        # 存储
        def save_offline_recall_hbase(partition):
            &quot;&quot;&quot;离线模型召回结果存储
            &quot;&quot;&quot;
            import happybase
            pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)
            for row in partition:
                with pool.connection() as conn:
                    # 获取历史看过的该频道文章
                    history_table = conn.table('history_recall')
                    # 多个版本
                    data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                               'channel:{}'.format(row.channel_id).encode())

                    history = []
                    if len(data) &gt;= 2:
                        for l in data[:-1]:
                            history.extend(eval(l))
                    else:
                        history = []

                    # 过滤reco_article与history
                    reco_res = list(set(row.article_list) - set(history))

                    if reco_res:

                        table = conn.table('cb_recall')
                        # 默认放在推荐频道
                        table.put('recall:user:{}'.format(row.user_id).encode(),
                                  {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()})
                        conn.close()

                        # 放入历史推荐过文章
                        history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                          {'channel:{}'.format(row.channel_id): str(reco_res).encode()})
                    conn.close()

        als_recall.foreachPartition(save_offline_recall_hbase)

    def update_content_recall(self):
        &quot;&quot;&quot;
        更新基于内容（画像）的推荐召回集, word2vec相似
        :return:
        &quot;&quot;&quot;
        # 基于内容相似召回（画像召回）
        ur.spark.sql(&quot;use profile&quot;)
        user_article_basic = self.spark.sql(&quot;select * from user_article_basic&quot;)
        user_article_basic = user_article_basic.filter(&quot;clicked=True&quot;)

        def save_content_filter_history_to__recall(partition):
            &quot;&quot;&quot;计算每个用户的每个操作文章的相似文章，过滤之后，写入content召回表当中（支持不同时间戳版本）
            &quot;&quot;&quot;
            import happybase
            pool = happybase.ConnectionPool(size=10, host='hadoop-master')

            # 进行为相似文章获取
            with pool.connection() as conn:

                # key:   article_id,    column:  similar:article_id
                similar_table = conn.table('article_similar')
                # 循环partition
                for row in partition:
                    # 获取相似文章结果表
                    similar_article = similar_table.row(str(row.article_id).encode(),
                                                        columns=[b'similar'])
                    # 相似文章相似度排序过滤，召回不需要太大的数据， 百个，千
                    _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True)
                    if _srt:
                        # 每次行为推荐10篇文章
                        reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10]

                        # 获取历史看过的该频道文章
                        history_table = conn.table('history_recall')
                        # 多个版本
                        data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                                   'channel:{}'.format(row.channel_id).encode())

                        history = []
                            if len(_history_data) &gt; 1:
                                for l in _history_data:
                                    history.extend(l)

                        # 过滤reco_article与history
                        reco_res = list(set(reco_article) - set(history))

                        # 进行推荐，放入基于内容的召回表当中以及历史看过的文章表当中
                        if reco_res:
                            # content_table = conn.table('cb_content_recall')
                            content_table = conn.table('cb_recall')
                            content_table.put(&quot;recall:user:{}&quot;.format(row.user_id).encode(),
                                              {'content:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                            # 放入历史推荐过文章
                            history_table.put(&quot;reco:his:{}&quot;.format(row.user_id).encode(),
                                              {'channel:{}'.format(row.channel_id).encode(): str(reco_res).encode()})

                conn.close()

        user_article_basic.foreachPartition(save_content_filter_history_to__recall)


if __name__ == '__main__':
    ur = UpdateRecall(500)
    ur.update_als_recall()
    ur.update_content_recall()
</code></pre>
<p>定时更新代码，在main.py和update.py中添加以下代码：</p>
<pre><code>from offline.update_recall import UpdateRecall
from schedule.update_profile import update_user_profile, update_article_profile, update_recall

def update_recall():
    &quot;&quot;&quot;
    更新用户的召回集
    :return:
    &quot;&quot;&quot;
    udp = UpdateRecall(200)
    udp.update_als_recall()
    udp.update_content_recall()
</code></pre>
<p>main中添加</p>
<pre><code>scheduler.add_job(update_recall, trigger='interval', hour=3)
</code></pre>
<h1 id="排序设计">排序设计</h1>
<h3 id="排序模型">排序模型</h3>
<p>宽模型 + 特征⼯程</p>
<pre><code>LR/MLR + 非ID类特征(⼈⼯离散/GBDT/FM)
spark 中可以直接使用
</code></pre>
<p>宽模型 + 深模型</p>
<pre><code>wide&amp;deep,DeepFM
使用TensorFlow进行训练
</code></pre>
<p>深模型：</p>
<pre><code>DNN + 特征embedding
使用TensorFlow进行训练
</code></pre>
<h3 id="特征处理原则">特征处理原则</h3>
<p>离散数据</p>
<pre><code>one-hot编码
</code></pre>
<p>连续数据</p>
<pre><code>归一化
</code></pre>
<p>图片/文本</p>
<pre><code>文章标签/关键词提取
embedding
</code></pre>
<h3 id="优化训练方式">优化训练方式</h3>
<p>使用Batch SGD优化</p>
<pre><code>加入正则化防止过拟合
</code></pre>
<h3 id="spark-lr-进行预估">spark LR 进行预估</h3>
<p>目的：通过LR模型进行CTR预估<br>
步骤：</p>
<pre><code>1、需要通过spark读取HIVE外部表，需要新的sparksession配置
	增加HBASE配置
2、读取用户点击行为表，与用户画像和文章画像，构造训练样本
3、LR模型进行训练
4、LR模型预测、结果评估
</code></pre>
<p>创建环境</p>
<pre><code>import os
import sys
# 如果当前代码文件运行测试需要加入修改路径，避免出现后导包问题
BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))
sys.path.insert(0, os.path.join(BASE_DIR))

PYSPARK_PYTHON = &quot;/miniconda2/envs/reco_sys/bin/python&quot;
# 当存在多个版本时，不指定很可能会导致出错
os.environ[&quot;PYSPARK_PYTHON&quot;] = PYSPARK_PYTHON
os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = PYSPARK_PYTHON

from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline
from pyspark.sql.types import *
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.classification import LogisticRegressionModel
from offline import SparkSessionBase

class CtrLogisticRegression(SparkSessionBase):

    SPARK_APP_NAME = &quot;ctrLogisticRegression&quot;
    ENABLE_HIVE_SUPPORT = True

    def __init__(self):

        self.spark = self._create_spark_hbase()

ctr = CtrLogisticRegression()
</code></pre>
<p>这里注意的是_create_spark_hbase，我们后面需要通过spark读取HIVE外部表，需要新的配置</p>
<pre><code>def _create_spark_hbase(self):

    conf = SparkConf()  # 创建spark config对象
    config = (
        (&quot;spark.app.name&quot;, self.SPARK_APP_NAME),  # 设置启动的spark的app名称，没有提供，将随机产生一个名称
        (&quot;spark.executor.memory&quot;, self.SPARK_EXECUTOR_MEMORY),  # 设置该app启动时占用的内存用量，默认2g
        (&quot;spark.master&quot;, self.SPARK_URL),  # spark master的地址
        (&quot;spark.executor.cores&quot;, self.SPARK_EXECUTOR_CORES),  # 设置spark executor使用的CPU核心数，默认是1核心
        (&quot;spark.executor.instances&quot;, self.SPARK_EXECUTOR_INSTANCES),
        (&quot;hbase.zookeeper.quorum&quot;, &quot;192.168.19.137&quot;),
        (&quot;hbase.zookeeper.property.clientPort&quot;, &quot;22181&quot;)
    )

    conf.setAll(config)

    # 利用config对象，创建spark session
    if self.ENABLE_HIVE_SUPPORT:
        return SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()
    else:
        return SparkSession.builder.config(conf=conf).getOrCreate()
</code></pre>
<p>读取用户点击行为表，与用户画像和文章画像，构造训练样本</p>
<ol>
<li>目标值：clicked</li>
<li>特征值：
<ul>
<li>
<p>用户画像关键词权重：权重值排序TOPK，这里取10个</p>
</li>
<li>
<p>文章频道号：channel_id, ID类型通常要做one_hot编码，变成25维度(25个频道)</p>
</li>
<li>
<p>这里由于我们的历史点击日志测试时候是只有18号频道，所以不进行转换</p>
</li>
<li>
<p>文章向量：articlevector<br>
进行行为日志数据读取</p>
<pre><code>  ctr.spark.sql(&quot;use profile&quot;)
  # +-------------------+----------+----------+-------+
  # |            user_id|article_id|channel_id|clicked|
  # +-------------------+----------+----------+-------+
  # |1105045287866466304|     14225|         0|  false|
  user_article_basic = ctr.spark.sql(&quot;select * from user_article_basic&quot;).select(
      ['user_id', 'article_id', 'channel_id', 'clicked'])
</code></pre>
</li>
</ul>
</li>
</ol>
<p>用户画像读取处理与日志数据合并</p>
<pre><code>user_profile_hbase = ctr.spark.sql(
    &quot;select user_id, information.birthday, information.gender, article_partial, env from user_profile_hbase&quot;)
user_profile_hbase = user_profile_hbase.drop('env')

# +--------------------+--------+------+--------------------+
# |             user_id|birthday|gender|     article_partial|
# +--------------------+--------+------+--------------------+
# |              user:1|     0.0|  null|Map(18:Animal -&gt; ...|

_schema = StructType([
    StructField(&quot;user_id&quot;, LongType()),
    StructField(&quot;birthday&quot;, DoubleType()),
    StructField(&quot;gender&quot;, BooleanType()),
    StructField(&quot;weights&quot;, MapType(StringType(), DoubleType()))
])

def get_user_id(row):
    return int(row.user_id.split(&quot;:&quot;)[1]), row.birthday, row.gender, row.article_partial
</code></pre>
<p>读取用户画像HIVE的外部表，构造样本:</p>
<pre><code>user_profile_hbase_temp = user_profile_hbase.rdd.map(get_user_id)
user_profile_hbase_schema = ctr.spark.createDataFrame(user_profile_hbase_temp, schema=_schema)

    train = user_article_basic.join(user_profile_hbase_schema, on=['user_id'], how='left').drop('channel_id')
</code></pre>
<p>文章频道与向量读取合并，删除无用的特征</p>
<pre><code># +-------------------+----------+-------+--------+------+--------------------+
# |            user_id|article_id|clicked|birthday|gender|             weights|
# +-------------------+----------+-------+--------+------+--------------------+
# |1106473203766657024|     13778|  false|     0.0|  null|Map(18:text -&gt; 0....|
ctr.spark.sql(&quot;use article&quot;)
article_vector = ctr.spark.sql(&quot;select * from article_vector&quot;)
train = train.join(article_vector, on=['article_id'], how='left').drop('birthday').drop('gender')

# +-------------------+-------------------+-------+--------------------+----------+--------------------+
# |         article_id|            user_id|clicked|             weights|channel_id|       articlevector|
# +-------------------+-------------------+-------+--------------------+----------+--------------------+
# |              13401|                 10|  false|Map(18:tp2 -&gt; 0.2...|        18|[0.06157120217893...|
</code></pre>
<p>结果格式(删除了gender和birthday)：</p>
<pre><code>article_id  use_id  channel_id  articlevector   weight   clicked
--------------
article_id: n
user_id:    n
channel_id: n
articlevector: []
weight: Map(channel_id: keyword -&gt; weight)
clicked: bool
</code></pre>
<p>合并文章画像的权重特征</p>
<pre><code>ctr.spark.sql(&quot;use article&quot;)
article_profile = ctr.spark.sql(&quot;select * from article_profile&quot;)

def article_profile_to_feature(row):

    try:
        weights = sorted(row.keywords.values())[:10]
    except Exception as e:
        weights = [0.0] * 10
    return row.article_id, weights
article_profile = article_profile.rdd.map(article_profile_to_feature).toDF(['article_id', 'article_weights'])

article_profile.show()

train = train.join(article_profile, on=['article_id'], how='left')
</code></pre>
<p>结果格式：</p>
<pre><code>article_id  use_id  channel_id  articlevector   weight  article_weight  clicked
--------------
article_id: n
user_id:    n
channel_id: n
articlevector: []
weight: Map(channel_id: keyword -&gt; weight)
article_weight: []
clicked: bool
</code></pre>
<p>进行用户的权重特征筛选处理，类型处理</p>
<ul>
<li>用户权重排序筛选，缺失值:
<ul>
<li>
<p>获取用户对应文章频道号的关键词权重</p>
</li>
<li>
<p>若无：生成默认值</p>
<pre><code>  columns = ['article_id', 'user_id', 'channel_id', 'articlevector', 'user_weights', 'article_weights', 'clicked']
  def get_user_weights(row):
      from pyspark.ml.linalg import Vectors
      try:
          user_weights = sorted([row.article_partial[key] for key in row.article_partial.keys() if key.split(':')[0] == str(row.channel_id)])[
                  :10]
      except Exception:
          user_weights = [0.0] * 10

      return row.article_id, row.user_id, row.channel_id, Vectors.dense(row.articlevector), Vectors.dense(
          user_weights), Vectors.dense(row.article_weights), int(row.clicked)

  train_vector = train_user_article.rdd.map(get_user_weights).toDF(columns)
</code></pre>
</li>
</ul>
</li>
</ul>
<p>结果格式：Note: Vectors.dense要根据列的数据来指定字段</p>
<pre><code>article_id  use_id  channel_id  articlevector   weight  article_weight  clicked
--------------
article_id: n
user_id:    n
channel_id: n
articlevector: []
weight: []                   # 和上面对比，这里从Map变成了 []
article_weight: []
clicked: bool
</code></pre>
<h1 id="lr点击率预测">LR点击率预测</h1>
<p>输入模型的特征格式指定，通过VectorAssembler()收集<br>
只要channel_id，articlevector，weights，article_weights作为输入特征</p>
<pre><code>train_version_two = VectorAssembler().setInputCols(cols[2:6]).setOutputCol(&quot;features&quot;).transform(train)
</code></pre>
<p>把输入特征平铺，再输入到LR中<br>
合并特征向量(channel_id1个+用户特征权重10个+文章向量100个+文章关键词权重10个) = 121个特征<br>
按理说，把channel做one-hot会更多： 121-1+25=145</p>
<pre><code>lr = LogisticRegression()
model = lr.setLabelCol(&quot;clicked&quot;).setFeaturesCol(&quot;features&quot;).fit(train_version_two)
model.save(&quot;hdfs://hadoop-master:9000/headlines/models/logistic_ctr_model.obj&quot;)
</code></pre>
<p>使用model模型加载预估</p>
<pre><code>online_model = LogisticRegressionModel.load(&quot;hdfs://hadoop-master:9000/headlines/models/logistic_ctr_model.obj&quot;)

res_transfrom = online_model.transform(train_version_two)

res_transfrom.select([&quot;clicked&quot;, &quot;probability&quot;, &quot;prediction&quot;]).show()
</code></pre>
<p>probability结果中有对某个文章点击(1为目标)的概率，和不点击（0为目标）的概率，是个列表<br>
eg: probability = [不点击的概率， 点击的概率]<br>
def vector_to_double(row):<br>
return float(row.clicked), float(row.probability[1])</p>
<pre><code>score_label = res_transfrom.select([&quot;clicked&quot;, &quot;probability&quot;]).rdd.map(vector_to_double)
</code></pre>
<h1 id="模型评估-accuracy与auc">模型评估-Accuracy与AUC</h1>
<h2 id="roc-曲线图">ROC 曲线图</h2>
<p>画出ROC图,使用训练的时候的模型model中会有</p>
<pre><code>import matplotlib.pyplot as plt
plt.figure(figsize=(5,5))
plt.plot([0, 1], [0, 1], 'r--')
plt.plot(model.summary.roc.select('FPR').collect(),
        model.summary.roc.select('TPR').collect())
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.show()
</code></pre>
<h2 id="计算auc值">计算AUC值</h2>
<p>方式1：spark内置方法</p>
<pre><code>from pyspark.mllib.evaluation import BinaryClassificationMetrics
# score_label包括[&quot;clicked&quot;, &quot;probability&quot;]
metrics = BinaryClassificationMetrics(score_label) 
metrics.areaUnderROC
0.7364334522585716
</code></pre>
<p>方式2：sklearn方法</p>
<pre><code>from sklearn.metrics import roc_auc_score, accuracy_score
import numpy as np

arr = np.array(score_label.collect())

#评估AUC与准确率
accuracy_score(arr[:, 0], arr[:, 1].round())
0.9051438053097345

roc_auc_score(arr[:, 0], arr[:, 1])
0.719274521004087
</code></pre>
<h1 id="离线ctr特征中心创建">离线ctr特征中心创建</h1>
<ul>
<li>特征服务中心可以作为离线计算用户与文章的高级特征。</li>
<li>不仅仅提供给离线使用。还可以作为实时的特征供其他场景读取进行</li>
<li>用户，文章能用到的特征都进行处理进行存储，便于实时推荐进行读取</li>
<li>存储到HBASE</li>
</ul>
<h2 id="创建user特征hbase表">创建User特征Hbase表：</h2>
<pre><code>create 'ctr_feature_user', 'channel'
    4                         column=channel:13, timestamp=1555647172980, value=[]                        
    4                         column=channel:14, timestamp=1555647172980, value=[]                        
    4                         column=channel:15, timestamp=1555647172980, value=[]                        
    4                         column=channel:16, timestamp=1555647172980, value=[]                        
    4                         column=channel:18, timestamp=1555647172980, value=[0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073, 0.2156294170196073]                                                      
    4                         column=channel:19, timestamp=1555647172980, value=[]                        
    4                         column=channel:20, timestamp=1555647172980, value=[]                        
    4                         column=channel:2, timestamp=1555647172980, value=[]                         
    4                         column=channel:21, timestamp=1555647172980, value=[]
</code></pre>
<h2 id="创建article特征hbase表">创建Article特征Hbase表</h2>
<pre><code>create 'ctr_feature_article', 'article'
    COLUMN                     CELL                                                                        
    article:13401             timestamp=1555635749357, value=[18.0,0.08196639249252607,0.11217275332895373,0.1353835167902181,0.16086650318453152,0.16356418791892943,0.16740082750337945,0.18091837445730974,0.1907214431716628,0.2........................-0.04634634410271921,-0.06451843378804649,-0.021564142420785692,0.10212902152136256]
</code></pre>
<h2 id="使用hive关联特种中心基本不需要">使用Hive关联特种中心（基本不需要）</h2>
<p>关联用户</p>
<pre><code>create external table ctr_feature_user_hbase(
user_id STRING comment &quot;user_id&quot;,
user_channel map comment &quot;user_channel&quot;)
COMMENT &quot;ctr table&quot;
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,channel:&quot;)
TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;ctr_feature_user&quot;);
</code></pre>
<p>关联文章</p>
<pre><code>create external table ctr_feature_article_hbase(
article_id STRING comment &quot;article_id&quot;,
article_feature map comment &quot;article&quot;)
COMMENT &quot;ctr table&quot;
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,article:&quot;)
TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;ctr_feature_article&quot;);
</code></pre>
<h1 id="用户特征中心更新">用户特征中心更新</h1>
<ul>
<li>目的：计算用户特征更新到HBASE</li>
<li>步骤：
<ul>
<li>获取特征进行用户画像权重过滤</li>
<li>特征批量存储</li>
</ul>
</li>
</ul>
<p>获取特征进行用户画像权重过滤</p>
<pre><code># 构造样本
ctr.spark.sql(&quot;use profile&quot;)

user_profile_hbase = ctr.spark.sql(
    &quot;select user_id, information.birthday, information.gender, article_partial, env from user_profile_hbase&quot;)

# 特征工程处理
# 抛弃获取值少的特征
user_profile_hbase = user_profile_hbase.drop('env', 'birthday', 'gender')

def get_user_id(row):
    return int(row.user_id.split(&quot;:&quot;)[1]), row.article_partial

user_profile_hbase_temp = user_profile_hbase.rdd.map(get_user_id)

from pyspark.sql.types import *

_schema = StructType([
    StructField(&quot;user_id&quot;, LongType()),
    StructField(&quot;weights&quot;, MapType(StringType(), DoubleType()))
])

user_profile_hbase_schema = ctr.spark.createDataFrame(user_profile_hbase_temp, schema=_schema)

def frature_preprocess(row):

    from pyspark.ml.linalg import Vectors

    channel_weights = []
    for i in range(1, 26):
        try:
            _res = sorted([row.weights[key] for key
                        in row.weights.keys() if key.split(':')[0] == str(i)])[:10]
            channel_weights.append(_res)
        except:
            channel_weights.append([0.0] * 10)

    return row.user_id, channel_weights

res = user_profile_hbase_schema.rdd.map(frature_preprocess).collect()
</code></pre>
<p>结果格式：</p>
<pre><code>[
    # weight 代表用户的标签权重
    ( user_id,  [ [weight], [], [], ..., 25个[]，代表25个channel ]   

]
</code></pre>
<p>特征批量存储，保存用户每个频道的特征</p>
<pre><code>import happybase
# 批量插入Hbase数据库中
pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)
with pool.connection() as conn:
    ctr_feature = conn.table('ctr_feature_user')
    with ctr_feature.batch(transaction=True) as b:
        for i in range(len(res)):
            for j in range(25):
                b.put(&quot;{}&quot;.format(res[i][0]).encode(),{&quot;channel:{}&quot;.format(j+1).encode(): str(res[i][1][j]).encode()})
    conn.close()
</code></pre>
<h1 id="文章特征中心更新">文章特征中心更新</h1>
<p>文章特征：</p>
<ul>
<li>关键词权重</li>
<li>文章的频道</li>
<li>文章向量结果</li>
</ul>
<p>存储这些特征以便于后面实时排序时候快速使用特征</p>
<p>步骤：</p>
<ul>
<li>读取相关文章画像</li>
<li>进行文章相关特征处理和提取</li>
<li>合并文章所有特征作为模型训练或者预测的初始特征</li>
<li>文章特征存储到HBASE</li>
</ul>
<p>读取相关文章画像</p>
<pre><code>ctr.spark.sql(&quot;use article&quot;)
article_profile = ctr.spark.sql(&quot;select * from article_profile&quot;)
</code></pre>
<p>进行文章相关特征处理和提取</p>
<pre><code>def article_profile_to_feature(row):
    try:
        weights = sorted(row.keywords.values())[:10]
    except Exception as e:
        weights = [0.0] * 10
    return row.article_id, row.channel_id, weights
article_profile = article_profile.rdd.map(article_profile_to_feature).toDF(['article_id', 'channel_id', 'weights'])

article_profile.show()
</code></pre>
<p>格式如下：</p>
<pre><code>article_id  channel_id  weights
                        []
</code></pre>
<p>再把文章向量join进来</p>
<pre><code>article_vector = ctr.spark.sql(&quot;select * from article_vector&quot;)
article_feature = article_profile.join(article_vector, on=['article_id'], how='inner')
def feature_to_vector(row):
    from pyspark.ml.linalg import Vectors
    return row.article_id, row.channel_id, Vectors.dense(row.weights), Vectors.dense(row.articlevector)
article_feature = article_feature.rdd.map(feature_to_vector).toDF(['article_id', 'channel_id', 'weights', 'articlevector'])
</code></pre>
<p>指定上面特征进行合并（atricle_id,channel_id, weights, articlevector）</p>
<pre><code># 保存特征数据
cols2 = ['article_id', 'channel_id', 'weights', 'articlevector']
# 做特征的指定指定合并
article_feature_two = VectorAssembler().setInputCols(cols2[1:4]).setOutputCol(&quot;features&quot;).transform(article_feature)
</code></pre>
<p>最终结果格式（可给LR使用）：</p>
<pre><code>article_id  channel_id  weights articlevector   features

# 格式样式说明
article_id: n
channel_id: n
weights:    []
articlevector:  []
features:   []          
# features就是我们最终合并成一个list的结果，它和 channel_id 用于给 lr 使用
</code></pre>
<p>保存到特征数据库HBASE中:</p>
<pre><code># 保存到特征数据库中
def save_article_feature_to_hbase(partition):
    import happybase
    pool = happybase.ConnectionPool(size=10, host='hadoop-master')
    with pool.connection() as conn:
        table = conn.table('ctr_feature_article')
        for row in partition:
            table.put('{}'.format(row.article_id).encode(),
                    {'article:{}'.format(row.article_id).encode(): str(row.features).encode()})

article_feature_two.foreachPartition(save_article_feature_to_hbase)
</code></pre>
<h1 id="离线特征中心定时更新">离线特征中心定时更新</h1>
<p>添加update.py更新程序</p>
<pre><code>def update_ctr_feature():
    &quot;&quot;&quot;
    定时更新用户、文章特征
    :return:
    &quot;&quot;&quot;
    fp = FeaturePlatform()
    fp.update_user_ctr_feature_to_hbase()
    fp.update_article_ctr_feature_to_hbase()
</code></pre>
<p>添加apscheduler定时运行</p>
<pre><code># 添加定时更新用户文章特征结果的程序，每个4小时更新一次
scheduler.add_job(update_ctr_feature, trigger='interval', hours=4)
</code></pre>
<p>完整代码：</p>
<pre><code>import os
import sys
# 如果当前代码文件运行测试需要加入修改路径，否则后面的导包出现问题
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(BASE_DIR))
from offline import SparkSessionBase
# from offline.utils import textrank, segmentation
import happybase
import pyspark.sql.functions as F
from datetime import datetime
from datetime import timedelta
import time
import gc


class FeaturePlatform(SparkSessionBase):
    &quot;&quot;&quot;特征更新平台
    &quot;&quot;&quot;
    SPARK_APP_NAME = &quot;featureCenter&quot;
    ENABLE_HIVE_SUPPORT = True

    def __init__(self):
        # _create_spark_session
        # _create_spark_hbase用户spark sql 操作hive对hbase的外部表
        self.spark = self._create_spark_hbase()

    def update_user_ctr_feature_to_hbase(self):
        &quot;&quot;&quot;
        :return:
        &quot;&quot;&quot;
        clr.spark.sql(&quot;use profile&quot;)

        user_profile_hbase = self.spark.sql(
            &quot;select user_id, information.birthday, information.gender, article_partial, env from user_profile_hbase&quot;)

        # 特征工程处理
        # 抛弃获取值少的特征
        user_profile_hbase = user_profile_hbase.drop('env', 'birthday', 'gender')

        def get_user_id(row):
            return int(row.user_id.split(&quot;:&quot;)[1]), row.article_partial

        user_profile_hbase_temp = user_profile_hbase.rdd.map(get_user_id)

        from pyspark.sql.types import *

        _schema = StructType([
            StructField(&quot;user_id&quot;, LongType()),
            StructField(&quot;weights&quot;, MapType(StringType(), DoubleType()))
        ])

        user_profile_hbase_schema = self.spark.createDataFrame(user_profile_hbase_temp, schema=_schema)

        def frature_preprocess(row):

            from pyspark.ml.linalg import Vectors

            channel_weights = []
            for i in range(1, 26):
                try:
                    _res = sorted([row.weights[key] for key
                                in row.weights.keys() if key.split(':')[0] == str(i)])[:10]
                    channel_weights.append(_res)
                except:
                    channel_weights.append([])

            return row.user_id, channel_weights

        res = user_profile_hbase_schema.rdd.map(frature_preprocess).collect()

        # 批量插入Hbase数据库中
        pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)
        with pool.connection() as conn:
            ctr_feature = conn.table('ctr_feature_user')
            with ctr_feature.batch(transaction=True) as b:
                for i in range(len(res)):
                    for j in range(25):
                        b.put(&quot;{}&quot;.format(res[i][0]).encode(),
                            {&quot;channel:{}&quot;.format(j + 1).encode(): str(res[i][1][j]).encode()})
            conn.close()

    def update_article_ctr_feature_to_hbase(self):
        &quot;&quot;&quot;
        :return:
        &quot;&quot;&quot;
        # 文章特征中心
        self.spark.sql(&quot;use article&quot;)
        article_profile = self.spark.sql(&quot;select * from article_profile&quot;)

        def article_profile_to_feature(row):
            try:
                weights = sorted(row.keywords.values())[:10]
            except Exception as e:
                weights = [0.0] * 10
            return row.article_id, row.channel_id, weights

        article_profile = article_profile.rdd.map(article_profile_to_feature).toDF(
            ['article_id', 'channel_id', 'weights'])

        article_vector = self.spark.sql(&quot;select * from article_vector&quot;)
        article_feature = article_profile.join(article_vector, on=['article_id'], how='inner')

        def feature_to_vector(row):
            from pyspark.ml.linalg import Vectors
            return row.article_id, row.channel_id, Vectors.dense(row.weights), Vectors.dense(row.articlevector)

        article_feature = article_feature.rdd.map(feature_to_vector).toDF(
            ['article_id', 'channel_id', 'weights', 'articlevector'])

        # 保存特征数据
        cols2 = ['article_id', 'channel_id', 'weights', 'articlevector']
        # 做特征的指定指定合并
        article_feature_two = VectorAssembler().setInputCols(cols2[1:4]).setOutputCol(&quot;features&quot;).transform(
            article_feature)

        # 保存到特征数据库中
        def save_article_feature_to_hbase(partition):
            import happybase
            pool = happybase.ConnectionPool(size=10, host='hadoop-master')
            with pool.connection() as conn:
                table = conn.table('ctr_feature_article')
                for row in partition:
                    table.put('{}'.format(row.article_id).encode(),
                            {'article:{}'.format(row.article_id).encode(): str(row.features).encode()})

        article_feature_two.foreachPartition(save_article_feature_to_hbase)
</code></pre>
<h1 id="离线总结">离线总结</h1>
<p>至此，离线部分已全部完成，包括如下内容</p>
<ul>
<li>
<p>文章画像</p>
<ul>
<li>文章信息拼接</li>
<li>文章分词</li>
<li>对分词结果TF-IDF</li>
<li>对分词结果TextRank （作为关键词，带权重）</li>
<li>将TF-IDF和TextRank结果交集，（作为 主题词）</li>
<li>对分词结果Word2Vec得到词向量</li>
<li>把文章的所有词向量的均值，（作为文章向量）</li>
<li>LSH计算相似文章，得到所有文章相似文章向量（为下面内容召回打下基础）</li>
</ul>
</li>
<li>
<p>用户画像</p>
<ul>
<li>对用户行为表的埋点日志的Json格式转到Hive的Map</li>
<li>对用户行为的map格式 explode爆炸处理</li>
<li>对用户与文章画像合并处理</li>
<li>根据用户对文章的喜好程度（行为权重打分）把文章画像的标签取TopK打在用户身上</li>
<li>最终得到用户的每个uid, channel的特征标签+权重</li>
</ul>
</li>
<li>
<p>用户召回</p>
<ul>
<li>模型ALS（模型结果自动推荐TopK个，及权重）</li>
<li>内容召回（文章相似的文章的TopK个）</li>
<li>召回的Item推荐用户</li>
</ul>
</li>
<li>
<p>根据用户/文章画像抽取各自的特征组成特征中心平台</p>
<ul>
<li>用户画像抽取用户特征，使用VectorAssembler拼进一个列表中，供后续实时排序等使用。</li>
<li>文章画像抽取文章特征，使用VectorAssembler拼进一个列表中，供后续实时排序等使用。</li>
</ul>
</li>
</ul>

                </div>
                <div class="clear"></div>
              </section>
            </article>
            <div class="clear"></div>

            <section class="related section">
              
              <article class="prev grid-50 tablet-grid-50 grid-parent">
                <div class="thumb cover lazy loaded" style="background-image: url('https://cythonlin.github.io/media/images/c137hZpK4.jpg');"></div>
                 <a href="https://cythonlin.github.io/post/py-greater-github-cli-quan-tao/" class="full-link"></a>
                 <div class="info">
                  <time datetime="2020-10-03">2020-10-03</time>
                  <h4 class="title white no-margin">PY =&gt; Github-Cli(1.6.2)</h4>
                </div>
                 <span class="epcl-button red">
                  <img src="https://cythonlin.github.io/media/images/left-arrow.svg" width="15" alt="Left Arrow">
                </span>
                <div class="overlay"></div>
              </article>
              
              
              <article class="next grid-50 tablet-grid-50 grid-parent">
                <div class="thumb cover lazy loaded" style="background-image: url('https://cythonlin.github.io/media/images/EjFvvnhFs.jpg');"></div>
                 <a href="https://cythonlin.github.io/post/py-greater-hbase/" class="full-link"></a>
                 <div class="info">
                  <time datetime="2020-09-29">2020-09-29</time>
                  <h4 class="title white no-margin">PY =&gt; HBase</h4>
                </div>
                 <span class="epcl-button red">
                  <img src="https://cythonlin.github.io/media/images/right-arrow.svg" width="15" alt="Left Arrow">
                </span>
                <div class="overlay"></div>
              </article>
              

                <div class="clear"></div>
            </section>

              <div class="clear"></div>
              
            
<!--               <div id="comments" class="bg-white hosted ">
                <p>请到客户端“主题--自定义配置--valine”中填入ID和KEY</p>
              </div> -->
              <div class="clear"></div>
            

            </div>
          </div>
      </main>

          <footer id="footer" class="grid-container">
        <div class="widgets row gradient-effect">
            <div class="default-sidebar border-effect">
              <div class="grid-33 tablet-grid-50 mobile-grid-100">
                <section id="tag_cloud-2" class="widget widget_epcl_posts_thumbs underline-effect">
                  <h4 class="widget-title title white bordered">最新文章</h4>
                  
                  
                  <article class="item post-0 post type-post status-publish format-standard has-post-thumbnail hentry">
                    <a href="https://cythonlin.github.io/post/py-greater-2017-2021-gao-ji-zhi-shi-fu-xi/" class="thumb hover-effect">
                      <span class="fullimage cover" style="display:block;border-radius:50%;background-image: url('/media/images/gridea.jpg');"></span>
                    </a>
                    <div class="info gradient-effect">
                      <time datetime="2021-03-10">2021-03-10</time>
                      <h4 class="title usmall">
                        <a href="https://cythonlin.github.io/post/py-greater-2017-2021-gao-ji-zhi-shi-fu-xi/">PY =&gt; 2017-2021高级冷门备忘</a>
                      </h4>
                    </div>
                    <div class="clear"></div>
                  </article>
                  
                  
                  
                  <article class="item post-1 post type-post status-publish format-standard has-post-thumbnail hentry">
                    <a href="https://cythonlin.github.io/post/ide-greater-my-sublimepy/" class="thumb hover-effect">
                      <span class="fullimage cover" style="display:block;border-radius:50%;background-image: url('/media/images/gridea.jpg');"></span>
                    </a>
                    <div class="info gradient-effect">
                      <time datetime="2021-03-03">2021-03-03</time>
                      <h4 class="title usmall">
                        <a href="https://cythonlin.github.io/post/ide-greater-my-sublimepy/">IDE =&gt; My Sublime（PY）</a>
                      </h4>
                    </div>
                    <div class="clear"></div>
                  </article>
                  
                  
                  
                  <article class="item post-2 post type-post status-publish format-standard has-post-thumbnail hentry">
                    <a href="https://cythonlin.github.io/post/rs-greater-9/" class="thumb hover-effect">
                      <span class="fullimage cover" style="display:block;border-radius:50%;background-image: url('/media/images/gridea.jpg');"></span>
                    </a>
                    <div class="info gradient-effect">
                      <time datetime="2021-03-03">2021-03-03</time>
                      <h4 class="title usmall">
                        <a href="https://cythonlin.github.io/post/rs-greater-9/"> IDE =&gt; My Pycharm</a>
                      </h4>
                    </div>
                    <div class="clear"></div>
                  </article>
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  <div class="clear"></div>
                </section>
              </div>

              <div class="grid-33 tablet-grid-50 mobile-grid-100">
                <section id="tag_cloud-2" class="widget widget_tag_cloud underline-effect">
                  <h4 class="widget-title title white bordered">标签云</h4>
                  <div class="tagcloud">
                    
                      <a href="https://cythonlin.github.io/tag/wAXYBHxvH/" class="ctag ctag-0 ctag-wAXYBHxvH" aria-label="">Python</a>
                    
                      <a href="https://cythonlin.github.io/tag/c137hZpK4/" class="ctag ctag-1 ctag-c137hZpK4" aria-label="">IDE</a>
                    
                      <a href="https://cythonlin.github.io/tag/EjFvvnhFs/" class="ctag ctag-2 ctag-EjFvvnhFs" aria-label="">RS</a>
                    
                      <a href="https://cythonlin.github.io/tag/XH05Gb5J6/" class="ctag ctag-3 ctag-XH05Gb5J6" aria-label="">Golang</a>
                    
                      <a href="https://cythonlin.github.io/tag/sKPdrpV6Y/" class="ctag ctag-4 ctag-sKPdrpV6Y" aria-label="">PR</a>
                    
                      <a href="https://cythonlin.github.io/tag/1L4lr0i2f/" class="ctag ctag-5 ctag-1L4lr0i2f" aria-label="">Docker</a>
                    
                      <a href="https://cythonlin.github.io/tag/n16me-6oV/" class="ctag ctag-6 ctag-n16me-6oV" aria-label="">AI</a>
                    
                      <a href="https://cythonlin.github.io/tag/FLS_eF6Eg/" class="ctag ctag-7 ctag-FLS_eF6Eg" aria-label="">KG</a>
                    
                      <a href="https://cythonlin.github.io/tag/sJAb6NuUK/" class="ctag ctag-8 ctag-sJAb6NuUK" aria-label="">DB</a>
                    
                  </div>
                  <div class="clear"></div>
                </section>
              </div>

              <div class="grid-33 tablet-grid-50 mobile-grid-100">
                <section id="epcl_about-2" class="widget widget_epcl_about underline-effect">
                  <h4 class="widget-title title white bordered">关于我</h4>
                  <div class="avatar">
                    <a href="" class="translate-effect thumb"><span class="fullimage cover" style="background-image: url(https://cythonlin.github.io/images/avatar.png);"></span></a>
                  </div>
                  <div class="info">
                    <h4 class="title small author-name gradient-effect no-margin"><a href="">Cython_lin</a></h4>
                    <p class="founder"></p>
                    <div class="social">
                      
                        
                      
                        
                      
                        
                      
                        
                      
                        
                      
                    </div> 
                  </div>
                  <div class="clear"></div>
                  </section>
              </div>

            </div>
            <div class="clear"></div>
        </div>

        <div class="logo">
          <a href="https://cythonlin.github.io"><img src="\media\images\custom-footerLogo.png" alt=""></a>
        </div>
        <p class="published border-effect">
          ©2019 共 79 篇文章
          <br/>

        </p>
        
        <a href="javascript:void(0)" id="back-to-top" class="epcl-button dark" style="display:none">
          <i class="fa fa-arrow"></i>
        </a>
    </footer>
    
    <div class="clear"></div>

        

      
    <script src="https://cythonlin.github.io/media/js/functions-post.js"></script>

    </div>
    <!-- end: #wrapper -->
  </body>
</html>
